<!DOCTYPE NETSCAPE-Bookmark-file-1>
	<HTML>
	<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=UTF-8">
	<Title>Bookmarks</Title>
	<H1>Bookmarks</H1>
	<DT><H3 FOLDED>Favourites</H3>
	<DL><p>
		<DT><A HREF="https://www.google.com/?client=safari&channel=mac_bm">Google</A>
		<DT><A HREF="https://www.wikipedia.org/">Wikipedia</A>
		<DT><A HREF="https://www.apple.com/es">Apple</A>
		<DT><A HREF="https://www.bing.com/">Bing</A>
		<DT><A HREF="https://www.google.com/?client=safari&channel=ipad_bm">Google</A>
		<DT><A HREF="https://es.yahoo.com/">Yahoo</A>
	</DL><p>
	<DT><H3 FOLDED>Bookmarks Menu</H3>
	<DL><p>
	</DL><p>
	<DT><H3 FOLDED>Tab Group Favourites</H3>
	<DL><p>
	</DL><p>
	<DT><H3 FOLDED id="com.apple.ReadingList">Reading List</H3>
	<DL><p>
		<DT><A HREF="https://www.youtube.com/watch?v=d0gS5TXarXc&t=80s">Signals. I spent 2 years to understand this part. - YouTube</A>
		<DT><A HREF="https://user-images.githubusercontent.com/3841370/253379177-38ba1531-ea0d-4851-b31a-a6d4ddc944b0.png">253379177-38ba1531-ea0d-4851-b31a-a6d4ddc944b0.png 7.178√ó4.390 pixels</A>
		<DT><A HREF="https://colab.research.google.com/github/corolla-johnson/mkultra/blob/master/tuning_finetune.ipynb#scrollTo=cE2jNS5UMXKh">tuning_finetune_alice.ipynb - Colaboratory</A>
		<DT><A HREF="https://www.amazon.es/dp/1452182787/?tag=halcyonreal0b-21">Descripci√≥n del producto</A>
		<DT><A HREF="https://www.ebay.com/itm/153542159104?chn=ps&_trkparms=ispr%3D1&amdata=enc%3A1sq8JBcRtT6KcLZ-t4nHwlA91&norover=1&mkevt=1&mkrid=711-117182-37290-0&mkcid=2&itemid=153542159104&targetid=1262843335169&device=c&mktype=&googleloc=1005421&poi=&campaignid=15275224983&mkgroupid=131097072938&rlsatarget=pla-1262843335169&abcId=9300697&merchantid=137762152&gclid=CjwKCAiAiKuOBhBQEiwAId_sK4Vo04TQeVRAMajvPpCiWSkXUUfG3ndA9DARYgVeIuHrVnJj3IawnRoCimQQAvD_BwE">Studio Ghibli Layout Design Exhibition Hayao Miyazaki Art Book | eBay</A>
		<DT><A HREF="https://www.google.com/search?q=Studio+Ghibli+Layout+Design+Exhibition+Art+Book+Hayao+Miyazaki+From&sxsrf=AOaemvLdkPamZzJFeOFzCcVI5u7vYqv05w:1640740904393&source=lnms&tbm=shop&sa=X&ved=2ahUKEwjryJiW7If1AhWOHhQKHXJPCDwQ_AUoA3oECAEQBQ&biw=2240&bih=1180&dpr=2">Studio Ghibli Layout Design Exhibition Art Book Hayao Miyazaki From - Google Shopping</A>
		<DT><A HREF="https://www.amazon.com/Art-Pixar-Complete-Scripts-Animation/dp/0811879631">Editorial Reviews</A>
		<DT><A HREF="https://www.google.com/search?q=pixar+layouts+and+background+art+book+by&source=lmns&tbm=shop&bih=1180&biw=2240&client=safari&hl=en&sa=X&ved=2ahUKEwj09tjS6of1AhUN2-AKHb9hAj4Q_AUoAnoECAEQAg">pixar layouts and background art book by - Google Shopping</A>
		<DT><A HREF="https://www.tatteredcover.com/book/9781423138662">Page not found | Tattered Cover Book Store</A>
		<DT><A HREF="https://www.tatteredcover.com/contact-us-1">Contact Us | Tattered Cover Book Store</A>
		<DT><A HREF="https://www.tatteredcover.com/search/site/Walt%20Disney%20Animation%20Studios%20The%20Archive%20Series%20Layout%20%26%20Background">Search | Tattered Cover Book Store</A>
		<DT><A HREF="https://www.google.com/search?q=They+Drew+as+They+Pleased+Volume+6:+The+Hidden+Art+of+Disney%27s+New+Golden+Age+(Disney+x+Chronicle+Books)&client=safari&rls=en&sxsrf=AOaemvJRzA-THgHGQjRiX72En-FKx8brUw:1640740384480&source=lnms&tbm=shop&sa=X&ved=2ahUKEwid_aOe6of1AhWB0eAKHcyWBVcQ_AUoA3oECAEQBQ&biw=2240&bih=1180&dpr=2">They Drew as They Pleased Volume 6: The Hidden Art of Disney's New Golden Age (Disney x Chronicle Books) - Google Shopping</A>
		<DT><A HREF="https://www.amazon.com/They-Drew-Pleased-Hidden-Disneys/dp/1797200933/ref=pd_sbs_3/145-2891899-2697556?pd_rd_w=JgRmc&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=Q0FP94JPPJA1B04P7AYN&pd_rd_r=b5b842be-d34d-482b-8b35-c39120b1a464&pd_rd_wg=vvyJu&pd_rd_i=1797200933&psc=1">They Drew as They Pleased Volume 6: The Hidden Art of Disney's New Golden Age (Disney x Chronicle Books) Hardcover ‚Äì August 4, 2020</A>
		<DT><A HREF="https://www.creativecavepublishers.com/books/layout-background-walt-disney-animation-archives/">Layout &amp; Background (Walt Disney Animation Archives)</A>
		<DT><A HREF="https://www.amazon.com/gp/product/1423134206?ref_=dbs_m_mng_rwt_calw_thcv_1&storeType=ebooks">Walt Disney Animation Studios The Archive Series #3: Design: Walt Disney Animation Research Libr: 9781423134206: Amazon.com: Books</A>
		<DT><A HREF="https://www.amazon.com/Layout-Background-Disney-Animation-Archives/dp/142313866X">Walt Disney Animation Studios The Archive Series #4: Layout &amp; Background Hardcover ‚Äì October 25, 2011</A>
		<DT><A HREF="https://www.google.com/search?q=Walt+Disney+Animation+Studios+The+Archive:+Layout+%26+Background+pdf&client=safari&rls=en&sxsrf=AOaemvLEx_g8hNo7U9hbzLTYYHeOjWStnw:1640739669128&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiMm5bJ54f1AhXRA2MBHSiVAMsQ_AUoAXoECBUQAw&biw=2240&bih=1180&dpr=2#imgrc=oPV_tLZoS1wjdM&imgdii=aUZ42t3Hxy4KdM">Walt Disney Animation Studios The Archive: Layout &amp; Background pdf - Google Search</A>
		<DT><A HREF="https://www.amazon.com/Paper-Dreams-Artists-Disney-Storyboards/dp/0786863072">Amazon.com: Paper Dreams: The Art And Artists Of Disney Storyboards: 9780786863075: Canemaker, John: Books</A>
		<DT><A HREF="https://www.karts.ac.kr/en/main.do">Korea National University of Arts</A>
		<DT><A HREF="https://www.mmca.go.kr/eng/publication/publicationMain.do">Íµ≠Î¶ΩÌòÑÎåÄÎØ∏Ïà†Í¥Ä</A>
		<DT><A HREF="http://mmcashop-en.co.kr/goods/goods_view.php?goodsNo=1000001313">MMCA SHOP</A>
		<DT><A HREF="https://www.google.com/search?q=south+korean+illustration+university+books&client=safari&rls=en&sxsrf=AOaemvIK65RkA4i92os9bBTTqD-dsb9aGA%3A1640738171253&ei=e63LYYzyDqi5gweJu7TwCg&ved=0ahUKEwjMm_f-4Yf1AhWo3OAKHYkdDa4Q4dUDCA0&uact=5&oq=south+korean+illustration+university+books&gs_lcp=Cgdnd3Mtd2l6EAM6BAgAEEc6BQgAEM0COgQIIRAKSgQIQRgASgQIRhgAUJEHWMQQYMUSaAFwAngAgAFriAGbBZIBAzUuMpgBAKABAcgBBMABAQ&sclient=gws-wiz">south korean illustration university books - Google Search</A>
		<DT><A HREF="https://www.e-flux.com/announcements/421733/book-release-korean-art-1900-2020/">Book release: Korean Art 1900-2020 - Announcements - e-flux</A>
		<DT><A HREF="https://www.phaidon.com/store/art/korean-art-from-1953-collision-innovation-and-interaction-9780714878331/">Korean Art from 1953: Collision, Innovation, Interaction</A>
		<DT><A HREF="https://github.com/gii-is-DP1/dp1-2020-g3-09/blob/master/docker-compose.yml">Page not found ¬∑ GitHub</A>
		<DT><A HREF="https://www.reddit.com/r/OpenAI/comments/u831xb/dalle_2_waitlist_length/">DALL-E 2 waitlist length : OpenAI</A>
		<DT><A HREF="https://medium.com/@DMeechan/fixing-the-installation-failed-virtualbox-error-on-mac-high-sierra-7c421362b5b5">Fixing ‚ÄòThe Installation Failed‚Äô VirtualBox Error on Mac High Sierra</A>
		<DT><A HREF="https://stackoverflow.com/questions/40523307/brew-install-docker-does-not-include-docker-engine/43365425#43365425">Brew install docker does not include docker engine?</A>
		<DT><A HREF="https://medium.com/crowdbotics/a-complete-one-by-one-guide-to-install-docker-on-your-mac-os-using-homebrew-e818eb4cfc3">A complete one-by-one guide to install Docker on your Mac OS using Homebrew</A>
		<DT><A HREF="https://formulae.brew.sh/formula/docker-compose#default">docker-compose ‚Äî Homebrew Formulae</A>
		<DT><A HREF="https://ragunathrajasekaran.medium.com/getting-started-with-spring-data-jpa-hibernate-orm-repository-part-4-95a6ef2af513">Spring Data JPA, Hibernate, ORM &amp; Repository (Part 4)</A>
		<DT><A HREF="https://docs.google.com/document/d/1x3TRnRXz8PCHAWAgb7pzNfgAfBuKPxsyvBVZKXKeBFc/edit#heading=h.7ye3de504xy7">Experimentation Prompts: Infographics - Google Docs</A>
		<DT><A HREF="https://docs.google.com/spreadsheets/d/1Og2u4LExfCgTckBDKte54tdBuI2JPdK3YN6v2L6scn8/edit#gid=2074678152">openai-humaneval - Google Sheets</A>
		<DT><A HREF="https://docs.google.com/spreadsheets/d/1zuQQnOzC_zrd4MBPQg4W0A7Ab5k0jEnPRSwy8AxiC4c/edit#gid=0">Function header - Google Sheets</A>
		<DT><A HREF="https://drive.google.com/drive/folders/1l6yNtgD-Xx0Awb0D5VZ-nuMG1ea-WwAr">Folder - Google Drive</A>
		<DT><A HREF="https://huggingface.co/settings/tokens">Hugging Face ‚Äì The AI community building the future.</A>
		<DT><A HREF="https://huggingface.co/datasets/bigcode/the-stack">bigcode/the-stack ¬∑ Datasets at Hugging Face</A>
		<DT><A HREF="https://huggingface.co/docs/datasets/repository_structure">Structure your repository</A>
		<DT><A HREF="https://huggingface.co/datasets/diversoailab/standard_humaneval/tree/main/data">diversoailab/standard_humaneval at main</A>
		<DT><A HREF="http://localhost:8888/notebooks/notebook/dataset.ipynb#2.1-Debugging-Tests">dataset - Jupyter Notebook</A>
		<DT><A HREF="http://localhost:8888/tree/notebook">notebook/</A>
		<DT><A HREF="https://mail.google.com/mail/u/1/#inbox?compose=GTvVlcSPFdJllBGQkSkhDnrBpFzLrGXDHVsZsQTLCtzPLdPbCJZhZjQhgpjDzgbWtdHRZkmhNrtlg">Inbox (2) - antonio.jfdominguez1@gmail.com - Gmail</A>
		<DT><A HREF="https://docs.google.com/document/d/1x3TRnRXz8PCHAWAgb7pzNfgAfBuKPxsyvBVZKXKeBFc/edit#">Experimentation Prompts: Infographics - Google Docs</A>
	</DL><p>
	<DT><H3 FOLDED>28-06-2024</H3>
	<DL><p>
		<DT><H3 FOLDED>Computational Research</H3>
		<DL><p>
			<DT><H3 FOLDED>Artificial Intelligence</H3>
			<DL><p>
				<DT><H3 FOLDED>Papers</H3>
				<DL><p>
					<DT><H3 FOLDED>Requests For Research</H3>
					<DL><p>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772472408559206798">Clive Chan</A>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772370709274038694">Sholto Douglas</A>
						<DT><A HREF="https://www.youtube.com/watch?v=cPu3SecmgUU&list=LL&index=10">How They Became Leading AI Researchers in Just 1 Year ‚Äì Sholto Douglas &amp; Trenton Bricken - YouTube</A>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1774583930500632917">Chip Huyen</A>
						<DT><A HREF="https://openai.com/research/requests-for-research-2">OpenAI: Requests for Research 2.0</A>
						<DT><A HREF="https://www.youtube.com/watch?v=UTuuTTnjxMQ&list=PLd7-bHaQwnthaNDpZ32TtYONGVk95-fhF">Sholto Douglas &amp; Trenton Bricken - How to Build &amp; Understand GPT-7's Mind - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>Reading</H3>
					<DL><p>
						<DT><H3 FOLDED>Reading Groups</H3>
						<DL><p>
							<DT><A HREF="https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/">probabilistic-machine-learning</A>
							<DT><A HREF="https://www.youtube.com/@SanDiegoMachineLearning/videos">videos</A>
							<DT><A HREF="https://mlcollective.org/events/">ML Collective</A>
						</DL><p>
						<DT><H3 FOLDED>reading-news</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=fGF3nPClUT0">LLaMA3 400B to beat GPT4? (&amp; more) | Trends in AI - May 2024 - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Papers With Code</H3>
						<DL><p>
							<DT><A HREF="https://paperswithcode.com/paper/the-distributed-information-bottleneck">The Distributed Information Bottleneck</A>
						</DL><p>
						<DT><H3 FOLDED>Books</H3>
						<DL><p>
							<DT><A HREF="https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/">Probabilistic Machine Learning</A>
							<DT><A HREF="https://mitpressbookstore.mit.edu/book/9780262048439">Probabilistic Machine Learning: Advanced Topics (Adaptive Computation and Machine Learning series) | mitpressbookstore</A>
							<DT><A HREF="https://www.amazon.com/dp/0262048434/">Probabilistic Machine Learning: Advanced Topics (Adaptive Computation and Machine Learning series): Murphy, Kevin P.: 9780262048439: Amazon.com: Books</A>
						</DL><p>
						<DT><H3 FOLDED>Papers implementations</H3>
						<DL><p>
							<DT><A HREF="https://nn.labml.ai/?_gl=1*v934qz*_ga*MTE0MzE1Mjg1My4xNzE5NjU5MzYx*_ga_PDCL9PHMHT*MTcxOTY1OTM2MS4xLjAuMTcxOTY1OTM2MS4wLjAuMA..">Annotated Research Paper Implementations: Transformers, StyleGAN, Stable Diffusion, DDPM/DDIM, LayerNorm, Nucleus Sampling and more</A>
						</DL><p>
						<DT><A HREF="https://scispace.com/">SCISPACE</A>
						<DT><A HREF="https://chatdoc.com/">ChatDOC - Chat with your documents</A>
						<DT><A HREF="https://kipp.ly/sept-oct-2023/">Things Read | Sept/Oct 2023 (kipply's blog) (MUST)</A>
						<DT><A HREF="https://www.simonwenkel.com/index.html">Summary &amp; Update Papers AI/ML</A>
						<DT><A HREF="https://search.zeta-alpha.com/?similar_to=CO_3527012&sort_by=relevance&q=&retrieval_method=knn">Zeta Alpha</A>
						<DT><A HREF="https://www.youtube.com/watch?v=QQIwfpOY-qA">20 papers to master Language modeling?</A>
						<DT><A HREF="https://kipp.ly/nov-dec-2023/">Kipply's blog: nov-dec-2023</A>
						<DT><A HREF="https://nn.labml.ai/unet/index.html">U-Net</A>
						<DT><A HREF="https://twitter.com/TheGregYang/status/1680358832789155842">Greg Yang: Books to read</A>
						<DT><A HREF="https://papers.cool/">Cool Papers - Immersive Paper Discovery</A>
						<DT><A HREF="https://gwern.net/complement">Laws of Tech: Commoditize Your Complement ¬∑ Gwern.net</A>
						<DT><A HREF="https://gist.github.com/matijagrcic/ae8353eb1e6be84a7c85d9fdc2f9631f">John_Carmack_Ilya_Sutskever.md</A>
					</DL><p>
					<DT><H3 FOLDED>Research tools</H3>
					<DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=hSTy_BInQs8">Obsidian: The King of Learning Tools (FULL GUIDE + SETUP) - YouTube</A>
						<DT><A HREF="https://www.connectedpapers.com/">Connected Papers | Find and explore academic papers</A>
					</DL><p>
					<DT><H3 FOLDED>people</H3>
					<DL><p>
						<DT><H3 FOLDED>High context people</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772472408559206798">Clive Chan</A>
							<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772370709274038694">Sholto Douglas</A>
							<DT><A HREF="https://x.com/MajmudarAdam/status/1794190796411027791">(3) adammaj en X: "I've spent the past ~3 weeks going through the entire history of deep learning and reimplementing all the core breakthroughs. It has completely changed my beliefs about deep learning progress and where we're headed. Progress tracker in thread (all resources at the end) üëá https://t.co/BjcEA2iv3f" / X</A>
							<DT><A HREF="https://www.youtube.com/watch?v=cPu3SecmgUU&list=LL&index=10">How They Became Leading AI Researchers in Just 1 Year ‚Äì Sholto Douglas &amp; Trenton Bricken - YouTube</A>
							<DT><A HREF="https://twitter.com/dwarkesh_sp">Dwarkesh Patel (main technical AI podcast)</A>
							<DT><A HREF="https://scholar.google.co.uk/citations?user=DaFHynwAAAAJ&hl=en">‚Ä™Alex Graves‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
							<DT><A HREF="https://twitter.com/kipperrii/status/1777814429838573861">Alex Graves: 8 years rule</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GU2K0kiHE1Q">AI Reading List (by Ilya Sutskever) - Part 1 - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GxjEjy5UYJU">AI Reading List (by Ilya Sutskever) - Part 2 - YouTube</A>
							<DT><A HREF="https://x.com/NoamShazeer">(1) Noam Shazeer (@NoamShazeer) / X</A>
							<DT><A HREF="https://x.com/papers_anon/status/1805084174510162112">(1) PapersAnon en X: "https://t.co/CJC3YWPoB6 Various links for ML and local models (not just LLMs) that's kept fairly updated. https://t.co/5pLfM330hp ML papers I've read that I think are interesting. Also keep a text file at the top of all the abstracts for easy searching." / X</A>
							<DT><A HREF="https://rentry.org/LocalModelsLinks">Local Models Related Links</A>
							<DT><A HREF="https://www.jasonwei.net/thoughts">Thoughts ‚Äî Jason Wei</A>
						</DL><p>
						<DT><H3 FOLDED>historical</H3>
						<DL><p>
							<DT><A HREF="https://en.wikipedia.org/wiki/Hermann_Weyl">Hermann Weyl - (Symmetry) Mathematician</A>
						</DL><p>
						<DT><A HREF="https://tacocohen.wordpress.com/">Taco Cohen-Qualcomm</A>
						<DT><A HREF="https://www.cs.ox.ac.uk/people/michael.bronstein/">Michael Bronstein-Imperial College/Twitter</A>
						<DT><A HREF="https://petar-v.com/">Petar Veliƒçkoviƒá-Deepmind</A>
						<DT><A HREF="https://arxiv.org/search/cs?searchtype=author&query=Bruna%2C+J">Joan Bruna</A>
						<DT><A HREF="https://www.stellabiderman.com/">Stella Biderman</A>
						<DT><A HREF="https://www.stellabiderman.com/">Stella Biderman (EleutherAI)</A>
						<DT><A HREF="https://people.csail.mit.edu/zhonge/index.html">Ellen D. Zhong</A>
						<DT><A HREF="https://scholar.google.com/citations?hl=en&user=7Fxbm0AAAAAJ&view_op=list_works">‚Ä™Phitchaya Mangpo Phothilimthana‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
					</DL><p>
					<DT><H3 FOLDED>Information Theory &amp; Communication</H3>
					<DL><p>
						<DT><H3 FOLDED>representation theory</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=IwYiETZEGY0&list=LL&index=11&t=25s">What does AI have to do with Plato's Allegory of the Cave? - YouTube</A>
							<DT><A HREF="https://arxiv.org/abs/2405.07987">[2405.07987] The Platonic Representation Hypothesis</A>
							<DT><A HREF="https://www.youtube.com/watch?v=1_xH2mUFpZw&t=802s">The Platonic Representation Hypothesis - YouTube</A>
						</DL><p>
						<DT><A HREF="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">(SHANNON) A Mathematical Theory of Communication</A>
						<DT><A HREF="https://twitter.com/realGeorgeHotz/status/1738013824987656635">(1) George Hotz üåë en X: ".@__tinygrad__ is really an entropics research group. Distilling neural networks down to their purest essence. The compression of the compressor. Someday net size will be measured in gates instead of FLOPS or weights." / X</A>
						<DT><A HREF="https://twitter.com/realGeorgeHotz/status/1690831164431585280">Thermodynamics is to Energy as &gt;???&gt;is to Intelligence</A>
						<DT><A HREF="https://www.youtube.com/watch?v=Z_sQg-Alg7E">Estimating the Information Flow in Deep Neural Networks - YouTube</A>
						<DT><A HREF="https://irhum.github.io/blog/spherical-harmonics/">irhum.github.io - Visual Notes on Spherical Harmonics</A>
						<DT><A HREF="https://x.com/din0s_/status/1801271625309937771">(1) dinos en X: Awesome Information Retrieval</A>
					</DL><p>
					<DT><H3 FOLDED>Signal processing</H3>
					<DL><p>
						<DT><A HREF="https://www.jezzamon.com/fourier/index.html">An Interactive Introduction to Fourier Transforms</A>
						<DT><A HREF="https://www.youtube.com/watch?v=2EZap2EoMR0">Recitation 1 - Why Vibrations and Waves Matter - YouTube</A>
						<DT><A HREF="https://twitter.com/taiyasaki/status/1782856728973045843">Band-limited Neural Fields for Levels of Detail Reconstruction</A>
						<DT><A HREF="https://irhum.github.io/blog/spherical-harmonics/">irhum.github.io - Visual Notes on Spherical Harmonics</A>
						<DT><A HREF="https://x.com/Ethan_smith_20/status/1767570870598279181">Ethan en X: "Created a new method of generative model (although kinda crappy lol) that works by autoregressive sequencing fourier coefficients. Inspired by the coarse to fine generation by Diffusion models. Full write up here: https://t.co/voD78qVwIB and tldr in thread üßµ (flowers102) https://t.co/VyhR0WA4vs" / X</A>
					</DL><p>
					<DT><H3 FOLDED>Computational Complexity</H3>
					<DL><p>
						<DT><A HREF="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov‚ÄìArnold representation theorem</A>
						<DT><A HREF="https://hadrien-montanelli.github.io/2019-06-25.html">Deep networks and the Kolmogorov‚ÄìArnold theorem</A>
						<DT><A HREF="https://math.stackexchange.com/questions/2518664/are-there-any-simple-examples-of-kolmogorov-arnold-representation">Are there any simple examples of Kolmogorov-Arnold representation?</A>
					</DL><p>
					<DT><H3 FOLDED>Numerical linear algebra</H3>
					<DL><p>
						<DT><H3 FOLDED>Random Matrix Theory</H3>
						<DL><p>
							<DT><A HREF="https://math.mit.edu/~edelman/publications/random_matrix_theory.pdf">MIT: Introduction</A>
							<DT><A HREF="https://www.youtube.com/watch?v=oPQ4mNcqY7k&t=9s">Terence Tao's central limit theorem, Double Factorials (!!) and the Moment Method #SoME3 - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=1aXOXHA7Jcw&t=3032s">Greg Yang | Large N Limits: Random Matrices &amp; Neural Networks</A>
							<DT><A HREF="https://www.youtube.com/watch?v=u32UEaFevNY">Math Reading Group - Random Matrix Theory III (12/11/23) - YouTube</A>
							<DT><A HREF="https://ericauld.github.io/2023/05/20/clt.html">Eric Auld | Math, Economics, and Computing</A>
						</DL><p>
						<DT><H3 FOLDED>Rand NLA</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2302.11474">[2302.11474] Randomized Numerical Linear Algebra : A Perspective on the Field With an Eye to Software</A>
							<DT><A HREF="https://www.youtube.com/watch?v=6htbyY3rH1w&t=1699s">Is the Future of Linear Algebra.. Random? - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=fJ2EyvR85ro">Randomized Singular Value Decomposition (SVD) - YouTube</A>
						</DL><p>
						<DT><A HREF="https://www.maths.ed.ac.uk/~v1ranick/papers/gantmacher1.pdf">The Theory of Matrices</A>
					</DL><p>
					<DT><H3 FOLDED>Statistics</H3>
					<DL><p>
						<DT><H3 FOLDED>statistics-physics-informed-learning</H3>
						<DL><p>
							<DT><A HREF="https://mitmath.github.io/18337/lecture15/diffeq_machine_learning">Mixing Differential Equations and Neural Networks for Physics-Informed Learning</A>
						</DL><p>
						<DT><H3 FOLDED>statistics-bayes</H3>
						<DL><p>
							<DT><A HREF="https://ermongroup.github.io/cs228-notes/representation/directed/">Probabilistic Graphical Models: Cascades and Nets</A>
						</DL><p>
						<DT><H3 FOLDED>optimal transport</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=C12o-60fmgE">IFML SEMINAR: 2/2/24 - Gromov-Wasserstein Alignment: Statistical and Computational Advancements... - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=EauDdCzxphE&t=3s">Optimal Transport and Information Geometry for Machine Learning and Data Science - YouTube</A>
						</DL><p>
						<DT><A HREF="https://matjaz.substack.com/p/why-are-there-complete-problems-really?s=r">Why are there complete problems, really?</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/permutation">permutation</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/probability+distribution">probability distribution</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/supervised+learning">supervised learning</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/probability+density">probability density</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Banach+space">Banach space</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Wasserstein+metric">Wasserstein metric</A>
						<DT><A HREF="https://simons.berkeley.edu/workshops/deep-learning-theory-workshop">Deep Learning Theory Workshop and Summer School | Simons Institute for the Theory of Computing</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Domain_adaptation#Domain_shift">Domain adaptation - (Distribution Shift)</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Log_probability">Log probability</A>
						<DT><A HREF="https://starai.cs.ucla.edu/papers/ZhangIJCAI23.pdf">On the Paradox of Learning to Reason from Data</A>
						<DT><A HREF="https://twitter.com/sirbayes/status/1765905163020325296">(1) Kevin Patrick Murphy en X: "@fchollet claimed that "learning to reason" (robustly across problem instances) is hard because of the nature of the MLE objective. This paper proves that claim. The reason is "statistical features inherently exist in data distributions, but can hinder model generalization..." / X</A>
						<DT><A HREF="https://www.youtube.com/watch?v=xkS9GyujiqQ">03 ‚Äì NaiÃàve Bayes parameters estimation and Laplace smoothing - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>Deep Learning</H3>
					<DL><p>
						<DT><H3 FOLDED>automatic differentiation</H3>
						<DL><p>
							<DT><A HREF="https://x.com/karpathy/status/1803963383018066272">These 94 lines of code are everything that is needed to train a neural network. Everything else is just efficiency.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=_ds0-daMESY">Automatic Differentiation - A Revisionist History and the State of the Art - AD meets SDG and PLT - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>optimization</H3>
						<DL><p>
							<DT><H3 FOLDED>convex optimization</H3>
							<DL><p>
								<DT><A HREF="https://web.stanford.edu/~boyd/cvxbook/">Convex Optimization</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Convex_optimization">Convex optimization - Wikipedia</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=NE88eqLngkg">Optimization for Deep Learning (Momentum, RMSprop, AdaGrad, Adam)</A>
							<DT><A HREF="https://www.youtube.com/watch?v=6CaUxbFX8Oc&list=PLiCLbsFQNFAxOmVeqPhI5er1LGf2-L9I4">Lecture 1 Part 1: Approximate Dynamic Programming Lectures</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mHO2ocZxqQM">IFML Seminar: 3/8/2024 - An Lyapunov Analysis of the Lion Optimizer - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GM6XPEQbkS4">Provably Faster Gradient Descent via Long Steps</A>
							<DT><A HREF="https://www.youtube.com/watch?v=AKMuA_TVz3A">An observation on Generalization</A>
							<DT><A HREF="https://web.stanford.edu/~boyd/cvxbook/">Convex Optimization ‚Äì Boyd and Vandenberghe</A>
							<DT><A HREF="https://www.youtube.com/watch?v=78vq6kgsTa8">Tom Goldstein: "What do neural loss surfaces look like?" - YouTube</A>
							<DT><A HREF="https://machine-learning-etc.ghost.io/">Generating functions approach to gradient descent analysis</A>
							<DT><A HREF="https://nn.labml.ai/optimizers/index.html">Optimizers implementations</A>
						</DL><p>
						<DT><H3 FOLDED>neural networks</H3>
						<DL><p>
							<DT><H3 FOLDED>nn-equivariant-invariant</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?app=desktop&v=kTvow5-eCCQ&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=5">Group Equivariant Deep Learning</A>
								<DT><A HREF="https://twitter.com/JustinMSolomon/status/1538780379062075392">I know nothing about "equivariant" NNs</A>
							</DL><p>
							<DT><H3 FOLDED>Learning Dynamics</H3>
							<DL><p>
								<DT><H3 FOLDED>gradient</H3>
								<DL><p>
									<DT><H3 FOLDED>Vanishing Gradients</H3>
									<DL><p>
										<DT><A HREF="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits: Transformers Residual Stream</A>
										<DT><A HREF="https://www.youtube.com/watch?v=P6sfmUTpUmc">Building makemore Part 3: Activations &amp; Gradients, BatchNorm - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=ncTHBi8a9uA">The Fundamental Problem with Neural Networks - Vanishing Gradients - YouTube</A>
									</DL><p>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1800585478539981094">What the gradient seems to really like: a thread</A>
									<DT><A HREF="https://arxiv.org/abs/2403.02241">[2403.02241] Neural Redshift: Random Networks are not Random Functions</A>
									<DT><A HREF="https://arxiv.org/abs/2004.01461">[2004.01461] Gradient Centralization: A New Optimization Technique for Deep Neural Networks</A>
								</DL><p>
								<DT><A HREF="https://transformer-circuits.pub/2022/toy_model/index.html#learning">Toy Models of Superposition</A>
								<DT><A HREF="https://www.fast.ai/posts/2023-09-04-learning-jumps/">fast.ai ‚Äì Can LLMs learn from a single example?</A>
								<DT><A HREF="https://arxiv.org/abs/1712.09913">[1712.09913] Visualizing the Loss Landscape of Neural Nets</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1806547801779863697">language model layer permutation</A>
							</DL><p>
							<DT><H3 FOLDED>backpropagation</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4186s">The spelled-out intro to neural networks and backpropagation: building micrograd - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=P6sfmUTpUmc">Building makemore Part 3: Activations &amp; Gradients, BatchNorm - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=q8SA3rM6ckI">Building makemore Part 4: Becoming a Backprop Ninja - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>MLP</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=TCH_1BHY58I">Building makemore Part 2: MLP - YouTube</A>
								<DT><A HREF="https://x.com/thomasahle/status/1798408687981297844">Using ùöùùöòùöõùöåùöë.ùöåùöòùöñùöôùöíùöïùöé makes KANs as fast as MLPs!</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=BjyZcSiVg5A">Deep Learning Theory Session. Ilya SutskeverIlya Sutskever - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M74WKIh0ciI&list=LL&index=24">Three weight matrices in a neural network growing connections as two parallel networks are merged. - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=g0gREwiDbis">Dr. Geoffrey Hinton: A brief study of neural networks</A>
							<DT><A HREF="https://www.youtube.com/watch?v=-PFIkkwWdnM">Kolmogorov-Arnold Networks: MLP vs KAN, Math, B-Splines, Universal Approximation Theorem - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=nL7zpkfKbPs">Human MNIST Challenge: Can you recognize the Fourier transform of handwritten digits? - YouTube</A>
							<DT><A HREF="https://github.com/cloneofsimo/insightful-nn-papers">cloneofsimo/insightful-nn-papers: These papers will provide unique insightful concepts that will broaden your perspective on neural networks and deep learning</A>
						</DL><p>
						<DT><H3 FOLDED>deep-learning-people</H3>
						<DL><p>
							<DT><H3 FOLDED>Michael Bronstein</H3>
							<DL><p>
								<DT><A HREF="https://towardsdatascience.com/predictions-and-hopes-for-geometric-graph-ml-in-2022-aa3b8b79f5cc">What does 2022 hold for Geometric &amp; Graph ML?</A>
								<DT><A HREF="https://arxiv.org/abs/2104.13478">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</A>
								<DT><A HREF="https://medium.com/stanford-cs224w/tackling-the-traveling-salesman-problem-with-graph-neural-networks-b86ef4300c6e">Tackling the Traveling Salesman Problem with Graph Neural Networks | by Michael Atkin | Stanford CS224W GraphML Tutorials | May, 2023 | Medium</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/agikoala/status/1641838208823468032/photo/1">(1) jason en X: "Three starter pokemon in deep learning</A>
							<DT><A HREF="https://scholar.google.com/citations?user=x04W_mMAAAAJ">‚Ä™Ilya Sutskever‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
							<DT><A HREF="https://scholar.google.com/citations?user=NkzyCvUAAAAJ&hl=en">‚Ä™Oriol Vinyals‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
							<DT><A HREF="https://scholar.google.com/citations?user=vfT6-XIAAAAJ&hl=en">‚Ä™Quoc V. Le‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
							<DT><A HREF="https://scholar.google.co.uk/citations?user=DaFHynwAAAAJ&hl=en">‚Ä™Alex Graves‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
							<DT><A HREF="https://www.yitay.net/">Yi Tay</A>
							<DT><A HREF="https://www.jasonwei.net/thoughts">Thoughts ‚Äî Jason Wei</A>
							<DT><A HREF="https://hwchung27.github.io/">Hyung Won Chung</A>
							<DT><A HREF="https://twitter.com/ilyasut/status/1790517455628198322">(1) Ilya Sutskever en X: "After almost a decade, I have made the decision to leave OpenAI. ¬†The company‚Äôs trajectory has been nothing short of miraculous, and I‚Äôm confident that OpenAI will build AGI that is both safe and beneficial under the leadership of @sama, @gdb, @miramurati and now, under the" / X</A>
							<DT><A HREF="https://twitter.com/merettm">akub Pachocki (OpenAI dota bot)</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Ckz8XA2hW84&list=LL&index=14&t=1s">Ilya Sutskever (OpenAI) and Jensen Huang (NVIDIA CEO) : AI Today and Vision of the Future (3/2023) - YouTube</A>
							<DT><A HREF="https://www.linkedin.com/in/alecradford/">Alec Radford</A>
							<DT><A HREF="https://www.linkedin.com/in/markchen90/">Mark Chen: Head of Fronteirs Research at OpenAI</A>
							<DT><A HREF="https://x.com/peterthedecent">(1) Peter (@peterthedecent) / X</A>
						</DL><p>
						<DT><H3 FOLDED>deep-learning-lectures</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=bZF4N8HR1cc">09 ‚Äì AE, DAE, and VAE with PyTorch; generative adversarial networks (GAN) and code - YouTube</A>
							<DT><A HREF="https://mlcollective.org/dlct/">ML Collective</A>
							<DT><A HREF="https://informationisbeautiful.net/">Information is Beautiful</A>
							<DT><A HREF="https://www.youtube.com/watch?v=v7arCzjQk38">Robust Gradient Descent: Agnostically Estimating an Unknown Affine Transformation... - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=MRNrbLRxNyg">Toward a Grand Unified Theory of Accelerations in Optimization and Machine Learning - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M74WKIh0ciI&list=LL&index=24">Three weight matrices in a neural network growing connections as two parallel networks are merged. - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=EI6k2g9CUHo">Baharan Mirzasoleiman - How Structure Helps in Machine Learning - YouTube</A>
							<DT><A HREF="https://nn.labml.ai/unet/index.html">U-Net</A>
							<DT><A HREF="https://github.com/Atcold/NYU-DLSP21">Atcold/NYU-DLSP21: NYU Deep Learning Spring 2021</A>
							<DT><A HREF="https://atcold.github.io/NYU-DLSP21/">DEEP LEARNING ¬∑ Deep Learning</A>
							<DT><A HREF="https://uvadlc.github.io/">UvA Deep Learning Course (Amsterdam)</A>
							<DT><A HREF="https://mathematical-tours.github.io/maths-ia-course/">The Mathematics of IA - Mathematical Tours of Data Sciences</A>
							<DT><A HREF="https://cs229.stanford.edu/lectures-spring2022/main_notes.pdf">CS229 Lecture Notes</A>
							<DT><A HREF="https://www.youtube.com/watch?v=UOvPeC8WOt8">The Neural Network, A Visual Introduction - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ys-G9uEW3O0">02 ‚Äì Discrete probability recap, NaiÃàve Bayes classification - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=BjyZcSiVg5A">Deep Learning Theory Session. Ilya SutskeverIlya Sutskever - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>deep-learning-books</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=kuvFoXzTK3E">Prof. Chris Bishop's NEW Deep Learning Textbook (Microsoft)</A>
						</DL><p>
						<DT><H3 FOLDED>Machine Learning</H3>
						<DL><p>
							<DT><H3 FOLDED>Physics-Informed Learning</H3>
							<DL><p>
								<DT><A HREF="https://mitmath.github.io/18337/lecture15/diffeq_machine_learning">Mixing Differential Equations and Neural Networks</A>
								<DT><A HREF="https://www.youtube.com/channel/UCDtsHjkOEMHYPGgpKX8VOPg/videos">Parallel Computing and Scientific Machine Learning (MIT 2021 Spring)</A>
								<DT><A HREF="https://twitter.com/martinmbauer/status/1529705943386275842">Protons are not fundamental particles, but dynamical systems made of quarks and gluons.</A>
								<DT><A HREF="https://twitter.com/thuereyGroup/status/1529177918479511552">How much does training with a differentiable physics simulator really improve things?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5P19hROy9vk">Why Quantum Mechanics Uses the Physics of SPRINGS - Quantum Harmonic Oscillators EXPLAINED - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=GCz6afDVy5Y">AI/ML+Physics: Recap and Summary [Physics Informed Machine Learning] - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>ml-cs</H3>
							<DL><p>
								<DT><A HREF="http://cs229.stanford.edu/syllabus.html">CS229: Machine Learning</A>
								<DT><A HREF="http://cs229.stanford.edu/#info">CS229: Machine Learning</A>
								<DT><A HREF="https://www.youtube.com/@machinelearningatberkeley8868">Machine Learning at Berkeley - YouTube</A>
							</DL><p>
							<DT><A HREF="https://www.simonwenkel.com/index.html">SimonWenkel.com</A>
							<DT><A HREF="https://developers.google.com/machine-learning/glossary/">Machine Learning Glossary ¬†|¬† Google Developers</A>
							<DT><A HREF="https://github.com/ageron/handson-ml2">handson-ml2: A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.</A>
							<DT><A HREF="https://www.eleuther.ai/beginners.pdf">A Beginner‚Äôs Guide to Machine Learning</A>
							<DT><A HREF="https://github.com/ageron/handson-ml2">Fundamentals of Machine Learning and Deep Learning in Python (Sklearn, Keras, TensorFlow v2)</A>
							<DT><A HREF="https://snap-research.github.io/BitsFusion/">Snap.Inc: BitsFusion</A>
						</DL><p>
						<DT><H3 FOLDED>Constractive Learning</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</A>
							<DT><A HREF="https://arxiv.org/abs/1911.05722">Momentum Contrast for Unsupervised Visual Representation Learning</A>
							<DT><A HREF="https://crfm.stanford.edu/2022/04/14/contrastive-learning.html">Understanding Deep Learning with Unlabeled Data: Contrastive Learning</A>
						</DL><p>
						<DT><H3 FOLDED>Self-Supervised Learning</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2103.00020">CLIP: Learning Transferable Visual Models From Natural Language Supervision</A>
							<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1744716166290096161">(Yann LeCun) Adaptive linear classifiers</A>
							<DT><A HREF="https://github.com/imbue-ai/self_supervised">imbue-ai/self_supervised: A Pytorch-Lightning implementation of self-supervised algorithms</A>
						</DL><p>
						<DT><H3 FOLDED>blueprint</H3>
						<DL><p>
							<DT><H3 FOLDED>Tensor Programs</H3>
							<DL><p>
								<DT><A HREF="https://thegregyang.com/">Greg Yang |¬†Professional page</A>
								<DT><A HREF="https://arxiv.org/abs/2310.02244">[2310.02244] Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks</A>
							</DL><p>
							<DT><H3 FOLDED>blueprint-geometric-deep-learninng</H3>
							<DL><p>
								<DT><H3 FOLDED>geometric-deep-learninng-papers</H3>
								<DL><p>
									<DT><A HREF="https://geometricdeeplearning.com/">Geometric Deep Learning - Grids, Groups, Graphs, Geodesics, and Gauges</A>
									<DT><A HREF="https://www.youtube.com/watch?v=e5233FYNfNQ">Math Reading Group - Geometric Deep Learning I (06/08/23) - YouTube</A>
									<DT><A HREF="https://geometricdeeplearning.com/lectures/">GDL Course</A>
								</DL><p>
								<DT><H3 FOLDED>geometric-deep-learninng-lectures</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=PtA0lg_e5nA&list=PLn2-dEmQeTfQ8YVuHBOvAhUlnIPYxkeu3">youtube</A>
									<DT><A HREF="https://medium.com/towards-data-science/geometric-deep-learning-da09e7c17aa3">Introduction to Geometric Deep Learning | by Ahmed A. A. Elhag</A>
								</DL><p>
								<DT><H3 FOLDED>geometric-deep-learninng-software</H3>
								<DL><p>
									<DT><A HREF="https://blog.paperspace.com/geometric-deep-learning-framework-comparison/">Geometric Deep Learning Library Comparison | Paperspace Blog</A>
									<DT><A HREF="https://medium.com/towards-data-science/graph-neural-networks-a-learning-journey-since-2008-from-python-to-jax-graph-attention-networks-692e4d6d7637">Graph Neural Networks: A learning journey since 2008 ‚Äî From Python to JAX: Graph Attention Networks | by Stefano Bosisio | Mar, 2022 | Towards Data Science</A>
									<DT><A HREF="https://twitter.com/phillip_lippe/status/1536340878960173057">"Are you interested in learning JAX with Flax? We have translated our popular Deep Learning tutorials on CNNs, GNNs, (Vision) Transformers, and more from PyTorch to JAX+Flax, with considerable speedups for smaller models!</A>
									<DT><A HREF="https://github.com/phlippe/uvadlc_notebooks">phlippe/uvadlc_notebooks: Repository of Jupyter notebook tutorials for teaching the Deep Learning Course at the University of Amsterdam (MSc AI), Fall 2021/Spring 2022</A>
									<DT><A HREF="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html">Tutorial 2 (JAX): Introduction to JAX+Flax ‚Äî UvA DL Notebooks v1.2 documentation</A>
									<DT><A HREF="https://uvadlc-notebooks.readthedocs.io/en/latest/index.html">Welcome to the UvA Deep Learning Tutorials! ‚Äî UvA DL Notebooks v1.2 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>Graph Neural Networks</H3>
								<DL><p>
									<DT><A HREF="http://snap.stanford.edu/graphsage/">GraphSAGE: embedding framework</A>
									<DT><A HREF="https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa">Transformers are Graph Neural Networks</A>
									<DT><A HREF="https://www.youtube.com/watch?v=7KMcXHwQzZs">Michael Bronstein | Neural diffusion PDEs, differential geometry, and graph neural networks</A>
									<DT><A HREF="https://towardsdatascience.com/manifold-learning-2-99a25eeb677d">Latent graph neural networks: Manifold learning 2.0?</A>
									<DT><A HREF="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks</A>
									<DT><A HREF="https://bookdown.org/omarlizardo/_main/4-6-network-composition-homophily-measures.html">4.6 Network Composition-Homophily Measures</A>
									<DT><H3 FOLDED>theory</H3>
									<DL><p>
										<DT><A HREF="https://ncatlab.org/nlab/show/Lipschitz+map">Lipschitz map</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/Banach+space">Banach space</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/permutation">permutation</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/linear%20order">linear order</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/cartesian+product">cartesian product</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/hypergraph">hypergraph in nLab</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/span">span in nLab</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/Bayesian+reasoning">Bayesian reasoning in nLab</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/probability+space">probability space in nLab</A>
										<DT><A HREF="https://en.wikipedia.org/wiki/Network_homophily">homophily</A>
									</DL><p>
									<DT><H3 FOLDED>PDE</H3>
									<DL><p>
										<DT><A HREF="https://blog.twitter.com/engineering/en_us/topics/insights/2021/graph-neural-networks-as-neural-diffusion-pdes">Graph Neural Networks as Neural Diffusion PDEs</A>
										<DT><A HREF="https://towardsdatascience.com/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a">Graph Neural Networks beyond Weisfeiler-Lehman and vanilla Message Passing</A>
										<DT><A HREF="https://www.youtube.com/watch?v=9SMbH18nMUg">Graph Neural Networks and Diffusion PDEs | Benjamin Chamberlain &amp; James Rowbottom - YouTube</A>
									</DL><p>
									<DT><A HREF="https://twitter.com/n_keriven/status/1529404489832308736">Theoretical analysis of graph (over)smoothing</A>
									<DT><A HREF="https://twitter.com/rampasek/status/1536534947825176577">Graph Transformers at scale</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Network_homophily">homophily</A>
									<DT><A HREF="https://towardsdatascience.com/predictions-and-hopes-for-geometric-graph-ml-in-2022-aa3b8b79f5cc">What does 2022 hold for Geometric &amp; Graph ML?</A>
									<DT><A HREF="https://towardsdatascience.com/using-subgraphs-for-more-expressive-gnns-8d06418d5ab">Using Subgraphs for More Expressive GNNs | by Michael Bronstein | Towards Data Science</A>
									<DT><A HREF="https://graphdeeplearning.github.io/post/benchmarking-gnns/">Benchmarking Graph Neural Networks | NTU Graph Deep Learning Lab</A>
									<DT><A HREF="https://distill.pub/2021/understanding-gnns/">Understanding Convolutions on Graphs</A>
								</DL><p>
								<DT><A HREF="https://michael-bronstein.medium.com/">Michael Bronstein ‚Äì Medium (Geometrical Deep Learning)</A>
								<DT><A HREF="https://www.quantamagazine.org/researchers-build-ai-that-builds-ai-20220125/">Researchers Build AI That Builds AI | Quanta Magazine</A>
								<DT><A HREF="https://www.youtube.com/watch?v=rie-9AEhYdY">WE MUST ADD STRUCTURE TO DEEP LEARNING BECAUSE... - YouTube</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/TheGregYang/status/1711803863177855033">Infinite Depth Neural Networks (Greg Yang)</A>
							<DT><A HREF="https://twitter.com/TheGregYang/status/1709700104951931077">Neural Nets: What if depth -&gt; inf as well?</A>
							<DT><A HREF="https://twitter.com/hayou_soufiane/status/1622594202042499073">Q: What happens to the neural covariance when both Width and Depth are taken to infinity?</A>
							<DT><A HREF="https://irhum.github.io/blog/spherical-harmonics/">irhum.github.io - Visual Notes on Spherical Harmonics</A>
						</DL><p>
						<DT><H3 FOLDED>The Bitter Leasson</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/polynoamial/status/1789381426187546644">(1) Noam Brown en X: "@jxmnop The point of the Bitter Lesson is that research and clever ideas are important, but people should think about how their ideas scale with data and compute rather than just relying on One Weird Trick to get them a little farther than SOTA." / X</A>
							<DT><A HREF="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</A>
							<DT><A HREF="https://twitter.com/polynoamial">(1) Noam Brown (@polynoamial) / X</A>
						</DL><p>
						<DT><H3 FOLDED>SDE vs ODE</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/wgilpin0/status/1737934963365056543">William Gilpin: "Can machine learning predict chaos?</A>
						</DL><p>
						<DT><H3 FOLDED>Neural Architecture Search</H3>
						<DL><p>
							<DT><A HREF="https://blog.research.google/2022/02/unlocking-full-potential-of-datacenter.html">Unlocking the Full Potential of Datacenter ML Accelerators with Platform-Aware Neural Architecture Search ‚Äì Google Research Blog</A>
							<DT><A HREF="https://x.com/antferdom/status/1799474799661691014">xLSTM won't replace the Transformer. Two bitter lessons</A>
						</DL><p>
						<DT><H3 FOLDED>differentiable neural computers</H3>
						<DL><p>
							<DT><A HREF="https://jaspock.github.io/funicular/dnc.html">A bit-by-bit guide to the equations governing differentiable neural computers</A>
							<DT><A HREF="https://gwern.net/doc/reinforcement-learning/model-free/2016-graves.pdf">https://gwern.net/doc/reinforcement-learning/model-free/2016-graves.pdf</A>
							<DT><A HREF="https://deepmind.google/discover/blog/differentiable-neural-computers/">Differentiable neural computers - Google DeepMind</A>
						</DL><p>
						<DT><H3 FOLDED>deep-learning-debugging</H3>
						<DL><p>
							<DT><H3 FOLDED>silent data corruption (SDC)</H3>
							<DL><p>
								<DT><A HREF="https://dl.acm.org/doi/abs/10.1145/3620666.3651349">Dr. DNA: Combating Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations | Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</A>
								<DT><A HREF="https://dl.acm.org/doi/pdf/10.1145/3620666.3651349">Dr. DNA: Combating Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>deep double descent</H3>
						<DL><p>
							<DT><A HREF="https://openai.com/index/deep-double-descent/">Deep double descent | OpenAI</A>
							<DT><A HREF="https://www.youtube.com/watch?v=W_TAKJRgrbs">Ilya Sutskever on Deep Double Descent - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=5I7-ItaOZFU&t=7s">Double Descent explained by Yann LeCun - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=qRHdQz_P_Lo&t=1s">Statistical Learning: 10.7 Interpolation and Double Descent - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Kih-VPHL3gA">Deep Double Descent and Overparameterization: Classical Machine Learning vs. Modern Deep Learning - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Grokking</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=dND-7llwrpw&t=230s">Grokking: Generalization beyond Overfitting on small algorithmic datasets (Paper Explained) - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Meta-Learning and Self-Play</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=9EN_HoEk3KY">Ilya Sutskever: OpenAI Meta-Learning and Self-Play | MIT Artificial General Intelligence (AGI) - YouTube</A>
						</DL><p>
						<DT><A HREF="https://x.com/MajmudarAdam/status/1794190796411027791">History of deep learning (core)</A>
						<DT><A HREF="https://github.com/adam-maj/deep-learning">adam-maj/deep-learning: A deep-dive on the entire history of deep-learning</A>
						<DT><A HREF="https://gist.github.com/matijagrcic/ae8353eb1e6be84a7c85d9fdc2f9631f">Ilya Sutskever reading list.md</A>
						<DT><A HREF="https://github.com/google-research/tuning_playbook">google-research/tuning_playbook: A playbook for systematically maximizing the performance of DNN</A>
						<DT><A HREF="https://twitter.com/agikoala/status/1641838208823468032/photo/1">(1) jason en X: "Three starter pokemon in deep learning https://t.co/u6RZqrvsAJ" / X</A>
						<DT><A HREF="https://twitter.com/prateeky2806/status/1665759148380758022">Mergin diff task-specific models into a multi-task model</A>
						<DT><A HREF="https://www.youtube.com/watch?v=pA-azVdihQc&t=1890s">What is Machine Learning Good For? - Alex Davies - YouTube</A>
						<DT><A HREF="https://arxiv.org/pdf/2002.09398.pdf">It‚Äôs Not What Machines Can Learn, It‚Äôs What We Cannot Teach (DeepMind Math Reasoning)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=5exL8UYxpsI&t=63s">IFML SEMINAR: 1/26/24 - Meta Optimization (Google DeepMind)</A>
						<DT><A HREF="https://starai.cs.ucla.edu/papers/ZhangIJCAI23.pdf">On the Paradox of Learning to Reason from Data</A>
						<DT><A HREF="https://github.com/gregorbachmann/scaling_mlps">gregorbachmann/scaling_mlps</A>
						<DT><A HREF="https://www.youtube.com/watch?v=1menqhfNzzo">Stanford EE364A Convex Optimization I Stephen Boyd I 2023 I Lecture 3 - YouTube</A>
						<DT><A HREF="https://www.youtube.com/watch?v=6_v9Ogi7P6Q">Topology for Energy-Based models</A>
						<DT><A HREF="https://arxiv.org/abs/2403.14606">[2403.14606] The Elements of Differentiable Programming</A>
						<DT><A HREF="https://scholar.google.co.uk/citations?view_op=view_citation&hl=en&user=DaFHynwAAAAJ&citation_for_view=DaFHynwAAAAJ:isC4tDSrTZIC">Neural turing machines (main)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=yZ-HT81qYuE">What is Differentiable Programming - YouTube</A>
						<DT><A HREF="https://twitter.com/BenTheEgg">Each new layer adds residual information,  so simple concepts are nailed early, complex ones late</A>
						<DT><A HREF="https://www.youtube.com/watch?v=EvSe0ktD95k&t=7s">A Path Towards Autonomous Machine Intelligence with Dr. Yann LeCun - YouTube</A>
						<DT><A HREF="https://www.youtube.com/watch?v=GyKlMcsl72w">00 ‚Äì Course introduction - YouTube</A>
						<DT><A HREF="https://stanford-cs336.github.io/spring2024/">Stanford CS336 | Language Modeling from Scratch</A>
						<DT><A HREF="https://rentry.org/LocalModelsPapers">Local Models Related Papers</A>
						<DT><A HREF="https://www.youtube.com/watch?v=GxjEjy5UYJU&t=25s">AI Reading List (by Ilya Sutskever) - Part 2 - YouTube</A>
						<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples/tree/master">NVIDIA/DeepLearningExamples: State-of-the-Art Deep Learning scripts organized by models</A>
					</DL><p>
					<DT><H3 FOLDED>Reinforcement Learning</H3>
					<DL><p>
						<DT><H3 FOLDED>search-and-learning</H3>
						<DL><p>
							<DT><H3 FOLDED>Game Theory</H3>
							<DL><p>
								<DT><A HREF="https://pub.towardsai.net/deepminds-clever-idea-to-master-asymmetric-games-79c3461ef6e">DeepMind‚Äôs Clever Idea to Master Asymmetric Games</A>
							</DL><p>
							<DT><H3 FOLDED>self-play</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>dynamic programming</H3>
							<DL><p>
								<DT><A HREF="https://developer.nvidia.com/blog/boosting-dynamic-programming-performance-using-nvidia-hopper-gpu-dpx-instructions/">Boosting Dynamic Programming Performance Using NVIDIA Hopper GPU DPX Instructions | NVIDIA Technical Blog</A>
								<DT><A HREF="https://web.mit.edu/dimitrib/www/abstractdp_MIT.html">LESSONS FROM ALPHAZERO FOR OPTIMAL, MODEL PREDICTIVE, AND ADAPTIVE CONTROL</A>
							</DL><p>
							<DT><H3 FOLDED>Monte Carlo Tree Search</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=Fbs4lnGLS8M">Monte Carlo Tree Search (MCTS) Tutorial - YouTube</A>
							</DL><p>
							<DT><A HREF="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</A>
							<DT><A HREF="https://web.mit.edu/dimitrib/www/abstractdp_MIT.html">LESSONS FROM ALPHAZERO FOR OPTIMAL, MODEL PREDICTIVE, AND ADAPTIVE CONTROL</A>
							<DT><A HREF="https://www.umut-acar.org/self-adjusting-computation">Umut A. Acar - Self-Adjusting Computation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=BJi6N4tDupk">OpenAI: Teaching AI Through Self-Play - Ilya Sutskever</A>
							<DT><A HREF="https://www.youtube.com/watch?v=uz83G-2ny8Q">DeepMind‚Äôs New AI Saw 15,000,000,000 Chess Boards! - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=EvSe0ktD95k&t=7s">A Path Towards Autonomous Machine Intelligence with Dr. Yann LeCun - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>MuZero</H3>
						<DL><p>
							<DT><A HREF="https://github.com/kaesve/muzero">A clean implementation of MuZero and AlphaZero following the AlphaZero General framework. Train and Pit both algorithms against each other, and investigate reliability of learned MuZero MDP models.</A>
							<DT><A HREF="https://github.com/chiamp/muzero-cartpole">Cartpole</A>
							<DT><A HREF="https://twitter.com/DrJimFan/status/1627354160529285120?lang=en">(Jim Fan)"What‚Äôs next after open source and open model? Open training</A>
							<DT><A HREF="https://github.com/google-deepmind/mctx">google-deepmind/mctx: Monte Carlo tree search in JAX</A>
							<DT><A HREF="https://www.youtube.com/watch?v=yIrFIOx4VP8&t=21939s">George Hotz | Programming | Can MuZero play Tic Tac Toe? | Part1 | DeepMind AI - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=xc0jGZYFQLQ&t=63799s">George Hotz | Programming | Fun with MuZero and MCTS on a lovely Sunday | CartPole | DeepMind AI - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=62nq4Zsn8vc&t=1027s">Alpha Zero and Monte Carlo Tree Search - YouTube</A>
							<DT><A HREF="https://lczero.org/">Leela Chess Zero</A>
							<DT><A HREF="https://twitter.com/DrJimFan/status/1625538305889820673">mctx</A>
							<DT><A HREF="https://arxiv.org/abs/1911.08265">[1911.08265] Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</A>
							<DT><A HREF="https://deepmind.google/technologies/alphazero-and-muzero/?_gl=1*1ai5qs2*_up*MQ..*_ga*MTk2ODY0MzQ4Ny4xNjk5MjE2MzUz*_ga_LS8HVHCNQ0*MTY5OTIxNjM1My4xLjAuMTY5OTIxNjM3My4wLjAuMA..">AlphaZero and MuZero - Google DeepMind</A>
							<DT><A HREF="https://www.youtube.com/watch?v=xc0jGZYFQLQ&t=63800s">George Hotz | Programming | Fun with MuZero and MCTS on a lovely Sunday | CartPole | DeepMind AI - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>AlphaGo</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>AlphaZero</H3>
						<DL><p>
							<DT><A HREF="https://web.mit.edu/dimitrib/www/abstractdp_MIT.html">LESSONS FROM ALPHAZERO FOR OPTIMAL, MODEL PREDICTIVE, AND ADAPTIVE CONTROL</A>
						</DL><p>
						<DT><A HREF="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">A Taxonomy of RL Algorithms</A>
						<DT><A HREF="https://arxiv.org/abs/1707.03497">(Deepmind) Value Prediction Network: predictions of FUTURE VALUES rather than of future observations.</A>
						<DT><A HREF="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">Introduction to Reinforcement Learning with David Silver (Deepmind)</A>
						<DT><A HREF="http://www.incompleteideas.net/book/the-book.html">Sutton &amp; Barto Book: Reinforcement Learning: An Introduction</A>
						<DT><A HREF="https://twitter.com/TheSequenceAI/status/1530591453243744256">Let's talk about Exploration-Exploitation Dilemma</A>
						<DT><A HREF="https://www.amazon.com/dp/0262039249/">Reinforcement Learning, second edition: An Introduction Sutton, Richard S., Barto, Andrew G</A>
						<DT><A HREF="https://arxiv.org/pdf/2201.02135.pdf">2022 June Arxiv Nature book</A>
						<DT><A HREF="http://www.incompleteideas.net/book/RLbook2020.pdf">Reinforcement Learning: An Introduction 2nd edition (Richard S. Sutton)</A>
						<DT><A HREF="https://github.com/lvwerra/rl-implementations">lvwerra/rl-implementations: This repo contains a set of notebooks to reproduce reinforcement learning algorithms.</A>
						<DT><A HREF="https://github.com/sotetsuk/pgx">sotetsuk/pgx: üé≤ Vectorized RL game environments written in JAX with end-to-end AlphaZero examples</A>
						<DT><A HREF="https://github.com/google-deepmind/dqn_zoo">google-deepmind/dqn_zoo: DQN Zoo is a collection of reference implementations of reinforcement learning agents developed at DeepMind based on the Deep Q-Network (DQN) agent.</A>
						<DT><A HREF="https://www.youtube.com/watch?v=-tZkb0vgaDk">George Hotz | Programming | RL is dumb and doesn't work | Reinforcement Learning LunarLander Part 2 - YouTube</A>
						<DT><A HREF="https://shield.ai/">Shield AI - Building The World‚Äôs Best AI Pilot</A>
						<DT><A HREF="https://x.com/RobertTLange">Rejax: JAX RL algorithms</A>
						<DT><A HREF="https://github.com/kvfrans/rlbase_stable">kvfrans/rlbase_stable</A>
						<DT><A HREF="https://github.com/openai/baselines">openai/baselines: OpenAI Baselines: high-quality implementations of reinforcement learning algorithms</A>
						<DT><A HREF="https://github.com/imbue-ai/garage">imbue-ai/garage: A toolkit for reproducible reinforcement learning research.</A>
					</DL><p>
					<DT><H3 FOLDED>Diffusion</H3>
					<DL><p>
						<DT><H3 FOLDED>Consistency Models</H3>
						<DL><p>
							<DT><A HREF="https://gsunshine.notion.site/Consistency-Models-Made-Easy-954205c0b4a24c009f78719f43b419cc?pvs=4">üí´ Consistency Models Made Easy</A>
							<DT><A HREF="https://twitter.com/ZhengyangGeng/status/1773069621211415003">Zhengyang Geng: fast Consistency Models inference</A>
							<DT><A HREF="https://twitter.com/DrYangSong">(1) Yang Song (@DrYangSong) / X</A>
							<DT><A HREF="https://sander.ai/2024/02/28/paradox.html">The paradox of diffusion distillation ‚Äì Sander Dieleman</A>
							<DT><A HREF="https://github.com/locuslab/ect">locuslab/ect: Consistency Models Made Easy</A>
						</DL><p>
						<DT><H3 FOLDED>Diffusion-distillation</H3>
						<DL><p>
							<DT><H3 FOLDED>Imagine Flash (Meta AI)</H3>
							<DL><p>
								<DT><A HREF="https://ai.meta.com/research/publications/imagine-flash-accelerating-emu-diffusion-models-with-backward-distillation/?utm_source=linkedin&utm_medium=organic_social&utm_content=image&utm_campaign=imagineflash">Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation | Research - AI at Meta</A>
								<DT><A HREF="https://scontent.fsvq5-1.fna.fbcdn.net/v/t39.2365-6/10000000_1650715635757274_3499348867570367129_n.pdf?_nc_cat=111&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=E__oAKOcVaIQ7kNvgG2Odel&_nc_ht=scontent.fsvq5-1.fna&oh=00_AfABQIIxviDeDBA4t-0_pSaK-QGovi86kWT62uEJom-57w&oe=66355F6B">Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation</A>
							</DL><p>
							<DT><H3 FOLDED>SDXL-Lightning: Progressive Adversarial Diffusion Distillation (ByteDance)</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2402.13929">[2402.13929] SDXL-Lightning: Progressive Adversarial Diffusion Distillation</A>
								<DT><A HREF="https://arxiv.org/html/2402.13929v1">SDXL-Lightning: Progressive Adversarial Diffusion Distillation</A>
								<DT><A HREF="https://developers.cloudflare.com/workers-ai/models/stable-diffusion-xl-lightning/">stable-diffusion-xl-lightning ¬∑ Cloudflare Workers AI docs</A>
								<DT><A HREF="https://huggingface.co/ByteDance/SDXL-Lightning">ByteDance/SDXL-Lightning ¬∑ Hugging Face</A>
							</DL><p>
							<DT><A HREF="https://scontent.fsvq5-1.fna.fbcdn.net/v/t39.2365-6/10000000_1650715635757274_3499348867570367129_n.pdf?_nc_cat=111&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=E__oAKOcVaIQ7kNvgG2Odel&_nc_ht=scontent.fsvq5-1.fna&oh=00_AfABQIIxviDeDBA4t-0_pSaK-QGovi86kWT62uEJom-57w&oe=66355F6B">Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation</A>
							<DT><A HREF="https://arxiv.org/abs/2311.17042">[2311.17042] Adversarial Diffusion Distillation</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-optimization</H3>
						<DL><p>
							<DT><A HREF="https://www.vrushankdes.ai/diffusion-inference-optimization">Diffusion Inference Optimization</A>
							<DT><A HREF="https://github.com/vdesai2014/inference-optimization-blog-post">vdesai2014/inference-optimization-blog-post</A>
							<DT><A HREF="https://arxiv.org/abs/2402.19481">[2402.19481] DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</A>
							<DT><A HREF="https://github.com/czg1225/AsyncDiff">czg1225/AsyncDiff: Official implementation of "AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising"</A>
						</DL><p>
						<DT><H3 FOLDED>DiT</H3>
						<DL><p>
							<DT><A HREF="https://github.com/Tencent/HunyuanDiT/">Tencent/HunyuanDiT: Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding</A>
							<DT><A HREF="https://dit.hunyuan.tencent.com/">ËÖæËÆØÊ∑∑ÂÖÉDiT</A>
							<DT><A HREF="https://github.com/facebookresearch/DiT">facebookresearch/DiT: Official PyTorch Implementation of "Scalable Diffusion Models with Transformers"</A>
							<DT><A HREF="https://github.com/kvfrans/jax-diffusion-transformer/?tab=readme-ov-file">kvfrans/jax-diffusion-transformer: Implementation of Diffusion Transformer (DiT) in JAX</A>
							<DT><A HREF="https://github.com/PipeFusion/PipeFusion">PipeFusion/PipeFusion: A Suite of Parallel Approaches for Inference of Diffusion Transformer Models on GPU Clusters</A>
						</DL><p>
						<DT><H3 FOLDED>cloneofsimo</H3>
						<DL><p>
							<DT><H3 FOLDED>minRF</H3>
							<DL><p>
								<DT><A HREF="https://github.com/cloneofsimo/minRF">cloneofsimo/minRF: Minimal implementation of scalable rectified flow transformers, based on SD3's approach</A>
								<DT><A HREF="https://arxiv.org/abs/2406.07480">[2406.07480] Image Neural Field Diffusion Models</A>
								<DT><A HREF="https://cdn.openai.com/papers/dall-e-3.pdf">https://cdn.openai.com/papers/dall-e-3.pdf</A>
								<DT><A HREF="https://arxiv.org/abs/2310.20550">[2310.20550] CapsFusion: Rethinking Image-Text Data at Scale</A>
								<DT><A HREF="https://google.github.io/imageinwords/">ImageInWords</A>
								<DT><A HREF="https://huggingface.co/datasets/graph-based-captions/GBC10M">graph-based-captions/GBC10M ¬∑ Datasets at Hugging Face</A>
								<DT><A HREF="https://arxiv.org/abs/1812.06162">[1812.06162] An Empirical Model of Large-Batch Training</A>
								<DT><A HREF="https://x.com/_xjdr/status/1801063628298412239">(2) xjdr en X: "Megablox, splash attention, pallas and automatically sharded named axis come free and built in with Jax and y'all are still using pytorch in production?!?!" / X</A>
								<DT><A HREF="https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py">jax/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py at main ¬∑ google/jax</A>
							</DL><p>
							<DT><A HREF="https://x.com/cloneofsimo/status/1803802198348103781">(1) Simo Ryu en X: "Lavenderflow-pretrained-256x256-6.8B Hybrid MMDiT just reached 0.597 on GenEval! ü•≥ It took me and @isidentical less than 7 weeks of part-time effort + 4k h100 hours to get to SDXL-level (and this is just pretrained model) Does two of us worth 1B valuation? https://t.co/mD3cSpOyvM" / X</A>
							<DT><A HREF="https://github.com/cloneofsimo/efae">cloneofsimo/efae</A>
							<DT><A HREF="https://gligen.github.io/">GLIGEN:Open-Set Grounded Text-to-Image Generation.</A>
							<DT><A HREF="https://arxiv.org/html/2311.12342v2">LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis</A>
							<DT><A HREF="https://github.com/lllyasviel/Omost">lllyasviel/Omost: Your image is almost there!</A>
							<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-inference-optimization</H3>
						<DL><p>
							<DT><A HREF="https://github.com/vdesai2014/inference-optimization-blog-post">vdesai2014/inference-optimization-blog-post</A>
							<DT><A HREF="https://www.vrushankdes.ai/diffusion-inference-optimization">Diffusion Inference Optimization</A>
							<DT><A HREF="https://github.com/PipeFusion/PipeFusion">PipeFusion/PipeFusion: A Suite of Parallel Approaches for Inference of Diffusion Transformer Models on GPU Clusters</A>
							<DT><A HREF="https://github.com/PipeFusion/PatchVAE">PipeFusion/PatchVAE: A patch parallelism VAE implement for high resolution generation</A>
							<DT><A HREF="https://github.com/mit-han-lab/distrifuser">mit-han-lab/distrifuser: [CVPR 2024 Highlight] DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</A>
						</DL><p>
						<DT><H3 FOLDED>image-captioning</H3>
						<DL><p>
							<DT><A HREF="https://huggingface.co/datasets/google/imageinwords">google/imageinwords ¬∑ Datasets at Hugging Face</A>
							<DT><A HREF="https://www.microsoft.com/en-us/research/publication/florence-2-advancing-a-unified-representation-for-a-variety-of-vision-tasks/">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks - Microsoft Research</A>
							<DT><A HREF="https://x.com/gytdau/status/1804266728781943003">(1) Gytis Daujotas en X: "Excited to release a research preview of Feature Lab, a playground where you can sculpt images by editing their features. It uses a SAE to extract the sparse features from an image embeddings model, and allows us to finely sculpt and tweak images. https://t.co/0Nv6f7dX2P" / X</A>
						</DL><p>
						<DT><H3 FOLDED>imgdataset_process</H3>
						<DL><p>
							<DT><A HREF="https://github.com/cloneofsimo/imgdataset_process/tree/main">cloneofsimo/imgdataset_process</A>
							<DT><A HREF="https://stackoverflow.com/questions/35392594/getting-facebook-original-image-url">python - Getting Facebook original image url - Stack Overflow</A>
							<DT><A HREF="https://meyerweb.com/eric/tools/dencoder/">URL Decoder/Encoder</A>
							<DT><A HREF="https://gist.github.com/cloneofsimo/d31eee8a5352655cb45869694adf0880">MDS-Multiprocessed-datamerging to NFS, because writing is async this is faster</A>
							<DT><A HREF="https://github.com/allenai/cached_path">allenai/cached_path: A file utility for accessing both local and remote files through a unified interface.</A>
						</DL><p>
						<DT><H3 FOLDED>VAE</H3>
						<DL><p>
							<DT><A HREF="https://github.com/PipeFusion/PatchVAE">PipeFusion/PatchVAE: A patch parallelism VAE implement for high resolution generation</A>
						</DL><p>
						<DT><H3 FOLDED>Rectified Flow</H3>
						<DL><p>
							<DT><A HREF="https://www.cs.utexas.edu/~lqiang/rectflow/html/intro.html">Rectified Flow ‚Äî Rectified Flow</A>
							<DT><A HREF="https://github.com/cloneofsimo/rectified-flow">cloneofsimo/rectified-flow</A>
						</DL><p>
						<DT><H3 FOLDED>Flow matching</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=5ZSwYogAxYg">Flow Matching: Simplifying and Generalizing Diffusion Models | Yaron Lipman - YouTube</A>
							<DT><A HREF="https://x.com/i/bookmarks?post_id=1803004045084197186">code snipet: conditional_flow_matching_loss</A>
							<DT><A HREF="https://bm371613.github.io/conditional-flow-matching/">conditional-flow-matching</A>
							<DT><A HREF="https://arxiv.org/abs/2210.02747">[2210.02747] Flow Matching for Generative Modeling</A>
						</DL><p>
						<DT><A HREF="https://kexue.fm/archives/10047">A brief talk on generative diffusion model (Part 22): Signal-to-noise ratio</A>
						<DT><A HREF="https://github.com/tmabraham/diffusion_reading_group">tmabraham/diffusion_reading_group: Diffusion Reading Group at EleutherAI</A>
						<DT><A HREF="https://sander.ai/2024/06/14/noise-schedules.html">Noise schedules considered harmful ‚Äì Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2023/01/09/diffusion-language.html">Diffusion language models ‚Äì Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2023/08/28/geometry.html">The geometry of diffusion guidance ‚Äì Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2023/07/20/perspectives.html">Perspectives on diffusion ‚Äì Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2022/05/26/guidance.html">Guidance: a cheat code for diffusion models ‚Äì Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2022/01/31/diffusion.html">Diffusion models are autoencoders ‚Äì Sander Dieleman</A>
						<DT><A HREF="https://imagen.research.google/">Imagen: Text-to-Image Diffusion Models</A>
						<DT><A HREF="https://arxiv.org/pdf/2205.09991.pdf">Planning with Diffusion for Flexible Behavior Synthesis</A>
						<DT><A HREF="https://arxiv.org/abs/2206.01714">Compositional Visual Generation with Composable Diffusion Models</A>
						<DT><A HREF="https://twitter.com/finbarrtimbers/status/1643283063017971713">diffusion-based models into 11 modular parts</A>
						<DT><A HREF="https://arxiv.org/abs/2206.00364">[2206.00364] Elucidating the Design Space of Diffusion-Based Generative Models</A>
						<DT><A HREF="https://github.com/NVIDIA/TensorRT/blob/3aaa97b91ee1dd61ea46f78683d9a3438f26192e/demo/experimental/HuggingFace-Diffusers/TensorRT-diffusers-txt2img.ipynb">TensorRT/demo/experimental/HuggingFace-Diffusers/TensorRT-diffusers-txt2img.ipynb</A>
						<DT><A HREF="https://twitter.com/_akhaliq/status/1742255547741544602">DIffusion Model with Perceptual Loss (ByteDance)</A>
						<DT><A HREF="https://huggingface.co/papers/2312.02696">Paper page - Analyzing and Improving the Training Dynamics of Diffusion Models</A>
						<DT><A HREF="https://www.youtube.com/watch?v=HoKDTa5jHvg">Diffusion Models | Paper Explanation | Math Explained - YouTube</A>
						<DT><A HREF="https://www.youtube.com/watch?v=ogJsCPqgFMk">Efficient Text-to-Image Training (16x cheaper than Stable Diffusion) | Paper Explained - YouTube</A>
						<DT><A HREF="https://github.com/neelnanda-io/Stable-Diffusion-Interp/blob/main/w3d5_part2_stablediff_solution.py">Stable-Diffusion-Interp/w3d5_part2_stablediff_solution.py at main ¬∑ neelnanda-io/Stable-Diffusion-Interp</A>
						<DT><A HREF="https://github.com/neelnanda-io/Stable-Diffusion-Interp/tree/main">neelnanda-io/Stable-Diffusion-Interp</A>
						<DT><A HREF="https://www.slideshare.net/slideshow/embed_code/key/BoSrT1r6h4kDTJ">ex-OpenAI slides</A>
						<DT><A HREF="https://twitter.com/PreetumNakkiran/status/1767268109784940867">Projection onto manifold perspective</A>
						<DT><A HREF="https://chenyang.co/diffusion.html">Chenyyang Yuan: Diffusion models from scratch, from a new theoretical perspective</A>
						<DT><A HREF="https://github.com/yuanchenyang/smalldiffusion">yuanchenyang/smalldiffusion: Simple and readable code for training and sampling from diffusion models</A>
						<DT><A HREF="https://sander.ai/2024/02/28/paradox.html">The paradox of diffusion distillation ‚Äì Sander Dieleman</A>
						<DT><A HREF="https://arxiv.org/abs/2403.18103">[2403.18103] Tutorial on Diffusion Models for Imaging and Vision</A>
						<DT><A HREF="https://github.com/neelnanda-io/Stable-Diffusion-Interp/blob/main/w3d5_part2_stablediff_solution.py">Stable-Diffusion-Interp/w3d5_part2_stablediff_solution.py</A>
						<DT><A HREF="https://twitter.com/rm_rafailov/status/1781145364810350689">hybrid auto-regressive + diffusion video generation models</A>
						<DT><A HREF="https://twitter.com/BenTheEgg/status/1783972218772373708">(1) Benjamin Lefaudeux en X: "And.. we're back in business ! Possibly placebo, but I can feel the heat just looking at the screen https://t.co/s6bO10acjA" / X</A>
						<DT><A HREF="https://github.com/bytedance">Bytedance Inc.</A>
						<DT><A HREF="https://github.com/cloneofsimo/imagenet.int8">cloneofsimo/imagenet.int8</A>
						<DT><A HREF="https://carpedm30.notion.site/SHIFT-UP-AI-Labs-2cc71f48eb1140d09a439ab0b10bdb7b?p=642900de802c444caf4e3b51d34079aa&pm=s">SHIFT UP AI Labs</A>
						<DT><A HREF="https://www.youtube.com/watch?v=zc5NTeJbk-k&t=124s">Why Does Diffusion Work Better than Auto-Regression? - YouTube</A>
						<DT><A HREF="https://github.com/snap-research/BitsFusion">snap-research/BitsFusion</A>
						<DT><A HREF="https://github.com/snap-research/HyperHuman">snap-research/HyperHuman: [ICLR 2024] Github Repo for "HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion"</A>
						<DT><A HREF="https://www.microsoft.com/en-us/research/publication/florence-2-advancing-a-unified-representation-for-a-variety-of-vision-tasks/">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks - Microsoft Research</A>
					</DL><p>
					<DT><H3 FOLDED>Vision</H3>
					<DL><p>
						<DT><H3 FOLDED>clip-based-models</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2103.00020">CLIP: Learning Transferable Visual Models From Natural Language Supervision</A>
							<DT><A HREF="https://arxiv.org/abs/2310.09199">[2310.09199] PaLI-3 Vision Language Models: Smaller, Faster, Stronger</A>
							<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/imagen">JAX-Toolbox/rosetta/rosetta/projects/imagen</A>
							<DT><A HREF="https://x.com/BenTheEgg/status/1788596702380769616">MobileCLIP (Apple)</A>
							<DT><A HREF="https://x.com/i/bookmarks?post_id=1805149804059607366">SigLIP SO400</A>
							<DT><A HREF="https://x.com/olivierhenaff/status/1805995802352910557">(1) Olivier H√©naff en X: "Thrilled to announce our latest work on active data curation: joint example selection (JEST) drastically accelerates large-scale multimodal pretraining, surpassing previous SoTA (SigLIP) with 10x fewer iterations and FLOPS: https://t.co/NFPSrPIs4C https://t.co/Ggkg2Pn5qS" / X</A>
						</DL><p>
						<DT><H3 FOLDED>ViT</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=ma3NYVo8Im0">All Things ViTs || CVPR 2023 Tutorial || Hila Chefer and Sayak Paul - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=vsqKGZT8Qn8">Vision Transformer Basics - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=j3VNqtJUoz0">Vision Transformer Quick Guide</A>
							<DT><A HREF="https://github.com/mit-han-lab/efficientvit">mit-han-lab/efficientvit: EfficientViT is a new family of vision models for efficient high-resolution vision.</A>
						</DL><p>
						<DT><H3 FOLDED>GAN</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/anand_bhattad/status/1664798414318518274">(1) Anand Bhattad en X: "1/ SUPER excited to share our latest paper! Would you believe it if I told you that a StyleGAN, trained only to generate images, can also generate normals, depth, albedo, shading &amp;amp; segmentation? We show that it can! ü§Ø w/ @danielbmckee , @HoiemDerek &amp;amp; David Forsyth @IllinoisCS https://t.co/ae6pNn9r03" / X</A>
							<DT><A HREF="https://arxiv.org/abs/2306.00987">[2306.00987] StyleGAN knows Normal, Depth, Albedo, and More</A>
							<DT><A HREF="https://blog.fal.ai/introducing-aurasr-an-open-reproduction-of-the-gigagan-upscaler-2/">Introducing AuraSR - An open reproduction of the GigaGAN Upscaler</A>
						</DL><p>
						<DT><H3 FOLDED>image-tokens</H3>
						<DL><p>
							<DT><A HREF="https://github.com/lucidrains/titok-pytorch">lucidrains/titok-pytorch: Implementation of TiTok, proposed by Bytedance in "An Image is Worth 32 Tokens for Reconstruction and Generation"</A>
						</DL><p>
						<DT><H3 FOLDED>Florence-2</H3>
						<DL><p>
							<DT><A HREF="https://huggingface.co/microsoft/Florence-2-large-ft">microsoft/Florence-2-large-ft ¬∑ Hugging Face</A>
							<DT><A HREF="https://arxiv.org/abs/2311.06242">[2311.06242] Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</A>
						</DL><p>
						<DT><A HREF="https://arxiv.org/abs/2310.05737">[2310.05737] Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation</A>
						<DT><A HREF="https://segment-anything.com/demo#">Segment Anything | Meta AI</A>
						<DT><A HREF="https://towardsdatascience.com/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b">META‚Äôs Hiera: reduce complexity to increase accuracy</A>
						<DT><A HREF="https://twitter.com/skalskip92/status/1728867156681732229">GPT-4V &amp; SLAM</A>
						<DT><A HREF="https://arxiv.org/html/2404.00498v1">94% on CIFAR-10 in 3.29 Seconds on a Single GPU</A>
						<DT><A HREF="https://arxiv.org/abs/2309.16588">[2309.16588] Vision Transformers Need Registers</A>
						<DT><A HREF="https://x.com/borisdayma/status/1806092526572503533">attention registers</A>
					</DL><p>
					<DT><H3 FOLDED>Large Transformer Model Inference Optimization</H3>
					<DL><p>
						<DT><H3 FOLDED>model compression</H3>
						<DL><p>
							<DT><H3 FOLDED>model distillation</H3>
							<DL><p>
								<DT><A HREF="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</A>
								<DT><A HREF="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#gemini-model-updates">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</A>
								<DT><A HREF="https://www.youtube.com/watch?v=FKsARHV3ZTI">Model Distilation: Student Models</A>
								<DT><A HREF="https://github.com/HuangOwen/Awesome-LLM-Compression">HuangOwen/Awesome-LLM-Compression: Awesome LLM compression research papers and tools.</A>
								<DT><A HREF="https://github.com/microsoft/TransformerCompression">microsoft/TransformerCompression: For releasing code related to compression methods for transformers, accompanying our publications</A>
								<DT><A HREF="https://arxiv.org/abs/2106.05237">[2106.05237] Knowledge distillation: A good teacher is patient and consistent</A>
								<DT><A HREF="https://x.com/giffmana/status/1402836863954599936">(1) Lucas Beyer (bl16) en X: "So you think you know distillation; it's easy, right?</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>throughput-bandwidth-latency</H3>
						<DL><p>
							<DT><H3 FOLDED>Continuity equation</H3>
							<DL><p>
								<DT><A HREF="https://www.fisimat.com.mx/ecuacion-de-continuidad/">Ecuaci√≥n de Continuidad - Ejercicios Resueltos - Fisimat</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=jlMAX2Oaht0">Text-generation-inference (TGI) deployment optimization and benchmarking - YouTube</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Continuity_equation">Continuity equation - Wikipedia</A>
							<DT><A HREF="https://cursor.sh/blog/instant-apply?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-9199">Cursor: Near-Instant Full-File Edits (OpenAI fund)</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-quantization</H3>
						<DL><p>
							<DT><H3 FOLDED>transformer-inference-quantization-people</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/Tim_Dettmers">Tim Dettmers (@Tim_Dettmers) / Twitter</A>
							</DL><p>
							<DT><H3 FOLDED>Post-training quantization (PTQ)</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2106.09685">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</A>
								<DT><A HREF="https://github.com/TimDettmers/bitsandbytes">TimDettmers/bitsandbytes: 8-bit CUDA functions for PyTorch</A>
								<DT><A HREF="https://arxiv.org/pdf/2004.09602.pdf">Integer Quantization For Deep Learning Inference: Principles and Empirical Evaluation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=AlASZb93rrc">Lecture 05 - Quantization (Part I) | MIT 6.S965 - YouTube</A>
								<DT><A HREF="https://twitter.com/atiorh/status/1684659953716584452">Atila @ ICML en Twitter: "Applying 1, 2, 4, 6 and 8-bit quantization via palettization yields much better results, e.g. We can use 1, 2, 4, 6 or 8-bits palettes to achieve the same compression rate as linear 8-bit quant (50%) but maintain correctness as high as 80dB (2dB loss vs 17dB for linear 8-bit) https://t.co/lA6ldVXYSW" / X</A>
								<DT><A HREF="https://twitter.com/amanrsanger/status/1690828443699847168">Quantitative quantization analysis at scale (bfp16 vs int)</A>
								<DT><A HREF="https://arxiv.org/pdf/2206.01861.pdf">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</A>
							</DL><p>
							<DT><H3 FOLDED>transformer-inference-quantization-1-BitNet</H3>
							<DL><p>
								<DT><A HREF="https://mobiusml.github.io/1bit_blog/">Towards 1-bit Machine Learning Models</A>
								<DT><A HREF="https://github.com/rafacelente/bllama?tab=readme-ov-file">rafacelente/bllama: 1.58-bit LLaMa model</A>
								<DT><A HREF="https://github.com/microsoft/BitBLAS">microsoft/BitBLAS: BitBLAS is a library to support mixed-precision matrix multiplications, especially for quantized LLM deployment.</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2212.09720">[2212.09720] The case for 4-bit precision: k-bit Inference Scaling Laws</A>
							<DT><A HREF="https://arxiv.org/abs/2305.19268?utm_source=Cohere_For_AI&utm_medium=LinkedIn">[2305.19268] Intriguing Properties of Quantization at Scale</A>
							<DT><A HREF="https://twitter.com/awnihannun/status/1773185727997825406">MLX low-RAM lazy quantiztion procedure</A>
							<DT><A HREF="https://github.com/Vahe1994/AQLM">Vahe1994/AQLM: Official Pytorch repository for Extreme Compression of Large Language Models via Additive Quantization</A>
							<DT><A HREF="https://arxiv.org/pdf/2401.06118.pdf">Extreme Compression of Large Language Models via Additive Quantization</A>
							<DT><A HREF="https://github.com/snap-research/BitsFusion">snap-research/BitsFusion</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-sparsity</H3>
						<DL><p>
							<DT><H3 FOLDED>transformer-inference-sparsity-deepspeed</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2302.02596">[2302.02596] Ten Lessons We Have Learned in the New "Sparseland": A Short Handbook for Sparse Neural Network Researchers</A>
							<DT><A HREF="https://arxiv.org/abs/1902.09574">[1902.09574] The State of Sparsity in Deep Neural Networks</A>
							<DT><A HREF="https://twitter.com/lzcemma15/status/1683916730052268032">Contextual Sparsity for Efficient LLMs at Inference Time</A>
							<DT><A HREF="https://www.youtube.com/watch?v=0g351WQTaf8">[REFAI Seminar 04/20/23] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</A>
							<DT><A HREF="https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/">What Is Sparsity in AI Inference and Machine Learning?</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mGDnOLcfE8g">Lecture 11: Sparsity - YouTube</A>
							<DT><A HREF="https://github.com/neuralmagic/sparseml">neuralmagic/sparseml: Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models</A>
							<DT><A HREF="https://github.com/neuralmagic/sparsezoo">neuralmagic/sparsezoo: Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes</A>
							<DT><A HREF="https://github.com/google-research/jaxpruner">google-research/jaxpruner</A>
							<DT><A HREF="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/">Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog</A>
							<DT><A HREF="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/">Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT</A>
							<DT><A HREF="https://arxiv.org/abs/2302.02596">[2302.02596] Ten Lessons We Have Learned in the New "Sparseland"</A>
							<DT><A HREF="https://twitter.com/_akhaliq/status/1734046582805344492">SparQ Attention: Bandwidth-Efficient LLM Inference</A>
							<DT><A HREF="https://arxiv.org/abs/2405.15743">[2405.15743] Sparse maximal update parameterization: A holistic approach to sparse training dynamics</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-pruning</H3>
						<DL><p>
							<DT><A HREF="https://openreview.net/pdf?id=0GRBKLBjJE">A Fast Post-Training Pruning Framework for Transformers</A>
							<DT><A HREF="https://www.youtube.com/watch?v=sZzc6tAtTrM">Lecture 03 - Pruning and Sparsity (Part I) | MIT 6.S965 - YouTube</A>
							<DT><A HREF="https://github.com/google-research/jaxpruner">google-research/jaxpruner</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-architectural-optimization</H3>
						<DL><p>
							<DT><H3 FOLDED>Attention</H3>
							<DL><p>
								<DT><H3 FOLDED>MHA (Multi-Head Attention</H3>
								<DL><p>
									<DT><A HREF="https://kexue.fm/archives/10091">ÁºìÂ≠ò‰∏éÊïàÊûúÁöÑÊûÅÈôêÊãâÊâØÔºö‰ªéMHA„ÄÅMQA„ÄÅGQAÂà∞MLA - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces</A>
								</DL><p>
								<DT><H3 FOLDED>MQA (Multi-Query Attention)</H3>
								<DL><p>
									<DT><A HREF="https://kexue.fm/archives/10091">ÁºìÂ≠ò‰∏éÊïàÊûúÁöÑÊûÅÈôêÊãâÊâØÔºö‰ªéMHA„ÄÅMQA„ÄÅGQAÂà∞MLA - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces</A>
								</DL><p>
								<DT><H3 FOLDED>GQA (Grouped-Query Attention)</H3>
								<DL><p>
									<DT><A HREF="https://kexue.fm/archives/10091">ÁºìÂ≠ò‰∏éÊïàÊûúÁöÑÊûÅÈôêÊãâÊâØÔºö‰ªéMHA„ÄÅMQA„ÄÅGQAÂà∞MLA - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces</A>
								</DL><p>
								<DT><H3 FOLDED>MLA (Multi-head Latent Attention)</H3>
								<DL><p>
									<DT><A HREF="https://kexue.fm/archives/10091">ÁºìÂ≠ò‰∏éÊïàÊûúÁöÑÊûÅÈôêÊãâÊâØÔºö‰ªéMHA„ÄÅMQA„ÄÅGQAÂà∞MLA - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces</A>
									<DT><A HREF="https://github.com/google/maxtext/blob/f7ee8c636fd500995e76c227b351d48680ab7890/MaxText/layers/attentions.py#L435">maxtext/MaxText/layers/attentions.py at f7ee8c636fd500995e76c227b351d48680ab7890 ¬∑ google/maxtext</A>
								</DL><p>
								<DT><H3 FOLDED>Ring attention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/mgmalek/ring-attention">mgmalek/ring-attention</A>
								</DL><p>
								<DT><H3 FOLDED>Linear Attention</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/1812.01243">[1812.01243] Efficient Attention: Attention with Linear Complexities</A>
									<DT><A HREF="https://arxiv.org/abs/2006.04768">[2006.04768] Linformer: Self-Attention with Linear Complexity</A>
									<DT><A HREF="https://arxiv.org/abs/2001.04451">[2001.04451] Reformer: The Efficient Transformer</A>
									<DT><A HREF="https://openai.com/research/requests-for-research-2">Transformers with Linear Attention (OpenAI)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=dVH1dRoMPBc">Do we need Attention? A Mamba Primer - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>Adaptive Attention</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2207.07061.pdf">Confident Adaptive Language Modeling (CALM)</A>
								</DL><p>
								<DT><H3 FOLDED>Attention Free</H3>
								<DL><p>
									<DT><H3 FOLDED>Mamba</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=dVH1dRoMPBc&t=10s">Do we need Attention? A Mamba Primer - YouTube</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2212.10544">[2212.10544] Pretraining Without Attention</A>
									<DT><A HREF="https://arxiv.org/abs/2105.14103">[2105.14103] An Attention Free Transformer</A>
									<DT><A HREF="https://github.com/rish-16/aft-pytorch">rish-16/aft-pytorch: Unofficial PyTorch implementation of Attention Free Transformer (AFT) layers by Apple Inc.</A>
								</DL><p>
								<DT><H3 FOLDED>RVKV</H3>
								<DL><p>
									<DT><A HREF="https://github.com/BlinkDL/RWKV-LM">BlinkDL/RWKV-LM: RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, "infinite" ctx_len, and free sentence embedding.</A>
								</DL><p>
								<DT><H3 FOLDED>PagedAttention</H3>
								<DL><p>
									<DT><A HREF="https://vllm.ai/">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</A>
								</DL><p>
								<DT><A HREF="https://kexue.fm/archives/10091">ÁºìÂ≠ò‰∏éÊïàÊûúÁöÑÊûÅÈôêÊãâÊâØÔºö‰ªéMHA„ÄÅMQA„ÄÅGQAÂà∞MLA - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces</A>
								<DT><A HREF="https://twitter.com/charles_irl/status/1724110196744835193">PagedAttention, Virtual Context, Speculative Decoding, Register Tokens</A>
								<DT><A HREF="https://twitter.com/giffmana/status/1659512770100973572/photo/1">Self-Attention Does Not Need O(n^2) Memory</A>
								<DT><A HREF="https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html">Rethinking Attention with Performers ‚Äì Google Research Blog</A>
								<DT><A HREF="https://twitter.com/ZihangDai/status/1281349893873897478/photo/1">Funnel-Transformer</A>
								<DT><A HREF="https://arxiv.org/abs/2404.07143">[2404.07143] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-attention.md">TensorRT-LLM/docs/source/advanced/gpt-attention.md</A>
								<DT><A HREF="https://www.youtube.com/watch?v=3a0_hAiFKag">TransformerFAM: Feedback attention is working memory - YouTube</A>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1764717117881094582">Based attention</A>
								<DT><A HREF="https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py">jax/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py at main ¬∑ google/jax</A>
								<DT><A HREF="https://github.com/feifeibear/long-context-attention">feifeibear/long-context-attention: Sequence Parallel Attention for Long Context LLM Model Training and Inference</A>
							</DL><p>
							<DT><H3 FOLDED>Adaptive Computation</H3>
							<DL><p>
								<DT><H3 FOLDED>adaptive-computation</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2207.07061.pdf">Confident Adaptive Language Modeling (CALM)</A>
									<DT><A HREF="https://blog.research.google/2022/12/accelerating-text-generation-with.html?m=1">Accelerating text generation with Confident Adaptive Language Modeling (CALM) ‚Äì Google Research Blog</A>
								</DL><p>
								<DT><A HREF="https://github.com/hao-ai-lab/LookaheadDecoding">hao-ai-lab/LookaheadDecoding</A>
								<DT><A HREF="https://blog.research.google/2024/01/introducing-aspire-for-selective.html">Introducing ASPIRE for selective prediction in LLMs ‚Äì Google Research Blog</A>
								<DT><A HREF="https://arxiv.org/abs/2404.16710">[2404.16710] LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding</A>
							</DL><p>
							<DT><H3 FOLDED>MoE</H3>
							<DL><p>
								<DT><A HREF="https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/">Demystifying AI Inference Deployments for Trillion Parameter Large Language Models | NVIDIA Technical Blog</A>
								<DT><A HREF="https://pytorch.org/blog/training-moes/?utm_content=298456196&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Training MoEs at Scale with PyTorch | PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>Elastic Inference</H3>
							<DL><p>
								<DT><A HREF="https://github.com/UKPLab/sentence-transformers/releases/tag/v2.7.0">Release v2.7.0 - CachedGISTEmbedLoss, easy Matryoshka inference &amp; evaluation, CrossEncoder, Intel Gaudi2 ¬∑ UKPLab/sentence-transformers</A>
								<DT><A HREF="https://arxiv.org/abs/2310.07707">[2310.07707] MatFormer: Nested Transformer for Elastic Inference</A>
								<DT><A HREF="https://twitter.com/adityakusupati/status/1714001115732427176">(1) Aditya Kusupati en X: "Announcing MatFormer - a nestedü™Ü(Matryoshka) Transformer that offers elasticity across deployment constraints. MatFormer is an architecture that lets us use 100s of accurate smaller models that we never actually trained for! https://t.co/wzR7To7HZu 1/9 https://t.co/mBF0ZOIjlz" / X</A>
								<DT><A HREF="https://github.com/RAIVNLab/MatFormer-OLMo">MatFormer: Nested Transformer for Elastic Inference</A>
							</DL><p>
							<DT><H3 FOLDED>Block Transformer</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2406.02657">Block Transformer: Global-to-Local Language Modeling for Fast Inference</A>
							</DL><p>
							<DT><A HREF="https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c">kernl</A>
							<DT><A HREF="https://research.google/blog/alternating-updates-for-efficient-transformers/">Alternating updates for efficient transformers</A>
							<DT><A HREF="https://arxiv.org/abs/2301.13310">[2301.13310] Alternating Updates for Efficient Transformers</A>
							<DT><A HREF="https://kexue.fm/archives/9948">(main: whole series) Transformer Upgrade Road: 16. "Review" Length Extrapolation Technology</A>
							<DT><A HREF="https://kexue.fm/archives/9844">VQ the key, and the complexity of Transformer becomes linear</A>
							<DT><A HREF="https://www.youtube.com/watch?v=PnWOeIgl3GA">Language Modeling with Reduced Densities</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-model-parallelism</H3>
						<DL><p>
							<DT><H3 FOLDED>deepspeed-inference</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2207.00032.pdf">DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeedExamples/tree/master/inference/mii">DeepSpeedExamples/inference/mii</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-server</H3>
						<DL><p>
							<DT><H3 FOLDED>transformer-inference-prefill</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/siyan_zhao">Prepacking</A>
								<DT><A HREF="https://arxiv.org/abs/2404.09529">[2404.09529] Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models</A>
								<DT><A HREF="https://github.com/siyan-zhao/prepacking">siyan-zhao/prepacking: The source code of our work "Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models"</A>
								<DT><A HREF="https://twitter.com/siyan_zhao/status/1780288750624612850">(1) Siyan Zhao en X: "üö®LLM RESEARCHERSüö®Want a free boost in speed and memory efficiency for your HuggingFaceü§óLLM with ZERO degradation in generation quality? Introducing Prepacking, a simple method to obtain up to 6x speedup and 16x memory efficiency gains in prefilling prompts of varying lengths.... https://t.co/O8XsjhOLGZ" / X</A>
							</DL><p>
							<DT><H3 FOLDED>transformer-inference-server-decode</H3>
							<DL><p>
								<DT><H3 FOLDED>speculative decoding</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2302.01318.pdf">Accelerating Large Language Model Decoding with Speculative Sampling</A>
									<DT><A HREF="https://arxiv.org/abs/2211.17192">[CRITICAL] Fast Inference from Transformers via Speculative Decoding</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM">NVIDIA/TensorRT-LLM: TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/docs/source/conceptual/speculation.md">text-generation-inference/docs/source/conceptual/speculation.md at main ¬∑ huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/speculative_decoding.md">TensorRT-LLM/docs/source/speculative_decoding.md</A>
									<DT><A HREF="https://github.com/lucidrains/speculative-decoding">lucidrains/speculative-decoding: Explorations into some recent techniques surrounding speculative decoding</A>
									<DT><A HREF="https://github.com/hao-ai-lab/LookaheadDecoding">hao-ai-lab/LookaheadDecoding</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>transformer-inference-server-kv-cache</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/XueFz/status/1768690833988174040">(1) Fuzhao Xue en X: "KV cache may be the most redundant memory usage, but it‚Äôs non-trivial to compress it in a lossless way. My Takeaway: 1) Adaptively append/merge the current token to KV cache. This is a simple but smart solution to achieve better trade-off between RNN and Transformer, 2) Designed..." / X</A>
								<DT><A HREF="https://github.com/SqueezeAILab/KVQuant">SqueezeAILab/KVQuant: KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</A>
								<DT><A HREF="https://www.youtube.com/watch?v=r_UBBfTPcF0">Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>transformer-inference-server-continous batching</H3>
							<DL><p>
								<DT><A HREF="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/">Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</A>
							</DL><p>
							<DT><H3 FOLDED>FlexGen</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2303.06865">[2303.06865] High-throughput Generative Inference of Large Language Models with a Single GPU</A>
							</DL><p>
							<DT><H3 FOLDED>MuxServe</H3>
							<DL><p>
								<DT><A HREF="https://x.com/haoailab/status/1805307696297689119">(1) Hao AI Lab en X: "Multiple LLM serving has emerged as a crucial and costly demand. Want to co-serve multiple LLMs with better utilization? Introducing MuxServe - flexible spatial-temporal multiplexing - up to 1.8x higher throughput Blog: https://t.co/Pep94vUFTw Paper: https://t.co/X1Jhov3QOY https://t.co/mXrHMSLPS1" / X</A>
								<DT><A HREF="https://hao-ai-lab.github.io/blogs/muxserve/">MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving | Hao AI Lab @ UCSD</A>
								<DT><A HREF="https://arxiv.org/abs/2404.02015">[2404.02015] MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving</A>
								<DT><A HREF="https://github.com/hao-ai-lab/MuxServe">hao-ai-lab/MuxServe</A>
								<DT><A HREF="https://github.com/EfficientLLMSys/MuxServe-vLLM">EfficientLLMSys/MuxServe-vLLM</A>
							</DL><p>
							<DT><A HREF="https://proceedings.mlsys.org/paper_files/paper/2023/hash/523f87e9d08e6071a3bbd150e6da40fb-Abstract-mlsys2023.html">Efficiently Scaling Transformer Inference</A>
							<DT><A HREF="https://arxiv.org/pdf/2204.02311.pdf">PaLM: Scaling Language Modeling with Pathways</A>
							<DT><A HREF="https://twitter.com/aleks_madry/status/1642972175572545545">AI deployment as suppling chain problem</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-flops</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/karpathy/status/1781047292486914189">Karpathy: Llama 3 model card (flops model training taxonomy)</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-benchmarking</H3>
						<DL><p>
							<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance (2024)</A>
							<DT><A HREF="https://arxiv.org/pdf/2011.02327.pdf">INFERBENCH: UNDERSTANDING DEEP LEARNING INFERENCE SERVING WITH AN AUTOMATIC BENCHMARKING SYSTEM</A>
							<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic | kipply's blog</A>
							<DT><A HREF="https://bytemlperf.ai/guide/introduction.html">Introduction - ByteMLPerf</A>
							<DT><A HREF="https://www.modular.com/blog/how-to-be-confident-in-your-performance-benchmarking">Modular: How to Be Confident in Your Performance Benchmarking</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-manifesto</H3>
						<DL><p>
							<DT><A HREF="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#methods-overview">Large Transformer Model Inference Optimization | Lil'Log</A>
							<DT><A HREF="https://arxiv.org/abs/2211.05102">[2211.05102] Efficiently Scaling Transformer Inference</A>
							<DT><A HREF="https://www.overleaf.com/project/64399db50ab28ba5e3335352">Empirical Analysis of Compute-Optimal Large Language Model Training - Online LaTeX Editor Overleaf</A>
							<DT><A HREF="https://github.com/DataCrunch-io/inference_playbook/blob/main/manifesto.MD">inference_playbook/manifesto.MD at main ¬∑ DataCrunch-io/inference_playbook ¬∑ GitHub</A>
							<DT><A HREF="https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c">Hugging Face Transformer Inference Under 1 Millisecond Latency | by Micha√´l Benesty | Towards Data Science</A>
							<DT><A HREF="https://gist.github.com/lattner/31ed37682ef1576b16bca1432ea9f782#overall-vision">Swift Concurrency Manifesto</A>
							<DT><A HREF="https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&hw=ph&test=db">Round 20 results - TechEmpower Framework Benchmarks</A>
							<DT><A HREF="https://els-rd.github.io/transformer-deploy/compare/">Which tool to choose for your inference? - transformer-deploy by Lefebvre Dalloz</A>
							<DT><A HREF="https://github.com/ELS-RD/kernl">ELS-RD/kernl: Kernl lets you run PyTorch transformer models several times faster on GPU with a single line of code, and is designed to be easily hackable.</A>
							<DT><A HREF="https://github.com/ELS-RD/transformer-deploy">ELS-RD/transformer-deploy: Efficient, scalable and enterprise-grade CPU/GPU inference server for ü§ó Hugging Face transformer models üöÄ</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-big-picture</H3>
						<DL><p>
							<DT><A HREF="https://karpathy.ai/stateofgpt.pdf">https://karpathy.ai/stateofgpt.pdf</A>
							<DT><A HREF="https://www.youtube.com/watch?v=bZQun8Y4L2A">State of GPT | BRK216HFS - YouTube</A>
							<DT><A HREF="https://excalidraw.com/#room=93d0feb81016e7e5d6c7,TYDGkAGBNB0A8Fa-5-EhDg">Excalidraw</A>
							<DT><A HREF="https://www.youtube.com/@rutgersefficientaiseminar9909">Rutgers Efficient AI Seminar - YouTube</A>
							<DT><A HREF="https://openreview.net/forum?id=wIPIhHd00i">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time | OpenReview</A>
							<DT><A HREF="https://www.youtube.com/@neubig/videos">Graham Neubig  (CMU)</A>
						</DL><p>
						<DT><H3 FOLDED>QPS</H3>
						<DL><p>
							<DT><A HREF="https://x.com/NoamShazeer/status/1803790708358410380">(1) Noam Shazeer en X: "Character AI is serving 20,000 QPS. Here are the technologies we use to serve hyper-efficiently. [https://t.co/R14Jt9Z5yo ]" / X</A>
							<DT><A HREF="https://research.character.ai/optimizing-inference/">Optimizing AI Inference at Character.AI</A>
							<DT><A HREF="https://x.com/cis_female/status/1803816791677808964">(1) sophia en X: "&amp;gt;20k QPS MQA everywhere local attention, 1/6 layers use global attn KV cache tied between layers, cached on host memory with "95% cache rate" trained in int8 precision" / X</A>
						</DL><p>
						<DT><A HREF="https://arxiv.org/abs/2211.05102">[2211.05102] Efficiently Scaling Transformer Inference</A>
						<DT><A HREF="https://www.zhihu.com/people/liang-de-peng">GiantPandaCV (chinese high-proffesional discussions)</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance (2024)</A>
						<DT><A HREF="https://horace.io/brrr_intro.html">Making Deep Learning go Brrrr From First Principles</A>
						<DT><A HREF="https://arxiv.org/pdf/2302.14017.pdf">Full Stack Optimization of Transformer Inference: a Survey</A>
						<DT><A HREF="https://arxiv.org/abs/2303.06865">[2303.06865] High-throughput Generative Inference of Large Language Models with a Single GPU</A>
						<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic | kipply's blog</A>
						<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic</A>
						<DT><A HREF="https://arxiv.org/pdf/2211.05102.pdf">Efficiently Scaling Transformer Inference</A>
						<DT><A HREF="https://huggingface.co/papers/2312.11514">Paper page - LLM in a flash: Efficient Large Language Model Inference with Limited Memory</A>
						<DT><A HREF="https://twitter.com/atiorh/status/1737912777153609918">(Apple) Atila en X: "My takeaways from Apple's ‚ÄúLLM in a flash" (1/n)" / X</A>
						<DT><A HREF="https://www.semianalysis.com/p/inference-race-to-the-bottom-make">Inference Race To The Bottom - Make It Up On Volume?</A>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1773268740806705266">optimal batch size (bytes per param, flops, bytes/token, arithmetic intensity )</A>
						<DT><A HREF="https://developer.nvidia.com/blog/accelerating-hpc-applications-with-nsight-compute-roofline-analysis/">Accelerating HPC Applications with NVIDIA Nsight Compute Roofline Analysis | NVIDIA Technical Blog</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance (2024 main)</A>
						<DT><A HREF="https://arxiv.org/abs/2401.08092">[2401.08092] A Survey of Resource-efficient LLM and Multimodal Foundation Models</A>
						<DT><A HREF="https://research.google/blog/alternating-updates-for-efficient-transformers/">Alternating updates for efficient transformers</A>
						<DT><A HREF="https://github.com/UbiquitousLearning/Efficient_Foundation_Model_Survey?tab=readme-ov-file">UbiquitousLearning/Efficient_Foundation_Model_Survey: Survey Paper List - Efficient LLM and Foundation Models</A>
						<DT><A HREF="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/">Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</A>
						<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/memory.md">TensorRT-LLM/docs/source/reference/memory.md</A>
						<DT><A HREF="https://mlcommons.org/2024/03/mlperf-inference-v4/">New MLPerf Inference Benchmark Results Highlight The Rapid Growth of Generative AI Models - MLCommons</A>
						<DT><A HREF="https://arxiv.org/pdf/2405.00208">A PRIMER ON THE INNER WORKINGS OF TRANSFORMER-BASED LANGUAGE MODELS</A>
						<DT><A HREF="https://github.com/cuda-mode/awesomeMLSys">cuda-mode/awesomeMLSys: An ML Systems Onboarding list</A>
						<DT><A HREF="https://github.com/xai-org/grok-1">xai-org/grok-1: Grok open release</A>
						<DT><A HREF="https://github.com/madsys-dev/deepseekv2-profile/blob/main/workspace/blog/optimizing-mla.md">deepseekv2-profile/workspace/blog/optimizing-mla.md at main ¬∑ madsys-dev/deepseekv2-profile</A>
						<DT><A HREF="https://kexue.fm/archives/10091">The ultimate struggle between cache and effects: from MHA, MQA, GQA (by Jianlin SU (the GOD creator of ROPE)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=ptGDaGUXInw">Mark Russinovich | Generative AI in the Cloud: Inside Microsoft AI Innovation - YouTube</A>
						<DT><A HREF="https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/">Demystifying AI Inference Deployments for Trillion Parameter Large Language Models | NVIDIA Technical Blog</A>
						<DT><A HREF="https://github.com/xjdr-alt/mla_blog_translation">DeepSeek-V2 High-performance Inference Optimization Notes: MLA Optimization</A>
						<DT><A HREF="https://github.com/DefTruth/Awesome-LLM-Inference">DefTruth/Awesome-LLM-Inference: üìñA curated list of Awesome LLM Inference Paper with codes, TensorRT-LLM, vLLM, streaming-llm, AWQ, SmoothQuant, WINT8/4, Continuous Batching, FlashAttention, PagedAttention etc.</A>
					</DL><p>
					<DT><H3 FOLDED>Language Models</H3>
					<DL><p>
						<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
						<DT><A HREF="https://arxiv.org/pdf/1910.10683.pdf">(T5): Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</A>
						<DT><A HREF="https://arxiv.org/abs/2001.08361">(Kaplan) Scaling Laws for Neural Language Models</A>
						<DT><A HREF="https://arxiv.org/pdf/2307.09288.pdf">Llama 2: Open Foundation and Fine-Tuned Chat Models</A>
						<DT><A HREF="https://arxiv.org/abs/2312.11805">Gemini: A Family of Highly Capable Multimodal Models</A>
						<DT><A HREF="https://arxiv.org/abs/2310.06825">[2310.06825] Mistral 7B</A>
						<DT><A HREF="https://arxiv.org/abs/2401.04088">[2401.04088] Mixtral of Experts</A>
						<DT><A HREF="https://arxiv.org/abs/2204.02311">[2204.02311] PaLM: Scaling Language Modeling with Pathways</A>
						<DT><A HREF="https://arxiv.org/abs/2205.05131">[Abletative &amp; Pareto-frontier] UL2: Unifying Language Learning Paradigms</A>
						<DT><A HREF="https://openreview.net/pdf?id=gEZrGCozdqR">FLAN: Finetuned Language Models are Zero-Shot Learners</A>
						<DT><A HREF="https://arxiv.org/abs/2205.01068">[2205.01068] OPT: Open Pre-trained Transformer Language Models</A>
						<DT><A HREF="https://arxiv.org/pdf/2203.02155.pdf">(OpenAI, 2022) Training language models to follow instructions</A>
						<DT><A HREF="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space (word2vec)</A>
						<DT><A HREF="https://arxiv.org/abs/2204.07705">Super-NaturalInstructions: Generalization via Instructions (Allen AI)</A>
						<DT><A HREF="https://arxiv.org/pdf/2303.17568.pdf">CodeGeeX</A>
						<DT><A HREF="https://papers.labml.ai/paper/18eb0daa07cb11edb9b9d35608ee6155">Formal Algorithms for Transformers</A>
						<DT><A HREF="https://arxiv.org/abs/2103.00020">CLIP: Learning Transferable Visual Models From Natural Language Supervision</A>
						<DT><A HREF="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf">Supplement GPT-3 few-shot generalizable learners</A>
						<DT><A HREF="https://arxiv.org/abs/2307.09793">[2307.09793] On the Origin of LLMs: An Evolutionary Tree and Graph</A>
						<DT><A HREF="https://arxiv.org/pdf/2307.06435.pdf">2023 Summary</A>
						<DT><A HREF="https://huggingface.co/transformers/v4.8.0/glossary.html">Glossary ‚Äî transformers 4.7.0 documentation</A>
						<DT><H3 FOLDED>language-models-courses</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=orDKvo8h71o">Stanford CS25: V4 I Hyung Won Chung of OpenAI - YouTube</A>
							<DT><A HREF="https://people.cs.umass.edu/~miyyer/cs685/">Advanced NLP - CS 685, Spring 2024, UMass Amherst</A>
							<DT><A HREF="https://people.cs.umass.edu/~miyyer/cs685/schedule.html">Schedule - CS 685, Spring 2022, UMass Amherst</A>
							<DT><A HREF="https://stanford-cs324.github.io/winter2022/">CS324 - Large Language Models</A>
							<DT><A HREF="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/">COS 597G: Understanding Large Language Models</A>
							<DT><A HREF="https://www.youtube.com/watch?v=AKMuA_TVz3A">Ilya Sutskever (OpenAI): An observation on Generalization</A>
							<DT><A HREF="https://phontron.com/class/anlp2024/assets/slides/anlp-15-tourofllms.pdf">CS11-711 Advanced NLP Tour of Modern LLMs</A>
							<DT><A HREF="https://phontron.com/class/anlp2024/lectures/">Lectures | 11-711 ANLP</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-people</H3>
						<DL><p>
							<DT><A HREF="https://www.yitay.net/papers">Papers ‚Äî Yi Tay</A>
							<DT><A HREF="https://scholar.google.com/citations?hl=en&user=wsGvgA8AAAAJ&view_op=list_works&citft=1&email_for_op=antonio.jfdominguez%40gmail.com&gmla=AOV7GLN3W4O6NIDNdCcBkV_iDC21-xJFgE9EtTcF0RmBzdcKrEF46210Wvjs822ZRzG_SBibT3xddH6GJgU3SPO710DKm_SctPk3WkFFHzFwhDdat8yKJ1X7TYY8-WYpbAyAHud1T-e8q4oYIVEKVcszYYDshws0si2Wiyf2DLsvrAwbWUqybXjXFvwMczvMx_ckXdir3sbM9FRUTfFY9wB3nYllfhw3gwJzC_k1AsIXvsdXsn4XioWf4Ik">‚Ä™Noam Shazeer‚Ä¨ - (Transformers author)</A>
							<DT><A HREF="https://scholar.google.com/citations?hl=en&user=oR9sCGYAAAAJ&view_op=list_works&citft=1&email_for_op=antonio.jfdominguez%40gmail.com&gmla=AOV7GLOU01RwzH9ABWVgReuJ4vYIX-R16ZT_WjsIZt7HhK2cY3eb5fbOdZNkOCbIlM91bVmTEhMFcavI9r8QcGTItl1zpC_q0beRNBvJIqorFTZN7iChYoX3jUywwyJ5CwLvpFYruduPtNhAPsDXUg7xpsGLiSh45meJipgi3nKsNJzMieDISepUsp8-3hmuKU7cdrhNGNZp3ztviA_HkpQpZQa3cuL70CtXUOwgBmAIue-nEexRvwjcglc">‚Ä™Ashish Vaswani‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
							<DT><A HREF="https://twitter.com/ThomasScialom">MetaAI: Thomas Scialom (Llama)</A>
						</DL><p>
						<DT><H3 FOLDED>Sampling Methods</H3>
						<DL><p>
							<DT><H3 FOLDED>Contrastive Search</H3>
							<DL><p>
								<DT><A HREF="https://github.com/yxuansu/SimCTG">yxuansu/SimCTG: [NeurIPS'22] A Contrastive Framework for Neural Text Generation</A>
								<DT><A HREF="https://arxiv.org/abs/2210.14140">[2210.14140] Contrastive Search Is What You Need For Neural Text Generation</A>
								<DT><A HREF="https://twitter.com/joao_gante/status/1590293010385760256">Contastive Search</A>
								<DT><A HREF="https://huggingface.co/blog/introducing-csearch">Generating Human-level Text with Contrastive Search in Transformers ü§ó</A>
							</DL><p>
							<DT><H3 FOLDED>deterministic</H3>
							<DL><p>
								<DT><H3 FOLDED>Greedy Search</H3>
								<DL><p>
									<DT><A HREF="https://en.wikipedia.org/wiki/Beam_search">Beam search - Wikipedia</A>
									<DT><A HREF="https://twitter.com/cwolferesearch/status/1659608476455256078">Greedy decoding steps and theory</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>stochastic</H3>
							<DL><p>
								<DT><H3 FOLDED>top-k</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>nNucleus-top-p</H3>
								<DL><p>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>Grammar-based</H3>
							<DL><p>
								<DT><A HREF="https://github.com/outlines-dev/outlines">outlines-dev/outlines: Structured Text Generation</A>
								<DT><A HREF="https://github.com/outlines-dev/outlines/blob/main/outlines/grammars/arithmetic.lark">outlines/outlines/grammars/arithmetic.lark at main ¬∑ outlines-dev/outlines</A>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/issues/4218">llama : speed-up grammar sampling ¬∑ Issue #4218 ¬∑ ggerganov/llama.cpp</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=AvHLJqtmQkE">Typical Decoding for Natural Language Generation</A>
							<DT><A HREF="https://huggingface.co/blog/how-to-generate">Basics</A>
							<DT><A HREF="https://arxiv.org/abs/2307.15337">[2307.15337] Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</A>
							<DT><A HREF="https://arxiv.org/pdf/1905.00174.pdf">Unsupervised Temperature Scaling</A>
							<DT><A HREF="https://blog.research.google/2024/01/introducing-aspire-for-selective.html">Introducing ASPIRE for selective prediction in LLMs ‚Äì Google Research Blog</A>
							<DT><A HREF="https://arxiv.org/abs/2406.16838">[2406.16838] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models</A>
							<DT><A HREF="https://x.com/wellecks/status/1806326626546036802">(1) Sean Welleck en X: "What do nucleus sampling, tree-of-thought, and PagedAttention have in common? They're all part of our new survey: "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models" https://t.co/faHwQcxLUX https://t.co/eY9cNstBbd" / X</A>
						</DL><p>
						<DT><H3 FOLDED>capabilities</H3>
						<DL><p>
							<DT><H3 FOLDED>interpretability</H3>
							<DL><p>
								<DT><H3 FOLDED>mechanistic interpretability</H3>
								<DL><p>
									<DT><H3 FOLDED>mechanistic-interpretability-people</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/NeelNanda5">(1) Neel Nanda (@NeelNanda5) / Twitter</A>
										<DT><A HREF="https://github.com/saprmarks">saprmarks</A>
									</DL><p>
									<DT><H3 FOLDED>mechanistic-interpretability-lectures</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=OI1we2bUseI&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T">Real-Time Research Walkthrough: Addition in GPT-J - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=KV5gbOmHbjU&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=3">A Walkthrough of A Mathematical Framework for Transformer Circuits - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=yo4QvDn-vsU">Real-Time Research Recording: Can a Transformer Re-Derive Positional Info? - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=pC4zRb_5noQ">CS25 I Stanford Seminar 2022 - Transformer Circuits, Induction Heads, In-Context Learning - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=4JKuyfejWTU&list=LL&index=3">Mor Geva: Transformer Feed Forward Layers are Key-Value Memories, and Build Predictions - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=m8tzXelUTLo">Real-Time Research Walkthrough: Mover Heads Part 1/2 - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=I1ELSZNFeHc">David Bau - Direct Model Editing and Mechanistic Interpretability - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=2Rdp9GvcYOE">Chris Olah - Looking Inside Neural Networks with Mechanistic Interpretability - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=_Ygf0GnlwmY&t=5193s">Mechanistic Interpretability - NEEL NANDA (DeepMind) - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>transformer debugger</H3>
									<DL><p>
										<DT><A HREF="https://github.com/openai/transformer-debugger?tab=readme-ov-file">openai/transformer-debugger</A>
										<DT><A HREF="https://github.com/neelnanda-io/TransformerLens">TransformerLens: A library for mechanistic interpretability of language models</A>
										<DT><A HREF="https://www.loom.com/share/6bd8c6bde84b42a98f9a26a969d4a3ad?sid=4a09ac29-58a2-433e-b55d-762414d9a7fa">4. TDB worked example: name mover heads, part 2</A>
										<DT><A HREF="https://github.com/joennlae/tensorli">joennlae/tensorli: Absolute minimalistic implementation of a GPT-like transformer using only numpy (&lt;650 lines).</A>
										<DT><A HREF="https://explainextended.com/2023/12/31/happy-new-year-15/">Happy New Year: GPT in 500 lines of SQL - EXPLAIN EXTENDED at EXPLAIN EXTENDED</A>
										<DT><A HREF="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html">Language models can explain neurons in language models</A>
									</DL><p>
									<DT><H3 FOLDED>SAE</H3>
									<DL><p>
										<DT><A HREF="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</A>
										<DT><A HREF="https://blog.withmartian.com/post/scaling-ai-interpret">Scaling AI Interpretability</A>
										<DT><A HREF="https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html">An Intuitive Explanation of Sparse Autoencoders for LLM Interpretability | Adam Karvonen</A>
										<DT><A HREF="https://thesephist.com/posts/prism/">Prism: mapping interpretable concepts and features in a latent space of language | thesephist.com</A>
									</DL><p>
									<DT><A HREF="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</A>
									<DT><A HREF="https://twitter.com/NeelNanda5/status/1682827872191348736">GPT-J arithmetic addtion</A>
									<DT><A HREF="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</A>
									<DT><A HREF="https://transformer-circuits.pub/2024/april-update/index.html#scaling-laws">Scaling Laws for Dictionary Learning</A>
									<DT><A HREF="https://github.com/saprmarks/dictionary_learning">saprmarks/dictionary_learning</A>
									<DT><A HREF="https://transformer-circuits.pub/2023/toy-double-descent/index.html">Superposition, Memorization, and Double Descent</A>
									<DT><A HREF="https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating">Toy Models of Superposition</A>
									<DT><A HREF="https://transformer-circuits.pub/2021/framework/index.html#def-privileged-basis">Privileged Bases in the Transformer Residual Stream</A>
									<DT><A HREF="https://github.com/anthropics/toy-models-of-superposition/blob/main/toy_models.ipynb">toy-models-of-superposition/toy_models.ipynb</A>
									<DT><A HREF="https://github.com/alan-cooney/CircuitsVis">CircuitsVis: Mechanistic Interpretability Visualizations using React</A>
									<DT><A HREF="https://arxiv.org/pdf/2307.09458.pdf">Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chincilla (DeepMind)</A>
									<DT><A HREF="https://copy-suppression.streamlit.app/">LLM: copy-supression</A>
									<DT><A HREF="https://github.com/ArthurConmy/Automatic-Circuit-Discovery">ArthurConmy/Automatic-Circuit-Discovery</A>
									<DT><A HREF="https://github.com/neelnanda-io/Exploring-2L-SAE">neelnanda-io/Exploring-2L-SAE</A>
									<DT><A HREF="https://github.com/yizhe-ang/interactive-transformer">interactive-transformer: A visual interface for understanding Transformers</A>
									<DT><A HREF="https://www.youtube.com/watch?v=PnwC74s1nmc">Theoretical and Practical Insights from Linear Transformers</A>
									<DT><A HREF="https://github.com/neelnanda-io/neelutils">neelnanda-io/neelutils: Random utils for personal use</A>
									<DT><A HREF="https://github.com/neelnanda-io/Grokking">Grokking: A Mechanistic Interpretability Analysis of Grokking</A>
									<DT><A HREF="https://github.com/neelnanda-io/TransformerLens">TransformerLens: A library for mechanistic interpretability of language models</A>
									<DT><A HREF="https://github.com/google-deepmind/tracr">(DeepMind) Tracr: TRAnsformer Compiler for RASP</A>
									<DT><A HREF="https://twitter.com/DrJimFan/status/1613918800444899328?lang=en">We train Transformers to encode algorithms in their weights, such as sorting, counting, and balancing parentheses from lots of data. I never thought we may also go in the *reverse* direction: *compile* Transformer weights directly from explicit code</A>
									<DT><A HREF="https://twitter.com/DrJimFan/status/1613966404721729536">Thinking Like Transformers</A>
									<DT><A HREF="https://srush.github.io/raspy/">Thinking like Transformer</A>
									<DT><A HREF="https://www.youtube.com/watch?v=zUCoxhExe0o">Stanford Seminar - Computing with High-Dimensional Vectors</A>
									<DT><A HREF="https://github.com/saprmarks/geometry-of-truth">saprmarks/geometry-of-truth</A>
									<DT><A HREF="https://www.youtube.com/watch?v=XuFWN0xcM_U">[short] Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models - YouTube</A>
									<DT><A HREF="https://arxiv.org/abs//2401.06102">[2401.06102] Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models</A>
									<DT><A HREF="https://arxiv.org/abs/2401.05566">[2401.05566] Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</A>
									<DT><A HREF="https://alan-cooney.github.io/CircuitsVis/?path=/docs/attention-attentionheads--induction-heads-layer">attention / AttentionHeads - Induction Heads Layer ‚ãÖ Storybook</A>
									<DT><A HREF="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">interpreting GPT: the logit lens ‚Äî LessWrong</A>
									<DT><A HREF="https://x.com/mlpowered/status/1792948212728524917">dictonary learning working on a fronteir model</A>
								</DL><p>
								<DT><A HREF="https://thesephist.com/posts/prism/">Prism: mapping interpretable concepts and features in a latent space of language | thesephist.com</A>
								<DT><A HREF="https://research.google/blog/patchscopes-a-unifying-framework-for-inspecting-hidden-representations-of-language-models/">Patchscopes: A unifying framework for inspecting hidden representations of language models</A>
							</DL><p>
							<DT><H3 FOLDED>compression</H3>
							<DL><p>
								<DT><H3 FOLDED>compression-people</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/drjwrae">(1) Jack Rae (@drjwrae) / X</A>
									<DT><A HREF="https://www.youtube.com/watch?v=boiW5qhrGH4">(George Hotz) Hutter Prize: Intelligence as Compression</A>
									<DT><A HREF="http://www.hutter1.net/">Homepage of Marcus Hutter</A>
									<DT><A HREF="https://arxiv.org/abs/2308.07037">[Alex Graves] Bayesian Flow Networks</A>
									<DT><A HREF="https://jveness.info/">Homepage of Joel Veness</A>
								</DL><p>
								<DT><A HREF="https://github.com/Ma-Lab-Berkeley/CRATE">Ma-Lab-Berkeley/CRATE: Code for CRATE (Coding RAte reduction TransformEr).</A>
								<DT><A HREF="https://twitter.com/YiMaTweets/status/1693302230664020449/photo/1">rate reduction flow equation for maximizing information gain</A>
								<DT><A HREF="https://arxiv.org/abs/2306.01129">[2306.01129] White-Box Transformers via Sparse Rate Reduction</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Sx2PVCAiGhQ">George Hotz | Programming | Context Tree Weighting: compression = AI</A>
								<DT><A HREF="https://github.com/GFNOrg/gfn-lm-tuning">Amortizing Intractable Inference in Large Language Models (uTransfer creator)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=AKMuA_TVz3A&t=1851s">Ilya Sutskever: An Observation on Generalization</A>
								<DT><A HREF="https://www.youtube.com/watch?v=dO4TPJkeaaU&t=1234s">Compression for AGI - (Jack Rae)</A>
								<DT><A HREF="https://www.lesswrong.com/posts/Kyc5dFDzBg4WccrbK/an-intuitive-explanation-of-solomonoff-induction">An Intuitive Explanation of Solomonoff Induction ‚Äî LessWrong</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference">Solomonoff's theory of inductive inference - Wikipedia</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Kolmogorov_complexity#Compression">Kolmogorov complexity - Wikipedia</A>
								<DT><A HREF="https://bellard.org/">Fabrice Bellard's Home Page</A>
								<DT><A HREF="https://bellard.org/nncp/">NNCP: Lossless Data Compression with Neural Networks</A>
								<DT><A HREF="https://bellard.org/nncp/nncp_v2.1.pdf">NNCP v2: Lossless Data Compression with Transformer</A>
								<DT><A HREF="https://blog.research.google/2022/12/accelerating-text-generation-with.html?m=1">Accelerating text generation with Confident Adaptive Language Modeling (CALM) ‚Äì Google Research Blog</A>
								<DT><A HREF="https://twitter.com/a_stadt/status/1737849248560066794">Sample-Efficient Pre-training</A>
								<DT><A HREF="https://www.youtube.com/watch?v=OPZxs6IXH00&t=10s">Ilya Sutskever - Opening Remarks: Confronting the Possibility of AGI</A>
								<DT><A HREF="https://twitter.com/arankomatsuzaki/status/1780073500536872990">Compression Represents Intelligence Linearly</A>
								<DT><A HREF="https://github.com/hkust-nlp/llm-compression-intelligence">hkust-nlp/llm-compression-intelligence: Official github repo for the paper "Compression Represents Intelligence Linearly"</A>
								<DT><A HREF="https://arxiv.org/abs/2404.09937">[2404.09937] Compression Represents Intelligence Linearly</A>
								<DT><A HREF="https://www.youtube.com/watch?v=xjnp42BuxOo">Compression Represents Intelligence Linearly HKU &amp; Tencent 2024 - YouTube</A>
								<DT><A HREF="https://deepmind.google/research/publications/39768/">Language Modeling Is Compression - Google DeepMind</A>
								<DT><A HREF="https://arxiv.org/abs/2401.14953">[2401.14953] Learning Universal Predictors</A>
								<DT><A HREF="https://github.com/google-deepmind/neural_networks_solomonoff_induction">google-deepmind/neural_networks_solomonoff_induction: Learning Universal Predictors</A>
								<DT><A HREF="https://github.com/KindXiaoming/pykan">KindXiaoming/pykan: Kolmogorov Arnold Networks</A>
								<DT><A HREF="https://github.com/nadavrot/compressor">nadavrot/compressor: An educational implementation of a modern compressor in Rust</A>
								<DT><A HREF="https://github.com/cedrickchee/awesome-ml-model-compression">cedrickchee/awesome-ml-model-compression: Awesome machine learning model compression research papers, tools, and learning material.</A>
							</DL><p>
							<DT><H3 FOLDED>reasoning</H3>
							<DL><p>
								<DT><H3 FOLDED>In-Context Learning</H3>
								<DL><p>
									<DT><H3 FOLDED>chain-of-thought</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2210.09261">[2210.09261] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them</A>
										<DT><A HREF="https://github.com/FranxYao/chain-of-thought-hub">FranxYao/chain-of-thought-hub: Benchmarking large language models' complex reasoning ability with chain-of-thought prompting</A>
										<DT><A HREF="https://arxiv.org/pdf/2310.07923.pdf">THE EXPRESSIVE POWER OF TRANSFORMERS WITH CHAIN OF THOUGHT</A>
										<DT><A HREF="https://arxiv.org/abs/2310.10845">[2310.10845] CoTFormer: More Tokens With Attention Make Up For Less Depth</A>
										<DT><A HREF="https://arxiv.org/pdf/2311.01460.pdf">IMPLICIT CHAIN OF THOUGHT REASONING VIA KNOWLEDGE DISTILLATION (Microsoft)</A>
										<DT><A HREF="https://twitter.com/ZeyuanAllenZhu/status/1706829354888798296">Physics of Language Models: Part 3.2, knowledge manipulation</A>
									</DL><p>
									<DT><H3 FOLDED>icl-continuous</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/daniel_m_cer/status/1528655068010106881/photo/1">(Lester, 2022)SPoT:  SOFT PROMPT</A>
										<DT><A HREF="https://ai.googleblog.com/2022/02/guiding-frozen-language-models-with.html">Google AI Blog: Guiding Frozen Language Models with Learned Soft Prompts</A>
										<DT><A HREF="https://arxiv.org/abs/2110.07904">LESTER SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</A>
										<DT><A HREF="https://arxiv.org/abs/2104.08691">(Lester, 2021) The Power of Scale for Parameter-Efficient Prompt Tuning</A>
										<DT><A HREF="https://arxiv.org/abs/2106.07704">HTP Text Generation with Efficient (Soft) Q-Learning</A>
										<DT><A HREF="https://arxiv.org/abs/2104.05240">Soft-prompt: Factual Probing Is [MASK]: Learning vs. Learning to Recall</A>
										<DT><A HREF="https://arxiv.org/abs/2104.06599">Soft Prompts: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</A>
										<DT><A HREF="https://arxiv.org/abs/2101.00190">HT: (0.1%params) Prefix-Tuning: Optimizing Continuous Prompts</A>
									</DL><p>
									<DT><H3 FOLDED>Tu Vu (Twiiter &amp; Goolge Research)</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/tuvuumass">Tu Vu (@tuvuumass) / Twitter</A>
										<DT><A HREF="https://twitter.com/sangmichaelxie/status/1554553711241805824">In-Context Learning as Bayesian Inference</A>
										<DT><A HREF="https://twitter.com/quocleix/status/1532072473763532802">Least-to-most prompting: Teach How To Breakdown Complex Problems</A>
										<DT><A HREF="https://twitter.com/_jasonwei/status/1529292177414799361">Self-Supervised Chain Of Thought by introducing "Step by step" reasoning</A>
										<DT><A HREF="https://twitter.com/AkariAsai/status/1528996269280022528">Akari Asai en Twitter: "Introducing ùóîùóßùóßùóòùó†ùó£ùóß, a new modular, multi-task, and parameter-efficient approach to combine knowledge from multiple tasks to solve a new task using a small trainable of parameters üî• while keeping the original LM *frozen* üßä [1/9] Paper üìú: https://t.co/IgJvwCX5tU https://t.co/u26WVGCwsc" / Twitter</A>
										<DT><A HREF="https://twitter.com/tsiprasd/status/1555302289824366592">LLMs can do in-context learning, but are they "learning" new tasks</A>
										<DT><A HREF="https://twitter.com/Fortune_VY/status/1550819534940114946">(Q,A,R): what should be learned to achieve the correct answer (VERIFIERS)</A>
									</DL><p>
									<DT><H3 FOLDED>icl-vision</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2203.05557.pdf">Conditional Prompt Learning for Vision-Language Models</A>
										<DT><A HREF="https://arxiv.org/abs/2109.01134">[2109.01134] Learning to Prompt for Vision-Language Models</A>
										<DT><A HREF="https://arxiv.org/abs/2110.08484">A Good Prompt Is Worth Millions of Parameters</A>
									</DL><p>
									<DT><H3 FOLDED>system prompt</H3>
									<DL><p>
										<DT><A HREF="https://x.com/NorthstarBrain/status/1804823489632723057">claude 3.5 Sonnet: system prompt</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2208.01066">[Universal Approximator] What Can Transformers Learn In-Context?</A>
									<DT><A HREF="https://openreview.net/forum?id=8p3fu56lKc&referrer=%5Bthe%20profile%20of%20Tengyu%20Ma%5D(%2Fprofile%3Fid%3D~Tengyu_Ma1)">One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention | OpenReview</A>
									<DT><A HREF="https://arxiv.org/pdf/2212.07677.pdf">Transformers Learn In-Context by Gradient Descent</A>
									<DT><A HREF="https://arxiv.org/abs/2305.08298">Symbol tuning improves in-context learning in language models</A>
									<DT><A HREF="https://arxiv.org/pdf/2206.07682.pdf">(Wei, 2022) Emergent Abilities of Large Language Models</A>
									<DT><A HREF="https://arxiv.org/pdf/2201.11903.pdf">(Wei, 2022) CHAIN OF THOUGHT</A>
									<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
									<DT><A HREF="https://rylanschaeffer.github.io/blog_posts/2022-01-20-google-brain-flan.html">FLAN: Finetuned Language Models are Zero-Shot Learners</A>
									<DT><A HREF="https://arxiv.org/abs/2401.14953">[2401.14953] Learning Universal Predictors</A>
									<DT><A HREF="https://github.com/google-deepmind/neural_networks_solomonoff_induction">google-deepmind/neural_networks_solomonoff_induction</A>
									<DT><A HREF="https://arxiv.org/html/2311.07772v4#bib.bib13">In-context Learning and Gradient Descent Revisited</A>
									<DT><A HREF="https://paperswithcode.com/paper/finetuned-language-models-are-zero-shot">Finetuned Language Models Are Zero-Shot Learners</A>
									<DT><A HREF="https://openreview.net/pdf?id=NiEtU7blzN">LARGE LANGUAGE MODELS CAN SELF-IMPROVE</A>
									<DT><A HREF="https://www.lesswrong.com/posts/qwqowdhnMreKQvxLv/paper-large-language-models-can-self-improve-linkpost">Large Language Models Can Self-improve (Semi-Supervised Learning)</A>
									<DT><A HREF="https://arxiv.org/pdf/2102.07350.pdf">Prompt Programming for Large Language Models</A>
									<DT><A HREF="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">(OpenAI GPT2, 2019) Language Models are Unsupervised Multitask Learners</A>
									<DT><A HREF="https://arxiv.org/pdf/2107.13586.pdf">MAIN (Liu, 2021) Pre-train, Prompt, and Predict: Systematic Survey</A>
									<DT><A HREF="https://arxiv.org/pdf/2012.15723.pdf">(Gao, 2021) Making Pre-LLM Better Few-shot Learners</A>
									<DT><A HREF="http://pretrain.nlpedia.ai/">PRETRAIN LANGUAGE MODELS</A>
									<DT><A HREF="http://pretrain.nlpedia.ai/data/pdf/basics.pdf">Formalization of Prompting</A>
									<DT><A HREF="https://arxiv.org/abs/2112.00114">(Nye, 2021) SCRATCHPADS for Intermediate Computation</A>
									<DT><A HREF="https://arxiv.org/abs/2110.08207">(Prompt Training, 2021) Multitask Prompted Training Enables Zero-Shot Task Generalization</A>
									<DT><A HREF="https://aclanthology.org/2022.acl-long.60.pdf">(THEORY, 2022) An Information-theoretic Approach</A>
									<DT><A HREF="https://arxiv.org/abs/2103.10385">(Liu, 2021) P-Tuning: GPT Understands, Too</A>
									<DT><A HREF="https://arxiv.org/abs/2104.08786">Overcoming Few-Shot Prompt ORDER SENSITIVITY</A>
									<DT><A HREF="https://arxiv.org/abs/2205.12548">[2205.12548] RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning</A>
									<DT><A HREF="https://www.youtube.com/watch?v=8HwHGGb1zpQ&t=2474s">Prompt-based learning</A>
									<DT><A HREF="https://arxiv.org/abs/2203.06566">PromptChainer: Visual Programming</A>
									<DT><A HREF="https://arxiv.org/abs/2103.08493">HT How Many Data Points is a Prompt Worth?</A>
									<DT><A HREF="https://arxiv.org/abs/2101.06804">(Liu, 2021) What Makes Good In-Context Examples for GPT-3?</A>
									<DT><A HREF="https://github.com/microsoft/semantic-kernel/blob/main/python/README.md">semantic-kernel/README.md at main</A>
									<DT><A HREF="https://arxiv.org/pdf/2303.03846.pdf">LARGER LANGUAGE MODELS DO IN-CONTEXT LEARNING DIFFERENTLY</A>
									<DT><A HREF="https://arxiv.org/abs/2210.09261">Challenging BIG-Bench Tasks and Whether CoT Can Solve Them</A>
									<DT><A HREF="https://twitter.com/_akhaliq/status/1736581357705314731">ReST meets ReAct: Self-Improvement for Multi-Step Reasoning</A>
									<DT><A HREF="https://docs.google.com/document/d/1x3TRnRXz8PCHAWAgb7pzNfgAfBuKPxsyvBVZKXKeBFc/edit#">Experimentation Prompts: Infographics - Google Docs</A>
									<DT><A HREF="https://x.com/davidsamuelcz/status/1800213563124097476">(1) David Samuel en X: "BERTs are not dead!üßü But just misunderstood and overshadowed by their GPT siblings. In this paper, we travel back into 2020 and speculate on an alternative history where DeBERTa is the first model to show in-context learning abilities. Paper: https://t.co/vTPxC3UtqU 1/6 https://t.co/ipVu3bAQvx" / X</A>
									<DT><A HREF="https://arxiv.org/abs/2406.04823">[2406.04823] BERTs are Generative In-Context Learners</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2311.11045">[2311.11045] Orca 2: Teaching Small Language Models How to Reason</A>
								<DT><A HREF="https://www.youtube.com/watch?v=pA-azVdihQc&t=1890s">What is Machine Learning Good For? - Alex Davies (DeepMind)</A>
								<DT><A HREF="https://huggingface.co/microsoft/Orca-2-13b">microsoft/Orca-2-13b ¬∑ Hugging Face</A>
								<DT><A HREF="https://www.youtube.com/watch?v=QDkm_BU9Deo">[short] Generative AI for Math: Part I MATHPILE: A Billion-Token-Scale Pretraining Corpus for Math - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>aligment</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/@AlignmentWorkshop">Alignment Workshop - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-8mKJd2SwI4">Owain Evans - Out-of-context Reasoning in LLMs - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>NEXT-TOKEN PREDICTION</H3>
							<DL><p>
								<DT><H3 FOLDED>multi-token-prediction</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/facebook/multi-token-prediction">facebook/multi-token-prediction ¬∑ Hugging Face</A>
									<DT><A HREF="https://x.com/syhw/status/1803112949566799902">facebook/multi-token-prediction</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2403.06963.pdf">THE PITFALLS OF NEXT-TOKEN PREDICTION</A>
								<DT><A HREF="https://github.com/gregorbachmann/Next-Token-Failures">gregorbachmann/Next-Token-Failures</A>
								<DT><A HREF="https://www.youtube.com/watch?v=OF8A_b-3q84">The mean and variance of inputs through the layers of a deep narrow neural network - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MFOb9sh9geY">Multi-token Prediction and RemoteCLIP - YouTube</A>
								<DT><A HREF="https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know/">The Many Ways that Digital Minds Can Know ‚Äì Ryan Moulton's Articles</A>
								<DT><A HREF="https://x.com/alesstolfo/status/1805976764705038708">LLMs don‚Äôt just output the next token, they also output confidence. How is this computed?</A>
							</DL><p>
							<DT><H3 FOLDED>emergence</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://en.wikipedia.org/wiki/Kolmogorov_complexity#Compression">Kolmogorov complexity - Wikipedia</A>
							<DT><A HREF="https://arxiv.org/pdf/1911.12543.pdf">How Can We Know What Language Models Know?</A>
							<DT><A HREF="https://openai.com/research/unsupervised-sentiment-neuron">Unsupervised sentiment neuron</A>
							<DT><A HREF="https://www.youtube.com/watch?v=KV5gbOmHbjU">A Mathematical Framework for Transformer Circuits</A>
							<DT><A HREF="https://towardsdatascience.com/visualizing-word-embedding-with-pca-and-t-sne-961a692509f5">Visualizing Word Embedding with PCA and t-SNE</A>
							<DT><A HREF="https://arxiv.org/pdf/2105.12202.pdf">LM Transformer based: Context Sensitive Models (Attention Mechanism)</A>
							<DT><A HREF="https://twitter.com/wesg52/status/1653750337373880322">What features are neurons in LLMs actualy extracting?</A>
							<DT><A HREF="https://www.youtube.com/watch?v=t5LjgczaS80&t=741s">Gail Weiss: Thinking Like Transformers</A>
							<DT><A HREF="https://arxiv.org/pdf/2211.03495.pdf">How Much Does Attention Actually Attend?</A>
							<DT><A HREF="https://arxiv.org/abs/2310.10348">[2310.10348] Attribution Patching Outperforms Automated Circuit Discovery</A>
							<DT><A HREF="https://twitter.com/aaquib_syed1/status/1714386165237776653">ttribution Patching Outperforms Automated Circuit Discovery (Thread)</A>
							<DT><A HREF="https://papers.labml.ai/paper/7719e8e6c2cf11edb95839eec3084ddd">Eliciting Latent Predictions from Transformers with the Tuned Lens</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Ohl5AGUOLXk">Quantifying and Understanding Memorization in DNN</A>
							<DT><A HREF="https://twitter.com/val_kharvd/status/1655397456652140545">Transformerlens</A>
							<DT><A HREF="https://www.youtube.com/watch?v=VY7SCl_DFho">Interpretable vs Explainable Machine Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2301.04589?utm_source=substack&utm_medium=email">Memory Augmented Large Language Models are Computationally Universal</A>
							<DT><A HREF="https://arxiv.org/abs/2212.08073">[2212.08073] Constitutional AI: Harmlessness from AI Feedback (Antrophic)</A>
							<DT><A HREF="https://bbycroft.net/llm">LLM Visualization</A>
							<DT><A HREF="https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling">Pythia: Analyzing Large Language Models Across Training and Scaling</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M74WKIh0ciI&list=LL&index=24">Three weight matrices in a neural network growing connections as two parallel networks are merged. - YouTube</A>
							<DT><A HREF="https://www.youtube.com/@AlignmentWorkshop">Alignment Workshop - YouTube</A>
							<DT><A HREF="https://twitter.com/IasonGabriel/status/1781262674711466483">The Ethics of LLMS</A>
							<DT><A HREF="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/ethics-of-advanced-ai-assistants/the-ethics-of-advanced-ai-assistants-2024-i.pdf">The Ethics of Advanced AI Assistants</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Z1bXBinTtnQ">LLM - Reasoning SOLVED (new research) - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-training</H3>
						<DL><p>
							<DT><H3 FOLDED>language-models-training-data</H3>
							<DL><p>
								<DT><H3 FOLDED>Datasets</H3>
								<DL><p>
									<DT><H3 FOLDED>datasets-google-research</H3>
									<DL><p>
										<DT><A HREF="https://github.com/google-research-datasets">Google Research Datasets</A>
										<DT><A HREF="https://github.com/google-research-datasets/natural-questions">google-research-datasets/natural-questions: Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems.</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-common-crawls</H3>
									<DL><p>
										<DT><H3 FOLDED>FineWeb</H3>
										<DL><p>
											<DT><A HREF="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb: decanting the web for the finest text data at scale - a Hugging Face Space by HuggingFaceFW</A>
											<DT><A HREF="https://huggingface.co/datasets/HuggingFaceFW/fineweb">HuggingFaceFW/fineweb ¬∑ Datasets at Hugging Face</A>
											<DT><A HREF="https://x.com/LoubnaBenAllal1/status/1797175938972606975">(1) Loubna Ben Allal en X: "üç∑ FineWeb technical report is out and so is üìö FineWeb-Edu, a 1.3 trillion tokens dataset that outperforms all other open web datasets, with remarkable improvements on educational benchmarks such as MMLU, ARC, and OpenBookQA. Technical report: https://t.co/lfOZYYJKxq Dataset: https://t.co/urC5qjmx3v" / X</A>
											<DT><A HREF="https://x.com/drjwrae/status/1797286202825232497">(1) Jack Rae en X: "Some really nice data research + artifacts coming out from HuggingFace these days. Creating the definitive open source dataset has probably more value than most open-sourced models at this stage." / X</A>
											<DT><A HREF="https://x.com/gui_penedo/status/1797173053123916036">(1) Guilherme Penedo en X: "We are (finally) releasing the üç∑ FineWeb technical report! In it, we detail and explain every processing decision we took, and we also introduce our newest dataset: üìö FineWeb-Edu, a (web only) subset of FW filtered for high educational content. Link: https://t.co/MRsc8Q5K9q https://t.co/HVfFnKbeso" / X</A>
											<DT><A HREF="https://x.com/jd_pressman/status/1797396190210015716/photo/1">The 4 types of synthetic data</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1796901742715498604">The only thing we know is "make it as diverse as possible"..</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1797320347999670368">CommonCrawl over time</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1797313173449764933">FineWeb-Edu llm.c</A>
											<DT><A HREF="https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py">datatrove/examples/fineweb.py</A>
										</DL><p>
										<DT><H3 FOLDED>DataComp</H3>
										<DL><p>
											<DT><A HREF="https://www.datacomp.ai/dclm/">DataComp</A>
											<DT><A HREF="https://arxiv.org/abs/2406.11794">[2406.11794] DataComp-LM: In search of the next generation of training sets for language models</A>
											<DT><A HREF="https://x.com/TsingYoga/status/1804728355239199181">It's Time to Scale Down the Data</A>
										</DL><p>
										<DT><A HREF="https://commoncrawl.org/">Common Crawl - Open Repository of Web Crawl Data</A>
										<DT><A HREF="https://huggingface.co/datasets/HuggingFaceFW/fineweb">HuggingFaceFW/fineweb ¬∑ Datasets at Hugging Face</A>
										<DT><A HREF="https://huggingface.co/datasets/THUDM/LongBench">THUDM/LongBench</A>
										<DT><A HREF="https://huggingface.co/datasets/c4">c4</A>
										<DT><A HREF="https://arxiv.org/abs/2309.04662">[2309.04662] MADLAD-400: A Multilingual And Document-Level Large Audited Dataset</A>
										<DT><A HREF="https://huggingface.co/datasets/tiiuae/falcon-refinedweb">tiiuae/falcon-refinedweb</A>
										<DT><A HREF="https://twitter.com/_philschmid/status/1714280568853303720">IBM's Granite LLMs</A>
										<DT><A HREF="https://commoncrawl.org/get-started">Common Crawl - Accessing the Data: AWS S3 buckets</A>
									</DL><p>
									<DT><H3 FOLDED>LAION</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/laion_ai/status/1714883417383346493">LAION: 608 B chess moves, 236 Rubik's Cube moves, 39 B A* moves in ASCII Mazes</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-math</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=QDkm_BU9Deo">Generative AI for Math: Part I MATHPILE: A Billion-Token-Scale Pretraining Corpus for Math</A>
										<DT><A HREF="https://gair-nlp.github.io/MathPile/">Generative AI for Math: Part I MATHPILE: A Billion-Token-Scale Pretraining Corpus for Math</A>
										<DT><A HREF="https://arxiv.org/abs/2310.06786">OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text</A>
										<DT><A HREF="https://huggingface.co/datasets/open-web-math/open-web-math">open-web-math/open-web-math</A>
										<DT><A HREF="https://huggingface.co/datasets/math-ai/StackMathQA">math-ai/StackMathQA</A>
										<DT><A HREF="https://huggingface.co/datasets/math-eval/TAL-SCQ5K">math-eval/TAL-SCQ5K</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-dolma</H3>
									<DL><p>
										<DT><A HREF="https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64">AI2 Dolma: 3 Trillion Token Open Corpus for LLMs | AI2 Blog</A>
										<DT><A HREF="https://drive.google.com/file/d/12gOf5I5RytsD159nSP7iim_5zN31FCXq/view">dolma-datasheet</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-madlad-400</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2309.04662">[2309.04662] MADLAD-400: A Multilingual And Document-Level Large Audited Dataset</A>
										<DT><A HREF="https://huggingface.co/datasets/allenai/MADLAD-400">allenai/MADLAD-400</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-orca</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/datasets/Open-Orca/OpenOrca">Open-Orca/OpenOrca</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-the-stack</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/datasets/bigcode/the-stack">bigcode/the-stack</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-the-pile</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/datasets/JeanKaddour/minipile">JeanKaddour/minipile</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-videos</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/papers/2402.19479">Paper page - Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</A>
									</DL><p>
									<DT><A HREF="https://jingfengyang.github.io/gpt">Why did all of the public reproduction of GPT-3 fail?</A>
									<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1693951301942759884">mixture dataset</A>
									<DT><A HREF="https://annas-blog.org/duxiu-exclusive.html">Anna's Blog: Exclusive access for LLM companies to largest Chinese non-fiction book collection in the world - Anna‚Äôs Blog</A>
									<DT><A HREF="http://www.gharchive.org/">GH Archive</A>
									<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1745900747810619436">Datasets that I really like</A>
									<DT><A HREF="https://huggingface.co/datasets/ropes">ropes (reasoning)</A>
									<DT><A HREF="https://huggingface.co/datasets/euirim/goodwiki">euirim/goodwiki</A>
									<DT><A HREF="https://huggingface.co/datasets/deepmind/code_contests">deepmind/code_contests</A>
									<DT><A HREF="https://allenai.org/data/entailmentbank">EntailmentBank Dataset ‚Äî Allen Institute for AI</A>
									<DT><A HREF="https://huggingface.co/datasets/gsm8k">gsm8k ¬∑ Datasets at Hugging Face</A>
								</DL><p>
								<DT><H3 FOLDED>Crawler</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=leCYxw0Qv1E">Scalable Extraction of Training Data from (Production) LLMs</A>
									<DT><A HREF="https://arxiv.org/abs/2309.04662">[2309.04662] MADLAD-400: A Multilingual And Document-Level Large Audited Dataset</A>
								</DL><p>
								<DT><H3 FOLDED>training-data-pre-post-processing</H3>
								<DL><p>
									<DT><H3 FOLDED>data-cleaning</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/lilac_ai">Lilac: Analyze, structure, and clean unstructured data with AI</A>
										<DT><A HREF="https://lilacml.com/blog/introducing-lilac.html">Introducing Lilac - üå∏ Lilac</A>
										<DT><A HREF="https://github.com/ChenghaoMou/text-dedup">ChenghaoMou/text-dedup: All-in-one text de-duplication</A>
										<DT><A HREF="https://publish.obsidian.md/chenghao/posts/20230220150602">Large-scale Near-deduplication Behind BigCode</A>
										<DT><A HREF="https://lmsys.org/blog/2023-11-14-llm-decontaminator/">LLM-based decontaminator</A>
									</DL><p>
									<DT><H3 FOLDED>text-dedup</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ChenghaoMou/text-dedup">ChenghaoMou/text-dedup: All-in-one text de-duplication</A>
									</DL><p>
									<DT><A HREF="https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf">GPT-3:Details of Common Crawl Filtering</A>
									<DT><A HREF="https://github.com/cloneofsimo/imgdataset_process">cloneofsimo/imgdataset_process</A>
									<DT><A HREF="https://developer.nvidia.com/blog/gpu-accelerated-json-data-processing-with-rapids/?ncid=so-link-395145-vt27#cid=an01_so-link_en-us">GPU-Accelerated JSON Data Processing with RAPIDS</A>
									<DT><A HREF="https://arxiv.org/pdf/2310.20707.pdf">WHAT‚ÄôS IN MY BIG DATA? (Allen Institute for AI)</A>
									<DT><A HREF="https://lilacml.com/blog/introducing-lilac.html">Introducing Lilac - üå∏ Lilac</A>
									<DT><A HREF="https://www.ibm.com/downloads/cas/X9W4O6BM">Granite Foundation Models</A>
									<DT><A HREF="https://beam.apache.org/">Apache Beam¬Æ: Batch and streaming data processing</A>
									<DT><A HREF="https://github.com/google/seqio">google/seqio: Task-based datasets, preprocessing, and evaluation for sequence models.</A>
									<DT><A HREF="https://github.com/nikitakit/sabertooth/blob/80ab40a6bd50f4e012e0e5489a5d9b8315bc6758/rust/create_pretraining_data/src/main.rs#L4">sabertooth/rust/create_pretraining_data/src/main.rs at 80ab40a6bd50f4e012e0e5489a5d9b8315bc6758 ¬∑ nikitakit/sabertooth</A>
									<DT><A HREF="https://voltrondata.com/benchmarks/theseus">Benchmarking Report: Theseus Engine</A>
									<DT><A HREF="https://github.com/NVIDIA/spark-rapids">NVIDIA/spark-rapids: Spark RAPIDS plugin - accelerate Apache Spark with GPUs</A>
									<DT><A HREF="https://www.youtube.com/watch?v=crRbIgNqetg">Lightning Talk: Tensor Query Processing - Matteo Interlandi, Microsoft - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>sample-quality</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2309.05463.pdf">Microsoft: Textbooks Are All You Need II: phi-1.5 technical report (phi-2)</A>
									<DT><A HREF="https://twitter.com/jerome_massot/status/1558122719698309126?s=20&t=ya4vHLuGuS3E88MMZhewsw">Dataset selection value</A>
									<DT><A HREF="https://twitter.com/ShayneRedford/status/1660670374206652419">When and where does pretraining (PT) data matter?</A>
									<DT><A HREF="https://twitter.com/jerome_massot/status/1558122719698309126?s=20&t=ya4vHLuGuS3E88MMZhewsw">Data-Centric AI</A>
									<DT><A HREF="https://github.com/shayne-longpre/a-pretrainers-guide/blob/main/A%20Pretrainer's%20Guide%20To%20Training%20Data.pdf">A Pretrainer's Guide To Training Data</A>
									<DT><A HREF="https://twitter.com/sbmaruf/status/1664965734831738881">Systematic Study of ChatGPT on Benchmarks Datasets</A>
								</DL><p>
								<DT><H3 FOLDED>training-data-document-processing</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/UDOP">microsoft/UDOP: Unifying Vision, Text, and Layout for Universal Document Processing</A>
									<DT><A HREF="https://twitter.com/_akhaliq/status/1742369195034099731">A layout-aware generative language model for multimodal document understanding (JPMorgan)</A>
									<DT><A HREF="https://twitter.com/mezaoptimizer/status/1725389571817378142">GPT-4V OCR problem of math -&gt; LaTeX</A>
								</DL><p>
								<DT><H3 FOLDED>training-data-synthetic-data</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/home">Synthetic Data from Diffusion Models Improves ImageNet</A>
									<DT><A HREF="https://arxiv.org/pdf/2312.06585.pdf">Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</A>
									<DT><H3 FOLDED>combine human and AI to annotate at super-human level!</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2302.04761">[2302.04761] Toolformer: Language Models Can Teach Themselves to Use Tools</A>
									<DT><A HREF="https://twitter.com/andrew_n_carr/status/1709754581184643089">Meta SAM: Progressive labeling with weak-&gt;strong models effectiveness</A>
									<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1709760111156093201">Computer vision papers data process</A>
									<DT><A HREF="https://arxiv.org/abs/2304.02643">[2304.02643] Segment Anything (SAM)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=zpqBiNV9XWE">A Tale of Tails: Model Collapse as a Change of Scaling Laws - YouTube</A>
									<DT><A HREF="https://arxiv.org/abs//2402.07043">[2402.07043] A Tale of Tails: Model Collapse as a Change of Scaling Laws</A>
									<DT><A HREF="https://huggingface.co/blog/cosmopedia">Cosmopedia: how to create large-scale synthetic data for pre-training Large Language Models</A>
									<DT><A HREF="https://www.youtube.com/watch?v=kgqDtfC_pRY">Synthetic data from scratch - Clip from livestream 7/May/2024 - Claude 3 Opus, phi-1, phi-3, ChatGPT - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>datasets-stack</H3>
								<DL><p>
									<DT><A HREF="https://github.com/allenai/unified-io-2">allenai/unified-io-2</A>
									<DT><A HREF="https://til.simonwillison.net/duckdb/remote-parquet">Summing columns in remote Parquet files using DuckDB</A>
									<DT><A HREF="https://medium.com/yandex/ytsaurus-exabyte-scale-storage-and-processing-system-is-now-open-source-42e7f5fa5fc6">YTsaurus: Exabyte-Scale Storage and Processing System</A>
									<DT><A HREF="https://github.com/google/seqio">google/seqio: Task-based datasets, preprocessing, and evaluation for sequence models.</A>
									<DT><A HREF="https://lilacml.com/blog/introducing-lilac.html">Introducing Lilac - üå∏ Lilac</A>
								</DL><p>
								<DT><A HREF="https://docs.ffcv.io/writing_datasets.html">Writing a dataset to FFCV format ‚Äî FFCV documentation</A>
								<DT><A HREF="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb: decanting the web for the finest text data at scale - a Hugging Face Space by HuggingFaceFW</A>
								<DT><A HREF="https://arxiv.org/abs/2406.11794v1">[2406.11794v1] DataComp-LM: In search of the next generation of training sets for language models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=HYvhj9W2qHQ">Cloud Native Data Loaders for Machine Learning Using Zarr and Xarray - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=W73Sp7XKuVE">Luca Soldaini - Curating Pretrain Data (AI2 / Dolma) - YouTube</A>
								<DT><A HREF="https://x.com/ZackAnkner/status/1797595682439901565">"New paper where we explore using a small LM‚Äôs perplexity to prune the pretraining data for larger LMs. We find that small LMs can prune data for up to 30x larger LMs, data pruning works in the overtrained and data-constrained regimes, and more! https://t.co/XYbI0Ijois" / X</A>
								<DT><A HREF="https://arxiv.org/abs/2405.20541">[2405.20541] Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models</A>
								<DT><A HREF="https://x.com/robertnishihara/status/1798759330868760680">(Robert Nishihara) Ray Server: Dataset preparation</A>
								<DT><A HREF="https://x.com/iamtrask/status/1797528618429468966">(1) Andrew Trask en X: "fantastic work by @huggingface! this stuff really moves the field forward. in 2015 I was listening to Thomas Mikolov talk about word2vec... and he didn't explain it as an algorithmic innovation... he described it as something like ~"i wanted to simplify the architecture so i" / X</A>
								<DT><A HREF="https://x.com/Vaishaal/status/1803198064364232918">(1) Vaishaal Shankar en X: "I am really excited to introduce DataComp for Language Models (DCLM), our new testbed for controlled dataset experiments aimed at improving language models. 1/x https://t.co/uNe5mUJJxb" / X</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-evaluation</H3>
							<DL><p>
								<DT><H3 FOLDED>TEST SET CONTAMINATION</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2310.17623.pdf">PROVING TEST SET CONTAMINATION IN BLACK BOX LANGUAGE MODELS</A>
									<DT><A HREF="https://twitter.com/suchenzang/status/1701615026648605095">Susan Zhang: I think Phi-1.5 trained on the benchmarks. Particularly, GSM8K.</A>
								</DL><p>
								<DT><H3 FOLDED>lm-evaluation-harness</H3>
								<DL><p>
									<DT><A HREF="https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file">EleutherAI/lm-evaluation-harness: A framework for few-shot evaluation of language models.</A>
									<DT><A HREF="https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md">lm-evaluation-harness/docs/interface.md at main ¬∑ EleutherAI/lm-evaluation-harness</A>
									<DT><A HREF="https://imbue.com/research/70b-evals/">Ensuring accurate model evaluations: open-sourced, cleaned datasets for models that reason and code - imbue</A>
									<DT><A HREF="https://x.com/Thom_Wolf/status/1805985020168769988">(1) Thomas Wolf en X: "Very excited to release the new version of the Open LLM Leaderboard, v2 ‚Äì it's much harder than the previous version as you can see on some of the v1 &amp;lt;&amp;gt; v2 scores comparison I'm posting below Updated: As open models keeps getting better and saturating some of the evaluations it https://t.co/zv6dSQCnhJ" / X</A>
								</DL><p>
								<DT><A HREF="https://github.com/EleutherAI/lm-evaluation-harness">EleutherAI/lm-evaluation-harness: A framework for few-shot evaluation of language models.</A>
								<DT><A HREF="https://www.jasonwei.net/blog/evals">Successful language model evals ‚Äî Jason Wei</A>
								<DT><A HREF="https://arxiv.org/pdf/2311.12022.pdf">GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark (Anthropic)</A>
								<DT><A HREF="https://x.com/haileysch__/status/1793957143910953185">(1) Hailey Schoelkopf en X: "My favorite bit in this paper: I and @bbrabbasi wrote an appendix formalizing what is done evaluating models with loglikelihood multiple choice and perplexity evals. afaik, none of this has been written up in one place in most papers and just been tacitly assumed before! https://t.co/OaZW7n35yb" / X</A>
								<DT><A HREF="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">Evaluation Metrics for Language Modeling</A>
								<DT><A HREF="https://cs.stanford.edu/people/karpathy/tsnejs/csvdemo.html">Perplexity</A>
								<DT><A HREF="https://twitter.com/tanmingxing/status/1519787578160869376">PolyLoss</A>
								<DT><A HREF="https://github.com/Sanger2000/human-eval/blob/master/run.py">HumanEval: pass@K</A>
								<DT><A HREF="https://twitter.com/AIatMeta/status/1715041427283902793">Meta: GenBench (Generalizatio testing)</A>
								<DT><A HREF="https://sjmielke.com/comparing-perplexities.htm">Can you compare perplexity across different segmentations?</A>
								<DT><A HREF="https://arxiv.org/abs/2311.07689?utm_source=twitter&utm_medium=organic_social&utm_campaign=research&utm_content=image">[2311.07689] MART: Improving LLM Safety with Multi-round Automatic Red-Teaming</A>
								<DT><A HREF="https://arxiv.org/abs/2311.07911">[2311.07911] Instruction-Following Evaluation for Large Language Models</A>
								<DT><A HREF="https://twitter.com/huihan_li/status/1724334480339710287">LINK: framework for systematically generating data in the long-tail distribution</A>
								<DT><A HREF="https://github.com/OpenBMB/UltraEval">OpenBMB/UltraEval: An open source framework for evaluating foundation models.</A>
								<DT><A HREF="https://cdn.openai.com/openai-preparedness-framework-beta.pdf">OpenAI: Preparedness Framework (Beta)</A>
								<DT><A HREF="https://www.lesswrong.com/posts/hQPfLsDKWtdvMwyyr/on-openai-s-preparedness-framework">On OpenAI‚Äôs Preparedness Framework ‚Äî LessWrong</A>
								<DT><A HREF="https://cdn.openai.com/papers/weak-to-strong-generalization.pdf">WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION</A>
								<DT><A HREF="https://arxiv.org/pdf/2312.11444.pdf">An In-depth Look at Gemini's Language Abilities</A>
								<DT><A HREF="https://crfm.stanford.edu/helm/v0.2.2/?group=core_scenarios">Holistic Evaluation of Language Models (HELM)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=2CIIQ5KZWUM">Evaluating LLM-based Applications - YouTube</A>
								<DT><A HREF="https://arxiv.org/abs/2311.15930">[2311.15930] WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models</A>
								<DT><A HREF="https://arxiv.org/abs/2311.12983">[2311.12983] GAIA: a benchmark for General AI Assistants</A>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1727111453117997185">Claude: Decline to answer</A>
								<DT><A HREF="https://www.parea.ai/">Parea AI</A>
								<DT><A HREF="https://github.com/meta-llama/llama3/blob/main/eval_details.md">llama3/eval_details.md at main ¬∑ meta-llama/llama3</A>
								<DT><A HREF="https://github.com/openai/simple-evals">openai/simple-evals</A>
								<DT><A HREF="https://twitter.com/gallabytes/status/1784694561861947823">ways of evaluating a langauge encoder</A>
								<DT><A HREF="https://twitter.com/srush_nlp/status/1785293229165818298">LMSys: Chatbot Arena</A>
								<DT><A HREF="https://arxiv.org/abs/2405.14782">[2405.14782] Lessons from the Trenches on Reproducible Evaluation of Language Models</A>
								<DT><A HREF="https://huggingface.co/spaces/open-llm-leaderboard/blog">Open-LLM performances are plateauing, let‚Äôs make the leaderboard steep again - a Hugging Face Space by open-llm-leaderboard</A>
								<DT><A HREF="https://www.youtube.com/watch?v=3gb-ZkVRemQ&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=27">Stanford CS25: V4 I Jason Wei &amp; Hyung Won Chung of OpenAI - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>tokenizer</H3>
							<DL><p>
								<DT><H3 FOLDED>BPE</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/docs/transformers/tokenizer_summary">Byte-Pair Encoding (BPE)</A>
									<DT><A HREF="https://github.com/youkaichao/fast_bpe_tokenizer">youkaichao/fast_bpe_tokenizer: fast bpe tokenizer, simple to understand, easy to use</A>
								</DL><p>
								<DT><H3 FOLDED>tiktokenizer</H3>
								<DL><p>
									<DT><A HREF="https://tiktokenizer.vercel.app/">Tiktokenizer</A>
									<DT><A HREF="https://github.com/openai/tiktoken">openai/tiktoken: tiktoken is a fast BPE tokeniser for use with OpenAI's models.</A>
								</DL><p>
								<DT><H3 FOLDED>sentencepiece</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=2QO3vzwHXhg&t=3551s">sentencepiece_model pb ParseFromString deserialize tokenizer vocab dict</A>
									<DT><A HREF="https://github.com/google/sentencepiece">google/sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation.</A>
									<DT><A HREF="https://github.com/google/sentencepiece/blob/master/python/add_new_vocab.ipynb">sentencepiece/python/add_new_vocab.ipynb</A>
								</DL><p>
								<DT><H3 FOLDED>tokenizers-rust</H3>
								<DL><p>
									<DT><A HREF="https://github.com/guillaume-be/rust-tokenizers">guillaume-be/rust-tokenizers: Rust-tokenizer offers high-performance tokenizers for modern language models, including WordPiece, Byte-Pair Encoding (BPE) and Unigram (SentencePiece) models</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=zduSFxRajkE">Let's build the GPT Tokenizer</A>
								<DT><A HREF="https://arxiv.org/abs/2310.05737">Language Model Beats Diffusion -- Tokenizer is Key to Visual Gen</A>
								<DT><A HREF="https://gist.github.com/Winston-503/39f7bc5fe1d93dcfa38361b119fbe2d7">json_yaml_tokens.py</A>
								<DT><A HREF="https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html">Exploring BERT's Vocabulary</A>
							</DL><p>
							<DT><H3 FOLDED>context-window</H3>
							<DL><p>
								<DT><H3 FOLDED>Linear Transformers</H3>
								<DL><p>
									<DT><A HREF="https://openai.com/research/requests-for-research-2">Transformers with Linear Attention</A>
								</DL><p>
								<DT><H3 FOLDED>KV cache</H3>
								<DL><p>
									<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8">LLM Inference Series: 4. KV caching, a deeper look | by Pierre Lienhart (main)</A>
									<DT><A HREF="https://arxiv.org/abs/2403.05527">[2403.05527] GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM</A>
									<DT><A HREF="https://openreview.net/pdf?id=e9D2STGwLJ">Adaptive KV Cache Compression for LLMs</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen">DeepSpeed/blogs/deepspeed-fastgen at master ¬∑ microsoft/DeepSpeed</A>
									<DT><A HREF="https://twitter.com/bio_bootloader/status/1790539323600949542">context caching for Google Gemini</A>
									<DT><A HREF="https://github.com/madsys-dev/deepseekv2-profile/blob/main/workspace/blog/optimizing-mla.md">deepseekv2-profile/workspace/blog/optimizing-mla.md at main ¬∑ madsys-dev/deepseekv2-profile</A>
									<DT><A HREF="https://x.com/haileysch__/status/1787583052039802887">MLA TL:DR</A>
									<DT><A HREF="https://x.com/teortaxesTex/status/1790407782325420277/photo/4">Teortaxes en X: "he blog on MLA by Jianlin SU (the GOD creator of ROPE) is also a good reference</A>
									<DT><A HREF="https://x.com/teortaxesTex/status/1793180779943543246">most KV cache is redundant, actually</A>
									<DT><A HREF="https://github.com/google/maxtext/blob/f7ee8c636fd500995e76c227b351d48680ab7890/MaxText/layers/attentions.py#L435">maxtext/MaxText/layers/attentions.py</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2302.10866.pdf">Hyena Hierarchy</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2023-03-07-hyena">Hyena Hierarchy: Towards Larger Convolutional Language Models ¬∑ Hazy Research</A>
								<DT><A HREF="https://github.com/HazyResearch/safari">HazyResearch/safari: Convolutions for Sequence Modeling</A>
								<DT><A HREF="https://arxiv.org/abs/2305.01625">[2305.01625] Unlimiformer: Long-Range Transformers with Unlimited Length Input</A>
								<DT><A HREF="https://twitter.com/haoliuhl/status/1664396377252667393">Blockwise Parallel Transformer</A>
								<DT><A HREF="https://arxiv.org/pdf/2305.19370.pdf">Blockwise Parallel Transformer for Large Context Models</A>
								<DT><A HREF="https://www.cerebras.net/blog/variable-sequence-length-training-for-long-context-large-language-models/?utm_content=258053990&utm_medium=social&utm_source=linkedin&hss_channel=lcp-10858000">Variable Sequence Length Training for Long-Context Large Language Models - Cerebras</A>
								<DT><A HREF="https://twitter.com/arankomatsuzaki/status/1637612922934382593">CoLT5</A>
								<DT><A HREF="https://arxiv.org/abs/2307.03172">[2307.03172] Lost in the Middle: How Language Models Use Long Contexts</A>
								<DT><A HREF="https://arxiv.org/pdf/2108.12409.pdf">ALiBi</A>
								<DT><A HREF="https://twitter.com/theemozilla/status/1726650665718685821">Yarn-LLAMA-2-70b-32k</A>
								<DT><A HREF="https://www.youtube.com/watch?v=_u47Esw4d8o">[short] On the Long Range Abilities of Transformers - YouTube</A>
								<DT><A HREF="https://arxiv.org/abs/2404.07143">[2404.07143] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</A>
								<DT><A HREF="https://arxiv.org/abs/2402.13753">[2402.13753] LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens</A>
								<DT><A HREF="https://arxiv.org/abs/2310.01889">[2310.01889] Ring Attention with Blockwise Transformers for Near-Infinite Context</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MRTTGMlKgb8">Leave No Context Behind</A>
								<DT><A HREF="https://arxiv.org/abs/2404.08801">[2404.08801] Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length</A>
								<DT><A HREF="https://www.youtube.com/watch?v=r_UBBfTPcF0">Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=T5haJyhVPCY">Leave No Context Behind Efficient Infinite Context Transformers with Infini attention Google 2024 - YouTube</A>
								<DT><A HREF="https://huggingface.co/crusoeai">crusoeai (Crusoe AI)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=RCSvpYb90qE">LongRoPE &amp; Theta Scaling to 1 Mio Token</A>
								<DT><A HREF="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#gemini-model-updates</A>
								<DT><A HREF="https://kexue.fm/archives/10122">Transformer Upgrade Road: 18. RoPE Base Design Principles</A>
								<DT><A HREF="https://arxiv.org/abs/2109.00301">[2109.00301] $\infty$-former: Infinite Memory Transformer</A>
								<DT><A HREF="https://arxiv.org/abs/2405.07719">[2405.07719] USP: A Unified Sequence Parallelism Approach for Long Context Generative AI</A>
								<DT><A HREF="https://arxiv.org/abs/2403.00071">[2403.00071] Resonance RoPE: Improving Context Length Generalization of Large Language Models</A>
								<DT><A HREF="https://arxiv.org/abs/2309.00071">[2309.00071] YaRN: Efficient Context Window Extension of Large Language Models</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-architecture</H3>
							<DL><p>
								<DT><H3 FOLDED>seq2seq</H3>
								<DL><p>
									<DT><H3 FOLDED>Transformer</H3>
									<DL><p>
										<DT><H3 FOLDED>Transformer++</H3>
										<DL><p>
											<DT><A HREF="https://arxiv.org/abs/2312.00752">[2312.00752] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</A>
											<DT><A HREF="https://twitter.com/rasbt/status/1729123424990068743">Simple Transformer Block</A>
											<DT><A HREF="https://github.com/RobertRiachi/nanoPALM">RobertRiachi/nanoPALM</A>
										</DL><p>
										<DT><H3 FOLDED>transformer-lectures</H3>
										<DL><p>
											<DT><A HREF="https://www.youtube.com/watch?v=orDKvo8h71o">Stanford CS25: V4 I Hyung Won Chung of OpenAI - YouTube</A>
											<DT><A HREF="https://www.youtube.com/watch?v=iqmjzecbJHE">Orignal transformer paper "Attention is all you need" introduced by a layman | Shawn's ML Notes - YouTube</A>
										</DL><p>
										<DT><A HREF="https://gist.github.com/sophiawisdom/ccdff5b7ebcd782393dbc5be3f0866f9">shittytransformer.py</A>
										<DT><A HREF="https://github.com/xjdr-alt/simple_transformer">xjdr-alt/simple_transformer: Simple Transformer in Jax</A>
										<DT><A HREF="https://github.com/joschu/jax-exp/blob/master/jax_transformer.py#L96">John Schulman: jax-exp/jax_transformer.py at master</A>
										<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes ‚Äî Good Coding Style | by Noam Shazeer | Medium</A>
										<DT><A HREF="https://github.com/openai/finetune-transformer-lm/">openai/finetune-transformer-lm: Code and model for the paper "Improving Language Understanding by Generative Pre-Training"</A>
										<DT><A HREF="https://docs.google.com/presentation/d/1u05yQQaw4QXLVYGLI6o3YoFHv6eC3YN8GvWD8JMumpE/edit#slide=id.g2885e521b53_0_0">Hyung Won (OpenAI): Shaping the future of AI from the history of Transformer</A>
										<DT><A HREF="https://x.com/hwchung27/status/1800676312916656592">Shaping the future of AI from the history of Transformer</A>
										<DT><A HREF="https://github.com/hyunwoongko/transformer">transformer step-by-step impl</A>
									</DL><p>
									<DT><H3 FOLDED>GPT</H3>
									<DL><p>
										<DT><H3 FOLDED>GPT-2</H3>
										<DL><p>
											<DT><H3 FOLDED>cloneofsimo-min-max-gpt</H3>
											<DL><p>
												<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt">cloneofsimo/min-max-gpt: Minimal (400 LOC) implementation Maximum (multi-node, FSDP) GPT training</A>
												<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
											</DL><p>
											<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt">cloneofsimo/min-max-gpt: Minimal (400 LOC) implementation Maximum (multi-node, FSDP) GPT training</A>
											<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
											<DT><A HREF="https://kexue.fm/archives/9529">Why are current LLMs all decoder-only architectures?</A>
											<DT><A HREF="https://github.com/openai/gpt-2">openai/gpt-2: Code for the paper "Language Models are Unsupervised Multitask Learners" (Tensorflow impl)</A>
											<DT><A HREF="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py">huggingface/transformers PyTorch implementation</A>
											<DT><A HREF="https://www.youtube.com/watch?v=l8pRSuU81PU">Let's reproduce GPT-2 (124M) - YouTube</A>
											<DT><A HREF="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</A>
											<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
											<DT><A HREF="https://x.com/kellerjordan0/status/1803566078985117903">Sophia optimizer</A>
											<DT><A HREF="https://github.com/KellerJordan/modded-nanogpt/tree/sophia">KellerJordan/modded-nanogpt at sophia</A>
											<DT><A HREF="https://github.com/pytorch-labs/gpt-fast">pytorch-labs/gpt-fast: Simple and efficient pytorch-native transformer text generation in &lt;1000 LOC of python.</A>
											<DT><A HREF="https://github.com/cchan/nanoGPT-fp8">cchan/nanoGPT-fp8</A>
											<DT><A HREF="https://x.com/itsclivetime/status/1655515089506820097">(1) Clive Chan en X: "WIP FP8 training on consumer graphics cards - üßµ/4 I hacked nanoGPT to use TransformerEngine on RTX 4090 and ran a few iterations of GPT-2 training: - nanoGPT Block (+flashattn) =&amp;gt; TE TransformerLayer (both BF16): 15% faster - BF16 =&amp;gt; FP8: additional +18% https://t.co/cJNWehoGeu" / X</A>
											<DT><A HREF="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/quickstart.html">Getting Started ‚Äî Transformer Engine 1.7.0 documentation</A>
											<DT><A HREF="https://openai.com/index/image-gpt/">Image GPT | OpenAI</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1805335482999750773">crossentropy loss is the main memory bottleneck (memory-bound)</A>
											<DT><A HREF="https://github.com/Lightning-AI/litgpt">Lightning-AI/litgpt: Load, pretrain, finetune, deploy 20+ LLMs on your own data. Uses state-of-the-art techniques: flash attention, FSDP, 4-bit, LoRA, and more.</A>
											<DT><A HREF="https://github.com/mgmalek/efficient_cross_entropy">mgmalek/efficient_cross_entropy</A>
											<DT><A HREF="https://mp.weixin.qq.com/s/8F3eAHDBjQkHHBmrAEoOfw">"Illustrated Large Model Training: Data Parallel Part 2 (ZeRO, Zero Redundancy Optimization)"</A>
											<DT><A HREF="https://zhuanlan.zhihu.com/p/624740065">Analyze the parameters, computation, intermediate activation, and KV cache of the transformer model</A>
											<DT><A HREF="https://huggingface.co/spaces/BBuf/megatron-lm-parallel-group-playground">Megatron Lm Parallel Group Playground - a Hugging Face Space by BBuf</A>
											<DT><A HREF="https://tspeterkim.github.io/posts/mixed-precision-from-scratch">Mixed Precision Training from Scratch | Taeksang Peter Kim</A>
											<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes ‚Äî Good Coding Style | by Noam Shazeer | Medium</A>
											<DT><A HREF="https://kidger.site/thoughts/jaxtyping/">No more shape errors! Type annotations for the shape+dtype of tensors/arrays. ¬∑ Patrick Kidger</A>
											<DT><A HREF="http://giantpandacv.com/project/PyTorch/AI%20Infra%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8A%E5%9C%A8LLM%E8%AE%AD%E7%BB%83%E4%B8%AD%E5%87%8F%E5%B0%91%E6%BF%80%E6%B4%BB%E5%80%BC%E5%86%85%E5%AD%98%E3%80%8B/">AI Infra paper reading: "Reducing activation value memory in LLM training"</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1805564665428164915">sparsity pattern 2:4 PyTorch training</A>
											<DT><A HREF="https://pytorch.org/blog/accelerating-neural-network-training/">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity | PyTorch</A>
											<DT><A HREF="https://imbue.com/research/70b-infrastructure/">From bare metal to a 70B model: infrastructure set-up and scripts - imbue</A>
											<DT><A HREF="https://imbue.com/research/70b-carbs/">Open-sourcing CARBS: a cost-effective hyperparameter optimizer that helps scale small experiments to large language models - imbue</A>
											<DT><A HREF="https://imbue.com/research/70b-intro/">Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings - imbue</A>
											<DT><A HREF="https://github.com/imbue-ai/cluster-health/tree/master">imbue-ai/cluster-health</A>
											<DT><A HREF="https://github.com/imbue-ai/carbs">imbue-ai/carbs: Cost aware hyperparameter tuning algorithm</A>
											<DT><A HREF="https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e">GPT from Scratch with MLX. Define and train GPT-2 on your MacBook | by Pranav Jadhav | Jun, 2024 | Towards Data Science</A>
											<DT><A HREF="https://github.com/pytorch/torchtune">pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning</A>
											<DT><A HREF="https://github.com/pytorch/ao">pytorch/ao: Create and integrate custom data types, layouts and kernels with up to 2x speedups with 65% less VRAM for inference and training</A>
										</DL><p>
										<DT><A HREF="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf">Supplement GPT-3 few-shot generalizable learners</A>
									</DL><p>
									<DT><H3 FOLDED>T5</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/pdf/1910.10683.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</A>
										<DT><A HREF="https://twitter.com/YiTayML/status/1651932898512211970">~2 x the parameters for the same compute cost. Basically free model sparsity (sparse w.r.t to enc/dec blocks).</A>
										<DT><A HREF="https://twitter.com/YiTayML/status/1668302949276356609">(Yi Tay): Some thoughts/observations (T5 models at 30B &amp; 65B)</A>
										<DT><A HREF="https://arxiv.org/abs/2306.04757">InstructEval: Table 2 (Flan vs Alpaca) EncDec vs Dec param claim</A>
										<DT><A HREF="https://arxiv.org/abs/2207.10551">[2207.10551] Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?</A>
									</DL><p>
									<DT><H3 FOLDED>EncDec</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=ORzGEnHTSfk">What BERT Can‚Äôt Do: The Transformer's Decoder</A>
									</DL><p>
									<DT><H3 FOLDED>Llama</H3>
									<DL><p>
										<DT><H3 FOLDED>Llama 3</H3>
										<DL><p>
											<DT><A HREF="https://ai.meta.com/blog/meta-llama-3/">Introducing Meta Llama 3: The most capable openly available LLM to date</A>
											<DT><A HREF="https://twitter.com/astonzhangAZ/status/1780990210576441844">pre-training, human data, scaling, long context, post-training &amp; eval</A>
											<DT><A HREF="https://github.com/meta-llama/llama3">meta-llama/llama3: the main Llama 3 GitHub site - will be moved under Meta-Llama</A>
											<DT><A HREF="https://github.com/meta-llama/llama3/blob/main/eval_details.md">llama3/eval_details.md at main ¬∑ meta-llama/llama3</A>
											<DT><A HREF="https://twitter.com/karpathy/status/1781047292486914189">(1) Andrej Karpathy en X: "The model card has some more interesting info too: https://t.co/EzNGMu57am Note that Llama 3 8B is actually somewhere in the territory of Llama 2 70B, depending on where you look. This might seem confusing at first but note that the former was trained for 15T tokens, while the..." / X</A>
											<DT><A HREF="https://twitter.com/karpathy/status/1781028605709234613">(1) Andrej Karpathy en X: "Congrats to @AIatMeta on Llama 3 release!! üéâ https://t.co/fSw615zE8S Notes: Releasing 8B and 70B (both base and finetuned) models, strong-performing in their model class (but we'll see when the rankings come in @ @lmsysorg :)) 400B is still training, but already encroaching..." / X</A>
											<DT><A HREF="https://github.com/naklecha/llama3-from-scratch">naklecha/llama3-from-scratch: llama3 implementation one matrix multiplication at a time</A>
											<DT><A HREF="https://x.com/danielhanchen/status/1792553901524472311">Llama 3 arch on 1 mathematics page</A>
											<DT><A HREF="https://github.com/meta-llama/llama3/blob/main/llama/model.py">llama3/llama/model.py</A>
											<DT><A HREF="https://huggingface.co/imbue/llama3-question-quality">imbue/llama3-question-quality ¬∑ Hugging Face</A>
										</DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2307.09288.pdf">Llama 2: Open Foundation and Fine-Tuned Chat Models</A>
										<DT><A HREF="https://arxiv.org/abs/2302.13971">[2302.13971] LLaMA: Open and Efficient Foundation Language Models</A>
										<DT><A HREF="https://github.com/alibaba/Megatron-LLaMA">alibaba/Megatron-LLaMA: Best practice for training LLaMA models in Megatron-LM</A>
										<DT><A HREF="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">Building Meta‚Äôs GenAI Infrastructure - Engineering at Meta</A>
									</DL><p>
									<DT><H3 FOLDED>RNN</H3>
									<DL><p>
										<DT><H3 FOLDED>LSTM</H3>
										<DL><p>
											<DT><H3 FOLDED>xLSTM</H3>
											<DL><p>
												<DT><A HREF="https://x.com/antferdom/status/1799474799661691014">xLSTM won't replace the Transformer. Two bitter lessons</A>
											</DL><p>
										</DL><p>
										<DT><A HREF="https://kexue.fm/archives/10017">Chapter on Space and Time: Viewing Attention as a Quadratic RNN</A>
										<DT><A HREF="https://www.youtube.com/watch?v=FRy7eLuosic">Transformers are Multi-State RNNs</A>
										<DT><A HREF="https://web.stanford.edu/~jurafsky/slp3/9.pdf">RNN and LSTM</A>
									</DL><p>
									<DT><H3 FOLDED>Gemma</H3>
									<DL><p>
										<DT><H3 FOLDED>Gemma 2</H3>
										<DL><p>
											<DT><A HREF="https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf">Gemma 2: Improving Open Language Models at a Practical Size</A>
											<DT><A HREF="https://x.com/danielhanchen/status/1806372357684220308">Gemma 2 Analysis</A>
											<DT><A HREF="https://x.com/mvpatel2000/status/1806344519333323258">tech report dicussion (Mihir Patel)</A>
											<DT><A HREF="https://huggingface.co/blog/gemma2">Welcome Gemma 2 - Google‚Äôs new open LLM</A>
										</DL><p>
										<DT><A HREF="https://ai.google.dev/gemma">Gemma - a family of lightweight, state-of-the art open models from Google ¬†|¬† Google for Developers</A>
										<DT><A HREF="https://github.com/google/gemma.cpp">google/gemma.cpp: lightweight, standalone C++ inference engine for Google's Gemma models.</A>
										<DT><A HREF="https://arxiv.org/abs/2403.08295">[2403.08295] Gemma: Open Models Based on Gemini Research and Technology</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>architecture-efficient</H3>
								<DL><p>
									<DT><H3 FOLDED>Parallel Layers</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://github.com/JeanKaddour/NoTrainNoGain">Revisiting Efficient Training Algorithms For Transformer-based Language Models</A>
									<DT><A HREF="https://github.com/Lightning-AI/lit-gpt">Lightning-AI/lit-gpt</A>
									<DT><A HREF="https://twitter.com/ZihangDai/status/1281349893873897478/photo/1">(1) Zihang Dai (@ZihangDai) xAI: Funnel-Transformer</A>
									<DT><A HREF="https://github.com/laiguokun/Funnel-Transformer">laiguokun/Funnel-Transformer</A>
									<DT><A HREF="https://arxiv.org/pdf/2009.06732.pdf">Efficient Transformers: A Survery</A>
									<DT><A HREF="https://arxiv.org/abs/2110.12894">[2110.12894] The Efficiency Misnomer</A>
									<DT><A HREF="https://arxiv.org/abs/2009.06732">[2009.06732] Efficient Transformers: A Survey</A>
								</DL><p>
								<DT><H3 FOLDED>Attention</H3>
								<DL><p>
									<DT><H3 FOLDED>Multi-Query Attention</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2310.06825">[2310.06825] Mistral 7B</A>
									</DL><p>
									<DT><H3 FOLDED>GQA</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html">Rethinking Attention with Performers ‚Äì Google Research Blog</A>
									<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention">HazyResearch/flash-attention: Fast and memory-efficient exact attention</A>
									<DT><A HREF="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">Self-Attention</A>
									<DT><A HREF="https://arxiv.org/pdf/2205.14135v2.pdf">Flash Attention</A>
									<DT><A HREF="https://theaisummer.com/self-attention/">Why multi-head self attention works</A>
								</DL><p>
								<DT><H3 FOLDED>Mixture of Expert Models</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-MoE</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/blog/training-moes/?utm_content=298456196&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Training MoEs at Scale with PyTorch | PyTorch</A>
										<DT><A HREF="https://x.com/mvpatel2000/status/1806391255297147218">Training MoEs at Scale with PyTorch (DBRX)</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</A>
									<DT><A HREF="https://arxiv.org/abs/2201.05596">[DeepSpeed-MoE: Advancing MoE Inference and Training</A>
									<DT><A HREF="https://github.com/stanford-futuredata/megablocks">stanford-futuredata/megablocks</A>
									<DT><A HREF="https://mistral.ai/news/mixtral-of-experts/">Mixtral of experts | Mistral AI | Open source models</A>
									<DT><A HREF="https://twitter.com/amanrsanger/status/1690072802454568960">WHY</A>
									<DT><A HREF="https://github.com/lucidrains/st-moe-pytorch">lucidrains/st-moe-pytorch: Implementation of ST-Moe</A>
									<DT><A HREF="https://arxiv.org/abs/2309.05444">[2309.05444] Pushing Mixture of Experts to the Limit</A>
									<DT><A HREF="https://www.youtube.com/watch?v=wLjJ34ygZVc">HuggingGPT</A>
									<DT><A HREF="https://twitter.com/arankomatsuzaki/status/1757229323126243620">Scaling Laws for Fine-Grained Mixture of Experts</A>
									<DT><A HREF="https://twitter.com/Tgale96/status/1773342371993751830">databricks/megablocks</A>
									<DT><A HREF="https://twitter.com/tedzadouri/status/1701579765973713285">Pushing MoE to the Limit: Extremely Parameter Efficient MoE</A>
									<DT><A HREF="https://x.com/cloneofsimo/status/1800228889341599963">Why are MoEs effective?</A>
									<DT><A HREF="https://arxiv.org/pdf/2305.14705">Mixture-of-Experts Meets Instruction Tuning</A>
									<DT><A HREF="http://incompleteideas.net/papers/sutton-86.pdf">Two Problems With Backpropagation and other steepest-descent learning procedures for networks</A>
									<DT><A HREF="https://x.com/teortaxesTex/status/1805066323409707336">(1) Teortaxes‚ñ∂Ô∏è en X: "On why serious labs (CAI, DS, probably all of the Western frontier) work on reducing KV cache You need MoEs with higher sparsity to save compute. But more experts = cache up = batch size down = MFU down = you lose to basic stuff like Mixtral, or even dense https://t.co/VUtsNg8A8I https://t.co/uWDxjtpevX" / X</A>
									<DT><A HREF="https://arxiv.org/abs/2404.02852">[2404.02852] Toward Inference-optimal Mixture-of-Expert Large Language Models</A>
									<DT><A HREF="https://pytorch.org/blog/training-moes/?utm_content=298456196&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Training MoEs at Scale with PyTorch | PyTorch</A>
									<DT><A HREF="https://x.com/mvpatel2000/status/1806391255297147218">Training MoEs at Scale with PyTorch (DBRX)</A>
								</DL><p>
								<DT><H3 FOLDED>State Space Model</H3>
								<DL><p>
									<DT><H3 FOLDED>SSM-Samba</H3>
									<DL><p>
										<DT><A HREF="https://github.com/microsoft/Samba">microsoft/Samba: Official implementation of "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling"</A>
									</DL><p>
									<DT><H3 FOLDED>SSM-simba</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2403.15360">[2403.15360] SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2312.00752">[2312.00752] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</A>
									<DT><A HREF="https://www.youtube.com/watch?v=hN0XvyuWlsM">Math Reading Group - State Space Models - (04/03/2024) - YouTube</A>
									<DT><A HREF="https://github.com/sophiawisdom/ssms">sophiawisdom/ssms: GPU kernels for state space models</A>
									<DT><A HREF="https://kexue.fm/archives/10114">Revisiting SSM (I): Linear Systems and HiPPO Matrix</A>
									<DT><A HREF="https://kexue.fm/archives/10180">ÈáçÊ∏©SSMÔºàÂõõÔºâÔºöÊúâÁêÜÁîüÊàêÂáΩÊï∞ÁöÑÊñ∞ËßÜËßí - ÁßëÂ≠¶Á©∫Èó¥|Scientific Spaces</A>
								</DL><p>
								<DT><H3 FOLDED>Parallel Layers</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>Activation Function</H3>
								<DL><p>
									<DT><H3 FOLDED>SwiGLU</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2002.05202">(SwiGLU) GLU Variants Improve Transformer</A>
									</DL><p>
									<DT><H3 FOLDED>GELU</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/1606.08415">[1606.08415] Gaussian Error Linear Units (GELUs)</A>
										<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html">GELU ‚Äî PyTorch 2.3 documentation</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/39853">[proposal] Add approx variant option to F.gelu ¬∑ Issue #39853 ¬∑ pytorch/pytorch (approximate=tanh)</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>Inductive Bias</H3>
								<DL><p>
									<DT><H3 FOLDED>No bias</H3>
									<DL><p>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2207.10551">[2207.10551] Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?</A>
								<DT><A HREF="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer (new version)</A>
								<DT><A HREF="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</A>
								<DT><A HREF="https://github.com/Mooler0410/LLMsPracticalGuide">Mooler0410/LLMsPracticalGuide</A>
								<DT><A HREF="https://twitter.com/m__dehghani/status/1524796275052498945">architecture of the model is not actually that important (T5 UL2)</A>
								<DT><A HREF="https://kipp.ly/blog/">Transformers Taxonomy</A>
								<DT><A HREF="https://bbycroft.net/llm">LLM Visualization</A>
								<DT><A HREF="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits</A>
								<DT><A HREF="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</A>
								<DT><A HREF="https://arxiv.org/abs/2305.19370">[2305.19370] Blockwise Parallel Transformer for Long Context Large Models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=1aXOXHA7Jcw&t=3032s">Greg Yang | Large N Limits: Random Matrices &amp; Neural Networks</A>
								<DT><A HREF="https://www.youtube.com/watch?v=KV5gbOmHbjU">A Mathematical Framework for Transformer Circuits</A>
								<DT><A HREF="https://www.youtube.com/watch?v=P2LTAUO1TdA">Change of basis | Chapter 13, Essence of linear algebra</A>
								<DT><A HREF="https://www.youtube.com/watch?v=PnWOeIgl3GA">Language Modeling with Reduced Densities</A>
								<DT><A HREF="https://atcold.github.io/NYU-DLSP21/en/week10/10-3/">Transformer Encoder-predictor-decoder architecture (NYU)</A>
								<DT><A HREF="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer ‚Äì Jay Alammar</A>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1651927473884655616">(Yi Tay): Clarifying misconceptions about architectures (training obj)</A>
								<DT><A HREF="https://gist.github.com/sbmaruf/4832bd9a7f66df3c8bab8ab913a7a31a">Architecture config mapping vs T5 Table 2</A>
								<DT><A HREF="https://x.com/andrewgwils/status/1800532164418867245">Andrew Gordon Wilson en X: "Another major barrier is hypers -- initializations, LR, etc. You could easily try a new structure and not realize why it fails. The naive hypers just don‚Äôt work. But adapting the great work of @TheGregYang on muP to structure-aware initialization, we achieve exciting results! 4/8 https://t.co/NucLouwFIF" / X</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-training-objectives</H3>
							<DL><p>
								<DT><H3 FOLDED>Causal</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/haileysch__/status/1691483230761857024">Causal &gt;&gt; UL2</A>
								</DL><p>
								<DT><H3 FOLDED>Non-Causal</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>UL2</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/Megatron-LM/pull/268">Megatron-LM: Add UL2 data sampling and pretraining</A>
								</DL><p>
								<DT><H3 FOLDED>Self-Play</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2312.06585.pdf">Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</A>
									<DT><A HREF="https://github.com/lucidrains/ReST-EM-pytorch">lucidrains/ReST-EM-pytorch: Implementations and explorations into the ReSTùê∏ùëÄ algorithm in the new deepmind paper "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"</A>
								</DL><p>
								<DT><H3 FOLDED>Self-Training</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2312.06585.pdf">Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</A>
								</DL><p>
								<DT><H3 FOLDED>Generative Information Retrieval</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/presentation/d/19lAeVzPkh20Ly855tKDkz1uv-1pHV_9GxfntiTJPUug/edit#slide=id.g22efd0a58a4_2_0">Google DeepMind (SIGIR 2023 keynote)</A>
								</DL><p>
								<DT><H3 FOLDED>Fill In The Middle</H3>
								<DL><p>
									<DT><A HREF="https://openai.com/blog/gpt-3-edit-insert/">New GPT-3 Capabilities: Edit &amp; Insert</A>
									<DT><A HREF="https://beta.openai.com/playground/p/default-translate-code?mode=edit&model=code-davinci-edit-001">Playground - OpenAI API</A>
									<DT><A HREF="https://community.openai.com/t/introducing-insert-and-edits-capabilities/15993">Introducing Insert and Edits Capabilities - Announcements - OpenAI API Community Forum</A>
									<DT><A HREF="https://github.com/cloneofsimo/fim-llama-deepspeed">cloneofsimo/fim-llama-deepspeed</A>
								</DL><p>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1651927473884655616">(Yi Tay): Clarifying misconceptions about architectures (training obj)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=YEUclZdj_Sc">Why next-token prediction is enough for AGI - Ilya Sutskever</A>
								<DT><A HREF="https://kexue.fm/archives/9797">EMO: Classification loss function designed based on optimal transmission</A>
								<DT><A HREF="https://twitter.com/haileysch__/status/1691483230761857024">Causal &gt;&gt; UL2</A>
								<DT><A HREF="https://github.com/NVIDIA/Megatron-LM/pull/268">Megatron-LM: Add UL2 data sampling and pretraining</A>
								<DT><A HREF="https://github.com/lucidrains/ReST-EM-pytorch">lucidrains/ReST-EM-pytorch: Implementations and explorations into the ReSTùê∏ùëÄ algorithm in the new deepmind paper "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"</A>
								<DT><A HREF="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5">OLMo-7B | OLMo-7B ‚Äì Weights &amp; Biases</A>
								<DT><A HREF="https://x.com/cloneofsimo/status/1792376397278953493/photo/1">3 days in, I see gradient norm sloooowly increasing (ref Olmo)</A>
								<DT><A HREF="https://arxiv.org/abs/2309.14322">[2309.14322] Small-scale proxies for large-scale Transformer training instabilities</A>
							</DL><p>
							<DT><H3 FOLDED>training-details</H3>
							<DL><p>
								<DT><H3 FOLDED>Vocabulary</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2204.14268.pdf">SetencePiece 256K Vocabulary</A>
									<DT><A HREF="https://x.com/karpathy/status/1621578354024677377?lang=en">Andrej Karpathy: increase vocab size from 50257 to 50304 (nearest multiple of 64) Careful with your powers of 2</A>
									<DT><A HREF="https://arxiv.org/abs/1909.08053">[1909.08053] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (Section 5.1)</A>
									<DT><A HREF="https://x.com/abhi_venigalla/status/1621710130051190784">MosaicML: GPT-1.3B MFU went from 49% -&gt; 53%</A>
									<DT><A HREF="https://x.com/cHHillee/status/1630274804795445248">Horace He en X: "Recently, Karpathy tweeted that *increasing* the size of his matmul made it run faster. But... why? Many people seem content to leave this as black magic. But luckily, this *can* be understood! Here's a plot of FLOPs achieved for square matmuls. Let's explain each curve! 1/19 https://t.co/9xLVzswVmv" / X</A>
									<DT><A HREF="https://gist.github.com/Chillee/f86675147366a7a0c6e244eaa78660f7#file-4-matmul-bench-py">PT 2.0 Benchmarks</A>
									<DT><A HREF="https://www.thonking.ai/p/answer-key-what-shapes-do-matrix">Solutions: What Shapes Do Matrix Multiplications Like?</A>
									<DT><A HREF="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications">What Shapes Do Matrix Multiplications Like? [medium]</A>
								</DL><p>
								<DT><H3 FOLDED>Bitwise determinism</H3>
								<DL><p>
									<DT><A HREF="https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html">Stanford CRFM</A>
									<DT><A HREF="https://arxiv.org/pdf/2204.02311.pdf">PaLM: Section 5 "Training Details"</A>
								</DL><p>
								<DT><H3 FOLDED>Weight initialization</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/cloneofsimo/status/1741381460274331916">effective weight init scheme</A>
									<DT><A HREF="https://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling with Pathways (fain-in variance scaling)</A>
									<DT><A HREF="https://thegregyang.com/">Greg Yang |¬†Professional page</A>
									<DT><A HREF="https://cs231n.github.io/neural-networks-2/">CS231n Convolutional Neural Networks for Visual Recognition</A>
								</DL><p>
								<DT><H3 FOLDED>Sequence length</H3>
								<DL><p>
									<DT><H3 FOLDED>RoPE</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2403.00071">[2403.00071] Resonance RoPE: Improving Context Length Generalization of Large Language Models</A>
									</DL><p>
									<DT><H3 FOLDED>YaRN</H3>
									<DL><p>
										<DT><A HREF="https://www.modular.com/ai-resources/yarn#:~:text=Perplexity%20Performance%3A%20YaRN%20achieves%20lower,up%20to%20128k%20context%20lengths.">Yarn key concepts</A>
										<DT><A HREF="https://arxiv.org/abs/2309.00071">[2309.00071] YaRN: Efficient Context Window Extension of Large Language Models</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>Batch size</H3>
								<DL><p>
									<DT><A HREF="https://openai.com/index/how-ai-training-scales/">How AI training scales | OpenAI</A>
									<DT><A HREF="https://arxiv.org/pdf/2204.02311.pdf">PALM: progresive larger batch size (GPT-3 style)</A>
									<DT><A HREF="https://arxiv.org/abs/1907.04164">[1907.04164] Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model</A>
									<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1790024970824740949">maxing batchsize and performance</A>
									<DT><A HREF="https://twitter.com/cloneofsimo/status/1789680163133018593">larger batch size leads to more compute-efficient optimization</A>
								</DL><p>
								<DT><H3 FOLDED>Regularization</H3>
								<DL><p>
									<DT><H3 FOLDED>RMS</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/1910.07467">[1910.07467] Root Mean Square Layer Normalization</A>
										<DT><A HREF="https://imbue.com/research/70b-intro/">Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings - imbue</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>Dropout</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/1912.10095">Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks</A>
									<DT><A HREF="https://arxiv.org/pdf/2303.01500">Dropout Reduces Underfitting</A>
									<DT><A HREF="https://developers.google.com/machine-learning/crash-course/training-neural-networks/best-practices">Training Neural Networks: Best Practices ¬†|¬† Machine Learning ¬†|¬† Google for Developers</A>
									<DT><A HREF="https://ramnathkumar181.github.io/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting/">Dropout- A Simple Way to Prevent Neural Networks from Overfitting ‚Äì Ramnath Kumar ‚Äì Predoctoral Researcher, Google DeepMind</A>
									<DT><A HREF="https://eitca.org/artificial-intelligence/eitc-ai-dltf-deep-learning-with-tensorflow/training-a-neural-network-to-play-a-game-with-tensorflow-and-open-ai/training-model/examination-review-training-model/what-is-the-purpose-of-the-dropout-process-in-the-fully-connected-layers-of-a-neural-network/">What is the purpose of the dropout process in the fully connected layers of a neural network? - EITCA Academy</A>
								</DL><p>
								<DT><H3 FOLDED>Training Instability</H3>
								<DL><p>
									<DT><A HREF="https://github.com/apple/ml-sigma-reparam">apple/ml-sigma-reparam</A>
									<DT><A HREF="https://twitter.com/Birchlabs/status/1735709582096302243">Stabilizing Transformer Training by Preventing Attention Entropy Collapse</A>
								</DL><p>
								<DT><H3 FOLDED>Gradient accumulation</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2212.14034">[2212.14034] Cramming: Training a Language Model on a Single GPU in One Day</A>
								</DL><p>
								<DT><H3 FOLDED>MFU &amp; training performance</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2304.01433.pdf">[TPUv4] Do peak FLOPS/second predict real performance?</A>
									<DT><A HREF="https://vram.asmirnov.xyz/">VRAM Calculator</A>
									<DT><A HREF="https://arxiv.org/abs/2205.05198">[2205.05198] Reducing Activation Recomputation in Large Transformer Models</A>
									<DT><A HREF="https://arxiv.org/pdf/2204.02311.pdf">PALM: 4.1 Training Efficiency</A>
									<DT><A HREF="https://pytorch.org/blog/maximizing-training-throughput/?utm_content=293931524&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Maximizing Training Throughput Using PyTorch FSDP and Torch.compile | PyTorch</A>
									<DT><A HREF="https://x.com/HamelHusain/status/1800315287574847701">managing and debugging GPU HBM</A>
									<DT><A HREF="https://parlance-labs.com/">Parlance Labs: A consultancy focused on LLMs</A>
								</DL><p>
								<DT><A HREF="https://openai.com/research/techniques-for-training-large-neural-networks">Techniques for training large neural networks</A>
								<DT><A HREF="https://www.youtube.com/watch?v=k_HMgpJKBso">LLaMA 2 w/ Thomas Scialom (LLaMA 2 lead) - YouTube</A>
								<DT><A HREF="https://github.com/stas00/ml-engineering/blob/master/insights/ai-battlefield.md#ml-engineers-heaven-and-hell">stas00: The AI Battlefield Engineering - What You Need To Know</A>
								<DT><A HREF="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf">metaseq/projects/OPT/chronicles/OPT175B_Logbook</A>
								<DT><A HREF="https://blog.replit.com/llm-training">Replit - How to train your own Large Language Models</A>
								<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/README.md">DeepLearningExamples/README.md at master</A>
								<DT><A HREF="https://jingfengyang.github.io/gpt">Why did all of the public reproduction of GPT-3 fail?</A>
								<DT><A HREF="https://ai.google/static/documents/palm2techreport.pdf">PaLM 2 Technical Report</A>
								<DT><A HREF="http://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks (Andrew Karphathy)</A>
								<DT><A HREF="https://scontent.fsvq5-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=sv_RQgqDkdAAX_BgWRs&_nc_ht=scontent.fsvq5-1.fna&oh=00_AfA6QXt9YOU7MOECaRK3gKak7CECYE6GJZ6SQn6y_aji5Q&oe=65A5C63F">Llama 2 (Table 2): training duration (hours)</A>
								<DT><A HREF="https://news.ycombinator.com/item?id=38222277">Google Cloud TPU Multislice Training | Hacker News</A>
								<DT><A HREF="http://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks (Andrew Karphathy) DATA</A>
								<DT><A HREF="https://console.cloud.google.com/storage/browser/_details/madlad-400-checkpoints/checkpoints/10b-mt/10b-mt.gin;tab=live_object">madlad-400...0b-mt.gin (training and model config file)</A>
								<DT><A HREF="https://github.com/axboe/fio">axboe/fio: Flexible I/O Tester</A>
								<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/storage">Filesystems and IO</A>
								<DT><A HREF="https://github.com/zhengzangw/awesome-huge-models">A collection of details things about LLMs</A>
								<DT><A HREF="https://arxiv.org/abs/1907.04164">[1907.04164] Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model</A>
								<DT><A HREF="https://blog.eleuther.ai/nyt-yi-34b-response/">Yi-34B, Llama 2, and common practices in LLM training: a fact check of the New York Times | EleutherAI Blog</A>
								<DT><A HREF="https://x.com/leilavclark/status/1805700631199642094">From bare metal to a 70B model: infrastructure set-up and scripts</A>
							</DL><p>
							<DT><H3 FOLDED>training-scaling</H3>
							<DL><p>
								<DT><H3 FOLDED>Scaling laws</H3>
								<DL><p>
									<DT><H3 FOLDED>Chinchilla</H3>
									<DL><p>
										<DT><A HREF="https://ai.meta.com/blog/meta-llama-3/">model performance continues to improve even after the model is trained on two orders of magnitude more data</A>
										<DT><A HREF="https://arxiv.org/abs/2404.10102">[2404.10102] Chinchilla Scaling: A replication attempt</A>
										<DT><A HREF="https://twitter.com/akbirthko/status/1782286554985107631">how good a text compressor we can make for a fixed compute budget</A>
									</DL><p>
									<DT><A HREF="https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training">An empirical analysis of compute-optimal large language model training</A>
									<DT><A HREF="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</A>
									<DT><A HREF="https://twitter.com/ZeyuanAllenZhu/status/1777513016592040248">Physics of Language Models: Knowledge Capacity Scaling Laws (Meta AI)</A>
									<DT><A HREF="https://arxiv.org/abs/2404.05405">[2404.05405] Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws</A>
									<DT><A HREF="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval">Gopher, ethical considerations, and retrieval</A>
									<DT><A HREF="https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications">Chinchilla's Scaling Laws</A>
									<DT><A HREF="https://irhum.github.io/blog/chinchilla/">Thoughts on Chinchilla (irhum's blog)</A>
									<DT><A HREF="https://thegregyang.com/">Tensor Programs</A>
									<DT><A HREF="https://arxiv.org/abs/2310.02244">Tensor Programs VI: Feature Learning in Infinite-Depth NN</A>
									<DT><A HREF="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws For Neural Languag Models</A>
									<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
									<DT><A HREF="https://arxiv.org/abs/1712.00409">[1712.00409] Deep Learning Scaling is Predictable, Empirically</A>
									<DT><A HREF="https://arxiv.org/abs/2010.14701">[2010.14701] Scaling Laws for Autoregressive Generative Modeling</A>
									<DT><A HREF="https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html">Scaling vision transformers to 22 billion parameters (Google)</A>
									<DT><A HREF="https://arxiv.org/pdf/2301.03728.pdf">Scaling Laws For Generative Mixed-Modal Language Models</A>
									<DT><A HREF="https://www.youtube.com/watch?v=0D23NeBjCeQ">A Theory for Emergence of Complex Skills in Language Models</A>
									<DT><A HREF="https://www.youtube.com/watch?v=dbo3kNKPaUA">Large Language Models (in 2023) - YouTube</A>
									<DT><A HREF="https://arxiv.org/pdf/2203.17189.pdf">Scaling Up Models and Data with t5x and seqio (James Bradbury)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=K-cXYoqHxBc">More Is Different for AI - Scaling Up, Emergence</A>
									<DT><A HREF="https://www.youtube.com/watch?v=J4LzMGn6FdM">TinyLlama: An Open-Source Small Language Model (model saturation)</A>
									<DT><A HREF="https://twitter.com/khoomeik/status/1769333863401107602">language model scaling laws appear to be sensitive to data complexity</A>
									<DT><A HREF="https://twitter.com/khoomeik/status/1773248562555486588">scaling laws: gzip compression ratio and model size</A>
									<DT><A HREF="https://twitter.com/ChombaBupe/status/1777352725858029727/photo/2">multimodal scaling laws: sample efficiency</A>
									<DT><A HREF="https://github.com/bethgelab/frequency_determines_performance">Pretraining Concept Frequency Determines Multimodal Model Performance</A>
									<DT><A HREF="https://ai.meta.com/blog/meta-llama-3/">Introducing Meta Llama 3: The most capable openly available LLM to date</A>
									<DT><A HREF="https://github.com/gregorbachmann/scaling_mlps">gregorbachmann/scaling_mlps</A>
									<DT><A HREF="https://arxiv.org/abs/2405.10938">[2405.10938] Observational Scaling Laws and the Predictability of Language Model Performance</A>
									<DT><A HREF="https://www.pnas.org/doi/10.1073/pnas.2311878121">Explaining neural scaling laws | PNAS (2024)</A>
									<DT><A HREF="https://arxiv.org/abs/2207.10551">[2207.10551] Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?</A>
								</DL><p>
								<DT><A HREF="https://github.com/gregorbachmann/scaling_mlps">gregorbachmann/scaling_mlps</A>
								<DT><A HREF="https://arxiv.org/abs/2212.10544">[2212.10544] Pretraining Without Attention</A>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1784696357892063565">(transformers) same parameter count, a wildly different architecture</A>
								<DT><A HREF="https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training">An empirical analysis of compute-optimal large language model training</A>
								<DT><A HREF="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</A>
								<DT><A HREF="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval">Gopher, ethical considerations, and retrieval</A>
								<DT><A HREF="https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications">Chinchilla's Scaling Laws</A>
								<DT><A HREF="https://irhum.github.io/blog/chinchilla/">Thoughts on Chinchilla (irhum's blog)</A>
								<DT><A HREF="https://thegregyang.com/">Tensor Programs</A>
								<DT><A HREF="https://arxiv.org/abs/2310.02244">[2310.02244] Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks</A>
								<DT><A HREF="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws For Neural Languag Models</A>
								<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
								<DT><A HREF="https://arxiv.org/abs/1712.00409">[1712.00409] Deep Learning Scaling is Predictable, Empirically</A>
								<DT><A HREF="https://arxiv.org/abs/2010.14701">[2010.14701] Scaling Laws for Autoregressive Generative Modeling</A>
								<DT><A HREF="https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html">Scaling vision transformers to 22 billion parameters (Google)</A>
								<DT><A HREF="https://arxiv.org/pdf/2301.03728.pdf">Scaling Laws For Generative Mixed-Modal Language Models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=0D23NeBjCeQ">A Theory for Emergence of Complex Skills in Language Models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=dbo3kNKPaUA">Large Language Models (in 2023) - YouTube</A>
								<DT><A HREF="https://arxiv.org/pdf/2203.17189.pdf">Scaling Up Models and Data with t5x and seqio (James Bradbury)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=K-cXYoqHxBc">More Is Different for AI - Scaling Up, Emergence</A>
								<DT><A HREF="https://www.youtube.com/watch?v=J4LzMGn6FdM">TinyLlama: An Open-Source Small Language Model (model saturation)</A>
								<DT><A HREF="https://twitter.com/khoomeik/status/1769333863401107602">language model scaling laws appear to be sensitive to data complexity</A>
								<DT><A HREF="https://twitter.com/khoomeik/status/1773248562555486588">scaling laws: gzip compression ratio and model size</A>
								<DT><A HREF="https://transformer-circuits.pub/2024/april-update/index.html#scaling-laws">Scaling Laws for Dictionary Learning</A>
								<DT><A HREF="https://arxiv.org/abs/2404.10102">[2404.10102] Chinchilla Scaling: A replication attempt</A>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1780639257389904013">Chinchilla Scaling: A replication attempt (twitter thread)</A>
								<DT><A HREF="https://github.com/hkust-nlp/llm-compression-intelligence">hkust-nlp/llm-compression-intelligence: Official github repo for the paper "Compression Represents Intelligence Linearly"</A>
								<DT><A HREF="https://x.com/hwchung27/status/1712209280529727705">A counterintuitive implication of scale: trying to solve a more general version of the problem is an easier way to solve the original problem than directly tackling it</A>
							</DL><p>
							<DT><H3 FOLDED>training-efficient</H3>
							<DL><p>
								<DT><H3 FOLDED>Activation Recomputation</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2205.05198">[2205.05198] Reducing Activation Recomputation in Large Transformer Models</A>
								</DL><p>
								<DT><H3 FOLDED>training-checkpointing</H3>
								<DL><p>
									<DT><H3 FOLDED>checkpointing-io</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2101.08734">[2101.08734] Clairvoyant Prefetching for Distributed Machine Learning I/O</A>
										<DT><A HREF="https://github.com/NVIDIA/aistore/blob/master/docs/overview.md">aistore/docs/overview.md at master ¬∑ NVIDIA/aistore</A>
										<DT><A HREF="https://arxiv.org/abs/2001.01858">[2001.01858] High Performance I/O For Large Scale Deep Learning</A>
										<DT><A HREF="https://www.youtube.com/watch?v=1CdduHa-KgA&t=2186s">File I/O for Game Developers: Past, Present, and Future with C++ - Guy Davidson - CppCon 2023 - YouTube</A>
										<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/storage">Filesystems and IO</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=bfexfASu9h4">Scalable and Efficient AI: From Supercomputers to Smartphones (Microsoft)</A>
									<DT><A HREF="https://pytorch.org/docs/stable/distributed.checkpoint.html">Distributed Checkpoint - torch.distributed.checkpoint ‚Äî PyTorch 2.1</A>
									<DT><A HREF="https://blog.research.google/2022/09/tensorstore-for-high-performance.html?m=1">TensorStore for High-Performance, Scalable Array Storage</A>
									<DT><A HREF="https://learn.microsoft.com/en-us/azure/machine-learning/reference-checkpoint-performance-for-large-models?view=azureml-api-2&tabs=PYTORCH">Optimize Checkpoint Performance for Large Models - Azure Machine Learning | Microsoft Learn</A>
									<DT><A HREF="https://github.com/libffcv/ffcv">libffcv/ffcv: FFCV: Fast Forward Computer Vision (and other ML workloads!)</A>
									<DT><A HREF="https://github.com/google/orbax">google/orbax: Orbax provides common utility libraries for JAX users.</A>
								</DL><p>
								<DT><H3 FOLDED>Parameter-Efficient Fine-Tuning-(PEFT)</H3>
								<DL><p>
									<DT><H3 FOLDED>LoRA</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=NDV65M-2T3g">Flat minima generalize for low-rank matrix recovery - YouTube</A>
										<DT><A HREF="https://github.com/punica-ai/punica">punica-ai/punica: Serving multiple LoRA finetuned LLM as one</A>
										<DT><A HREF="https://gist.github.com/Chillee/a8d2070b1b7b3f97d8c87bac3c366f8e">lora_example.py</A>
										<DT><A HREF="https://irhum.github.io/blog/lorawd/">irhum.github.io - LoRA and Weight Decay</A>
										<DT><A HREF="https://x.com/datavistics/status/1805929136390656137">(1) Derek Thomas en X: "Check out this animation on LoRA Inference! https://t.co/0ixOHmUcQR" / X</A>
									</DL><p>
									<DT><H3 FOLDED>Prompt-Tuning</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2102.07350.pdf">Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</A>
										<DT><A HREF="https://arxiv.org/pdf/2202.13169.pdf">A SYSTEMATIC EVALUATION OF LARGE LANGUAGE MODELS OF CODE</A>
										<DT><A HREF="https://arxiv.org/pdf/2208.05950.pdf">Interactive Code Generation via Test-Driven User-Intent Formalization</A>
										<DT><A HREF="https://openreview.net/pdf?id=NiEtU7blzN">LARGE LANGUAGE MODELS CAN SELF-IMPROVE</A>
										<DT><A HREF="https://arxiv.org/pdf/2107.03374.pdf">Evaluating Large Language Models Trained on Code</A>
										<DT><A HREF="https://www.youtube.com/watch?v=80jwVkYOu0w">The Power of Scale for Parameter-Efficient Prompt Tuning - YouTube</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2403.14608">[2403.14608] Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</A>
									<DT><A HREF="https://arxiv.org/pdf/2401.01286.pdf">A Comprehensive Study of Knowledge Editing for Large Language Models</A>
									<DT><A HREF="https://arxiv.org/pdf/2205.05638.pdf">T-FEW: Few-Shot Parameter-Efficient Fine-Tuning vs In-Context Learning</A>
									<DT><A HREF="https://www.youtube.com/watch?v=MQwryfkydc0">Unsloth.ai: Easily finetune &amp; train LLMs - YouTube</A>
								</DL><p>
								<DT><A HREF="https://github.com/JeanKaddour/NoTrainNoGain">NoTrainNoGain: Revisiting Efficient Training Algorithms For Transformer-based Language Models</A>
								<DT><A HREF="https://github.com/Lightning-AI/lit-gpt">Lightning-AI/lit-gpt</A>
								<DT><A HREF="https://twitter.com/ZihangDai/status/1281349893873897478/photo/1">Funnel-Transformer</A>
								<DT><A HREF="https://twitter.com/edwardjhu/status/1714283903908020386">ŒºTransfer</A>
								<DT><A HREF="https://twitter.com/itsclivetime/status/1650254229620260864">LLM rules of thumb (Clive Shan)</A>
								<DT><A HREF="https://bettergpt.chat////">Better GPT</A>
							</DL><p>
							<DT><H3 FOLDED>training-model-parallelism</H3>
							<DL><p>
								<DT><H3 FOLDED>mode-parallelism-lectures</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=4h7YBUXiCZE">Abstractions for Expressive, Efficient Parallel and Distributed Comp</A>
									<DT><A HREF="https://www.youtube.com/watch?v=xtxxLWZznBI">Demystifying Parallel and Distributed Deep Learning (ETH) (Microsoft)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=0qoUqE695X0">Lecture 12.4 Scaling up (Mixed precision, Data-parallelism, FSDP) - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>Megatron-LM</H3>
								<DL><p>
									<DT><H3 FOLDED>megatron-lm-rocm</H3>
									<DL><p>
										<DT><A HREF="https://github.com/TurkuNLP/Megatron-DeepSpeed">TurkuNLP/Megatron-DeepSpeed: Ongoing research training transformer language models at scale, including: BERT &amp; GPT-2</A>
										<DT><A HREF="https://github.com/vosen/ZLUDA">vosen/ZLUDA: CUDA on AMD GPUs</A>
									</DL><p>
									<DT><H3 FOLDED>NeMO</H3>
									<DL><p>
										<DT><A HREF="https://github.com/NVIDIA/NeMo">NVIDIA/NeMo: NeMo: a toolkit for conversational AI</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/1909.08053">[1909.08053] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</A>
									<DT><A HREF="https://arxiv.org/abs/2104.04473">[2104.04473] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</A>
									<DT><A HREF="https://github.com/alibaba/Megatron-LLaMA">alibaba/Megatron-LLaMA: Best practice for training LLaMA models in Megatron-LM</A>
									<DT><A HREF="https://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/353/files#diff-df547fcaa58ea4cb2f31099e838dd94f10c92fbb14a5111ed4ffced4ee5f0c4e">Pre-train UL2</A>
									<DT><A HREF="https://twitter.com/YangYou1991/status/1770334665678970882">Dynamic Sequence Parallelism: parallel axis</A>
									<DT><A HREF="https://huggingface.co/spaces/BBuf/megatron-lm-parallel-group-playground">Megatron Lm Parallel Group Playground - a Hugging Face Space by BBuf</A>
									<DT><A HREF="http://giantpandacv.com/project/PyTorch/AI%20Infra%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8A%E5%9C%A8LLM%E8%AE%AD%E7%BB%83%E4%B8%AD%E5%87%8F%E5%B0%91%E6%BF%80%E6%B4%BB%E5%80%BC%E5%86%85%E5%AD%98%E3%80%8B/">AI Infra paper reading: "Reducing activation value memory in LLM training"</A>
								</DL><p>
								<DT><H3 FOLDED>DeepSpeed</H3>
								<DL><p>
									<DT><H3 FOLDED>ZeRO-infinity</H3>
									<DL><p>
										<DT><H3 FOLDED>Videos</H3>
										<DL><p>
											<DT><A HREF="https://www.youtube.com/watch?v=smDC_mOGQ5U">[REFAI Seminar 03/30/23] Efficient Trillion Parameter Scale Training</A>
											<DT><A HREF="https://www.youtube.com/watch?v=YEdtLCjSZFY">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale</A>
											<DT><A HREF="https://www.youtube.com/watch?v=7IposV4_LY4">Unleashing the Power of BLOOM 176B with AWS</A>
										</DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2104.07857.pdf">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</A>
										<DT><A HREF="https://dl.acm.org/doi/10.1145/3458817.3476205">ZeRO-infinity | Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</A>
										<DT><A HREF="https://arxiv.org/pdf/2306.09782.pdf">LOMO: Heterogeneous Training System (ZeRO-Infinity)</A>
										<DT><A HREF="https://sliu583.gitbook.io/blog/specific-work/shivarams-group/group-papers/lists/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training">ZeRO-Infinity and DeepSpeed: Unlocking unprecedented model</A>
										<DT><A HREF="https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html">TRAIN 1 TRILLION+ PARAMETER MODELS</A>
										<DT><A HREF="https://www.amazon.science/blog/scaling-to-trillion-parameter-model-training-on-aws">Scaling to trillion-parameter model training on AWS</A>
									</DL><p>
									<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt">cloneofsimo/min-max-gpt: minGPT that scales</A>
									<DT><A HREF="https://arxiv.org/abs/1910.02054">[1910.02054] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</A>
									<DT><A HREF="https://github.com/microsoft/Megatron-DeepSpeed">microsoft/Megatron-DeepSpeed</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/main/main_classes/deepspeed#deepspeed-trainer-integration">DeepSpeed Integration</A>
									<DT><A HREF="https://www.youtube.com/watch?v=wbG2ZEDPIyw">Microsoft DeepSpeed introduction at KAUST</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/824f2ffd5124d24935811bc738ed903796ab13ac/benchmark/deepspeed/benchmark_gpt2.py#L86">alpa/benchmark/deepspeed/benchmark_gpt2.py</A>
									<DT><A HREF="https://github.com/cloneofsimo/reverse_eng_deepspeed_study">cloneofsimo/reverse_eng_deepspeed_study: DeepSpeed Study</A>
									<DT><A HREF="https://mp.weixin.qq.com/s/8F3eAHDBjQkHHBmrAEoOfw">Illustrated explanation of large model training: Data parallelism, Part 2 (ZeRO, zero redundancy optimization)</A>
									<DT><A HREF="https://nn.labml.ai/scaling/zero3/index.html">Zero-DP Memory Optimization</A>
								</DL><p>
								<DT><H3 FOLDED>model-parallelism-google</H3>
								<DL><p>
									<DT><H3 FOLDED>GSPMD</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2105.04663.pdf">General and Scalable Parallellization for ML Computation Graphs</A>
									</DL><p>
									<DT><H3 FOLDED>Alpa</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/GoogleAI/status/1521938108568154112">Google AI en Twitter: "Alpa is a framework that uses just one line of code to easily automate the complex model parallelism process for large #DeepLearning models. Learn more and check out the code. https://t.co/xFfW5tml9v https://t.co/qYIhHnzwSG" / Twitter</A>
										<DT><A HREF="https://ai.googleblog.com/2022/05/alpa-automated-model-parallel-deep.html">Alpa: Automated Model-Parallel Deep Learning ‚Äì Google AI Blog</A>
										<DT><A HREF="https://github.com/alpa-projects/alpa">alpa-projects/alpa: Training and serving large-scale neural networks</A>
										<DT><A HREF="https://www.youtube.com/watch?v=y1NXHjcl6V0">Alpa: Automated Model-Parallel Deep Learning - Zhuohan Li | Stanford MLSys #59 - YouTube</A>
										<DT><A HREF="https://github.com/alpa-projects/mms">alpa-projects/mms: Multi model serving</A>
										<DT><A HREF="https://www.youtube.com/watch?v=oVC3SB3GqrI">OSDI '22 - Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>t5x + seqio</H3>
									<DL><p>
										<DT><A HREF="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=GprA5UsAAAAJ&sortby=pubdate&citation_for_view=GprA5UsAAAAJ:roLk4NBRz8UC">Scaling Up Models and Data with t5x and seqio</A>
										<DT><A HREF="https://arxiv.org/pdf/2203.17189.pdf">Scaling Up Models and Data with t5x and seqio</A>
										<DT><A HREF="https://twitter.com/rasbt/status/1638538494887821313">(1) Sebastian Raschka en X: ""Meet in the Middle: A New Pre-training Paradigm" for large language models (LLM). In this paper, the authors propose to develop a bidirectional LLM using the full sequence information during pretraining and using context from both sides during inference. https://t.co/FHY4Vof90I... https://t.co/QFxtKO3iJb" / X</A>
										<DT><A HREF="https://twitter.com/rasbt/status/1638538494887821313">Meet in the Middle: A New Pre-training Paradigm</A>
										<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x</A>
									</DL><p>
									<DT><A HREF="https://twitter.com/dlwh/status/1677923909763747840">JAX/Haliax/Levanter</A>
									<DT><A HREF="https://colab.research.google.com/drive/18_BrtDpe1lu89M4T6fKzda8DdSLtFJhi">Tensor Parallelism in Haliax - Colaboratory</A>
									<DT><A HREF="https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf">TensorFlow: A System for Large-Scale Machine Learning</A>
									<DT><A HREF="https://arxiv.org/abs/2204.06514">[2204.06514] Scalable Training of Language Models using JAX pjit and TPUv4</A>
									<DT><A HREF="https://arxiv.org/abs/1904.00962">[1904.00962] Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</A>
									<DT><A HREF="https://github.com/stanford-crfm/levanter">stanford-crfm/levanter: Legibile, Scalable, Reproducible Foundation Models with Named Tensors and Jax</A>
									<DT><A HREF="https://github.com/froystig">froystig (Roy Frostig)</A>
								</DL><p>
								<DT><H3 FOLDED>training-distributed-parallelism-heterogenous</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2206.01288v1">Decentralized Training of Foundation Models in Heterogeneous Environments</A>
								</DL><p>
								<DT><H3 FOLDED>data-parallelism</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/Muennighoff/status/1661895337248686081">How to keep scaling LLMs when data runs out?</A>
									<DT><A HREF="https://github.com/huggingface/datablations">huggingface/datablations: Scaling Data-Constrained Language Models</A>
								</DL><p>
								<DT><H3 FOLDED>pipeline-parallelism</H3>
								<DL><p>
									<DT><H3 FOLDED>pipeline-parallelism-torch</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=_rC49PeqrWc">PiPPy: Automated Pipeline Parallelism for PyTorch - YouTube</A>
										<DT><A HREF="https://github.com/pytorch/PiPPy">pytorch/PiPPy: Pipeline Parallelism for PyTorch</A>
									</DL><p>
									<DT><H3 FOLDED>GPipe</H3>
									<DL><p>
										<DT><A HREF="https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html">GPipe</A>
										<DT><A HREF="https://www.youtube.com/watch?v=9s2cum25Kkc">GPipe</A>
									</DL><p>
									<DT><H3 FOLDED>zero-bubble-parallelism</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sail-sg/zero-bubble-pipeline-parallelism">sail-sg/zero-bubble-pipeline-parallelism: Zero Bubble Pipeline Parallelism</A>
									</DL><p>
									<DT><A HREF="https://siboehm.com/articles/22/pipeline-parallel-training">Pipeline-Parallelism: Distributed Training via Model Partitioning</A>
								</DL><p>
								<DT><H3 FOLDED>tensor-parallelism</H3>
								<DL><p>
									<DT><A HREF="https://irhum.github.io/blog/pjit/">irhum.github.io - Tensor Parallelism with jax.pjit</A>
									<DT><A HREF="https://twitter.com/__tinygrad__/status/1742365883048284421">Tinygrad's multiGPU tensor sharding</A>
									<DT><A HREF="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html">SPMD: Parallel Evaluation in JAX</A>
									<DT><A HREF="https://twitter.com/MishaLaskin/status/1628880374255296512">Model, data &amp; pipeline parallelism</A>
									<DT><A HREF="https://blog.eleuther.ai/transformer-math/">Transformer Math 101 | EleutherAI Blog</A>
								</DL><p>
								<DT><H3 FOLDED>sequence-parallelism</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2405.07719">[2405.07719] USP: A Unified Sequence Parallelism Approach for Long Context Generative AI</A>
								</DL><p>
								<DT><H3 FOLDED>FSDP</H3>
								<DL><p>
									<DT><A HREF="https://engineering.fb.com/2021/07/15/open-source/fsdp/">Fully Sharded Data Parallel: faster AI training with fewer GPUs Engineering at Meta -</A>
									<DT><A HREF="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/">Scaling services with Shard Manager - Engineering at Meta</A>
									<DT><A HREF="https://scholar.google.co.uk/citations?view_op=view_citation&hl=en&user=_cHRq1kAAAAJ&citft=1&email_for_op=antonio.jfdominguez%40gmail.com&citation_for_view=_cHRq1kAAAAJ:mVmsd5A6BfQC">PyTorch FSDP: experiences on scaling fully sharded data parallel</A>
									<DT><A HREF="https://www.youtube.com/watch?v=a3iW6Cggccw">Part 4: FSDP Sharding Strategies</A>
									<DT><A HREF="https://www.youtube.com/watch?v=NiL7egqyJEI">[Long Review] Fully Sharded Data Parallel</A>
									<DT><A HREF="https://www.youtube.com/watch?v=By_O0k102PY">How Fully Sharded Data Parallel (FSDP) works</A>
									<DT><A HREF="https://github.com/facebookresearch/llama-recipes/blob/main/docs/multi_gpu.md">llama-recipes/docs/multi_gpu.md</A>
									<DT><A HREF="https://www.youtube.com/watch?v=3XUG7cjte2U">Invited Talk: PyTorch Distributed (DDP, RPC) - By Facebook Research Scientist Shen Li - YouTube</A>
									<DT><A HREF="https://pytorch.org/blog/maximizing-training-throughput/?utm_content=293931524&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Maximizing Training Throughput Using PyTorch FSDP and Torch.compile | PyTorch</A>
									<DT><A HREF="https://github.com/foundation-model-stack/fms-fsdp">foundation-model-stack/fms-fsdp: üöÄ Efficiently (pre)training foundation models with native PyTorch features, including FSDP for training and SDPA implementation of Flash attention v2.</A>
									<DT><A HREF="https://github.com/pytorch/torchtitan">pytorch/torchtitan: A native PyTorch Library for large model training</A>
								</DL><p>
								<DT><H3 FOLDED>distributed-training</H3>
								<DL><p>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1724732329740976187">(1) Guardados / X</A>
									<DT><A HREF="https://arxiv.org/abs/2311.08105">[2311.08105] DiLoCo: Distributed Low-Communication Training of Language Models</A>
									<DT><A HREF="https://www.databricks.com/blog/mosaic-ai-training-capabilities">Building DBRX-class Custom LLMs with Mosaic AI Training | Databricks Blog</A>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1803941734629388590">(1) Guardados / X</A>
								</DL><p>
								<DT><A HREF="https://imbue.com/research/70b-infrastructure/">From bare metal to a 70B model: infrastructure set-up and scripts - imbue</A>
								<DT><A HREF="https://sumanthrh.com/post/distributed-and-efficient-finetuning/">Everything about Distributed Training and Efficient Finetuning</A>
								<DT><A HREF="https://github.com/siboehm/shallowspeed">siboehm/ShallowSpeed: Small scale distributed training of sequential deep learning models, built on Numpy and MPI.</A>
								<DT><A HREF="https://github.com/FZJ-JSC/tutorial-multi-gpu">FZJ-JSC/tutorial-multi-gpu: Efficient Distributed GPU Programming for Exascale, an SC/ISC Tutorial</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Collective_operation">Collective operation - Wikipedia</A>
								<DT><A HREF="https://arxiv.org/abs/2205.01068">[2205.01068] OPT: Open Pre-trained Transformer Language Models</A>
								<DT><A HREF="https://arxiv.org/abs/2203.12533">[2203.12533] Pathways: Asynchronous Distributed Dataflow for ML</A>
								<DT><A HREF="https://pytorch.org/docs/stable/distributed.html#distributed-key-value-store">Distributed communication package - torch.distributed</A>
								<DT><A HREF="https://github.com/stanford-crfm/levanter">stanford-crfm/levanter</A>
								<DT><A HREF="https://arxiv.org/abs/2204.06745">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</A>
								<DT><A HREF="https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e">the world‚Äôs largest distributed LLM training job on TPU v5e</A>
								<DT><A HREF="https://arxiv.org/abs/2303.06318">[2303.06318] A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training</A>
								<DT><A HREF="http://giantpandacv.com/project/PyTorch/AI%20Infra%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8A%E5%9C%A8LLM%E8%AE%AD%E7%BB%83%E4%B8%AD%E5%87%8F%E5%B0%91%E6%BF%80%E6%B4%BB%E5%80%BC%E5%86%85%E5%AD%98%E3%80%8B/">AI Infra paper reading: "Reducing activation value memory in LLM training"</A>
								<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1805629542914211951">Imbue blogs</A>
								<DT><A HREF="https://github.com/imbue-ai/cluster-health">imbue-ai/cluster-health</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1803446449796989021">Nemotron-340B was trained on 768x8 H100: visualization</A>
							</DL><p>
							<DT><H3 FOLDED>training-optimizer</H3>
							<DL><p>
								<DT><H3 FOLDED>optimization-hyperparameters</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437/photo/1">Aggressive learning rate</A>
									<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437">The right moment to decay is always as late as possible</A>
									<DT><A HREF="https://twitter.com/borisdayma/status/1489292077313703939">Impact of learning rate</A>
									<DT><A HREF="https://arxiv.org/pdf/1803.02021.pdf">UNDERSTANDING SHORT-HORIZON BIAS IN STOCHASTIC META-OPTIMIZATION (aggresive)</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=5exL8UYxpsI&t=63s">IFML SEMINAR: 1/26/24 - Meta Optimization (Google DeepMind)</A>
								<DT><A HREF="https://github.com/google/automl/tree/master/lion#language-modeling">Google AutoML: Lion Optimizer over Adam</A>
								<DT><A HREF="https://arxiv.org/pdf/1510.01799.pdf">efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters</A>
								<DT><A HREF="https://arxiv.org/abs/2304.09871">theory on Adam Instability in Large-Scale Machine Learning</A>
								<DT><A HREF="https://arxiv.org/abs/2308.01814">[2308.01814] Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit</A>
								<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437/photo/1">Aggressive learning rate</A>
								<DT><A HREF="https://arxiv.org/pdf/2002.09018.pdf">Scalable Second Order Optimization for Deep Learning (Shampo)</A>
								<DT><A HREF="https://twitter.com/_arohan_/status/1713343013563576540">Rohan Anil en X: "From ICLR, ŒºP ü§ù Shampoo</A>
								<DT><A HREF="https://twitter.com/tengyuma/status/1714708089205912004">Sophia</A>
								<DT><A HREF="https://github.com/Liuhong99/Sophia#tuning-the-hyperparameter-rho">Sophia: The official implementation of ‚ÄúSophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training‚Äù</A>
								<DT><A HREF="https://arxiv.org/abs/2305.14342">[2305.14342] Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</A>
								<DT><A HREF="https://twitter.com/sytelus/status/1713462676838588565">loss scaling FP16 training</A>
								<DT><A HREF="https://twitter.com/zacharynado/status/1729582361115848789">training algorithm* that is faster** than Adam**</A>
								<DT><A HREF="https://x.com/thecharlieblake/status/1800875303486861735">Adam Per Parameter Memory Usage, Excluding Activations</A>
								<DT><A HREF="https://arxiv.org/abs/2310.18313">[2310.18313] FP8-LM: Training FP8 Large Language Models</A>
								<DT><A HREF="https://nn.labml.ai/optimizers/index.html">Optimizers</A>
							</DL><p>
							<DT><H3 FOLDED>training-hyperparameters</H3>
							<DL><p>
								<DT><H3 FOLDED>Maximal Update Parametrization (ŒºP) and Hyperparameter Transfer</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=1aXOXHA7Jcw&t=3032s">Greg Yang | Large N Limits: Random Matrices &amp; Neural Networks</A>
									<DT><A HREF="https://www.microsoft.com/en-us/research/blog/%C2%B5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/">¬µtransfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks</A>
									<DT><A HREF="https://arxiv.org/abs/2203.03466">[2203.03466] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer</A>
									<DT><A HREF="https://arxiv.org/abs/2305.19268">[2305.19268] Intriguing Properties of Quantization at Scale (Emergent LLM properties &gt; 6B)</A>
									<DT><A HREF="https://github.com/microsoft/mup">mup</A>
									<DT><A HREF="https://decentdescent.org/tp5.html">Tuning GPT-3 on a Single GPU</A>
									<DT><A HREF="https://github.com/cloneofsimo/ezmup">cloneofsimo/ezmup: Simple implementation of muP, based on Spectral Condition for Feature Learning</A>
									<DT><A HREF="https://www.youtube.com/watch?v=z8-C42mAwBc">ŒºTransfer</A>
									<DT><A HREF="https://blog.speechmatics.com/mup">Reduce Model Tuning Costs with MuP</A>
									<DT><A HREF="https://x.com/CerebrasSystems/status/1796578819400442033">(1) Cerebras en X: "(1/n) Paper drop: https://t.co/fcr3Jr2ckD TLDR: We introduce the sparse maximal update parameterization (SŒºPar), which ensures optimal HPs remain the same for any width or sparsity level. This dramatically reduces HP tuning costs, allowing SŒºPar to achieve superior losses. üßµ üëá https://t.co/K0sY4kOKVT" / X</A>
									<DT><A HREF="https://arxiv.org/abs/2405.15743">[2405.15743] Sparse maximal update parameterization: A holistic approach to sparse training dynamics</A>
									<DT><A HREF="https://x.com/andrewgwils/status/1800532164418867245">Andrew Gordon Wilson en X: "Another major barrier is hypers -- initializations, LR, etc. You could easily try a new structure and not realize why it fails. The naive hypers just don‚Äôt work. But adapting the great work of @TheGregYang on muP to structure-aware initialization, we achieve exciting results! 4/8 https://t.co/NucLouwFIF" / X</A>
									<DT><A HREF="https://github.com/imbue-ai/carbs">imbue-ai/carbs: Cost aware hyperparameter tuning algorithm</A>
								</DL><p>
								<DT><H3 FOLDED>learning rate</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437">The right moment to decay is always as late as possible</A>
									<DT><A HREF="https://twitter.com/borisdayma/status/1489292077313703939">Impact of learning rate</A>
								</DL><p>
								<DT><H3 FOLDED>clip gradient norm</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>CARBS</H3>
								<DL><p>
									<DT><A HREF="https://imbue.com/research/70b-intro/">Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings - imbue</A>
								</DL><p>
								<DT><A HREF="https://twitter.com/cloneofsimo/status/1741381460274331916">effective weight init scheme</A>
							</DL><p>
							<DT><H3 FOLDED>training-scheduler</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>language-models-training-profiling</H3>
							<DL><p>
								<DT><A HREF="https://github.com/bigcode-project/bigcode-analysis/blob/main/multi_query_experiments/attention_types_imp.py">BigCode: Experiment analysis and profiling. Attention Types</A>
								<DT><A HREF="https://mlcommons.org/en/">MLCommons</A>
								<DT><A HREF="https://gist.github.com/jboner/2841832">Latency Numbers Every Programmer Should Know</A>
							</DL><p>
							<DT><H3 FOLDED>training-multilingual</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2205.06266.pdf">Lifting the Curse of Multilinguality by Pre-training Modular Transformers</A>
								<DT><A HREF="https://twitter.com/PfeiffJo/status/1525009949478330368">Lifting the Curse of Miltilinguality by Pre-training Modular Transformers</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ghL31wWlpFY">Massively Multilingual Shallow Fusion with Large Language Model</A>
								<DT><A HREF="https://arxiv.org/abs/2304.09151">[2304.09151] UniMax (mT5)</A>
								<DT><A HREF="https://arxiv.org/abs/2010.11934">[2010.11934] mT5: A massively multilingual pre-trained text-to-text transformer</A>
								<DT><A HREF="https://twitter.com/LucasBandarkar/status/1697650503726125294">Bebele: multilingual capabilities of LLMs</A>
								<DT><A HREF="https://unbabel.com/announcing-tower-an-open-multilingual-llm-for-translation-related-tasks/?utm_campaign=Tower%20Announcement&utm_content=278163588&utm_medium=social&utm_source=linkedin&hss_channel=lcp-3327165">Tower : An Open Multilingual LLM for Translation-Related Tasks</A>
								<DT><A HREF="https://www.microsoft.com/en-us/research/project/llmlingua/">LLMLingua - Microsoft Research</A>
								<DT><A HREF="https://twitter.com/arankomatsuzaki/status/1742369432091976085">Llama Beyond English: An Empirical Study on Language Capability</A>
								<DT><A HREF="https://www.youtube.com/watch?v=f1ZayIAz210">NeurIPS 2023 Oral 2A Efficient Learning - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-training-lectures</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=k_HMgpJKBso">LLaMA 2 w/ Thomas Scialom (LLaMA 2 lead) - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5RUOrXl3nag&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb">OPT-175B: Open Pretrained Transformer</A>
								<DT><A HREF="https://www.youtube.com/watch?v=g0gREwiDbis">Neural Networks Efficient Training Fundamentals</A>
								<DT><A HREF="https://www.youtube.com/watch?v=DK-QXsiycpk&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb&index=2">GPT-NeoX-20B | BigScience BLOOM</A>
								<DT><A HREF="https://www.youtube.com/watch?v=hc0u4avAkuM&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb&index=3">Megatron-LM | ZeRO | DeepSpeed | Mixed Precision</A>
								<DT><A HREF="https://www.youtube.com/watch?v=pTChDs5uD8I&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb&index=4">BigScience BLOOM | 3D Parallelism</A>
								<DT><A HREF="https://www.youtube.com/watch?v=iJ0IVZgGjTM&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb&index=5">T0: Multitask Prompted Training Enables</A>
								<DT><A HREF="https://www.youtube.com/watch?v=CAbHbm9769Q">Hugging Face: Accelerating Transformers in Production</A>
								<DT><A HREF="https://www.youtube.com/watch?v=jyOqtw4ry2w">8-bit Methods for Efficient Deep Learning with Tim Dettmers</A>
								<DT><A HREF="https://www.youtube.com/watch?v=B3Az2EONCHE">Large Language Models - Michael Douglas</A>
								<DT><A HREF="https://www.youtube.com/watch?v=R_o6nUC1Nzo&list=PLgKuh-lKre11GbZWneln-VZDLHyejO7YD&index=14">Tutorial on Deep Learning II - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=b1c1yh6I594">Math Reading Group - Neural Tangent Kernels (07/05/23)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=XfpMkf4rD6E">Transformers United 2023: Introduction to Transformers</A>
								<DT><A HREF="https://www.youtube.com/watch?v=tfWPCeyh77k">Lessons from scale for large language models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=bZQun8Y4L2A&t=723s">State of GPT | BRK216HFS - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=nUSvsLLlz9U">Tesla CVPR2023 Workshop - YouTube (Data Engine)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=DHjwbleAgPQ">Why do Neural Networks use Linear Algebra?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=tndPB8z98Zg">Machine Learning and Theory Calculations</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ytbYRIN0N4g">Softmax Function Explained In Depth with 3D Visuals</A>
								<DT><A HREF="https://www.youtube.com/watch?v=DZoICV92VGE">The Magic Behind QLORA: Efficient Finetuning of Quantized LLMs</A>
								<DT><A HREF="https://www.youtube.com/watch?v=0QczhVg5HaI&t=558s">Why Neural Networks can learn (almost) anything</A>
								<DT><A HREF="https://www.youtube.com/watch?v=j4yz1k3Ueso">The Power of Symmetry III</A>
								<DT><A HREF="https://www.youtube.com/watch?v=SGInyKjzF7A">Optimizing Large Language Models with Reinforcement Learning</A>
								<DT><A HREF="https://www.youtube.com/watch?v=2Zi6wFQQl3E">Generalization bounds for Neural Network Based Decoders</A>
								<DT><A HREF="https://www.youtube.com/watch?v=g0gREwiDbis">SLXCA 2021 featuring Dr. Geoffrey Hinton &amp; Jennifer Smith</A>
							</DL><p>
							<DT><H3 FOLDED>mixed-precision-training</H3>
							<DL><p>
								<DT><H3 FOLDED>amp-dtypes</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2209.05433">[2209.05433] FP8 Formats for Deep Learning</A>
									<DT><A HREF="https://github.com/P3109/Public/blob/main/Value%20Tables/make-value-tables.ipynb">P3109 - Arithmetic Formats for Machine Learning</A>
									<DT><A HREF="https://github.com/P3109/Public/blob/main/Shared%20Reports/P3109%20WG%20Interim%20Report.pdf">8-bit formats</A>
									<DT><A HREF="https://axil.github.io/a-comprehensive-guide-to-numpy-data-types.html">A Comprehensive Guide to NumPy Data Types</A>
									<DT><A HREF="https://github.com/google/jaxtyping">google/jaxtyping: Type annotations and runtime checking for shape and dtype of JAX/NumPy/PyTorch/etc. arrays.</A>
								</DL><p>
								<DT><A HREF="https://github.com/tspeterkim/mixed-precision-from-scratch">tspeterkim/mixed-precision-from-scratch: Mixed precision training from scratch with Tensors and CUDA</A>
								<DT><A HREF="https://arxiv.org/pdf/1710.03740.pdf">MIXED PRECISION TRAINING</A>
								<DT><A HREF="https://tspeterkim.github.io/posts/mixed-precision-from-scratch">Mixed Precision Training from Scratch | Taeksang Peter Kim</A>
								<DT><A HREF="https://arxiv.org/abs/2310.10537">[2310.10537] Microscaling Data Formats for Deep Learning</A>
								<DT><A HREF="https://github.com/Azure/MS-AMP">Azure/MS-AMP: Microsoft Automatic Mixed Precision Library (fp8)</A>
								<DT><A HREF="https://azure.github.io/MS-AMP/">MS-AMP Documentation | MS-AMP</A>
								<DT><A HREF="https://arxiv.org/abs/2309.17224">(Graphcore) Training and inference of LLMs using FP8</A>
								<DT><A HREF="https://twitter.com/sytelus/status/1713462676838588565">loss scaling FP16 training (training stability)</A>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">NVIDIA: Train With Mixed Precision</A>
								<DT><A HREF="https://arxiv.org/abs/2310.10537">language models at sub-8-bit weights, activations, and gradients</A>
								<DT><A HREF="https://arxiv.org/abs/1905.12322">[1905.12322] A Study of BFLOAT16 for Deep Learning Training</A>
								<DT><A HREF="https://github.com/P3109/Public/blob/main/Briefs/Discusson%20on%20Rounding.pdf">Public/Briefs/Discusson on Rounding.pdf</A>
								<DT><A HREF="https://arxiv.org/abs/2310.18313">[2310.18313] FP8-LM: Training FP8 Large Language Models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=xnBDg0lYQdU">Weekly AI paper overview- 6/18/24 - YouTube</A>
							</DL><p>
							<DT><A HREF="https://openai.com/research/techniques-for-training-large-neural-networks">Techniques for training large neural networks</A>
							<DT><A HREF="https://www.youtube.com/watch?v=fKMB5UlVY1E&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=27">Stanford CS25: V4 I Overview of Transformers - YouTube</A>
							<DT><A HREF="https://github.com/JonasGeiping/cramming">JonasGeiping/cramming: Cramming the training of a (BERT-type) language model into limited compute.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=2Zi6wFQQl3E">Generalization bounds for Neural Network Based Decoders</A>
							<DT><A HREF="https://www.youtube.com/watch?v=0D23NeBjCeQ">A Theory for Emergence of Complex Skills in Language Models</A>
							<DT><A HREF="https://www.youtube.com/watch?v=g0gREwiDbis">Dr. Geoffrey Hinton: A brief study of neural networks</A>
							<DT><A HREF="https://www.lesswrong.com/posts/diutNaWF669WgEt3v/the-scaling-inconsistency-openai-s-new-insight">One Epoch: the scaling¬†‚Äúinconsistency‚Äù</A>
							<DT><A HREF="https://twitter.com/Yampeleg/status/1672600308365623296">Cheatsheet: Full Training vs. LoRA Adapters VS. VS VS. Smaller Models</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-code-generation</H3>
						<DL><p>
							<DT><H3 FOLDED>language-models-code-generation-evaluation</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/amanrsanger/status/1588585377946021888?s=12&amp;t=WP151JoXlub3KMUA8iqo0g">Model pass@k evaluation comparison</A>
								<DT><A HREF="https://huggingface.co/spaces/evaluate-metric/code_eval">Hugging Face Evaluation: pass@k</A>
								<DT><A HREF="https://arxiv.org/pdf/2107.03374.pdf">Evaluating Large Language Models Trained on Code</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-code-generation-benchmarks</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/CodeXGLUE">microsoft/CodeXGLUE: CodeXGLUE</A>
								<DT><A HREF="https://paperswithcode.com/dataset/humaneval">HumanEval Dataset | Papers With Code</A>
								<DT><A HREF="https://openai.com/blog/grade-school-math/">Solving Math Word Problems</A>
								<DT><A HREF="https://arxiv.org/pdf/2210.14868.pdf">Multi-Lingual Code Generation (Execution Evaluation)</A>
								<DT><A HREF="https://huggingface.co/datasets/THUDM/humaneval-x">THUDM/humaneval-x ¬∑ Datasets at Hugging Face</A>
								<DT><A HREF="https://arxiv.org/abs/2211.11501">[2211.11501] DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-code-generation-multilingual</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/TransCoder">facebookresearch/TransCoder: Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf</A>
								<DT><A HREF="https://github.com/facebookresearch/TransCoder">facebookresearch/TransCoder</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-code-generation-architectures</H3>
							<DL><p>
								<DT><H3 FOLDED>DeepSeek-Coder</H3>
								<DL><p>
									<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-Coder-V2?tab=readme-ov-file">deepseek-ai/DeepSeek-Coder-V2</A>
								</DL><p>
								<DT><H3 FOLDED>Code Llama (Llama 3 70b)</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>Codestral</H3>
								<DL><p>
									<DT><A HREF="https://mistral.ai/news/codestral/">Codestral: Hello, World! | Mistral AI | Frontier AI in your hands</A>
								</DL><p>
								<DT><H3 FOLDED>StarCoder2</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>Codex</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2207.14255">Efficient Training of Language Models to Fill in the Middle</A>
									<DT><A HREF="https://arxiv.org/abs/2006.03511">Unsupervised Translation of Programming Languages</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Wc7dcwF7QaA&t=4846s">Codex: Evaluating Large Language Models Trained on Code - YouTube</A>
									<DT><A HREF="https://arxiv.org/pdf/2107.03374.pdf">Codex: Evaluating Large Language Models Trained on Code</A>
									<DT><H3 FOLDED>codex-training-data</H3>
									<DL><p>
										<DT><A HREF="https://help.openai.com/en/articles/5480054-understanding-codex-training-data-and-outputs">Understanding Codex training data and outputs | OpenAI Help Center</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>InCoder</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2204.05999.pdf">InCoder</A>
									<DT><A HREF="https://arxiv.org/abs/2204.05999">InCoder: A Generative Model for Code Infilling and Synthesis</A>
									<DT><A HREF="https://sites.google.com/view/incoder-code-models">InCoder</A>
								</DL><p>
								<DT><H3 FOLDED>CodeGen</H3>
								<DL><p>
									<DT><A HREF="https://github.com/salesforce/CodeGen/tree/main/codegen25">CodeGen/codegen25 at main ¬∑ salesforce/CodeGen</A>
									<DT><A HREF="https://github.com/facebookresearch/CodeGen">facebookresearch/CodeGen: Reference implementation of code generation projects from Facebook AI Research. General toolkit to apply machine learning to code, from dataset creation to model training and evaluation. Comes with pretrained models.</A>
									<DT><A HREF="https://arxiv.org/pdf/2203.13474.pdf">CodeGen (JAX): Conversational</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://arxiv.org/pdf/2107.03374.pdf">(Chen, 2021) Codex: Evaluating Large Language Models Trained on Code</A>
							<DT><A HREF="https://arxiv.org/pdf/2203.07814.pdf">(Deepmind, 2022) Competition-Level Code Generationwith AlphaCode</A>
							<DT><A HREF="https://arxiv.org/pdf/2207.14502.pdf">(HOT) Languages Models Can Teach Themselves (Kaplan Theorem)</A>
							<DT><A HREF="https://arxiv.org/abs/2207.10397">CodeT: Code Generation with Generated Tests</A>
							<DT><A HREF="https://github.com/microsoft/CodeT/tree/main/DIVERSE">CodeT/DIVERSE at main ¬∑ microsoft/CodeT</A>
							<DT><A HREF="https://arxiv.org/pdf/2208.05950.pdf">Interactive Code Generation via Test-Driven User-Intent Formalization</A>
							<DT><A HREF="https://arxiv.org/pdf/2102.07350.pdf">Prompt Programming: choice of prompts that determine the quality of output</A>
							<DT><A HREF="https://twitter.com/davisblalock/status/1558347542101839873">"Language Models Can Teach Themselves to Program Better"</A>
							<DT><A HREF="https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/?utm_source=linkedin&utm_medium=organic_social&utm_content=image&utm_campaign=fair">Meta Large Language Model Compiler: Foundation Models of Compiler Optimization | Research - AI at Meta</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-embeddings</H3>
						<DL><p>
							<DT><H3 FOLDED>embeddings-semantic-search</H3>
							<DL><p>
								<DT><A HREF="https://www.patterns.app/blog/2023/02/19/ask-hn-gpt-embeddings-question-answering/">AskHN - The collective GPT-embodied wisdom of Hacker News | Patterns</A>
								<DT><A HREF="https://www.youtube.com/watch?v=HAseTSX6FT8">Fast, Accurate and Robust Multilingual Syntactic Analysis ‚Äì Slav Petrov (Google) - 2012 - YouTube</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=RkYuH_K7Fx4">What is?</A>
							<DT><A HREF="https://twitter.com/_akhaliq/status/1742034428619096327">Improving Text Embeddings with Large Language Models (MS)</A>
							<DT><A HREF="https://twitter.com/din0s_/status/1742235150530851120">trained on synthetic retrieval data</A>
							<DT><A HREF="https://arxiv.org/abs/1911.02116">[1911.02116] Unsupervised Cross-lingual Representation Learning at Scale (Meta AI)</A>
							<DT><A HREF="https://ai.meta.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/">XLM-R: State-of-the-art cross-lingual understanding through self-supervision</A>
							<DT><A HREF="https://arxiv.org/abs/2401.00368">[ Improving Text Embeddings with Large Language Models (MSFT)</A>
							<DT><A HREF="https://github.com/RAIVNLab/MRL">RAIVNLab/MRL: Code repository for the paper - "Matryoshka Representation Learning"</A>
							<DT><A HREF="https://twitter.com/adityakusupati/status/1541554858829815809">(1) Aditya Kusupati ü™Ü en X: "Introductingü™ÜMatryoshka Representations for Adaptive Deploymentü™Ü TL;DR: up to 14√ó lower real-world classification &amp;amp; retrival costs at web-scale at no loss in accuracy &amp;amp; w/o any overhead across setups. Paper: https://t.co/JMP9ED72L2 Code: https://t.co/SEccseeDxz [1/11] https://t.co/dXl03V1CUc" / X</A>
							<DT><A HREF="https://huggingface.co/blog/embedding-quantization">Binary and Scalar Embedding Quantization for Significantly Faster &amp; Cheaper Retrieval</A>
							<DT><A HREF="https://github.com/tensorchord/pgvecto.rs">tensorchord/pgvecto.rs: Scalable, Low-latency and Hybrid-enabled Vector Search in Postgres. Revolutionize Vector Search, not Database.</A>
							<DT><A HREF="https://github.com/UKPLab/sentence-transformers/releases/tag/v2.7.0">Release v2.7.0 - CachedGISTEmbedLoss, easy Matryoshka inference &amp; evaluation, CrossEncoder, Intel Gaudi2 ¬∑ UKPLab/sentence-transformers</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-fine-tuning</H3>
						<DL><p>
							<DT><H3 FOLDED>Instruction Tuning</H3>
							<DL><p>
								<DT><H3 FOLDED>Datasets</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/yuntiandeng/status/1724503257601458575">WildChat: 650K user-ChatGPT interactions in the wild</A>
								</DL><p>
								<DT><H3 FOLDED>FLAN</H3>
								<DL><p>
									<DT><H3 FOLDED>flan-prompt-template</H3>
									<DL><p>
										<DT><A HREF="https://jrodthoughts.medium.com/this-google-model-combines-reasoning-and-acting-in-a-single-language-model-935317c3b111">Combine Reasoning and Acting in a Single Language Model</A>
									</DL><p>
									<DT><A HREF="https://jrodthoughts.medium.com/this-google-model-combines-reasoning-and-acting-in-a-single-language-model-935317c3b111">Combine Reasoning and Acting in a Single Language Model</A>
									<DT><A HREF="https://github.com/google-research/FLAN/blob/main/flan/baseline_templates.py">Templates for baseline guide language models (GLM) prompts</A>
									<DT><A HREF="https://arxiv.org/abs/2109.01652">[2109.01652] Finetuned Language Models Are Zero-Shot Learners</A>
									<DT><A HREF="https://arxiv.org/abs/2301.13688">[2301.13688] The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/model_doc/flan-t5">FLAN-T5</A>
									<DT><A HREF="https://arxiv.org/abs/2210.11416">Flan-PaLM: Scaling Instruction-Finetuned Language Models</A>
									<DT><A HREF="https://chat.openai.com/c/e261dbd6-8206-4e2d-8fdc-17cb3aafd611">Textual graph structures representations</A>
								</DL><p>
								<DT><A HREF="https://openai.com/blog/instruction-following/">InstructGPT</A>
								<DT><A HREF="https://twitter.com/ShayneRedford/status/1683927467436937219">The FLAN Collection</A>
								<DT><A HREF="https://twitter.com/burkov/status/1715541145881624617">chatting capability</A>
								<DT><A HREF="https://twitter.com/karpathy/status/1728143712059056467">(Andrej Karpathy) special tokens &lt;|BROWSE|&gt;  tooling usage</A>
								<DT><A HREF="https://twitter.com/Muennighoff/status/1717212559584149674">complexity of instruction data</A>
								<DT><A HREF="https://www.youtube.com/watch?v=mcep6W8oB1I&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=22">Stanford CS25: V3 I Recipe for Training Helpful Chatbots - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>Supervised-Fine-Tuning (SFT)</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>language-models-fine-tuning-rl</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=SGInyKjzF7A">Optimizing Large Language Models with Reinforcement Learning</A>
								<DT><A HREF="https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81">Reinforcement Learning for Language Models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=dbo3kNKPaUA">Large Language Models (in 2023)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=zjrM-MW-0y0&t=5s">Instruction finetuning and RLHF lecture (NYU CSCI 2590)</A>
								<DT><H3 FOLDED>RLHF</H3>
								<DL><p>
									<DT><A HREF="https://www.lesswrong.com/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research">Thoughts on the impact of RLHF research</A>
									<DT><A HREF="https://www.youtube.com/watch?v=SGInyKjzF7A">Optimizing Large Language Models with Reinforcement Learning</A>
									<DT><A HREF="https://twitter.com/vwxyzjn/status/1716818343133659598">RLHF codebase</A>
									<DT><A HREF="https://www.youtube.com/watch?v=fqC3D-zNJUM">AI Safety, RLHF, and Self-Supervision - Jared Kaplan</A>
									<DT><A HREF="https://twitter.com/_robertkirk/status/1712083230965280784">understanding the effects of RLHF on LLM generalisation</A>
									<DT><A HREF="https://huggingface.co/blog/rlhf">Illustrating Reinforcement Learning from Human Feedback (RLHF)</A>
									<DT><H3 FOLDED>PPO</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2307.04964">[2307.04964] Secrets of RLHF in Large Language Models Part I: PPO</A>
									</DL><p>
									<DT><H3 FOLDED>DPO</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2305.18290.pdf">Direct Preference Optimization (DPO): Your Language Model is Secretly a Reward Model</A>
									</DL><p>
									<DT><A HREF="https://twitter.com/rm_rafailov/status/1781145338759533016">Language models are not a reward function, but Q func</A>
								</DL><p>
								<DT><A HREF="https://github.com/huggingface/trl">huggingface/trl: Train transformer language models with reinforcement learning.</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-fine-tuning-peft</H3>
							<DL><p>
								<DT><H3 FOLDED>LoRA</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2106.09685">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</A>
									<DT><A HREF="https://www.youtube.com/watch?v=NDV65M-2T3g">Flat minima generalize for low-rank matrix recovery - YouTube</A>
									<DT><A HREF="https://github.com/punica-ai/punica">punica-ai/punica: Serving multiple LoRA finetuned LLM as one</A>
								</DL><p>
								<DT><H3 FOLDED>Prompt-Tuning Large Languages Models for Code-to-Code Generation</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2102.07350.pdf">Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</A>
									<DT><A HREF="https://arxiv.org/pdf/2202.13169.pdf">A SYSTEMATIC EVALUATION OF LARGE LANGUAGE MODELS OF CODE</A>
									<DT><A HREF="https://arxiv.org/pdf/2208.05950.pdf">Interactive Code Generation via Test-Driven User-Intent Formalization</A>
									<DT><A HREF="https://openreview.net/pdf?id=NiEtU7blzN">LARGE LANGUAGE MODELS CAN SELF-IMPROVE</A>
									<DT><A HREF="https://arxiv.org/pdf/2107.03374.pdf">Evaluating Large Language Models Trained on Code</A>
									<DT><A HREF="https://www.youtube.com/watch?v=80jwVkYOu0w">The Power of Scale for Parameter-Efficient Prompt Tuning - YouTube</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2401.01286.pdf">A Comprehensive Study of Knowledge Editing for Large Language Models</A>
								<DT><A HREF="https://arxiv.org/pdf/2205.05638.pdf">T-FEW: Few-Shot Parameter-Efficient Fine-Tuning vs In-Context Learning</A>
								<DT><A HREF="https://twitter.com/cloneofsimo/status/1739452121119150288">ICL inferior compared to prompt-tuning and instruct-prompt tuning (IPT)</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-fine-tuning-merge</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1725513067293724897">CombLM</A>
								<DT><A HREF="https://arxiv.org/abs/2312.15166">[2312.15166] SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling</A>
								<DT><A HREF="https://huggingface.co/papers/2312.15166">Paper page - SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling</A>
								<DT><A HREF="https://arxiv.org/abs/2305.15296">[2305.15296] MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation</A>
								<DT><A HREF="https://x.com/_akhaliq/status/1794938544336568380">Stacking Your Transformers A Closer Look at Model Growth for Efficient LLM Pre-Training LLMs are computationally expensive to pre-train</A>
								<DT><A HREF="https://x.com/nikdimitriadis/status/1797638468610466044">(1) Dimitriadis Nikos en X: "Wouldn't it be great if we could merge the knowledge of 20 specialist models into a single one without losing performance? üí™üèª Introducing our new ICML paper "Localizing Task Information for Improved Model Merging and Compression". üéâ üìú: https://t.co/JC4mdujKkd üßµ1/9 https://t.co/JrCE0DBYlT" / X</A>
								<DT><A HREF="https://x.com/gabriel_ilharco/status/1603415656699162624">(1) Gabriel Ilharco en X: "Introducing task vectors! A new way to steer models by doing arithmetic with model weights. Subtract to make models forget, add to make them learn üìú: https://t.co/YNQvdYtdSN üñ•Ô∏è: https://t.co/CVFM68u322 https://t.co/FBgdpByhUB" / X</A>
								<DT><A HREF="https://github.com/mlfoundations/task_vectors">mlfoundations/task_vectors: Editing Models with Task Arithmetic</A>
								<DT><A HREF="https://arxiv.org/pdf/2405.07813">Localizing Task Information for Improved Model Merging and Compression</A>
								<DT><A HREF="https://x.com/francoisfleuret/status/1797652614366339545">(1) Fran√ßois Fleuret en X: "TL;DR: You fine-tune a model on T tasks, and store a bit per task / parameter that indicates to use the base or the multi-task value. So only 2 x 16 + T bits per parameter, and performance are virtually as good as with T dedicated models." / X</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2405.05904#:~:text=We%20demonstrate%20that%20large%20language,consistent%20with%20the%20model's%20knowledge.">[2405.05904] Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</A>
							<DT><A HREF="https://blog.scottlogic.com/2023/11/24/llm-mem.html">LLM finetuning memory requirements</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mcep6W8oB1I">Stanford CS25: V3 I Recipe for Training Helpful Chatbots</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Ckz8XA2hW84&list=LL&index=14&t=1s">Ilya Sutskever (OpenAI) and Jensen Huang (NVIDIA CEO) : AI Today and Vision of the Future (3/2023) - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-retrieval</H3>
						<DL><p>
							<DT><A HREF="https://media.licdn.com/dms/image/D4E22AQENEUGr4PY3Tg/feedshare-shrink_800/0/1699343585425?e=1702512000&v=beta&t=V7uFCIOwllfoE0Is1EpAWSqidGHcZJ2VBGEOpkgmzu8">OpenAI: RAG Success Story</A>
							<DT><A HREF="https://www.youtube.com/watch?v=g-VvYLhYhOg">Jerry Liu‚ÄìLlamaIndex ‚Äì Practical Data Considerations for building Production-Ready LLM Applications - YouTube</A>
							<DT><A HREF="https://github.com/tantaraio/voy">tantaraio/voy: üï∏Ô∏èü¶Ä A WASM vector similarity search written in Rust</A>
							<DT><A HREF="https://docs.google.com/presentation/d/19lAeVzPkh20Ly855tKDkz1uv-1pHV_9GxfntiTJPUug/edit#slide=id.g2584b5dafc1_0_2025">SIGIR 2023 keynote - Google DeepMind</A>
							<DT><A HREF="https://twitter.com/_akhaliq/status/1742369195034099731">DocLLM: A layout-aware generative language model for multimodal document understanding</A>
							<DT><A HREF="https://arxiv.org/pdf/2209.11755.pdf">Promptagator: Few-shot Dense Retrieval From 8 Examples</A>
							<DT><A HREF="https://arxiv.org/abs/2401.00368">[2401.00368] Improving Text Embeddings with Large Language Models</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mE7IDf2SmJg">Stanford CS25: V3 I Retrieval Augmented Language Models - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=SgVZBn5QNHk">UMass CS685 S24 (Advanced NLP) #21: Detecting LLM-generated text / LLM security - YouTube</A>
							<DT><H3 FOLDED>Semantic Search</H3>
							<DL><p>
								<DT><H3 FOLDED>Embedding</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=RkYuH_K7Fx4">What is?</A>
									<DT><A HREF="https://twitter.com/_akhaliq/status/1742034428619096327">Improving Text Embeddings with Large Language Models (MS)</A>
									<DT><A HREF="https://twitter.com/din0s_/status/1742235150530851120">trained on synthetic retrieval data</A>
									<DT><A HREF="https://arxiv.org/abs/1911.02116">[1911.02116] Unsupervised Cross-lingual Representation Learning at Scale (Meta AI)</A>
									<DT><A HREF="https://ai.meta.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/">XLM-R: State-of-the-art cross-lingual understanding through self-supervision</A>
									<DT><A HREF="https://arxiv.org/abs/2401.00368">[ Improving Text Embeddings with Large Language Models (MSFT)</A>
									<DT><A HREF="https://github.com/RAIVNLab/MRL">RAIVNLab/MRL: Code repository for the paper - "Matryoshka Representation Learning"</A>
									<DT><A HREF="https://twitter.com/adityakusupati/status/1541554858829815809">(1) Aditya Kusupati ü™Ü en X: "Introductingü™ÜMatryoshka Representations for Adaptive Deploymentü™Ü TL;DR: up to 14√ó lower real-world classification &amp;amp; retrival costs at web-scale at no loss in accuracy &amp;amp; w/o any overhead across setups. Paper: https://t.co/JMP9ED72L2 Code: https://t.co/SEccseeDxz [1/11] https://t.co/dXl03V1CUc" / X</A>
									<DT><A HREF="https://huggingface.co/blog/embedding-quantization">Binary and Scalar Embedding Quantization for Significantly Faster &amp; Cheaper Retrieval</A>
									<DT><A HREF="https://github.com/tensorchord/pgvecto.rs">tensorchord/pgvecto.rs: Scalable, Low-latency and Hybrid-enabled Vector Search in Postgres. Revolutionize Vector Search, not Database.</A>
									<DT><A HREF="https://github.com/UKPLab/sentence-transformers/releases/tag/v2.7.0">Release v2.7.0 - CachedGISTEmbedLoss, easy Matryoshka inference &amp; evaluation, CrossEncoder, Intel Gaudi2 ¬∑ UKPLab/sentence-transformers</A>
								</DL><p>
								<DT><A HREF="https://www.patterns.app/blog/2023/02/19/ask-hn-gpt-embeddings-question-answering/">AskHN - The collective GPT-embodied wisdom of Hacker News | Patterns</A>
								<DT><A HREF="https://www.youtube.com/watch?v=HAseTSX6FT8">Fast, Accurate and Robust Multilingual Syntactic Analysis ‚Äì Slav Petrov (Google) - 2012 - YouTube</A>
							</DL><p>
							<DT><A HREF="https://x.com/headinthebox/status/1799642956657508585">(1) Erik Meijer en X: "The world's ugliest poster for https://t.co/duXtHCj3mF Yet it captures everything I want to say. https://t.co/ieQGpOdm4q" / X</A>
							<DT><A HREF="https://x.com/din0s_/status/1801271625309937771">(1) dinos en X: "üìö Awesome Information Retrieval üîç I‚Äôve compiled a list of some of my favorite IR papers from the past few years. If you‚Äôre new to the field and want to understand how Transformer-based retrieval models work before building your RAG application, this should serve as a great https://t.co/SiMm6WKTZR" / X</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-multimodal</H3>
						<DL><p>
							<DT><H3 FOLDED>adept labs</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/adept/fuyu-8b">adept/fuyu-8b ¬∑ Hugging Face</A>
								<DT><A HREF="https://www.adept.ai/">Adept: Useful General Intelligence</A>
								<DT><A HREF="https://www.adept.ai/blog/fuyu-8b">Fuyu-8B: A Multimodal Architecture for AI Agents</A>
								<DT><A HREF="https://www.adept.ai/blog/adept-fuyu-heavy">Adept Fuyu-Heavy: A new multimodal model</A>
								<DT><A HREF="https://www.adept.ai/blog/persimmon-8b">Releasing Persimmon-8B</A>
							</DL><p>
							<DT><H3 FOLDED>LLaVA</H3>
							<DL><p>
								<DT><A HREF="https://github.com/haotian-liu/LLaVA#install">haotian-liu/LLaVA: [NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond.</A>
							</DL><p>
							<DT><H3 FOLDED>multimodal-bytedance</H3>
							<DL><p>
								<DT><A HREF="https://github.com/bytedance/MoMA">bytedance/MoMA: MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</A>
							</DL><p>
							<DT><A HREF="https://huggingface.co/papers/2311.00571">Paper page - LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing</A>
							<DT><A HREF="https://www.youtube.com/watch?v=DTpKUnbkT_k">Multimodal Reasoning: PaLM-E &amp; Gemini - Aakanksha Chowdhery | Stanford MLSys #90 - YouTube</A>
							<DT><A HREF="https://arxiv.org/abs//2404.04125">[2404.04125] No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</A>
							<DT><A HREF="https://github.com/xinyu1205/recognize-anything">xinyu1205/recognize-anything: Open-source and strong foundation image recognition models.</A>
							<DT><A HREF="https://github.com/mlfoundations/clip_quality_not_quantity">mlfoundations/clip_quality_not_quantity</A>
							<DT><A HREF="https://publications.reka.ai/reka-core-tech-report.pdf">Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models</A>
							<DT><A HREF="https://x.com/DrJimFan/status/1793318771932995793">(1) Jim Fan en X: "What makes up the abstract concept of an apple? We read the word "apple" as a string, see 2D pictures online, 3D shape in real life, and moving apples in videos. We touch the apple, feel its geometry in our palms and texture through the rich tactile sensation on our fingers. Do https://t.co/2LzxYa4f3N" / X</A>
							<DT><A HREF="https://x.com/Ethan_smith_20/status/1792024324464857197">(1) Ethan en X: "if you like this, you'll really like this https://t.co/NHgBACYxGT pretraining on text is merely an adequately challenging modality to learn computational primitives for universal transfer. there are other modalities which we can do causal modeling on and develop similar https://t.co/A8X0NRZpuP" / X</A>
							<DT><A HREF="https://arxiv.org/abs/2103.05247">[2103.05247] Pretrained Transformers as Universal Computation Engines</A>
							<DT><A HREF="https://www.youtube.com/watch?v=IwYiETZEGY0">What does AI have to do with Plato's Allegory of the Cave? - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=IwYiETZEGY0&list=LL&index=11&t=25s">What does AI have to do with Plato's Allegory of the Cave? - YouTube</A>
							<DT><A HREF="https://arxiv.org/abs/2405.07987">[2405.07987] The Platonic Representation Hypothesis</A>
							<DT><A HREF="https://github.com/facebookresearch/ImageBind">facebookresearch/ImageBind: ImageBind One Embedding Space to Bind Them All</A>
							<DT><A HREF="https://x.com/olivierhenaff/status/1805995802352910557">multimodal dataset creation: joint example selection (JEST)</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-safety</H3>
						<DL><p>
							<DT><H3 FOLDED>Red Teaming</H3>
							<DL><p>
								<DT><A HREF="https://openai.com/blog/red-teaming-network">OpenAI Red Teaming Network</A>
								<DT><A HREF="https://twitter.com/rharang/status/1725161975976497627">NVIDIA NeMo-Guardrails</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback (Antrophic)</A>
							<DT><A HREF="https://ai.meta.com/llama/purple-llama/">Purple Llama - AI at Meta</A>
							<DT><A HREF="https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/">Best Practices for Securing LLM-Enabled Applications</A>
							<DT><A HREF="https://github.com/NVIDIA/NeMo-Guardrails">NVIDIA/NeMo-Guardrails: NeMo Guardrails</A>
							<DT><A HREF="https://arxiv.org/abs/2302.10149">[2302.10149] Poisoning Web-Scale Training Datasets is Practical</A>
							<DT><A HREF="https://arxiv.org/pdf/2404.13208">The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions</A>
							<DT><A HREF="https://openai.com/index/openai-safety-update/">OpenAI safety practices | OpenAI</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-formal-proving</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=u-pkmdkQoMU">Alex Gu | LeanDojo: Theorem Proving with Retrieval-Augmented Language Models - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=dOplrIJEYBo">Alpha Everywhere: AlphaGeometry, AlphaCodium and the Future of LLMs - YouTube</A>
							<DT><A HREF="https://github.com/google-deepmind/alphageometry">google-deepmind/alphageometry</A>
							<DT><A HREF="https://www.youtube.com/watch?v=AayZuuDDKP0">Terence Tao, "Machine Assisted Proof" - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=eZbYSOpga2U">Trieu H. Trinh | Solving olympiad geometry without human demonstrations - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-ASR</H3>
						<DL><p>
							<DT><H3 FOLDED>whisper</H3>
							<DL><p>
								<DT><A HREF="https://github.com/openai/whisper">openai/whisper: Robust Speech Recognition via Large-Scale Weak Supervision</A>
								<DT><A HREF="https://github.com/ggerganov/whisper.cpp/pull/1500">whisper : quantize encoder only by ggerganov ¬∑ Pull Request #1500 ¬∑ ggerganov/whisper.cpp</A>
							</DL><p>
							<DT><A HREF="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices">Navigating the Challenges and Opportunities of Synthetic Voices</A>
							<DT><A HREF="https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0">mozilla-foundation/common_voice_17_0 ¬∑ Datasets at Hugging Face</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-biology</H3>
						<DL><p>
							<DT><A HREF="https://www.abzu.ai/">Bring drugs to market faster with Abzu¬Æ</A>
							<DT><A HREF="https://github.com/epfLLM/meditron">epfLLM/meditron: Meditron is a suite of open-source medical Large Language Models (LLMs).</A>
							<DT><H3 FOLDED>Protein</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2305.16634">[2305.16634] Machine Learning for Protein Engineering</A>
								<DT><A HREF="https://310.ai/">End 2 End AI Molecule Design ‚Äì We generate diverse de novo protein sequences from just a text description of the desired properties by Mol.E , a state-of-the-art ML model</A>
								<DT><A HREF="https://www.exscientia.ai/">Exscientia | AI Drug Discovery | Pharmatech</A>
							</DL><p>
							<DT><A HREF="https://atelfo.github.io/2023/12/23/biopharma-from-janssen-to-today.html">The pharma industry from Paul Janssen to today: why drugs got harder to develop and what we can do about it | Alex‚Äôs blog</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-byte-models</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=9U1L5qEvly0">[short] Beyond Language Models: Byte Models are Digital World Simulators - YouTube</A>
							<DT><A HREF="https://arxiv.org/abs//2402.19155">[2402.19155] Beyond Language Models: Byte Models are Digital World Simulators</A>
							<DT><A HREF="https://arxiv.org/abs/2401.13660">[2401.13660] MambaByte: Token-free Selective State Space Model</A>
						</DL><p>
						<DT><H3 FOLDED>Deepseek Core Readings</H3>
						<DL><p>
							<DT><A HREF="https://x.com/teortaxesTex/status/1787866166242763217">(1) Teortaxes‚ñ∂Ô∏è en X: "It's still only 6 papers. Deepseek Core Readings, Volume 1: 1 LLM (hyperparams, dataset basics) https://t.co/qJJx3Jj4FP 2 Coder (data engineering, continued pretraining) https://t.co/WP0HxIaqTF 3 Fine-grained MoE (architecture design) https://t.co/yb11AiI7T7 4 Math (better data," / X</A>
							<DT><A HREF="https://arxiv.org/abs/2401.02954">[2401.02954] DeepSeek LLM: Scaling Open-Source Language Models with Longtermism</A>
							<DT><A HREF="https://arxiv.org/abs/2401.14196">[2401.14196] DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence</A>
							<DT><A HREF="https://arxiv.org/abs/2401.06066">[2401.06066] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</A>
							<DT><A HREF="https://arxiv.org/abs/2402.03300">[2402.03300] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</A>
							<DT><A HREF="https://arxiv.org/abs/2403.05525">[2403.05525] DeepSeek-VL: Towards Real-World Vision-Language Understanding</A>
							<DT><A HREF="https://arxiv.org/abs/1911.02150">[1911.02150] Fast Transformer Decoding: One Write-Head is All You Need</A>
							<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/deepseek-v2-tech-report.pdf">DeepSeek-V2/deepseek-v2-tech-report.pdf at main ¬∑ deepseek-ai/DeepSeek-V2</A>
							<DT><A HREF="https://github.com/deepseek-ai">DeepSeek</A>
							<DT><A HREF="https://x.com/AdeptAILabs/status/1699835884097651145">AdeptAI Persimmon--8B</A>
							<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf">DeepSeek-Coder-V2/paper.pdf at main ¬∑ deepseek-ai/DeepSeek-Coder-V2</A>
							<DT><A HREF="https://x.com/teortaxesTex/status/1805055350011232352/photo/1">DeepSeek timeline</A>
						</DL><p>
						<DT><H3 FOLDED>VLM</H3>
						<DL><p>
							<DT><A HREF="https://www.microsoft.com/en-us/research/publication/florence-2-advancing-a-unified-representation-for-a-variety-of-vision-tasks/">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks - Microsoft Research</A>
						</DL><p>
						<DT><H3 FOLDED>large-languages-models-intuitions</H3>
						<DL><p>
							<DT><A HREF="https://x.com/_jasonwei/status/1729585618311950445">(1) Jason Wei en X: "It was an honor to give a guest lecture yesterday at Stanford‚Äôs CS330 class, "Deep Multi-Task and Meta-Learning"! I discussed a few very simple intuitions for how I personally think about large language models. Slides: https://t.co/NmusNTUVXb Here are the six intuitions: (1) https://t.co/qjmy7FGWWv" / X</A>
							<DT><A HREF="https://docs.google.com/presentation/d/1hQUd3pF8_2Gr2Obc89LKjmHL0DlH-uof9M0yFVd3FA4/edit#slide=id.g16197112905_0_0">jason wei cs330 talk - Google Slides</A>
							<DT><A HREF="https://x.com/hwchung27/status/1710003293223821658">(1) Hyung Won Chung en X: "I gave a talk at Seoul National University. I titled the talk ‚ÄúLarge Language Models (in 2023)‚Äù. This was an ambitious attempt to summarize our exploding field. Video: https://t.co/91GKf7kLQy Slides: https://t.co/cigAj0M4PD Trying to summarize the field forced me to think https://t.co/PjzWf5H8vU" / X</A>
						</DL><p>
						<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-V2">deepseek-ai/DeepSeek-V2: DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</A>
						<DT><A HREF="https://x.com/teortaxesTex/status/1787866166242763217">papers. Deepseek Core Readings, Volume 1: 1 LLM (hyperparams, dataset basics)</A>
						<DT><A HREF="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2: Language Models are Unsupervised Multitask Learners</A>
						<DT><A HREF="https://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling with Pathways (Arch &amp; Training Setup)</A>
						<DT><A HREF="https://arxiv.org/abs/2305.10403">[2305.10403] PaLM 2 Technical Report</A>
						<DT><A HREF="https://openai.com/research/better-language-models">GPT-2: Better language models and their implications</A>
						<DT><A HREF="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</A>
						<DT><A HREF="https://research.nvidia.com/publication/2024-06_nemotron-4-340b">Nemotron-4 340B | Research</A>
						<DT><A HREF="https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T.pdf">Nemotron-4 340B Technical Report</A>
						<DT><A HREF="https://arxiv.org/abs/2210.11416">FLAN: Scaling Instruction-Finetuned Language Models</A>
						<DT><A HREF="https://arxiv.org/abs/1706.03762">[1706.03762] Attention Is All You Need</A>
						<DT><A HREF="https://openai.com/research/ai-and-compute#appendix-methods">AI and compute</A>
						<DT><A HREF="https://github.com/lucidrains/self-rewarding-lm-pytorch">lucidrains/self-rewarding-lm-pytorch: Implementation of the training framework proposed in Self-Rewarding Language Model, from MetaAI</A>
						<DT><A HREF="https://www.youtube.com/watch?v=KCXDr-UOb9A">Large Language Models in Five Formulas</A>
						<DT><A HREF="https://arxiv.org/abs/2403.15360">[2403.15360] SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series</A>
						<DT><A HREF="https://twitter.com/_akhaliq/status/1772833121609679216">InternLM2 Technical Report</A>
						<DT><A HREF="https://github.com/badripatro/Simba">badripatro/simba: Simba</A>
						<DT><A HREF="https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/">Snowflake Arctic - LLM for Enterprise AI</A>
						<DT><A HREF="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm">Introducing DBRX: A New State-of-the-Art Open LLM | Databricks Blog</A>
						<DT><A HREF="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330">Granite Code Models - a ibm-granite Collection</A>
						<DT><A HREF="https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8">Yi-1.5 (2024/05) - a 01-ai Collection</A>
						<DT><A HREF="https://cohere.com/blog/command-r">Command R: RAG at Production Scale</A>
						<DT><A HREF="https://github.com/xai-org/grok-1">xai-org/grok-1: Grok open release</A>
						<DT><A HREF="https://www.youtube.com/watch?v=zJKGhiZp1uU">Math Reading Group - Kolmogorov Arnold Networks (EvelynM) - (08/06/2024) - YouTube</A>
						<DT><A HREF="https://github.com/kyegomez/Reka-Torch">kyegomez/Reka-Torch: Implementation of the model: "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models" in PyTorch</A>
						<DT><A HREF="https://www.evolutionaryscale.ai/">EvolutionaryScale</A>
						<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
						<DT><A HREF="https://imbue.com/research/70b-intro/">Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings - imbue</A>
						<DT><A HREF="https://github.com/imbue-ai/cluster-health">imbue-ai/cluster-health</A>
						<DT><A HREF="https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/?utm_source=twitter&utm_medium=organic_social&utm_content=link&utm_campaign=fair">Meta Large Language Model Compiler: Foundation Models of Compiler Optimization | Research - AI at Meta</A>
						<DT><A HREF="https://proceedings.mlr.press/v202/geiping23a.html">Cramming: Training a Language Model on a single GPU in one day.</A>
						<DT><A HREF="https://www.youtube.com/watch?v=orDKvo8h71o">Stanford CS25: V4 I Hyung Won Chung of OpenAI - YouTube</A>
						<DT><A HREF="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g2885e521b53_0_0">Language Language Models (in 2023) - Google Slides</A>
						<DT><A HREF="https://arxiv.org/abs/2207.09238">[2207.09238] Formal Algorithms for Transformers</A>
						<DT><A HREF="https://publications.reka.ai/reka-core-tech-report.pdf">Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models</A>
					</DL><p>
					<DT><H3 FOLDED>Hardware</H3>
					<DL><p>
						<DT><H3 FOLDED>hw-GPU</H3>
						<DL><p>
							<DT><H3 FOLDED>gpu-architecture</H3>
							<DL><p>
								<DT><H3 FOLDED>Ampere</H3>
								<DL><p>
									<DT><A HREF="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/">NVIDIA Ampere Architecture In-Depth</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/pdf/Ampere_Tuning_Guide.pdf">Ampere Tuning Guide</A>
									<DT><A HREF="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">A100 Tensor Core GPU Architecture</A>
								</DL><p>
								<DT><H3 FOLDED>Ada Lovelace</H3>
								<DL><p>
									<DT><A HREF="https://forums.developer.nvidia.com/t/ada-geforce-rtx-4090-fp8-cublaslt-performance/250737">Ada GeForce (RTX 4090) FP8 cuBLASLt performance</A>
									<DT><A HREF="https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf">NVIDIA ADA GPU ARCHITECTURE</A>
									<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/606">FP8 Unable to achieve the expected FLOPS indicator in 4090 ¬∑ Issue #606 ¬∑ NVIDIA/TransformerEngine</A>
								</DL><p>
								<DT><H3 FOLDED>Hopper</H3>
								<DL><p>
									<DT><H3 FOLDED>H100</H3>
									<DL><p>
										<DT><A HREF="https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper">NVIDIA H100 Tensor Core GPU Architecture Overview</A>
										<DT><A HREF="https://www.youtube.com/watch?v=MC223HlPdK0">Stanford Seminar - Nvidia‚Äôs H100 GPU - YouTube</A>
										<DT><A HREF="https://lambdalabs.com/blog/flashattention-2-on-lambda-cloud-h100-vs-a100">NVIDIA H100 vs A100 Benchmarks for FlashAttention-2 on Lambda Cloud</A>
										<DT><A HREF="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">NVIDIA Hopper Architecture In-Depth</A>
									</DL><p>
									<DT><H3 FOLDED>Grace Hopper: GH200</H3>
									<DL><p>
										<DT><A HREF="https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/">NVIDIA Grace Hopper Superchip Architecture In-Depth</A>
										<DT><A HREF="https://cf-store.widencdn.net/nvdam/6/9/4/694f8b68-0f19-4f59-af54-84cb33f1e27b.pdf?response-content-disposition=inline%3B%20filename%3D%22grace-hopper-superchip-datasheet-2705455.pdf%22&response-content-type=application%2Fpdf&Expires=1710177055&Signature=lsuy6fnxVz6h-NHXjUsDtFA~pGYyWOT5uF3CZgu8H7C9rROz4w2dvCVLHg2Z4D-TFW-aqq3Z4GmPs6MGcZ6Xqj7nfwegQDQP~~9Ew-TMxsRhiEjbbQqscqnDRysCaqJph6VxudMlh-kU-c67Thwxb1VrlkhHgBOEYIO2x4nQs0LFYs92nANSSCHe71~HWCEM35rl1Kzk2yLJbAlFNUpb-c0lTPP9HoYFAq4c5tJbWcH~nOwHAjcd~uqi3EVqJk8QuHwiVysZ-GSJL7HnbVh~5lnEkBmGFvRQT7lSBcNCCWovWqX~NAhmx7VC7tvkyX5IcC7NlyirXrGeOxdHGkFOzg__&Key-Pair-Id=APKAJD5XONOBVWWOA65A">NVIDIA GH200 GraceHopper Superchip</A>
										<DT><A HREF="https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper?ncid=so-link-825427-vt25#cid=hpc012_so-link_en-us">NVIDIA Grace Hopper Superchip Architecture Whitepaper</A>
										<DT><A HREF="https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-cpu-superchip?ncid=so-link-825427-vt25">NVIDIA Grace CPU Superchip Whitepaper</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2402.13499.pdf">Benchmarking and Dissecting the Nvidia Hopper GPU Architecture</A>
									<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-flashattention-2/">A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library ‚Äì Colfax Research</A>
									<DT><A HREF="https://docs.nvidia.com/grace-performance-tuning-guide.pdf">NVIDIA Grace Performance Tuning Guide</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/pdf/Hopper_Tuning_Guide.pdf">Hopper Tuning Guide</A>
									<DT><A HREF="https://gist.github.com/edecoux/a0dd3f446bb901be25f2b11be231820e">Tensor Core GPU.md</A>
								</DL><p>
								<DT><H3 FOLDED>Blackwell</H3>
								<DL><p>
									<DT><A HREF="https://nvdam.widen.net/s/xqt56dflgh/nvidia-blackwell-architecture-technical-brief">nvidia-blackwell-architecture-technical-brief.pdf</A>
									<DT><A HREF="https://www.nvidia.com/en-us/data-center/gb200-nvl72/">GB200 NVL72 | NVIDIA</A>
								</DL><p>
								<DT><H3 FOLDED>Instinct</H3>
								<DL><p>
									<DT><H3 FOLDED>MI300X</H3>
									<DL><p>
										<DT><A HREF="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/data-sheets/amd-instinct-mi300x-platform-data-sheet.pdf">DATA SHEET</A>
										<DT><A HREF="https://www.colfax-intl.com/Servers/CX8850s-EI9">Colfax CX8850s-EI9 8U Rackmount Server with 8x AMD Instinct‚Ñ¢ MI300X Accelerators</A>
										<DT><A HREF="https://www.reddit.com/r/AMD_Technology_Bets/comments/15lqkiz/no_its_not_comparable_to_the_mi300x_nvidia/">No it's not comparable to the MI300X! - "Nvidia Unveils DGX GH200 Superchip as Response to AMD‚Äôs MI300X" - chiplets of the MI300 is unique offers custom nothing nVidia's comes close! : r/AMD_Technology_Bets</A>
										<DT><A HREF="https://www.servethehome.com/amd-instinct-mi300x-gpu-and-mi300a-apus-launched-for-ai-era/amd-instinct-mi300a-vs-gh200-perf-per-watt/">AMD Instinct MI300A Vs GH200 Perf Per Watt - ServeTheHome</A>
									</DL><p>
									<DT><A HREF="https://repo.radeon.com/.hidden/cfa27af7066b8ebd5c73d75110183a62/docs/Change%20Summary_6.0.3_Known_Issues%20%281%29.pdf">AMD GPU kernel driver change summary</A>
									<DT><A HREF="https://chipsandcheese.com/2024/06/25/testing-amds-giant-mi300x/">Testing AMD‚Äôs Giant MI300X ‚Äì Chips and Cheese</A>
								</DL><p>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/66ef1df492f7bc9c8eeb01d7e14db01838e3f0bd/tensorrt_llm/auto_parallel/cluster_info.py">TensorRT-LLM/tensorrt_llm/auto_parallel/cluster_info.py</A>
								<DT><A HREF="https://news.futunn.com/en/post/42190604/nvidia-s-next-generation-gpu-revealed-integrating-eight-hbm-4?level=1&data_ticket=1715948857342483">Nvidia's next-generation GPU revealed: integrating eight HBM 4, TSMC N3 process</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-micro-benchmarking</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/1903.07486.pdf">Dissecting the NVidia Turing T4 GPU via Microbenchmarking</A>
								<DT><A HREF="https://arxiv.org/pdf/1912.03413.pdf">IPU</A>
								<DT><A HREF="https://arxiv.org/pdf/1804.06826.pdf%5B/url%5D">Volta</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-intel-gaudi</H3>
							<DL><p>
								<DT><A HREF="https://habana.ai/products/gaudi2/">Intel Gaudi 2 Neural Network Deep Learning Inference Processor</A>
								<DT><A HREF="https://twitter.com/EMostaque/status/1767199048337932719">(1) Emad acc/acc en X: "The @intel Gaudi2 chips are awesome &amp;amp; run the multimodal diffusion transformer arch that powers #SD3 faster than H100s (!) in scaled training pre fp8 Way cheaper TCO &amp;amp; Gaudi3 set to be 4x faster.. We also saw 673 tok/s inference on our upcoming StableBeluga 2.5 70b model (!)" / X</A>
								<DT><A HREF="https://www.intel.com/content/www/us/en/docs/graphics-for-linux/developer-reference/1-0/alchemist-arctic-sound-m.html">2023 Intel¬Æ Processors - Alchemist/Arctic Sound-M Platform</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-amd</H3>
							<DL><p>
								<DT><A HREF="https://github.com/geohot/7900xtx">geohot/7900xtx</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-power-limited</H3>
							<DL><p>
								<DT><A HREF="https://www.thonking.ai/p/strangely-matrix-multiplications">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</A>
								<DT><A HREF="https://twitter.com/dwarkesh_sp/status/1780990840179187715">(1) Dwarkesh Patel en X: "Zuck on: energy-bounded AI progress"</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=jys92_j627A">Digital Design &amp; Computer Architecture -  L23: Memory Hierarchy and Caches (Spring 2022)</A>
							<DT><A HREF="https://github.com/Jokeren/Awesome-GPU">Jokeren/Awesome-GPU: Awesome resources for GPUs</A>
							<DT><A HREF="https://github.com/NVIDIA/open-gpu-doc">NVIDIA/open-gpu-doc: Documentation of NVIDIA chip/hardware interfaces</A>
							<DT><A HREF="https://nvidia.github.io/open-gpu-doc/">open-gpu-doc</A>
							<DT><A HREF="https://www.colfax-intl.com/Servers/CX8850s-EI9">Colfax CX8850s-EI9 8U Rackmount Server with 8x AMD Instinct‚Ñ¢ MI300X Accelerators</A>
							<DT><A HREF="https://twitter.com/TechPowerUp/status/1776242252974559508">RISC-V CPU, GPU, and NPU</A>
							<DT><A HREF="https://www.youtube.com/watch?v=gofI47kfD28">Bill Dally | Directions in Deep Learning Hardware - YouTube</A>
							<DT><A HREF="https://techcommunity.microsoft.com/t5/azure-high-performance-computing/annual-roundup-of-ai-infrastructure-breakthroughs-for-2023/ba-p/4097737">Annual Roundup on AI Infrastructure Breakthroughs for 2023</A>
							<DT><A HREF="https://github.com/adam-maj/tiny-gpu">adam-maj/tiny-gpu: A minimal GPU design in Verilog to learn how GPUs work from the ground up</A>
							<DT><A HREF="https://x.com/jimkxa/status/1806105468575846509">AI on RISC-V: 10 years of mistakes</A>
						</DL><p>
						<DT><H3 FOLDED>hw-CPU</H3>
						<DL><p>
							<DT><A HREF="https://cpu.land/">Intro | Putting the "You" in CPU</A>
							<DT><A HREF="https://github.com/hackclub/putting-the-you-in-cpu">hackclub/putting-the-you-in-cpu: A technical explainer by @kognise of how your computer runs programs, from start to finish.</A>
							<DT><A HREF="https://www.anandtech.com/show/15578/cloud-clash-amazon-graviton2-arm-against-intel-and-amd">Amazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GA4ONupSl8Y">Two Decades of Hardware Optimizations Down The Drain - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>hw-TPU</H3>
						<DL><p>
							<DT><A HREF="https://github.com/ayaka14732/tpu-starter#72-parallelism">ayaka14732/tpu-starter: Everything you want to know about Google Cloud TPU</A>
							<DT><A HREF="https://github.com/ayaka14732/llama-2-jax">ayaka14732/llama-2-jax: JAX implementation of the Llama 2 model</A>
							<DT><A HREF="https://github.com/stanford-crfm/haliax">stanford-crfm/haliax: Named Tensors for Legible Deep Learning in JAX</A>
							<DT><A HREF="https://github.com/stanford-crfm/levanter">stanford-crfm/levanter: Legibile, Scalable, Reproducible Foundation Models with Named Tensors and Jax</A>
							<DT><A HREF="https://dl.acm.org/doi/pdf/10.1145/3360307">Domain Specific Hardware for training Deep Neural Networks</A>
							<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/design.html">Pallas Design ‚Äî JAX documentation</A>
							<DT><A HREF="https://cloud.google.com/tpu/docs/multislice-introduction">Cloud TPU Multislice Overview [Public Preview] ¬†|¬† Google Cloud</A>
							<DT><A HREF="https://github.com/google/maxtext">google/maxtext: A simple, performant and scalable Jax LLM!</A>
							<DT><A HREF="https://github.com/google/paxml">google/paxml: Pax is a Jax-based machine learning framework for training large scale models. Pax allows for advanced and fully configurable experimentation and parallelization, and has demonstrated industry leading model flop utilization rates.</A>
							<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
							<DT><A HREF="https://www.youtube.com/watch?v=NFKubflDb1A">Memory Model</A>
							<DT><A HREF="https://github.com/jinhachung/tptpu-sim">jinhachung/tptpu-sim: A Toy-Purpose TPU Simulator</A>
							<DT><A HREF="https://github.com/ayaka14732/tpu-starter#72-parallelism">ayaka14732/tpu-starter</A>
							<DT><A HREF="https://cloud.google.com/tpu/docs/multislice-introduction">Cloud TPU Multislice Overview [Public Preview]</A>
							<DT><A HREF="https://www.youtube.com/watch?v=rArv2NUXGU8&t=588s">George Hotz | Programming | tinygrad: on the Google Coral Edge TPU | reverse engineering part2 - YouTube</A>
							<DT><A HREF="https://cloud.google.com/kubernetes-engine/docs/concepts/tpus#configuration">Mapping of TPU configuration</A>
							<DT><A HREF="https://www.youtube.com/watch?v=oSCRZkSQ1CE&t=1529s">Jeff Dean (Google): Exciting Trends in Machine Learning</A>
							<DT><A HREF="https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus?hl=en">Introducing Trillium, sixth-generation TPUs | Google Cloud Blog</A>
							<DT><A HREF="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">System architecture ¬†|¬† Cloud TPU ¬†|¬† Google Cloud</A>
						</DL><p>
						<DT><H3 FOLDED>hw-Microsoft</H3>
						<DL><p>
							<DT><A HREF="https://news.microsoft.com/source/features/ai/in-house-chips-silicon-to-service-to-meet-ai-demand/">Maia 100</A>
							<DT><A HREF="https://twitter.com/highyieldYT/status/1724908672265150906">Maia 100 (AI accelerator) specs</A>
						</DL><p>
						<DT><H3 FOLDED>hw-IPU</H3>
						<DL><p>
							<DT><H3 FOLDED>Graphcore</H3>
							<DL><p>
								<DT><A HREF="https://www.zdnet.com/article/ai-computer-maker-graphcore-unveils-3-d-chip-promises-500-trillion-parameter-ultra-intelligence-machine/">AI computer maker Graphcore unveils 3-D chip, promises 500-trillion-parameter</A>
								<DT><A HREF="https://www.graphcore.ai/apply-for-our-machine-intelligence-academy">Apply for our Machine Intelligence Academy</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>hw-FPGA</H3>
						<DL><p>
							<DT><A HREF="http://www.clifford.at/icestorm/">Project IceStorm</A>
							<DT><A HREF="https://www.ebay.es/b/Laser-cnc-router/92150/bn_7005611421">Laser cnc router</A>
							<DT><A HREF="https://www.intel.com/content/www/us/en/products/programmable.html">Intel¬Æ FPGAs and Programmable Devices - Intel¬Æ FPGA</A>
							<DT><A HREF="https://www.xilinx.com/">Xilinx - Adaptable. Intelligent.</A>
							<DT><A HREF="https://link--springer--com.us.debiblio.com/book/10.1007/978-1-4302-6248-0">Beginning FPGA: Programming Metal</A>
							<DT><A HREF="https://link--springer--com.us.debiblio.com/book/10.1007/978-1-4757-3393-8">Functional Decomposition to FPGA Synthesis</A>
							<DT><A HREF="https://www.maxeler.com/">Maxeler Technologies</A>
							<DT><A HREF="https://taalas.com/">Taalas | The model is The Computer</A>
						</DL><p>
						<DT><H3 FOLDED>hw-Analog</H3>
						<DL><p>
							<DT><A HREF="https://escholarship.org/uc/item/5kb812qd">Stochastic Analog Computation for Machine Learning</A>
							<DT><A HREF="https://semiengineering.com/using-analog-for-ai/">Using Analog For AI</A>
						</DL><p>
						<DT><H3 FOLDED>hw-reverse-engineering</H3>
						<DL><p>
							<DT><A HREF="https://ffri.github.io/ProjectChampollion/part1/">Reverse-engineering Rosetta 2 part1: Analyzing AOT files and Rosetta 2 runtime - Project Champollion</A>
							<DT><A HREF="https://www.infoq.com/news/2020/11/rosetta-2-translation/">How x86 to arm64 Translation Works in Rosetta 2</A>
							<DT><A HREF="https://semiwiki.com/semiconductor-manufacturers/307494-the-semiconductor-ecosystem-explained/">The Semiconductor Ecosystem Explained - SemiWiki</A>
							<DT><H3 FOLDED>Ghidra</H3>
							<DL><p>
								<DT><A HREF="https://ghidra-sre.org/">Ghidra</A>
								<DT><A HREF="https://github.com/NationalSecurityAgency/ghidra">NationalSecurityAgency/ghidra: Ghidra is a software reverse engineering (SRE) framework</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Z04xTlLdZnc">George Hotz | Reverse engineering | same thing we do every weekend documenting the AMD 7900XTX Part2 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Cb2KwcnDKrk">George Hotz | Programming | tinygrad: 2.8k stars! CIFAR, ANE, speed, memory management | Part7 - YouTube</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>hw-Tenstorrent</H3>
						<DL><p>
							<DT><A HREF="https://tenstorrent.com/category/research/">Category: Research - Tenstorrent</A>
							<DT><A HREF="https://github.com/geohot/tt-twitch">geohot/tt-twitch: tenstorrent kernel from twitch</A>
						</DL><p>
						<DT><H3 FOLDED>hw-asic</H3>
						<DL><p>
							<DT><H3 FOLDED>hw-asic-mtia</H3>
							<DL><p>
								<DT><A HREF="https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/?utm_source=linkedin">Our next generation Meta Training and Inference Accelerator</A>
							</DL><p>
							<DT><H3 FOLDED>hw-asic-tesla-dojo</H3>
							<DL><p>
								<DT><A HREF="https://perspectives.mvdirona.com/2021/08/tesla-project-dojo-overview/">Tesla Project Dojo Overview ‚Äì Perspectives</A>
							</DL><p>
							<DT><H3 FOLDED>hw-asic-aws</H3>
							<DL><p>
								<DT><A HREF="https://perspectives.mvdirona.com/2018/11/aws-inferentia-machine-learning-processor/">AWS Inferentia Machine Learning Processor ‚Äì Perspectives</A>
								<DT><A HREF="https://perspectives.mvdirona.com/2022/05/graviton3-ec2-c7g-general-availability/">Graviton3 &amp; EC2 C7g General Availability ‚Äì Perspectives</A>
								<DT><A HREF="https://perspectives.mvdirona.com/2020/03/anandtech-on-aws-graviton2/">Anandtech on AWS Graviton2 ‚Äì Perspectives</A>
							</DL><p>
							<DT><A HREF="https://perspectives.mvdirona.com/2017/04/tensor-processing-unit/">Tensor Processing Unit ‚Äì Perspectives</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M_BZOQLw6KU">Digital Design and Comp. Arch. - L13: MIPS Assembly II &amp; Memories (Spring 2024) - YouTube</A>
							<DT><A HREF="https://bytemlperf.ai/">ByteMLPerf</A>
						</DL><p>
						<DT><H3 FOLDED>hw-neuromorphic</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>hw-memory</H3>
						<DL><p>
							<DT><H3 FOLDED>HBM</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=CPqdZZooS2g">HBM vs. GDDR6</A>
								<DT><A HREF="https://www.youtube.com/watch?v=k0ROr9_WNCI">SK hynix HBM3E Video - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=yAw63F1W_Us&t=658s">(Asiannometry) The Special Memory Powering the AI Revolution</A>
								<DT><A HREF="https://www.youtube.com/watch?v=JyhN_9MfPNA">HBM3 In The Data Center</A>
								<DT><A HREF="https://www.youtube.com/watch?v=oKyKAiEpOHU">Tackle Memory Bottlenecks with the Versal HBM Series - YouTube</A>
								<DT><A HREF="https://www.yolegroup.com/product/report/generative-ai-2024---deep-impacts-on-processors-memory-advanced-packaging-and-substrates/">Yole Group - Follow the latest trend news in the Semiconductor Industry</A>
								<DT><A HREF="https://investors.micron.com/news-releases/news-release-details/micron-delivers-industrys-fastest-highest-capacity-hbm-advance">Micron Delivers Industry‚Äôs Fastest, Highest-Capacity HBM to Advance Generative AI Innovation | Micron Technology</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=nZNd5FjSquk">CppCon 2017: John Lakos ‚ÄúLocal ('Arena') Memory Allocators (part 1 of 2)‚Äù - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=IRZKCBp9Q7U">Seminar in Computer Architecture - L5: Memory-Centric Computing (Spring 2024) - YouTube</A>
							<DT><A HREF="https://gwern.net/doc/ai/scaling/hardware/2021-jouppi.pdf">bandwidth-capacity pareto front of all digital memory technologies</A>
							<DT><A HREF="https://www.youtube.com/watch?v=aLttVmgYRGw">Modern Solid-State Drives (SSDs) - Lecture 2: Read/Write Operations in Modern SSDs (Spring 2024) - YouTube</A>
							<DT><A HREF="https://www.linkedin.com/pulse/tearing-down-memory-wall-sharada-yeluri/?trackingId=dn9NrSPARD2R%2BUQ9KN6CXQ%3D%3D">(1) Tearing Down the Memory Wall | LinkedIn</A>
						</DL><p>
						<DT><H3 FOLDED>hw-people</H3>
						<DL><p>
							<DT><A HREF="https://substack.com/@tanjb">@tanjb (Clive Chan reading)</A>
							<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772472408559206798">Clive Chan</A>
						</DL><p>
						<DT><H3 FOLDED>Computational Lithography</H3>
						<DL><p>
							<DT><A HREF="https://developer.nvidia.com/culitho">cuLitho - Accelerate Computational Lithography | NVIDIA Developer</A>
							<DT><A HREF="https://www.youtube.com/watch?v=HxyM2Chu9Vc">Nvidia's Computational Lithography Breakthrough - YouTube</A>
						</DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=gofI47kfD28">Bill Dally | Directions in Deep Learning Hardware - YouTube</A>
						<DT><A HREF="https://cs217.stanford.edu/">Hardware Accelerators for Machine Learning (CS 217) by cs217</A>
						<DT><A HREF="https://rain.ai/">Rain AI</A>
						<DT><A HREF="https://datacrunchltd-my.sharepoint.com/personal/ruben_datacrunch_io/_layouts/15/onedrive.aspx?FolderCTID=0x0120003C33ADC3F352284DAB5968CA9FA1A319&id=%2Fpersonal%2Fruben%5Fdatacrunch%5Fio%2FDocuments%2FDataCrunch%5FShared%2FMisc%2FSemianalysis">Semianalysis - OneDrive</A>
						<DT><A HREF="https://eliyan.com/eliyan-news/eliyan-closes-60-million-series-b-funding-round/">Chiplet Interconnect Pioneer Eliyan Closes $60 Million Series B Funding Round, Co-led by Samsung Catalyst Fund</A>
						<DT><A HREF="https://www.youtube.com/watch?v=tQhn6Fpw5HU">Future Computing Platforms - Talk at Stanford University SystemX Seminar - 08.02.2024 - YouTube</A>
						<DT><A HREF="https://github.com/fengbintu/Neural-Networks-on-Silicon">fengbintu/Neural-Networks-on-Silicon: This is originally a collection of papers on neural network accelerators. Now it's more like my selection of research on deep learning and computer architecture.</A>
						<DT><A HREF="https://x.com/i/bookmarks?post_id=1803893996093345983">tinybox-fpga</A>
					</DL><p>
					<DT><H3 FOLDED>efficient-ai</H3>
					<DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=bfexfASu9h4">Scalable and Efficient AI: From Supercomputers to Smartphones (Microsoft)</A>
						<DT><H3 FOLDED>efficient-ai-i/o</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2101.08734">[2101.08734] Clairvoyant Prefetching for Distributed Machine Learning I/O</A>
							<DT><A HREF="https://github.com/NVIDIA/aistore/blob/master/docs/overview.md">aistore/docs/overview.md at master ¬∑ NVIDIA/aistore</A>
							<DT><A HREF="https://arxiv.org/abs/2001.01858">[2001.01858] High Performance I/O For Large Scale Deep Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2101.08734">Prefetching for Distributed Machine Learning I/O (bitwise determinisn)</A>
							<DT><A HREF="https://github.com/NVIDIA/aistore/blob/master/docs/overview.md">NVIDIA/aistore</A>
							<DT><A HREF="https://arxiv.org/abs/2203.17189">Scaling Up Models and Data with t5x + seqio</A>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/datasets.html">Datasets ‚Äî NVIDIA NeMo (AIStore)</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-compute</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2007.00072">[2007.00072] Data Movement Is All You Need: A Case Study on Optimizing Transformers</A>
							<DT><A HREF="https://github.com/spcl/dace">spcl/dace: DaCe - Data Centric Parallel Programming</A>
							<DT><A HREF="https://arxiv.org/abs/2210.17323">[2210.17323] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</A>
							<DT><A HREF="https://research.nvidia.com/publication/2021-04_vs-quant-vector-scaled-quantization-accurate-low-precision-neural-network">VS-QUANT: Per-Vector Scaled Quantization for Accurate Low-Precision Neural Network Inference | Research</A>
							<DT><A HREF="https://arxiv.org/pdf/2102.04503.pdf">VS-QUANT: PER-VECTOR SCALED QUANTIZATION FOR ACCURATE LOW-PRECISION NEURAL NETWORK INFERENCE</A>
							<DT><A HREF="https://arxiv.org/abs/2102.00554">[2102.00554] Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks</A>
							<DT><A HREF="https://arxiv.org/abs/2306.03078">[2306.03078] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</A>
							<DT><A HREF="https://sparse.tamu.edu/">SuiteSparse Matrix Collection</A>
							<DT><A HREF="https://arxiv.org/abs/2304.07613">[2304.07613] STen: Productive and Efficient Sparsity in PyTorch</A>
							<DT><A HREF="https://github.com/spcl/sten">spcl/sten: Sparsity support for PyTorch</A>
							<DT><A HREF="https://sc23.supercomputing.org/presentation/?id=pap438&sess=sess178">VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores</A>
							<DT><A HREF="https://arxiv.org/abs/1712.05877">[1712.05877] Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</A>
							<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu">Kobzol/hardware-effects-gpu: Demonstration of various hardware effects on CUDA GPUs.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=q38V66bqhfU">EfficientML.ai lecture - YouTube</A>
							<DT><A HREF="https://github.com/srush/LLM-Training-Puzzles">srush/LLM-Training-Puzzles: What would you do with 1000 H100s...</A>
							<DT><A HREF="https://arxiv.org/abs/2310.07707">[2310.07707] MatFormer: Nested Transformer for Elastic Inference</A>
							<DT><A HREF="https://arxiv.org/abs/2007.00072">Data Movement Is All You Need: Case Study on Optimizing Transformers</A>
							<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu">hardware-effects-gpu: Demonstration of various hardware effects on CUDA GPUs.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mP4BL6URdxc">EfficientML.ai Lecture 18: Distributed Training (Part II) (MIT Fall 2023)</A>
							<DT><A HREF="https://arxiv.org/abs/2306.03078">SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</A>
							<DT><A HREF="https://research.nvidia.com/publication/2021-04_vs-quant-vector-scaled-quantization-accurate-low-precision-neural-network">VS-QUANT: Per-Vector Scaled Quantization for Accurate Low-Precision Neural Network Inference</A>
							<DT><A HREF="https://arxiv.org/abs/2102.00554">Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks</A>
							<DT><A HREF="https://arxiv.org/abs/1712.05877">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</A>
							<DT><A HREF="https://twitter.com/tianle_cai/status/1770587583086764288">compute-memory bandwidth bottleneck</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-communication</H3>
						<DL><p>
							<DT><A HREF="https://insujang.github.io/2022-06-11/parallelism-in-distributed-deep-learning/">Parallelism in Distributed Deep Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2201.12023">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</A>
							<DT><A HREF="https://arxiv.org/abs/1802.09941">Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis</A>
							<DT><A HREF="https://arxiv.org/abs/1802.08021">SparCML: High-Performance Sparse Communication for Machine Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2106.15565">[2106.15565] Flare: Flexible In-Network Allreduce</A>
							<DT><A HREF="https://arxiv.org/abs/2209.01346">[TPUv4] HammingMesh: A Network Topology for Large-Scale Deep Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2302.03337">[Network Cloud Standard] Datacenter Ethernet and RDMA: Issues at scale</A>
							<DT><A HREF="https://www.youtube.com/watch?v=KYNHe_XyUBc">NVIDIA Networking: Understanding Ethernet Switches - YouTube</A>
							<DT><A HREF="https://dl.acm.org/doi/10.1145/3544216.3544265">Jupiter evolving | Proceedings of the ACM SIGCOMM 2022 Conference</A>
							<DT><A HREF="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41684.pdf">Google Omega: flexible, scalable schedulers for large compute clusters</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Collective_operation">Collective operation</A>
							<DT><A HREF="https://github.com/facebookresearch/ucc">Unified Collective Communication (UCC)</A>
							<DT><A HREF="https://network.nvidia.com/pdf/whitepapers/IB_Intro_WP_190.pdf">Introduction to InfiniBand</A>
							<DT><A HREF="https://github.com/open-mpi/ompi">open-mpi/ompi: Open MPI main development repository</A>
							<DT><A HREF="https://github.com/leandromoreira/linux-network-performance-parameters">leandromoreira/linux-network-performance-parameters</A>
							<DT><A HREF="https://arxiv.org/abs/2311.08105">[2311.08105] DiLoCo: Distributed Low-Communication Training LM</A>
							<DT><A HREF="https://arxiv.org/abs/1802.09941">Demystifying Parallel and Distributed Deep Learning: An In-Depth Analysis</A>
							<DT><A HREF="https://arxiv.org/abs/1802.08021">SparCML: High-Performance Sparse Communication for ML</A>
							<DT><A HREF="https://www.youtube.com/watch?v=KYNHe_XyUBc">NVIDIA Networking: Understanding Ethernet Switches</A>
							<DT><A HREF="https://community.juniper.net/blogs/sharada-yeluri/2024/01/02/gpu-fabrics-for-genai-workloads">GPU Fabrics for GenAI Workloads (Juniper)</A>
							<DT><A HREF="https://developer.nvidia.com/blog/doubling-all2all-performance-with-nvidia-collective-communication-library-2-12/">Doubling all2all Performance with NCCL</A>
							<DT><A HREF="https://chatgpt.com/c/e57a5ce7-00de-486a-a8c2-c95db90cf417">non-blocking flat tree</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-supercomputing</H3>
						<DL><p>
							<DT><H3 FOLDED>supercomputing-high-performance-transaction-systems</H3>
							<DL><p>
								<DT><A HREF="https://perspectives.mvdirona.com/about-perspectives/">About Perspectives ‚Äì Perspectives</A>
							</DL><p>
							<DT><H3 FOLDED>superpod</H3>
							<DL><p>
								<DT><A HREF="https://docs.nvidia.com/https:/docs.nvidia.com/dgx-superpod-reference-architecture-dgx-h100.pdf">NVIDIA DGX SuperPOD</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/tim_zaman/status/1636981863477809152?t=_WTQgylsaggk5zy5JJrlAA&s=31">Tim Zaman: Azure next-gen AI datacenter (training &amp; inference)</A>
							<DT><A HREF="https://coreweave.com/events/supercomputing-2023?utm_content=270153292&utm_medium=social&utm_source=twitter&hss_channel=tw-979803443681349632">CoreWeave @ SC23 ‚Äî Events ‚Äî CoreWeave</A>
							<DT><A HREF="https://colocatedeventsna2023.sched.com/event/1Rj1g">CNCF-Hosted Co-located Events North America 2023: How We Power the Largest AI Deployments</A>
							<DT><A HREF="https://www.youtube.com/@cncf">CNCF [Cloud Native Computing Foundation] - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=aSwfT4oRrvI">From Zero to Infinity: How AI-Powered Hedge Fund Build Cloud-Native AI</A>
							<DT><A HREF="https://www.youtube.com/watch?v=hHZxjH8u84s">Practice of Building AI Training Cluster Based on Kubernetes+RoCEv2</A>
							<DT><A HREF="https://cs217.stanford.edu/">Hardware Accelerators for Machine Learning (CS 217) by cs217</A>
							<DT><A HREF="https://blogs.nvidia.com/blog/nemo-amazon-titan/">NVIDIA Powers Training for Some of the Largest Amazon Titan Foundation Models | NVIDIA Blogs</A>
							<DT><A HREF="https://github.com/baidu-research/DeepBench">baidu-research/DeepBench: Benchmarking Deep Learning operations on different hardware (MUST)</A>
							<DT><A HREF="https://www.youtube.com/@cncf">CNCF [Cloud Native Computing Foundation]</A>
							<DT><A HREF="https://blogs.nvidia.com/blog/nemo-amazon-titan/">NVIDIA Training for Some of the Largest Amazon Fronteir Models</A>
							<DT><A HREF="https://github.com/baidu-research/DeepBench">DeepBench: Benchmarking Deep Learning operations on different hardware (MUST)</A>
							<DT><A HREF="https://gpulist.ai/">gpulist</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-people</H3>
						<DL><p>
							<DT><H3 FOLDED>efficient-ai-people-coreweave</H3>
							<DL><p>
								<DT><A HREF="https://github.com/anthr76">anthr76 (Anthony Rabbito)</A>
								<DT><A HREF="https://github.com/bradbeam">bradbeam (Brad Beam)</A>
								<DT><A HREF="https://github.com/ddymko">ddymko (David Dymko)</A>
								<DT><A HREF="https://github.com/dfinster">dfinster (David Finster)</A>
							</DL><p>
							<DT><A HREF="http://htor.ethz.ch/">Torsten Hoefler's Home Page</A>
							<DT><A HREF="https://arxiv.org/search/cs?searchtype=author&query=Hoefler%2C+T">Search | arXiv e-print repository</A>
							<DT><A HREF="https://github.com/suryabhupa">suryabhupa (Surya Bhupatiraju) (DeepMind)</A>
							<DT><A HREF="https://github.com/ranxian">ranxian (Ran Xian) CTO at Metabit Trading</A>
							<DT><A HREF="https://twitter.com/songhan_mit?lang=en">Instructor: Prof. Song Han</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-algorithms-structures</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=7WeraZ0LLlg&list=LL&index=840&t=3s">EfficientML.ai Lecture 13 - Transformer and LLM (Part II) (MIT 6.5940, Fall 2023) - YouTube</A>
							<DT><A HREF="https://hanlab.mit.edu/courses/2023-fall-65940">MIT 6.5940 Fall 2023 TinyML and Efficient Deep Learning Computing</A>
							<DT><A HREF="https://github.com/BearNinja123/data_structures/blob/main/stack.c">data_structures/stack.c at main</A>
						</DL><p>
						<DT><A HREF="https://arxiv.org/abs/2211.05102">[2211.05102] Efficiently Scaling Transformer Inference</A>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1773268740806705266">optimal batch size (bytes per param, flops, bytes/token, arithmetic intensity )</A>
						<DT><A HREF="https://www.youtube.com/watch?v=gofI47kfD28">Bill Dally | Directions in Deep Learning Hardware - YouTube</A>
						<DT><A HREF="https://www.coreweave.com/blog/coreweaves-tensorizer-decrease-pytorch-model-load-times">Decrease PyTorch Model Load Times with CoreWeave‚Äôs Tensorizer</A>
						<DT><A HREF="https://www.youtube.com/watch?v=eZdOkDtYMoo">Song Han: Efficient Methods and Hardware for Deep Learning</A>
						<DT><A HREF="https://github.com/coreweave/tensorizer/#available-pre-tensorized-models-on-the-coreweave-cloud">coreweave/tensorizer: Module, Model, &amp; Tensor Serialization/Deserialization</A>
						<DT><A HREF="https://www.p99conf.io/?utm_source=google&utm_medium=cpc&utm_campaign=20420977396&utm_content=P99_Custom-Intent_P99-2023&utm_placement=youtube.com&gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdqVhRjjyo7gkpYIqZexrbTarZXUuFDYlrFHrlUkpWLbWgMlC8t1mv4aAoY2EALw_wcB">P99 CONF - The Event on All Things Performance</A>
						<DT><A HREF="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/pages/lecture-slides/">Lecture Slides | Performance Engineering of Software Systems</A>
						<DT><A HREF="https://arxiv.org/pdf/2211.05102.pdf">Efficiently Scaling Transformer Inference</A>
						<DT><A HREF="https://www.youtube.com/watch?v=iXSfUw7VfNw">Using Pytorch 2.0 Compile in IBM's Watsonx.AI Inference</A>
						<DT><A HREF="https://www.youtube.com/watch?v=1pg1JE2CPaA">Quatization Scaling</A>
						<DT><A HREF="https://github.com/cloud-hypervisor/cloud-hypervisor">cloud-hypervisor/cloud-hypervisor: A Virtual Machine Monitor</A>
						<DT><A HREF="https://horace.io/brrr_intro.html">Making Deep Learning go Brrrr From First Principles</A>
						<DT><A HREF="https://www.youtube.com/@rutgersefficientaiseminar9909">Rutgers Efficient AI Seminar - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>Companies</H3>
					<DL><p>
						<DT><H3 FOLDED>Deep tech</H3>
						<DL><p>
							<DT><H3 FOLDED>fundig-rounds</H3>
							<DL><p>
								<DT><A HREF="https://hackmd.io/@0RrvBhdwQya4V6lCDWvmew/BJScF5y2n?utm_source=preview-mode&utm_medium=rec">Investment Memo - CoreWeave - HackMD</A>
								<DT><A HREF="https://artificialanalysis.ai/speech-to-text">Speech to Text (ASR) Providers Leaderboard &amp; Comparison | Artificial Analysis</A>
								<DT><A HREF="https://www.replicated.com/blog/series-c-announcement">Replicated $50M Series C To Advance Multi-Prem Software Adoption</A>
								<DT><A HREF="https://fireworks.ai/blog/why-gpus-on-demand">GPUs on-demand: Not serverless, not reserved, but some third thing</A>
								<DT><A HREF="https://www.mercatopartners.com/editorials/why-we-invested-in-lambda">Why Mercato Led Lambda‚Äôs $44M Series B</A>
								<DT><A HREF="https://artificialanalysis.ai/models/deepseek-v2">DeepSeek-V2 - Quality, Performance &amp; Price Analysis | Artificial Analysis</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ptGDaGUXInw">Mark Russinovich | Generative AI in the Cloud: Inside Microsoft AI Innovation - YouTube</A>
								<DT><A HREF="https://gwern.net/complement">Laws of Tech: Commoditize Your Complement ¬∑ Gwern.net</A>
								<DT><A HREF="https://drive.google.com/file/d/1gquqRqiT-2Be85p_5w0izGQGgHvVzncQ/view">mistral.ai strategic memo.pdf - Google Drive</A>
								<DT><A HREF="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">Building Meta‚Äôs GenAI Infrastructure - Engineering at Meta</A>
								<DT><A HREF="https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness">Training great LLMs entirely from ground up in the wilderness as a startup ‚Äî Yi Tay</A>
							</DL><p>
							<DT><A HREF="https://gwern.net/complement">Laws of Tech: Commoditize Your Complement ¬∑ Gwern.net</A>
						</DL><p>
						<DT><H3 FOLDED>xAI</H3>
						<DL><p>
							<DT><H3 FOLDED>xai-people</H3>
							<DL><p>
								<DT><A HREF="https://people.eecs.berkeley.edu/~hendrycks/">Dan Hendrycks</A>
								<DT><A HREF="https://twitter.com/ibab_ml">(1) Igor Babuschkin (@ibab_ml) / X</A>
								<DT><A HREF="https://twitter.com/thegregyang">(1) Greg Yang (@TheGregYang) / X</A>
								<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437/photo/1">(1) Guodong Zhang (@Guodzh) / X</A>
								<DT><A HREF="https://thegregyang.com/">Greg Yang |¬†Professional page</A>
								<DT><A HREF="https://github.com/kykosic">kykosic (Kyle Kosic)</A>
							</DL><p>
							<DT><A HREF="https://x.ai/">xAI: Understand the Universe</A>
						</DL><p>
						<DT><H3 FOLDED>Reka</H3>
						<DL><p>
							<DT><A HREF="https://www.yitay.net/">Yi Tay</A>
						</DL><p>
						<DT><H3 FOLDED>NVIDIA</H3>
						<DL><p>
							<DT><A HREF="https://www.nvidia.com/en-us/data-center/dgx-cloud/">DGX Cloud | AI Supercomputer in the Cloud | NVIDIA</A>
							<DT><A HREF="https://www.hpcwire.com/2023/03/21/nvidias-ai-factory-services-start-at-37000/">DGX Cloud Is Here: Nvidia's AI Factory Services Start at $37,000</A>
						</DL><p>
						<DT><H3 FOLDED>Imbue</H3>
						<DL><p>
							<DT><A HREF="https://imbue.com/">imbue (General Intelligence):  AI system that can reason (agents)</A>
						</DL><p>
						<DT><H3 FOLDED>EU</H3>
						<DL><p>
							<DT><H3 FOLDED>EuroHPC</H3>
							<DL><p>
								<DT><H3 FOLDED>LUMI</H3>
								<DL><p>
									<DT><A HREF="https://www.lumi-supercomputer.eu/">Front Page - LUMI</A>
								</DL><p>
								<DT><H3 FOLDED>Jupiter</H3>
								<DL><p>
									<DT><A HREF="https://www.fz-juelich.de/en/ias/jsc/jupiter">JUPITER - Exascale for Europe</A>
									<DT><A HREF="https://www.datacenterdynamics.com/en/news/europes-first-exascale-supercomputer-will-feature-24000-nvidia-gh200-superchips/">Europe's first exascale supercomputer will feature 24,000 Nvidia GH200 Superchips - DCD</A>
								</DL><p>
								<DT><H3 FOLDED>AI Act</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/BertuzLuca/status/1722997599932678450">AI Act (November 2023)</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>Germany</H3>
							<DL><p>
								<DT><A HREF="https://aleph-alpha.com/">ALEPH ALPHA - AI for Enterprises and Governments</A>
								<DT><A HREF="https://www.cnbc.com/2023/11/06/aleph-alpha-raises-500-million-to-build-a-european-rival-to-openai.html">Aleph Alpha raises $500 million to build a European rival to OpenAI</A>
							</DL><p>
							<DT><H3 FOLDED>France</H3>
							<DL><p>
								<DT><H3 FOLDED>Mistral AI</H3>
								<DL><p>
									<DT><A HREF="https://mistral.ai/">Mistral AI | Open source models</A>
								</DL><p>
								<DT><H3 FOLDED>Kyutai</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/kyutai_labs">(1) kyutai (@kyutai_labs) / X</A>
									<DT><A HREF="https://twitter.com/kyutai_labs/status/1725483922652283211">Founding Research Team</A>
									<DT><A HREF="http://kyutai.org/">kyutai: open science AI lab</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://cip.org/whitepaper">Whitepaper ‚Äî The Collective Intelligence Project</A>
							<DT><A HREF="https://twitter.com/scienceisstrat1/status/1693355056634958128">economic consequences of tech stagnation for Europe‚Äôs prosperity</A>
						</DL><p>
						<DT><H3 FOLDED>YC</H3>
						<DL><p>
							<DT><A HREF="https://www.ycombinator.com/library/4r-yc-and-hard-tech-startups">YC and Hard Tech Startups : YC Startup Library | Y Combinator</A>
						</DL><p>
						<DT><H3 FOLDED>Financial</H3>
						<DL><p>
							<DT><A HREF="https://www.jpmorgan.com/technology/artificial-intelligence">artificial-intelligence</A>
						</DL><p>
						<DT><H3 FOLDED>MS: GeneralAI</H3>
						<DL><p>
							<DT><A HREF="https://thegenerality.com/agi/index.html">Advancing AI for humanity | Foundation of AI</A>
							<DT><A HREF="https://github.com/microsoft/unilm">microsoft/unilm: Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities</A>
						</DL><p>
					</DL><p>
					<DT><H3 FOLDED>papers-ai-compilers-pl</H3>
					<DL><p>
						<DT><H3 FOLDED>papers-ai-compilers-pl-correctness</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2306.06884">[2306.06884] A Survey of Modern Compiler Fuzzing</A>
							<DT><A HREF="https://comby.dev/blog/2022/04/11/comby-decomposer-compiler-fuzzing">Deconstructing programs for compiler fuzzing ¬∑ Comby</A>
							<DT><A HREF="https://agroce.github.io/cc22.pdf">Making No-Fuss Compiler Fuzzing Effective</A>
						</DL><p>
						<DT><H3 FOLDED>papers-ai-compilers-pl-MLIR</H3>
						<DL><p>
							<DT><A HREF="https://mlir.llvm.org/docs/LangRef/">MLIR Language Reference - MLIR</A>
							<DT><A HREF="https://www.youtube.com/watch?v=5OSP5DNAozU">Building domain-specific compilers quickly with MLIR compiler infrastructure | Chris Lattner</A>
							<DT><A HREF="https://www.youtube.com/watch?v=A3qbcwasEUY">Groq Spotlight: Groq‚Ñ¢ Compiler Overview - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=3LLzHKeL2hs">MLIR-based code generation for GPU tensor cores - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=LPlRLt9w4b0&t=306s">2023 EuroLLVM - What's new in MLIR?</A>
							<DT><A HREF="https://www.youtube.com/watch?v=xNe9fPvU7-U">Open MLIR Meeting 11-16-2023: Targeting H100 with NVGPU and NVVM Dialects - YouTube</A>
							<DT><A HREF="https://github.com/llvm/llvm-project/pull/87065">[mlir][nvgpu] NVGPU Tutorials by grypp ¬∑ Pull Request #87065 ¬∑ llvm/llvm-project</A>
							<DT><A HREF="https://github.com/llvm/llvm-project/pull/87065/files">[mlir][nvgpu] NVGPU Tutorials by grypp ¬∑ Pull Request #87065 ¬∑ llvm/llvm-project</A>
							<DT><A HREF="https://grypp.github.io/papers/nvdsl.pdf">Zero to Hero: Programming Nvidia Hopper with MLIR‚Äôs NVGPU Dialect</A>
							<DT><H3 FOLDED>byteIR</H3>
							<DL><p>
								<DT><A HREF="https://github.com/bytedance/byteir">bytedance/byteir: A model compilation solution for various hardware</A>
								<DT><A HREF="https://github.com/bytedance/byteir/blob/main/talks/ChinaSoftCon-ByteIR.pdf">byteir/talks/ChinaSoftCon-ByteIR.pdf at main ¬∑ bytedance/byteir</A>
								<DT><A HREF="https://github.com/bytedance/byteir/blob/main/talks/c4ml23_poster.pdf">Linalg is All You Need to Optimize Attention</A>
							</DL><p>
							<DT><H3 FOLDED>NVGPU</H3>
							<DL><p>
								<DT><A HREF="https://mlir.llvm.org/docs/Dialects/NVGPU/">'nvgpu' Dialect - MLIR</A>
								<DT><A HREF="https://github.com/llvm/llvm-project/pull/87065">[mlir][nvgpu] NVGPU Tutorials by grypp ¬∑ Pull Request #87065 ¬∑ llvm/llvm-project</A>
								<DT><A HREF="https://github.com/llvm/llvm-project/pull/87065/files">[mlir][nvgpu] NVGPU Tutorials by grypp ¬∑ Pull Request #87065 ¬∑ llvm/llvm-project</A>
								<DT><A HREF="https://grypp.github.io/papers/nvdsl.pdf">Programming
Nvidia Hopper with MLIR‚Äôs NVGPU Dialect</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>papers-ai-compilers-pl-lectures</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=lPX1H3jW8ZQ">AI Hardware w/ Jim Keller - YouTube</A>
							<DT><A HREF="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/pages/lecture-slides/">Lecture Slides | Performance Engineering of Software Systems</A>
						</DL><p>
						<DT><H3 FOLDED>papers-ai-compilers-pl-dtypes</H3>
						<DL><p>
							<DT><H3 FOLDED>papers-ai-compilers-pl-dtypes-fp8</H3>
							<DL><p>
								<DT><A HREF="https://tesla-cdn.thron.com/static/MXMU3S_tesla-dojo-technology_1WDVZN.pdf">A Guide to Tesla's Configurable Floating Point foRMATS &amp; Arithmetic</A>
								<DT><A HREF="https://arxiv.org/pdf/2209.05433.pdf">FP8 FORMATS FOR DEEP LEARNING</A>
								<DT><A HREF="https://github.com/pytorch/rfcs/blob/master/RFC-0030-native-fp8-dtype.md">rfcs/RFC-0030-native-fp8-dtype.md at master ¬∑ pytorch/rfcs</A>
								<DT><A HREF="https://arxiv.org/pdf/1710.03740.pdf">MIXED PRECISION TRAINING</A>
								<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization ‚Äî PyTorch 2.0 documentation</A>
								<DT><A HREF="https://github.com/Azure/MS-AMP">Azure/MS-AMP: Microsoft Automatic Mixed Precision Library</A>
								<DT><A HREF="https://azure.github.io/MS-AMP/">MS-AMP Documentation | MS-AMP</A>
								<DT><A HREF="https://arxiv.org/abs/2309.17224">[2309.17224] Training and inference of large language models using 8-bit floating point</A>
								<DT><A HREF="https://github.com/pytorch-labs/float8_experimental">pytorch-labs/float8_experimental</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Primitive_data_type">Primitive data type - Wikipedia</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">Single-precision floating-point format - Wikipedia</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760">numeric_conversion.h</A>
								<DT><A HREF="https://research.colfax-intl.com/adding-fp8-to-flashattention/">Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2 ‚Äì Colfax Research</A>
								<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/606">FP8 Unable to achieve the expected FLOPS indicator in 4090 ¬∑ Issue #606 ¬∑ NVIDIA/TransformerEngine</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Vp1zZGBUy9o">ü§ó Hugging Cast S2E2 - Accelerating AI with NVIDIA! - YouTube</A>
								<DT><A HREF="https://github.com/P3109/Public/blob/main/Shared%20Reports/P3109%20WG%20Interim%20Report.pdf">Public/Shared Reports/P3109 WG Interim Report.pdf at main ¬∑ P3109/Public</A>
								<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
								<DT><A HREF="https://arxiv.org/pdf/2209.05433">FP8 FORMATS FOR DEEP LEARNING</A>
								<DT><A HREF="https://arxiv.org/abs/2402.00025">[2402.00025] Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK work decomposition</A>
							</DL><p>
							<DT><H3 FOLDED>papers-ai-compilers-pl-dtypes-mx</H3>
							<DL><p>
								<DT><H3 FOLDED>fp8</H3>
								<DL><p>
									<DT><A HREF="https://tesla-cdn.thron.com/static/MXMU3S_tesla-dojo-technology_1WDVZN.pdf">A Guide to Tesla's Configurable Floating Point foRMATS &amp; Arithmetic</A>
									<DT><A HREF="https://arxiv.org/pdf/2209.05433.pdf">FP8 FORMATS FOR DEEP LEARNING</A>
									<DT><A HREF="https://github.com/pytorch/rfcs/blob/master/RFC-0030-native-fp8-dtype.md">rfcs/RFC-0030-native-fp8-dtype.md at master ¬∑ pytorch/rfcs</A>
									<DT><A HREF="https://arxiv.org/pdf/1710.03740.pdf">MIXED PRECISION TRAINING</A>
									<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization ‚Äî PyTorch 2.0 documentation</A>
									<DT><A HREF="https://github.com/Azure/MS-AMP">Azure/MS-AMP: Microsoft Automatic Mixed Precision Library</A>
									<DT><A HREF="https://azure.github.io/MS-AMP/">MS-AMP Documentation | MS-AMP</A>
									<DT><A HREF="https://arxiv.org/abs/2309.17224">[2309.17224] Training and inference of large language models using 8-bit floating point</A>
									<DT><A HREF="https://github.com/pytorch-labs/float8_experimental">pytorch-labs/float8_experimental</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Primitive_data_type">Primitive data type - Wikipedia</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">Single-precision floating-point format - Wikipedia</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760">numeric_conversion.h</A>
									<DT><A HREF="https://research.colfax-intl.com/adding-fp8-to-flashattention/">Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2 ‚Äì Colfax Research</A>
									<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/606">FP8 Unable to achieve the expected FLOPS indicator in 4090 ¬∑ Issue #606 ¬∑ NVIDIA/TransformerEngine</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Vp1zZGBUy9o">ü§ó Hugging Cast S2E2 - Accelerating AI with NVIDIA! - YouTube</A>
									<DT><A HREF="https://github.com/P3109/Public/blob/main/Shared%20Reports/P3109%20WG%20Interim%20Report.pdf">Public/Shared Reports/P3109 WG Interim Report.pdf at main ¬∑ P3109/Public</A>
								</DL><p>
								<DT><A HREF="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">Open Compute Project ‚Ä¢ OCP Microscaling Formats (MX) Specification</A>
								<DT><A HREF="https://azure.microsoft.com/en-us/blog/fostering-ai-infrastructure-advancements-through-standardization/">Fostering AI infrastructure advancements through standardization | Microsoft Azure Blog</A>
								<DT><A HREF="https://arxiv.org/abs/2302.08007">[2302.08007] With Shared Microexponents, A Little Shifting Goes a Long Way</A>
								<DT><A HREF="https://arxiv.org/abs/2310.10537">[2310.10537] Microscaling Data Formats for Deep Learning</A>
								<DT><A HREF="https://github.com/microsoft/microxcaling">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024">DeepSpeed/blogs/deepspeed-fp6/03-05-2024 at master ¬∑ microsoft/DeepSpeed</A>
								<DT><A HREF="https://www.opencompute.org/blog/open-compute-project-tackles-data-center-hardware-and-firmware-security">Open Compute Project Tackles Data Center Hardware and Firmware Security ¬ª Open Compute Project</A>
							</DL><p>
						</DL><p>
						<DT><A HREF="https://github.com/llvm/circt">llvm/circt: Circuit IR Compilers and Tools</A>
						<DT><A HREF="https://www.youtube.com/watch?v=ZI198eFghJk">Modernizing Compiler Design for Carbon Toolchain - Chandler Carruth - CppNow 2023 - YouTube</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/FLOPS">FLOPS</A>
						<DT><A HREF="https://dl.acm.org/doi/pdf/10.1145/3620666.3651369">EVT: Accelerating Deep Learning Training with Epilogue Visitor Tree</A>
					</DL><p>
					<DT><H3 FOLDED>Conferences</H3>
					<DL><p>
						<DT><A HREF="https://scie.lcc.uma.es:8443/ratingSearch.jsf">Conference Rating - Search the GGS Rating 2021</A>
						<DT><H3 FOLDED>Call For Papers (CFP)</H3>
						<DL><p>
							<DT><A HREF="https://groups.google.com/g/ml-news/c/780yWzTWR1g/m/5ho81CbCAQAJ">European Workshop on Reinforcement Learning (EWRL 2022)</A>
						</DL><p>
						<DT><H3 FOLDED>LoG: Learning on Graphs (non rated)</H3>
						<DL><p>
							<DT><A HREF="https://logconference.github.io/cfp/">Learning on Graphs Conference</A>
							<DT><A HREF="https://towardsdatascience.com/announcing-the-learning-on-graphs-conference-c63caed7347">Announcing the Learning on Graphs Conference | by Michael Bronstein | Apr, 2022</A>
							<DT><A HREF="https://docs.google.com/forms/d/e/1FAIpQLSfacoaCBxzMXPz-AisU1DV6qya6Q1Hj3idgqWwYV61B4jC8Uw/viewform">LoG 2022 Reviewer Sign-up Form</A>
						</DL><p>
						<DT><H3 FOLDED>ICLR (A++)</H3>
						<DL><p>
							<DT><A HREF="https://openreview.net/group?id=ICLR.cc/2022/Conference">ICLR 2022 Conference | OpenReview</A>
						</DL><p>
						<DT><H3 FOLDED>NeurIPS (A++)</H3>
						<DL><p>
							<DT><A HREF="https://nips.cc/">NeurIPS</A>
							<DT><A HREF="https://ogb.stanford.edu/neurips2022/">OGB-LSC @ NeurIPS 2022 | Open Graph Benchmark</A>
						</DL><p>
						<DT><H3 FOLDED>ICML (A++)</H3>
						<DL><p>
							<DT><A HREF="https://icml.cc/">ICML</A>
						</DL><p>
						<DT><H3 FOLDED>CVPR (A++)</H3>
						<DL><p>
							<DT><A HREF="https://cvpr2022.thecvf.com/">Home | CVPR 2022</A>
						</DL><p>
						<DT><H3 FOLDED>ICCV (A++)</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>NAACL (A+)</H3>
						<DL><p>
							<DT><A HREF="https://huggingface.co/spaces/NAACL2022/papers">NAACL 2022 Papers - a Hugging Face Space by NAACL2022</A>
						</DL><p>
						<DT><H3 FOLDED>ICSE (NIER) (A++)</H3>
						<DL><p>
							<DT><A HREF="https://conf.researchr.org/track/icse-2023/icse-2023-NIER">ICSE 2023 - NIER - New Ideas and Emerging Results - ICSE 2023</A>
							<DT><A HREF="https://www.ieee.org/conferences/publishing/templates.html">IEEE - Manuscript Templates for Conference Proceedings</A>
							<DT><A HREF="https://osl.ugr.es/CTAN/macros/latex/contrib/IEEEtran/IEEEtran_HOWTO.pdf">How to Use thE IEEEtran LaTeX Class</A>
							<DT><A HREF="https://www.connectedpapers.com/">Connected Papers | Find and explore academic papers</A>
						</DL><p>
						<DT><H3 FOLDED>ASPLOS (ML SYS)</H3>
						<DL><p>
							<DT><A HREF="https://www.asplos-conference.org/">ASPLOS 2024 ‚Äì San Diego, USA ‚Äî April 27- May 1, 2024</A>
							<DT><A HREF="https://www.asplos-conference.org/asplos2024/">ASPLOS 2024 ‚Äì ASPLOS 2024</A>
						</DL><p>
						<DT><A HREF="https://openreview.net/group?id=ICLR.cc/2022/Conference">ICLR 2022 Conference | OpenReview</A>
						<DT><A HREF="https://towardsdatascience.com/announcing-the-learning-on-graphs-conference-c63caed7347">Announcing the Learning on Graphs Conference | by Michael Bronstein | Apr, 2022</A>
					</DL><p>
					<DT><H3 FOLDED>Robotics &amp; manufacturing</H3>
					<DL><p>
						<DT><H3 FOLDED>SLAM</H3>
						<DL><p>
							<DT><A HREF="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">Simultaneous localization and mapping - Wikipedia</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Particle_filter">Particle filter - Wikipedia</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter - Wikipedia</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation‚Äìmaximization algorithm - Wikipedia</A>
							<DT><A HREF="https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html#ac4107fb146a762454a8a87715d9b7c96">OpenCV: cv::VideoCapture Class Reference</A>
							<DT><A HREF="https://www.amazon.com/Introduction-Simulation-SLAM-Alan-Pritsker/dp/0470265884">Introduction to simulation and SLAM: Pritsker, A. Alan B: 9780470265888: Amazon.com: Books</A>
							<DT><A HREF="https://fama.us.es/discovery/fulldisplay?docid=alma991000286959704987&context=L&vid=34CBUA_US:VU1&lang=es&search_scope=all_data_not_idus&adaptor=Local%20Search%20Engine&tab=all_data_not_idus&query=any,contains,Introduction%20to%20simulation%20and%20SLAM">Introduction to simulation and SLAM II</A>
							<DT><A HREF="https://pjreddie.com/darknet/">Darknet: Open Source Neural Networks in C</A>
						</DL><p>
						<DT><H3 FOLDED>Aerospace</H3>
						<DL><p>
							<DT><H3 FOLDED>Rockets</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=1reS83Njcio">Actively Stabilized Model Rocket with Real-Time Telemetry - YouTube</A>
							<DT><A HREF="https://shield.ai/">Shield AI - Building The World‚Äôs Best AI Pilot</A>
						</DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=6cbayQAuvvw&t=305s">S3: They're Reimagining How to Build Anything | Hadrian</A>
						<DT><A HREF="https://www.hadrian.co/">Hadrian ‚Ä¢ Manufacturing the future</A>
						<DT><A HREF="https://twitter.com/GoogleDeepMind/status/1714627619742683245">MuJoCO 3.0: GPU &amp; TPU acceleration through JAX</A>
						<DT><A HREF="https://twitter.com/lethic1">(1) Lethic (@lethic1) / X</A>
						<DT><A HREF="https://www.youtube.com/@lethic">Lethic Z - YouTube</A>
						<DT><A HREF="http://zlethic.com/">Lingkang Zhang</A>
						<DT><A HREF="https://github.com/zlingkang">zlingkang (Lingkang Zhang)</A>
						<DT><A HREF="https://www.anduril.com/">Anduril - Home</A>
					</DL><p>
					<DT><H3 FOLDED>papers-miscelaneous</H3>
					<DL><p>
						<DT><A HREF="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov‚ÄìArnold representation theorem - Wikipedia</A>
						<DT><A HREF="https://blog.ntrlab.com/why-do-neural-networks-need-an-activation-function/">Why do neural networks need an activation function?</A>
						<DT><A HREF="https://hadrien-montanelli.github.io/2019-06-25.html">Deep networks and the Kolmogorov‚ÄìArnold theorem</A>
						<DT><A HREF="https://blog.ntrlab.com/">NTRLab.Blog | Turning ideas into software</A>
						<DT><A HREF="https://math.stackexchange.com/questions/2518664/are-there-any-simple-examples-of-kolmogorov-arnold-representation">simple examples of Kolmogorov-Arnold representation?</A>
					</DL><p>
					<DT><A HREF="https://www.youtube.com/@ArxivPapers/videos">Arxiv Papers - YouTube</A>
					<DT><A HREF="https://logconference.github.io/cfp/">Learning on Graphs Conference</A>
				</DL><p>
				<DT><H3 FOLDED>Software</H3>
				<DL><p>
					<DT><H3 FOLDED>AI Compilers &amp; PL</H3>
					<DL><p>
						<DT><H3 FOLDED>CUDA</H3>
						<DL><p>
							<DT><H3 FOLDED>cuda-programming-model</H3>
							<DL><p>
								<DT><H3 FOLDED>cuda-memory-model</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy">Memory Hierarchy</A>
									<DT><A HREF="https://www.youtube.com/watch?v=uHN5fpfu8As">Memory hierarchy</A>
									<DT><A HREF="https://developer.nvidia.com/gtc/2020/video/cwe21754">GTC 2020: Memory Management on Modern GPU | NVIDIA Developer</A>
									<DT><A HREF="https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21819-optimizing-applications-for-nvidia-ampere-gpu-architecture.pdf">GPU memory architecture</A>
									<DT><A HREF="https://developer.download.nvidia.com/CUDA/training/register_spilling.pdf">Local memory/register spilling</A>
									<DT><A HREF="https://lwn.net/Articles/253361/">Memory part 3: Virtual Memory [LWN.net]</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-thread-divergence</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=95fFNzV4sd0">Parallel C++: Thread Affinity - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-branchless-computing</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>cuda-array-interface</H3>
								<DL><p>
									<DT><A HREF="https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html">CUDA Array Interface (Version 3) ‚Äî Numba 0+untagged.2155.g9ce83ef.dirty documentation</A>
									<DT><A HREF="https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.interface.html#__array_interface__">The Array Interface ‚Äî NumPy v1.13 Manual</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-learning</H3>
								<DL><p>
									<DT><H3 FOLDED>PMPP</H3>
									<DL><p>
										<DT><A HREF="https://github.com/drisspg/simple_cuda">drisspg/simple_cuda: Learnings + Exercises from the PMPP book!</A>
										<DT><A HREF="https://github.com/olcf/cuda-training-series">olcf/cuda-training-series: Training materials associated with NVIDIA's CUDA Training Series (www.olcf.ornl.gov/cuda-training-series/)</A>
										<DT><A HREF="https://www.amazon.es/Programming-Massively-Parallel-Processors-Hands/dp/0323912311">Programming Massively Parallel Processors: A Hands-on Approach : Hwu, Wen-mei W., Kirk, David B., El Hajj, Izzat: Amazon.es: Libros</A>
										<DT><A HREF="https://shop.elsevier.com/books/programming-massively-parallel-processors/hwu/978-0-323-91231-0">Programming Massively Parallel Processors - 4th Edition | Elsevier Shop</A>
										<DT><A HREF="http://gpu.di.unimi.it/books/PMPP-3rd-Edition.pdf">‚Äégpu.di.unimi.it/books/PMPP-3rd-Edition.pdf</A>
										<DT><A HREF="https://www.cse.iitd.ac.in/~rijurekha/col730_2022/cudabook.pdf">NVIDIA version</A>
										<DT><A HREF="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311">Programming Massively Parallel Processors: A Hands-on Approach: Hwu, Wen-mei W., Kirk, David B., El Hajj, Izzat: 9780323912310: Amazon.com: Books</A>
									</DL><p>
									<DT><A HREF="https://github.com/cuda-mode">CUDA MODE</A>
									<DT><A HREF="http://giantpandacv.com/project/CUDA/%E3%80%90BBuf%E7%9A%84CUDA%E7%AC%94%E8%AE%B0%E3%80%91%E4%B8%80%EF%BC%8C%E8%A7%A3%E6%9E%90OneFlow%20Element-Wise%20%E7%AE%97%E5%AD%90%E5%AE%9E%E7%8E%B0/">„ÄêBBufÁöÑCUDAÁ¨îËÆ∞„Äë‰∏ÄÔºåËß£ÊûêOneFlow Element-Wise ÁÆóÂ≠êÂÆûÁé∞ - GiantPandaCV</A>
									<DT><A HREF="https://github.com/BBuf/how-to-optim-algorithm-in-cuda">BBuf/how-to-optim-algorithm-in-cuda: how to optimize some algorithm in cuda.</A>
									<DT><A HREF="https://github.com/NVIDIA/cuda-samples">NVIDIA/cuda-samples: Samples for CUDA Developers which demonstrates features in CUDA Toolkit</A>
									<DT><A HREF="https://github.com/Tony-Tan/CUDA_Freshman">Tony-Tan/CUDA_Freshman</A>
									<DT><A HREF="https://github.com/cuda-mode/lectures">cuda-mode/lectures: Material for cuda-mode lectures</A>
									<DT><A HREF="https://github.com/mlecauchois/micrograd-cuda/blob/main/micrograd_cuda/operations.py">micrograd-cuda/micrograd_cuda/operations.py at main ¬∑ mlecauchois/micrograd-cuda</A>
									<DT><A HREF="https://github.com/shineyruan/CUDA-Stream-Compaction">shineyruan/CUDA-Stream-Compaction</A>
									<DT><A HREF="https://ahgamut.github.io/">Blog Needs a Name</A>
									<DT><A HREF="https://www.youtube.com/playlist?list=PL5Q2soXY2Zi_OwkTgEyA6tk3UsoPBH737">Livestream - P&amp;S Hands-on Acceleration on Heterogeneous Computing Systems (Fall 2021) - YouTube</A>
									<DT><A HREF="https://www.youtube.com/playlist?list=PL5Q2soXY2Zi-Mnk1PxjEIG32HAGILkTOF">Livestream - Computer Architecture - ETH Z√ºrich (Fall 2021) - YouTube</A>
									<DT><A HREF="https://github.com/FZJ-JSC/tutorial-multi-gpu">FZJ-JSC/tutorial-multi-gpu: Efficient Distributed GPU Programming for Exascale, an SC/ISC Tutorial</A>
								</DL><p>
								<DT><A HREF="https://github.com/IonThruster/CudaTutorials/blob/master/strategy.md">CudaTutorials/strategy.md</A>
								<DT><A HREF="https://www.olcf.ornl.gov/calendar/cuda-shared-memory/">CUDA Shared Memory ‚Äì Oak Ridge Leadership Computing Facility</A>
								<DT><A HREF="https://www.olcf.ornl.gov/cuda-training-series/">CUDA Training Series ‚Äì Oak Ridge Leadership Computing Facility</A>
								<DT><A HREF="https://github.com/olcf/cuda-training-series">olcf/cuda-training-series: Training materials associated with NVIDIA's CUDA Training Series (www.olcf.ornl.gov/cuda-training-series/)</A>
								<DT><A HREF="https://github.com/NVIDIA/multi-gpu-programming-models">NVIDIA/multi-gpu-programming-models: Examples demonstrating available options to program multiple GPUs in a single node or a cluster</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">1. Introduction ‚Äî CUDA C Programming Guide</A>
								<DT><A HREF="https://www.youtube.com/watch?v=KHa-OSrZPGo">CppCon 2016: ‚ÄúBringing Clang and C++ to GPUs: An Open-Source, CUDA-Compatible GPU C++ Compiler" - YouTube</A>
								<DT><A HREF="https://tripack45.github.io/2019/10/20/intern2019/">Parallelism on Its Own Feet: 90 Days as an NVidia Intern | Patricium</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md">cutlass/media/docs/cute/00_quickstart.md at main ¬∑ NVIDIA/cutlass</A>
								<DT><A HREF="https://github.com/NVIDIA/cuda-samples">NVIDIA/cuda-samples: Samples for CUDA Developers which demonstrates features in CUDA Toolkit</A>
								<DT><A HREF="https://github.com/Tony-Tan/CUDA_Freshman">Tony-Tan/CUDA_Freshman</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cXpTDKjjKZE">03 CUDA Fundamental Optimization Part 1 - YouTube</A>
								<DT><A HREF="https://github.com/cuda-mode/lecture2">cuda-mode/lecture2: lecture 2 - 2024-01-20</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NQ-0D5Ti2dc">Lecture 2 Ch1-3 PMPP book</A>
								<DT><A HREF="https://www.youtube.com/watch?v=OSpy-HoR0ac">Intro to CUDA (part 5): Memory Model - YouTube</A>
								<DT><A HREF="https://github.com/torstem/demo-cuda-pybind11">demo-cuda-pybind11</A>
								<DT><A HREF="https://github.com/cuda-mode/resource-stream">cuda-mode/resource-stream: CUDA related news and material links</A>
								<DT><A HREF="https://github.com/cuda-mode/lectures">cuda-mode/lectures: Material for cuda-mode lectures</A>
								<DT><A HREF="https://www.youtube.com/watch?v=u70a9ssZnjI&list=LL&index=17">George Hotz | Programming | cherry computer: no more superscalar? the thneed lesson - YouTube</A>
								<DT><A HREF="https://blog.speechmatics.com/pointless-gpu-optimization-exercise">An Almost Pointless Exercise in GPU Optimization</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/">CUDA C++ Best Practices Guide</A>
								<DT><A HREF="https://github.com/NVIDIA/multi-gpu-programming-models">NVIDIA/multi-gpu-programming-models: Examples demonstrating available options to program multiple GPUs</A>
								<DT><A HREF="https://leimao.github.io/blog/Row-Major-VS-Column-Major/">Row-Major VS Column-Major - Lei Mao's Log Book</A>
								<DT><A HREF="https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/">CUDA Refresher: The CUDA Programming Model</A>
							</DL><p>
							<DT><H3 FOLDED>PTX-Parallel Thread Execution</H3>
							<DL><p>
								<DT><H3 FOLDED>NVVM IR</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/7_libNVVM">cuda-samples/Samples/7_libNVVM at master ¬∑ NVIDIA/cuda-samples</A>
								</DL><p>
								<DT><H3 FOLDED>source-in-ptx</H3>
								<DL><p>
									<DT><A HREF="https://forums.developer.nvidia.com/t/how-to-see-ptx-cu-source-code/220043">How to see PTX/CU/source code? - Developer Tools / Nsight Compute - NVIDIA Developer Forums</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/pdf/CUDA_Compiler_Driver_NVCC.pdf">NVIDIA CUDA Compiler Driver Release 12.4</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html">PTX Writers Guide to Interoperability</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma">matrix-multiply-accumulate (mma)</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix">ldmatrix</A>
								<DT><A HREF="https://github.com/openai/triton/blob/8a83b141fc832dfb83dac3f335b21f3efb888afd/python/triton/tools/compile.py#L18">triton: compile.py</A>
								<DT><A HREF="https://segmentfault.com/a/1190000041878026/en">Ê∑±Â∫¶Â≠¶‰π† - Practice torch.fx Part 1 - Pytorch-based Model Optimization Quantization Artifact - ‰∏™‰∫∫ÊñáÁ´† - SegmentFault ÊÄùÂê¶</A>
								<DT><A HREF="https://segmentfault.com/a/1190000041878026/en">Practice torch.fx Part 1 - Pytorch-based Model Optimization</A>
								<DT><A HREF="https://twitter.com/cis_female/status/1660390038226751490">microcode: ncu -k "kernel" --list-page sass --page source python3 file.py</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html">PTX ISA 8.5</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk">cp.async.bulk</A>
							</DL><p>
							<DT><H3 FOLDED>NVCC</H3>
							<DL><p>
								<DT><H3 FOLDED>NVVM IR</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/7_libNVVM">cuda-samples/Samples/7_libNVVM at master ¬∑ NVIDIA/cuda-samples</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">CUDA NVCC</A>
								<DT><A HREF="https://leimao.github.io/blog/CUDA-Compilation/">CUDA Compilation - Lei Mao's Log Book</A>
							</DL><p>
							<DT><H3 FOLDED>cubin</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/cHHillee/status/1779141387876962469">Triton kernels can be precompiled into .cubin files</A>
							</DL><p>
							<DT><H3 FOLDED>cudarc</H3>
							<DL><p>
								<DT><A HREF="https://github.com/akhildevelops/cudaz">akhildevelops/cudaz: A Zig Cuda wrapper</A>
								<DT><A HREF="https://github.com/coreylowman/cudarc/tree/main">coreylowman/cudarc: Safe rust wrapper around CUDA toolkit</A>
								<DT><A HREF="https://leimao.github.io/blog/Proper-CUDA-Error-Checking/">Proper CUDA Error Checking - Lei Mao's Log Book</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-sass</H3>
							<DL><p>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">1. Overview ‚Äî cuda-binary-utilities (SaSS)</A>
								<DT><A HREF="https://github.com/openai/triton/blob/8a83b141fc832dfb83dac3f335b21f3efb888afd/python/triton/tools/disasm.py#L69">triton: disasm.py#L69</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-kernels</H3>
							<DL><p>
								<DT><H3 FOLDED>Apex</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/apex">NVIDIA/apex: A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch</A>
									<DT><A HREF="https://nvidia.github.io/apex/">Apex (A PyTorch Extension) ‚Äî Apex 0.1.0 documentation</A>
									<DT><A HREF="https://github.com/cat-state/tinypar">cat-state/tinypar</A>
									<DT><A HREF="https://github.com/stas00/tinypar">stas00/tinypar: TP/PP/DP implementation of llama using apex blocks</A>
									<DT><A HREF="https://github.com/cat-state/tinypar/blob/main/llama.py#L130">tinypar/llama.py at main ¬∑ cat-state/tinypar</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-tensor-cores</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/discussions/4066">Tinygrad: CUDA Tensor Core Integration #2684</A>
									<DT><A HREF="https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/">NVIDIA Tensor Core Programming - Lei Mao's Log Book</A>
									<DT><A HREF="https://github.com/jbaron34/torchwindow">jbaron34/torchwindow: Display tensors directly from GPU</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-transformer-engine</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/te_llama/tutorial_accelerate_hf_llama_with_te.ipynb">TransformerEngine/docs/examples/te_llama/tutorial_accelerate_hf_llama_with_te.ipynb at main ¬∑ NVIDIA/TransformerEngine</A>
									<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/15">Ada Lovelace support</A>
									<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
									<DT><A HREF="http://giantpandacv.com/project/CUDA/%E8%AF%A6%E8%A7%A3%20NVIDIA%20H100%20TransformerEngine/">ËØ¶Ëß£ NVIDIA H100 TransformerEngine - GiantPandaCV</A>
								</DL><p>
								<DT><A HREF="https://github.com/enp1s0/cutf">enp1s0/cutf: CUDA Template Functions</A>
								<DT><A HREF="https://jhui.github.io/2017/03/06/CUDA/">‚ÄúCUDA Tutorial‚Äù</A>
								<DT><A HREF="https://github.com/mcarilli/cuda-memory/tree/master">mcarilli/cuda-memory: Playing around with GPU memory optimization</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index">cuda-mode/triton-index: Cataloging released Triton kernels.</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index/blob/main/kernel_overview.md">triton-index/kernel_overview.md</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-graphs</H3>
							<DL><p>
								<DT><A HREF="https://developer.nvidia.com/blog/cuda-graphs/">Getting Started with CUDA Graphs | NVIDIA Technical Blog</A>
								<DT><A HREF="https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/">Accelerating PyTorch with CUDA Graphs | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/issues/99397">Internal errors with cuda graph (CUBLAS_STATUS_NOT_INITIALIZED and jit failure) ¬∑ Issue #99397 ¬∑ pytorch/pytorch</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/understanding-cudagraph-trees/1967">Understanding CUDAGraph Trees - compiler - PyTorch Developer Mailing List</A>
								<DT><A HREF="https://fireworks.ai/blog/speed-python-pick-two-how-cuda-graphs-enable-fast-python-code-for-deep-learning">Speed, Python: Pick Two. How CUDA Graphs Enable Fast Python Code for Deep Learning</A>
							</DL><p>
							<DT><H3 FOLDED>cccl</H3>
							<DL><p>
								<DT><H3 FOLDED>llm.cpp</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=WiB_3Csfj_Q">CUDA C++ llm.cpp - YouTube</A>
									<DT><A HREF="https://github.com/karpathy/llama2.c">karpathy/llama2.c: Inference Llama 2 in one file of pure C</A>
									<DT><A HREF="https://github.com/gevtushenko/llm.c">gevtushenko/llm.c: LLM training in simple, raw C/CUDA</A>
								</DL><p>
								<DT><H3 FOLDED>Thrust</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/thrust/index.html">Thrust</A>
									<DT><A HREF="https://thrust.github.io/">Thrust - Parallel Algorithms Library</A>
									<DT><A HREF="https://github.com/NVIDIA/thrust">NVIDIA/thrust: The C++ parallel algorithms library.</A>
									<DT><H3 FOLDED>Fancy Iterators</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/archive/12.0.0/pdf/Thrust_Quick_Start_Guide.pdf">Thrust_Quick_Start_Guide.pdf</A>
									<DT><A HREF="https://www.youtube.com/watch?v=zlJg9mCNfkQ">Thrust and the C++ Standard Algorithms - Conor Hoekstra - GTC 2021 - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=W2tWOdzgXHA">GoingNative 2013 C++ Seasoning</A>
									<DT><A HREF="https://www.youtube.com/watch?v=h4Jl1fk3MkQ">CppCon 2016: Marshall Clow ‚ÄúSTL Algorithms - why you should use them, and how to write your own" - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=2olsGf6JIkU">CppCon 2018: Jonathan Boccara ‚Äú105 STL Algorithms in Less Than an Hour‚Äù - YouTube</A>
									<DT><A HREF="https://github.com/codereport/Content/tree/main/Talks">Content/Talks at main ¬∑ codereport/Content</A>
									<DT><A HREF="https://github.com/codereport/Content/tree/main/Talks/2021-04-GTC/ThrustAndTheCppStandardAlgorithms">Content/Talks/2021-04-GTC/ThrustAndTheCppStandardAlgorithms</A>
									<DT><A HREF="https://research.nvidia.com/publication/2011-10_thrust-productivity-oriented-library-cuda">Thrust: A Productivity-Oriented Library for CUDA | Research</A>
									<DT><A HREF="https://developer.nvidia.com/blog/expressive-algorithmic-programming-thrust/">Expressive Algorithmic Programming with Thrust | NVIDIA Technical Blog</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=WiB_3Csfj_Q">CUDA C++ llm.cpp - YouTube</A>
								<DT><A HREF="https://github.com/NVIDIA/cccl">NVIDIA/cccl: CUDA C++ Core Libraries</A>
								<DT><A HREF="https://github.com/eyalroz/cuda-api-wrappers">eyalroz/cuda-api-wrappers: Thin, unified, C++-flavored wrappers for the CUDA APIs</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-python</H3>
							<DL><p>
								<DT><H3 FOLDED>pycuda</H3>
								<DL><p>
									<DT><A HREF="https://github.com/inducer/pycuda">inducer/pycuda: CUDA integration for Python, plus shiny features</A>
									<DT><A HREF="https://documen.tician.de/pycuda/driver.html">Device Interface</A>
									<DT><A HREF="https://forums.developer.nvidia.com/t/pycuda-best-practice-to-keep-c-kernels-separate-from-python-code/72829/4">load_kernels</A>
								</DL><p>
								<DT><A HREF="https://github.com/NVIDIA/cuda-python">NVIDIA/cuda-python: CUDA Python Low-level Bindings</A>
								<DT><A HREF="https://github.com/inducer/pyopencl">inducer/pyopencl: OpenCL integration for Python, plus shiny features</A>
								<DT><A HREF="https://github.com/MatthieuDartiailh/pyclibrary">MatthieuDartiailh/pyclibrary: C parser and ctypes automation for python</A>
								<DT><A HREF="https://github.com/trolldbois/ctypeslib/blob/master/ctypeslib/clang2py.py">clang2py.py</A>
							</DL><p>
							<DT><H3 FOLDED>CUTLASS</H3>
							<DL><p>
								<DT><H3 FOLDED>Mixed-input matrix multiplication performance optimizations</H3>
								<DL><p>
									<DT><A HREF="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">Mixed-input matrix multiplication performance optimizations ‚Äì Google Research Blog</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/pull/1084">Support for Mixed Input TensorOp by manishucsd ¬∑ Pull Request #1084 ¬∑ NVIDIA/cutlass</A>
								</DL><p>
								<DT><H3 FOLDED>Tensor Memory Accelerator (TMA)</H3>
								<DL><p>
									<DT><A HREF="https://research.colfax-intl.com/tutorial-hopper-tma/">CUTLASS Tutorial: Mastering the NVIDIA¬Æ Tensor Memory Accelerator (TMA) ‚Äì Colfax Research</A>
									<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/tree/master/tma">cfx-article-src/tma at master ¬∑ ColfaxResearch/cfx-article-src</A>
								</DL><p>
								<DT><A HREF="https://github.com/NVIDIA/cutlass">NVIDIA/cutlass: CUDA Templates for Linear Algebra Subroutines</A>
								<DT><A HREF="https://www.youtube.com/watch?v=PWWOGrLZtZg">CUTLASS: A CUDA C++ Template Library for Accelerating Deep Learning... Aniket Shivam &amp; Vijay Thakkar - YouTube</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md">Getting Started With CuTe</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md">CuTe Layouts</A>
								<DT><A HREF="https://www.youtube.com/watch?v=yCyZEJrlrfY&t=126s">Lightning Talk: Harnessing NVIDIA Tensor Cores: An Exploration of CUTLASS &amp; OpenAI..- Matthew Nicely - YouTube</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/python/README.md">cutlass/python/README.md</A>
								<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31883/">Accelerating Convolution with Tensor Cores in CUTLASS</A>
								<DT><A HREF="https://www.nvidia.com/ko-kr/on-demand/session/gtcspring22-s41996/">Accelerating Backward Data Gradient by Increasing Tensor Core Utilization in CUTLASS</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/pull/1084">Support for Mixed Input TensorOp by manishucsd (Google PR)</A>
								<DT><H3 FOLDED>CuTe</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md">CuTe Layouts</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md">Getting Started With CuTe</A>
									<DT><A HREF="https://research.colfax-intl.com/a-note-on-the-algebra-of-cute-layouts/">A note on the algebra of CuTe Layouts ‚Äì Colfax Research</A>
								</DL><p>
								<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-flashattention-2/">A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library ‚Äì Colfax Research</A>
								<DT><A HREF="https://research.colfax-intl.com/wp-content/uploads/2023/12/colfax-flashattention.pdf">A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library</A>
								<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/">GEMM kernels Hopper</A>
								<DT><A HREF="https://www.youtube.com/watch?v=G6q719ck7ww">Lecture 15: CUTLASS - YouTube</A>
								<DT><A HREF="https://pypi.org/project/nvidia-cutlass/">nvidia-cutlass ¬∑ PyPI</A>
								<DT><A HREF="https://dl.acm.org/doi/pdf/10.1145/3620666.3651369">EVT: Accelerating Deep Learning Training with Epilogue Visitor Tree</A>
								<DT><A HREF="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS: Fast Linear Algebra in CUDA C++ | NVIDIA Technical Blog</A>
								<DT><A HREF="https://github.com/ericauld/cutlass-playground">ericauld/cutlass-playground</A>
								<DT><A HREF="https://research.colfax-intl.com/tutorial-matrix-transpose-in-cutlass/">Tutorial: Matrix Transpose in CUTLASS ‚Äì Colfax Research</A>
								<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/tree/master/transpose-cute">cfx-article-src/transpose-cute at master ¬∑ ColfaxResearch/cfx-article-src</A>
								<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/tree/master/cutlass_gemm">cfx-article-src/cutlass_gemm at master ¬∑ ColfaxResearch/cfx-article-src</A>
								<DT><A HREF="https://github.com/ColfaxResearch/cutlass-kernels">ColfaxResearch/cutlass-kernels</A>
								<DT><A HREF="https://github.com/yester31/Cutlass_EX">yester31/Cutlass_EX: study of cutlass</A>
							</DL><p>
							<DT><H3 FOLDED>cuBLAS</H3>
							<DL><p>
								<DT><A HREF="https://siboehm.com/articles/22/CUDA-MMM">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog (main)</A>
								<DT><A HREF="https://github.com/wangzyon/NVIDIA_SGEMM_PRACTICE">wangzyon/NVIDIA_SGEMM_PRACTICE: Step-by-step optimization of CUDA SGEMM</A>
								<DT><A HREF="https://github.com/Bruce-Lee-LY/cuda_hgemm">Bruce-Lee-LY/cuda_hgemm: Several optimization methods of half-precision general matrix multiplication (HGEMM) using tensor core with WMMA API and MMA PTX instruction.</A>
								<DT><A HREF="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/tinyblas.cu">llamafile/llamafile/tinyblas.cu at main ¬∑ Mozilla-Ocho/llamafile</A>
							</DL><p>
							<DT><H3 FOLDED>NCCL</H3>
							<DL><p>
								<DT><H3 FOLDED>NVLINK</H3>
								<DL><p>
									<DT><A HREF="https://www.nvidia.com/en-us/data-center/nvlink/">nvlink</A>
								</DL><p>
								<DT><H3 FOLDED>nccl-installation</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html">Installation Guide :: NVIDIA Deep Learning NCCL Documentation</A>
								</DL><p>
								<DT><A HREF="https://en.wikipedia.org/wiki/Collective_operation">Collective operation - Wikipedia</A>
								<DT><A HREF="https://twitter.com/ProjectPhysX/status/1637789116363407362">(1) Dr. Moritz Lehmann en X: "There is a supppsed vendor-independent (but not cross-vendor) way of #GPU P2P communication in #OpenCL: have all GPUs in the same context &amp;amp; pass buffers to other devices' kernels.üí° Drivers should automatically handle P2P comm via PCIe/SLI/NVLink/CrossFire/InfFabric. üßµ1/9 https://t.co/h0DsEPPkFq" / X</A>
								<DT><A HREF="https://github.com/coreweave/nccl-tests">coreweave/nccl-tests: NVIDIA NCCL Tests for Distributed Training</A>
								<DT><A HREF="https://github.com/open-mpi/ompi">open-mpi/ompi: Open MPI main development repository</A>
								<DT><A HREF="https://github.com/horovod/horovod/blob/master/docs/concepts.rst">horovod/docs/concepts.rst at master ¬∑ horovod/horovod</A>
								<DT><A HREF="https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/">MPI Scatter, Gather, and Allgather ¬∑ MPI Tutorial</A>
								<DT><A HREF="https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/">MPI Broadcast and Collective Communication ¬∑ MPI Tutorial</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/tests/onebit/test_nccl_perf.py">DeepSpeed/tests: test_nccl_perf.py</A>
								<DT><A HREF="https://www.youtube.com/watch?v=nD6PUe3d6ec">Lecture #2 - Scatter-to-Gather Transformation - YouTube</A>
								<DT><A HREF="https://github.com/cuda-mode/p2p-perf">cuda-mode/p2p-perf: measuring peer-to-peer (p2p) transfer on different cuda devices</A>
								<DT><A HREF="https://www.youtube.com/watch?v=T22e3fgit-A">Lecture 17: NCCL - YouTube</A>
								<DT><A HREF="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/p2pBandwidthLatencyTest">cuda-samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest at master ¬∑ NVIDIA/cuda-samples</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-profiling</H3>
							<DL><p>
								<DT><H3 FOLDED>DrGPU: A Top-Down Profiler for GPU</H3>
								<DL><p>
									<DT><A HREF="https://github.com/FindHao/drgpu">FindHao/drgpu</A>
									<DT><A HREF="https://about.findhao.net/ICPE2023.pdf">DrGPU: A Top-Down Profiler for GPU</A>
									<DT><A HREF="https://github.com/FindHao/drgpu">FindHao/drgpu (Optimization suggestions)</A>
									<DT><A HREF="https://chat.openai.com/c/74ad9a19-e6f0-4ba3-a0d7-0d7a0b6c5c61">Performance Logging &amp; Saving Kernel ASM &amp; IR</A>
									<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/de17730a993b1d2cce4fd09e3654b5f79fd23c96/kernels/triton/inference/gptq/a100_qlinear.py#L8">applied-ai/kernels/triton/inference/gptq/a100_qlinear.py (ASM &amp; IR)</A>
								</DL><p>
								<DT><H3 FOLDED>Nsight</H3>
								<DL><p>
									<DT><A HREF="https://gpuhackshef.readthedocs.io/en/latest/">Sheffield GPU Hackathon ‚Äî GPUHackSheffield documentation</A>
									<DT><A HREF="https://gpuhackshef.readthedocs.io/en/latest/tools/nvidia-profiling-tools.html">NVIDIA Profiling Tools ‚Äî GPUHackSheffield documentation</A>
									<DT><A HREF="https://docs.nvidia.com/nsight-systems/UserGuide/index.html">User Guide :: Nsight Systems Documentation</A>
									<DT><A HREF="https://gist.github.com/mcarilli/376821aa1a7182dfcf59928a7cde3223">Favorite nsight systems profiling commands for Pytorch scripts</A>
									<DT><A HREF="https://gist.github.com/mcarilli/213a4e698e4a0ae2234ddee56f4f3f95">Single- and multiprocess profiling workflow with nvprof and NVVP (Nsight Systems coming soon...)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=nhTjq0P9uc8">08 GPU Performance Analysis - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=fsC3QeZHM1U">Introduction to Kernel Performance Analysis with NVIDIA Nsight Compute - YouTube</A>
									<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf">https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf</A>
									<DT><H3 FOLDED>High Pipe Utilization</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>Issue Slot Utilization</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>CPI Stall ‚ÄòLong Scoreboard‚Äô</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>videos</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=GCkdiHk6fUY">Memory Analysis with NVIDIA Nsight Compute (memory chart)</A>
										<DT><A HREF="https://www.youtube.com/watch?v=uHN5fpfu8As">Memory hierarchy</A>
										<DT><A HREF="https://www.youtube.com/watch?v=Iuy_RAvguBM">Intro to NVIDIA Nsight Compute</A>
										<DT><A HREF="https://www.youtube.com/watch?v=nhTjq0P9uc8">08 GPU Performance Analysis</A>
										<DT><A HREF="https://www.youtube.com/watch?v=fsC3QeZHM1U">Introduction to Kernel Performance Analysis with NVIDIA Nsight Compute</A>
										<DT><A HREF="https://www.youtube.com/watch?v=3DAYN-onSzY">GPU Series: Hands-On Session with NSight Systems and Compute - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=nhTjq0P9uc8">08 GPU Performance Analysis - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=fsC3QeZHM1U">Introduction to Kernel Performance Analysis with NVIDIA Nsight Compute - YouTube</A>
									</DL><p>
									<DT><A HREF="https://blog.speechmatics.com/pointless-gpu-optimization-exercise">An Almost Pointless Exercise in GPU Optimization</A>
									<DT><A HREF="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#memory-tables">Memory tables Profiling Guide</A>
									<DT><A HREF="https://github.com/NVIDIA/nsight-training/tree/master/cuda/nsight_compute/vlog_memory_workload">nsight-training/cuda/nsight_compute/vlog_memory_workload</A>
									<DT><A HREF="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">2. Kernel Profiling Guide ‚Äî NsightCompute 12.4 documentation</A>
									<DT><A HREF="http://tspeterkim.github.io/posts/nsight-setup-on-ec2">How to set up Nsight Compute Locally to profile Remote GPUs | Taeksang Peter Kim</A>
									<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf">ornl: Nsight Compute OLCF Webinar</A>
								</DL><p>
								<DT><H3 FOLDED>(NVML) NVIDIA Management Library</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html">NVML API Reference Guide :: GPU Deployment and Management Documentation</A>
								</DL><p>
								<DT><H3 FOLDED>nvprof</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/profiler-users-guide/">nvprof: Profiler Users Guide</A>
								</DL><p>
								<DT><H3 FOLDED>TensorBoard</H3>
								<DL><p>
									<DT><A HREF="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">TensorFlow Profiler: Profile model performance ¬†|¬† TensorBoard</A>
									<DT><A HREF="https://github.com/pytorch/kineto/blob/main/tb_plugin/docs/gpu_utilization.md">pytorch gpu_utilization.md</A>
									<DT><A HREF="https://github.com/pytorch/kineto/tree/main/tb_plugin">tb_plugin</A>
									<DT><A HREF="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard ‚Äî PyTorch Tutorials 2.0.1+cu117 documentation</A>
									<DT><A HREF="https://github.com/LaurentMazare/tboard-rs">LaurentMazare/tboard-rs: Read and write tensorboard data using Rust</A>
								</DL><p>
								<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/15">Proper benchmarkign with CUDA synchronization NVIDIA/TransformerEngine</A>
								<DT><A HREF="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">TensorFlow Profiler: Profile model performance ¬†|¬† TensorBoard</A>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html">GPU Performance Background User's Guide - NVIDIA Docs</A>
								<DT><A HREF="https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py">Step Demo ‚Äî Matplotlib 3.7.2 documentation</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_latency.py">vllm/benchmarks/benchmark_latency.py at main ¬∑ vllm-project/vllm</A>
								<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu">Kobzol/hardware-effects-gpu: Demonstration of various hardware effects on CUDA GPUs.</A>
								<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu/tree/master/memory-coalescing">hardware-effects-gpu/memory-coalescing at master ¬∑ Kobzol/hardware-effects-gpu</A>
								<DT><A HREF="https://github.com/mcarilli/cuda-memory/tree/master">mcarilli/cuda-memory: Playing around with GPU memory optimization</A>
								<DT><A HREF="https://github.com/cupy/cupy/blob/5a4c7a1c461776c779afc1e614aa06db7be594fa/docs/source/user_guide/performance.rst#L10">Performance Best Practices: CuPy</A>
								<DT><A HREF="https://github.com/Lin-Mao/DrGPUM">Lin-Mao/DrGPUM: A memory profiler for NVIDIA GPUs to explore memory inefficiencies in GPU-accelerated applications.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch - YouTube</A>
								<DT><A HREF="https://docs.google.com/presentation/d/110dnMW94LX1ySWxu9La17AVUxjgSaQDLOotFC3BZZD4/edit#slide=id.p">Lecture 1 How to profile CUDA kernels in PyTorch</A>
								<DT><A HREF="https://www.speechmatics.com/company/articles-and-news/timing-operations-in-pytorch">How to Accurately Time CUDA Kernels in Pytorch</A>
								<DT><A HREF="https://developer.nvidia.com/tools-overview">NVIDIA Developer Tools Overview | NVIDIA Developer</A>
								<DT><A HREF="https://developer.nvidia.com/nsight-visual-studio-code-edition">Nsight Visual Studio Code Edition</A>
								<DT><A HREF="https://developer.nvidia.com/nsight-dl-designer">Nsight Deep Learning Designer (earlier access)</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_latency.py">vllm/benchmarks/benchmark_latency.py at main</A>
								<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu/tree/master/memory-coalescing">hardware-effects-gpu/memory-coalescing at master</A>
								<DT><A HREF="https://github.com/Lin-Mao/DrGPUM">Lin-Mao/DrGPUM: A memory profiler for NVIDIA GPUs to explore memory inefficiencies in GPU</A>
								<DT><A HREF="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.pdf">Understanding Latency Hiding on GPUs (2016)</A>
								<DT><A HREF="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives</A>
								<DT><A HREF="https://github.com/NVIDIA/nvbandwidth">NVIDIA/nvbandwidth: A tool for bandwidth measurements on NVIDIA GPUs.</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-micro-benchmarking</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/1903.07486.pdf">Dissecting the NVidia Turing T4 GPU via Microbenchmarking</A>
								<DT><A HREF="https://arxiv.org/pdf/1912.03413.pdf">IPU</A>
								<DT><A HREF="https://arxiv.org/pdf/1804.06826.pdf%5B/url%5D">Volta</A>
								<DT><A HREF="https://github.com/sophiawisdom/benchmarks">sophiawisdom/benchmarks</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-reproducibility</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/docs/stable/notes/randomness.html">Reproducibility ‚Äî PyTorch 1.13 documentation</A>
								<DT><A HREF="https://vandurajan91.medium.com/random-seeds-and-reproducible-results-in-pytorch-211620301eba">Random seeds and reproducible results in PyTorch | by Vandana Rajan | Medium</A>
								<DT><A HREF="https://developer.nvidia.com/deep-learning-performance-training-inference">Reproducible Performance</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-error-correction-code</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>MGI</H3>
							<DL><p>
								<DT><A HREF="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html">NVIDIA Multi-Instance GPU User Guide :: NVIDIA Tesla Documentation</A>
								<DT><A HREF="https://developer.nvidia.com/blog/getting-the-most-out-of-the-a100-gpu-with-multi-instance-gpu/">Getting the Most Out of the NVIDIA A100 GPU with Multi-Instance GPU | NVIDIA Technical Blog</A>
								<DT><A HREF="https://pytorch.org/docs/stable/cuda.html">torch.cuda ‚Äî PyTorch 1.13 documentation</A>
								<DT><A HREF="https://discuss.pytorch.org/t/access-gpu-partitions-in-mig/142272">Access GPU partitions in MIG - PyTorch Forums</A>
							</DL><p>
							<DT><H3 FOLDED>IB</H3>
							<DL><p>
								<DT><A HREF="https://pytorchtoatoms.substack.com/p/nvidia-quantum-x800-next-generation">NVIDIA Quantum-X800: Next Generation Infiniband 800Gbit/s Network Topology</A>
								<DT><A HREF="https://pytorchtoatoms.substack.com/p/benchmarking-nvlink-and-infiniband">Benchmarking Nvlink &amp; Infiniband Network Speeds</A>
							</DL><p>
							<DT><H3 FOLDED>NVLink</H3>
							<DL><p>
								<DT><A HREF="https://pytorchtoatoms.substack.com/p/benchmarking-nvlink-and-infiniband">Benchmarking Nvlink &amp; Infiniband Network Speeds</A>
							</DL><p>
							<DT><H3 FOLDED>GPUDirect</H3>
							<DL><p>
								<DT><H3 FOLDED>gds-installation</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">NVIDIA GPUDirect Storage Installation and Troubleshooting Guide - NVIDIA Docs</A>
									<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mofed-req-install">NVIDIA GPUDirect Storage Installation and Troubleshooting Guide - NVIDIA Docs</A>
									<DT><A HREF="https://docs.nvidia.com/networking/display/mlnxofedv531001">NVIDIA MLNX_OFED Documentation Rev 5.3-1.0.0.1 - NVIDIA Docs</A>
									<DT><A HREF="https://docs.nvidia.com/networking/display/mlnxofedv461000/downloading+mellanox+ofed">Downloading Mellanox OFED - NVIDIA Docs</A>
									<DT><A HREF="https://docs.nvidia.com/networking/display/mlnxofedv461000/general+support+in+mlnx_ofed">General Support in MLNX_OFED - NVIDIA Docs</A>
									<DT><A HREF="https://network.nvidia.com/products/infiniband-drivers/linux/mlnx_ofed/">Linux InfiniBand Drivers</A>
									<DT><A HREF="https://ubuntu.com/server/docs/nvidia-drivers-installation">NVIDIA drivers installation | Ubuntu</A>
									<DT><A HREF="https://github.com/developer-onizuka/gpudirect_storage">developer-onizuka/gpudirect_storage (global view)</A>
								</DL><p>
								<DT><H3 FOLDED>cuFile</H3>
								<DL><p>
									<DT><H3 FOLDED>kvikio</H3>
									<DL><p>
										<DT><A HREF="https://github.com/rapidsai/kvikio/blob/branch-24.04/python/tests/test_defaults.py">kvikio/python/tests/test_defaults.py (compat_mode)</A>
										<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/topics/cufile-compatibility.html">cuFile Compatibility Mode - NVIDIA Docs</A>
										<DT><A HREF="https://docs.rapids.ai/api/libkvikio/nightly/">Compatibility Mode (KVIKIO_COMPAT_MODE)</A>
									</DL><p>
									<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html">cuFile API Reference Guide - NVIDIA Docs</A>
									<DT><A HREF="https://github.com/rapidsai/kvikio">rapidsai/kvikio</A>
									<DT><A HREF="https://github.com/rapidsai/kvikio/pull/135">Overload `numpy.fromfile()` and `cupy.fromfile()` by madsbk ¬∑ Pull Request #135 ¬∑ rapidsai/kvikio</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py at main ¬∑ alpa-projects/alpa</A>
									<DT><A HREF="https://github.com/pfnet/pytorch-pfn-extras/blob/f6b127063ec910b71788db2ae6ef96a3d89832b1/tests/pytorch_pfn_extras_tests/cuda_tests/test_allocator.py">pytorch-pfn-extras/tests/pytorch_pfn_extras_tests/cuda_tests/test_allocator.py at f6b127063ec910b71788db2ae6ef96a3d89832b1 ¬∑ pfnet/pytorch-pfn-extras</A>
									<DT><A HREF="https://pytorch-pfn-extras.readthedocs.io/en/latest/user_guide/cuda.html">CUDA (CuPy Interoperability) ‚Äî pytorch-pfn-extras documentation</A>
									<DT><A HREF="https://github.com/NVIDIA/apex/blob/810ffae374a2b9cb4b5c5e28eaeca7d7998fca0c/apex/contrib/csrc/gpu_direct_storage/gds.cpp">apex/apex/contrib/csrc/gpu_direct_storage/gds.cpp</A>
									<DT><A HREF="https://github.com/NVIDIA/MagnumIO/blob/main/gds/samples/cufile_sample_022.cc">cuFile Batch APIs</A>
									<DT><A HREF="https://chat.openai.com/c/61bb588b-35a7-42a6-90e9-c2355e5646a9">Identify Disk Storage Type</A>
								</DL><p>
								<DT><A HREF="https://github.com/NVIDIA/gds-nvidia-fs">NVIDIA/gds-nvidia-fs: NVIDIA GPUDirect Storage Driver</A>
								<DT><A HREF="https://developer.nvidia.com/blog/gpudirect-storage/">GPUDirect Storage: A Direct Path Between Storage and GPU Memory</A>
								<DT><A HREF="https://arxiv.org/pdf/2203.04910.pdf?">GPU-Initiated On-Demand High-Throughput Storage Access in the BaM System Architecture</A>
								<DT><A HREF="https://on-demand.gputechconf.com/supercomputing/2019/pdf/sc1922-gpudirect-storage-transfer-data-directly-to-gpu-memory-alleviating-io-bottlenecks.pdf">GPUDIRECT STORAGE:A DIRECT GPU-STORAGE DATA PATH</A>
								<DT><A HREF="https://www.snia-j.org/cmm/images/2022/10/3-1NVIDIA.pdf">NVIDIA GPUDirect Storage</A>
								<DT><A HREF="https://github.com/michaelbe2/write_to_gpu">michaelbe2/write_to_gpu: Using RC or DC with new post send APIs</A>
								<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py at main ¬∑ alpa-projects/alpa</A>
								<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html">NVIDIA GPUDirect Storage O_DIRECT Requirements Guide - NVIDIA Docs</A>
								<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html">NVIDIA GPUDirect Storage Best Practices Guide - NVIDIA Docs</A>
								<DT><A HREF="https://medium.com/@kaiyongx2/quick-guide-to-gpudirect-storage-gds-592037bdc046">Learning Nvidia GPUDirect. Impetus of Using GPUDirect Storage | by KY | Medium</A>
								<DT><A HREF="https://chat.openai.com/c/61bb588b-35a7-42a6-90e9-c2355e5646a9">Identify Disk Storage Type</A>
								<DT><A HREF="https://man7.org/linux/man-pages/man8/lsblk.8.html">lsblk(8) - Linux manual page</A>
								<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">NVIDIA GPUDirect Storage Overview Guide - NVIDIA Docs</A>
								<DT><A HREF="https://ieeexplore.ieee.org/document/7973709">Offloading Communication Control Logic in GPU</A>
								<DT><A HREF="https://github.com/Mellanox/gpu_direct_rdma_access">Mellanox/gpu_direct_rdma_access: example code for using DC QP for providing RDMA READ and WRITE operations to remote GPU memory</A>
								<DT><A HREF="https://github.com/NVIDIA/gdrcopy">NVIDIA/gdrcopy: A fast GPU memory copy library based on NVIDIA GPUDirect RDMA technology</A>
								<DT><A HREF="https://github.com/lw?tab=stars">lw (Luca Wehrstedt)</A>
								<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-installation</H3>
							<DL><p>
								<DT><H3 FOLDED>symlinks &amp; /etc/alternatives</H3>
								<DL><p>
									<DT><A HREF="https://chat.openai.com/c/21c4fe05-b3de-4a20-b96c-6d5c1bc7bea4">update_alternatives: multiple cuda installations</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-conda-installation</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#conda-installation">CUDA Installation Guide for Linux</A>
									<DT><A HREF="https://anaconda.org/nvidia">https://anaconda.org/nvidia</A>
									<DT><A HREF="https://anaconda.org/nvidia/repo?label=cuda-12.4.1">Package repository for nvidia :: Anaconda.org</A>
									<DT><A HREF="https://chatgpt.com/c/fc80f38a-f3a1-46a5-8a48-537e6fefe62b">LD_LIBRARY_PATH &amp; cuda lib</A>
									<DT><A HREF="https://cmake.org/cmake/help/latest/envvar/LDFLAGS.html">LDFLAGS ‚Äî CMake 3.29.3 Documentation</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/">CUDA Installation Guide for Linux</A>
								<DT><A HREF="https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation">cuda - How to verify CuDNN installation</A>
								<DT><A HREF="https://nvidia.github.io/cuda-python/install.html">Installation - CUDA Python 12.1.0 documentation</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/archive/10.2/cuda-installation-guide-linux/index.html">nvcc</A>
								<DT><A HREF="https://developer.download.nvidia.com/compute/cuda/redist/">Index of /compute/cuda/redist</A>
								<DT><A HREF="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/">Index of /compute/cuda/repos/ubuntu2204</A>
								<DT><A HREF="https://chat.openai.com/c/21c4fe05-b3de-4a20-b96c-6d5c1bc7bea4">CUDA_HOME</A>
								<DT><A HREF="https://faculty.cc.gatech.edu/~hyesoon/spr09/installcuda.html">https://faculty.cc.gatech.edu/~hyesoon/spr09/installcuda.html</A>
							</DL><p>
							<DT><H3 FOLDED>nvidia-driver</H3>
							<DL><p>
								<DT><H3 FOLDED>cuda-driver-installation</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#uninstallation">8.6 Uninstallation (official docs)</A>
									<DT><A HREF="https://ubuntu.com/server/docs/nvidia-drivers-installation">NVIDIA drivers installation | Ubuntu</A>
									<DT><A HREF="https://help.ubuntu.com/community/NvidiaDriversInstallation">NvidiaDriversInstallation - Community Help Wiki</A>
									<DT><A HREF="https://gist.github.com/jhelgert/b50d2b33d59eb935d8c37cd7d5d891d1">Installing the NVIDIA driver, CUDA, cuDNN, NCCL and Tensorflow on Linux</A>
									<DT><A HREF="https://linux.die.net/man/8/modprobe">modprobe(8): add/remove modules from Kernel - Linux man page</A>
									<DT><A HREF="https://www.nvidia.com/Download/index.aspx">NVIDIA Driver Downloads</A>
									<DT><A HREF="https://fossies.org/linux/misc/">Linux: Free open source software (misc) | Fossies Archive</A>
									<DT><A HREF="https://ubuntuforums.org/showthread.php?t=2454347">[SOLVED] dpkg: error processing archive /var/cache/apt/archives/libnvidia-compute-440_440.118.</A>
									<DT><A HREF="https://morgangiraud.medium.com/multi-gpu-tinygrad-patch-4904a75f8e16">Multi-GPU Tinygrad Patch. Unlocking Multi-GPU P2P Capabilities... | by Morgan | May, 2024 | Medium</A>
									<DT><A HREF="https://github.com/tinygrad/tinyos/blob/main/setup/driverinstall.sh">tinyos/setup/driverinstall.sh at main ¬∑ tinygrad/tinyos</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/datacenter/tesla/index.html">NVIDIA Data Center GPU Driver Documentation</A>
								<DT><A HREF="https://download.nvidia.com/XFree86/Linux-x86_64/470.223.02/README/">NVIDIA Accelerated Linux Graphics Driver README and Installation Guide</A>
								<DT><A HREF="https://github.com/tinygrad/open-gpu-kernel-modules">tinygrad/open-gpu-kernel-modules: NVIDIA Linux open GPU with P2P support</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/acc466751b2723eb913fd3148b4f054189bbf1ab/.devcontainer/README.md">pytorch/.devcontainer/README.md: NVIDIA Container Toolkit for GPU Usage</A>
								<DT><H3 FOLDED>open-gpu-kernel-modules</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tinygrad/tinyos/blob/main/setup/driverinstall.sh">tinyos/setup/driverinstall.sh at main ¬∑ tinygrad/tinyos</A>
									<DT><A HREF="https://morgangiraud.medium.com/multi-gpu-tinygrad-patch-4904a75f8e16">Multi-GPU Tinygrad Patch. Unlocking Multi-GPU P2P Capabilities... | by Morgan | May, 2024 | Medium</A>
									<DT><A HREF="https://github.com/tinygrad/open-gpu-kernel-modules">tinygrad/open-gpu-kernel-modules: NVIDIA Linux open GPU with P2P support</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>nvidia-smi</H3>
							<DL><p>
								<DT><A HREF="https://forums.developer.nvidia.com/t/nvidia-smi-drain-failed-to-parse-device-specified-at-the-command-line/180402">Nvidia-smi drain "Failed to parse device specified at the command-line" - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums</A>
								<DT><A HREF="https://unix.stackexchange.com/questions/654075/how-can-i-disable-and-later-re-enable-one-of-my-nvidia-gpus">linux - How can I disable (and later re-enable) one of my NVIDIA GPUs? - Unix &amp; Linux Stack Exchange</A>
							</DL><p>
							<DT><H3 FOLDED>cupy</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>cuda-sharing</H3>
							<DL><p>
								<DT><H3 FOLDED>Multi-Process Service (MPS)</H3>
								<DL><p>
									<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2021/06/MPS_ORNL_20210817.pdf">Introduction (slides)</A>
								</DL><p>
								<DT><H3 FOLDED>Time-Slicing</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html">Time-Slicing GPUs in Kubernetes ‚Äî NVIDIA GPU Operator 23.9.2 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>NVIDIA Multi-Instance GPU</H3>
								<DL><p>
									<DT><A HREF="https://www.nvidia.com/en-us/technologies/multi-instance-gpu/">Multi-Instance GPU (MIG) | NVIDIA</A>
								</DL><p>
								<DT><H3 FOLDED>CUDA Multi-Process Service (MPS)</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/deploy/mps/index.html">Multi-Process Service :: GPU Deployment and Management Documentation</A>
									<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2021/06/MPS_ORNL_20210817.pdf">Introduction (slides)</A>
								</DL><p>
								<DT><A HREF="https://research.colfax-intl.com/sharing-nvidia-gpus-at-the-system-level-time-sliced-and-mig-backed-vgpus/">Sharing NVIDIA¬Æ GPUs at the System Level: Time-Sliced and MIG-Backed vGPUs ‚Äì Colfax Research</A>
								<DT><A HREF="https://www.youtube.com/watch?v=8VQHwNwX-BU">NSDI '23 - Transparent GPU Sharing in Container Clouds for Deep Learning Workloads - YouTube</A>
								<DT><A HREF="https://github.com/pkusys/TGS">pkusys/TGS: Artifacts for our NSDI'23 paper TGS</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-people</H3>
							<DL><p>
								<DT><A HREF="https://github.com/wangzyon">wangzyon (Wang Zhiyong)</A>
								<DT><A HREF="https://www.youtube.com/@OnurMutluLectures/videos">Onur Mutlu Lectures - YouTube</A>
								<DT><A HREF="https://siboehm.com/">siboehm</A>
								<DT><A HREF="https://leimao.github.io/tags/CUDA/">Tag: CUDA - Lei Mao's Log Book</A>
							</DL><p>
							<DT><A HREF="https://github.com/geohot/cuda_ioctl_sniffer">geohot/cuda_ioctl_sniffer: Sniff CUDA ioctls</A>
							<DT><A HREF="https://developer.nvidia.com/deep-learning-performance-training-inference">Reproducible Performance</A>
							<DT><A HREF="https://catalog.ngc.nvidia.com/collections">Collections - Use-Case Based AI Software Packages | NVIDIA NGC</A>
							<DT><A HREF="https://forums.fast.ai/t/clearing-gpu-memory-pytorch/14637">Clearing GPU Memory - PyTorch - Part 1 (2018) / Beginner (2018) - Deep Learning Course Forums</A>
							<DT><A HREF="https://discuss.pytorch.org/t/why-moving-model-and-tensors-to-gpu/41498">Moving memory from CPU-RAM to GPU VRAM</A>
							<DT><A HREF="https://www.programcreek.com/python/?CodeExample=clear+memory">Python clear memory</A>
							<DT><A HREF="https://groups.google.com/a/anaconda.com/g/numba-users/c/7jkf-X_U7B8">Best way to clean up GPU memory</A>
							<DT><A HREF="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094">CUDA Runtime API :: CUDA Toolkit Documentation</A>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html">cuDNN Documentation</A>
							<DT><A HREF="https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation">cuda - How to verify CuDNN installation</A>
							<DT><A HREF="https://nvidia.github.io/cuda-python/install.html">Installation - CUDA Python 12.1.0 documentation</A>
							<DT><A HREF="https://twitter.com/marius/status/1657530968801181696/photo/1">GPU Computing</A>
							<DT><A HREF="https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics">CUDA semantics ‚Äî PyTorch 2.0 documentation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=QQceTDjA4f4">GTC 2022 - How CUDA Programming Works - Stephen Jones, CUDA Architect, NVIDIA - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=-J8YyfrSwTk">Effective ML - YouTube</A>
							<DT><A HREF="https://github.com/enp1s0/cutf">enp1s0/cutf: CUDA Template Functions</A>
							<DT><A HREF="https://jhui.github.io/2017/03/06/CUDA/">‚ÄúCUDA Tutorial‚Äù</A>
							<DT><A HREF="https://arxiv.org/pdf/1903.07486.pdf">https://arxiv.org/pdf/1903.07486.pdf</A>
							<DT><A HREF="https://arxiv.org/abs/1903.07486">[1903.07486] Dissecting the NVidia Turing T4 GPU via Microbenchmarking</A>
							<DT><A HREF="https://gist.github.com/mcarilli">mcarilli‚Äôs gists</A>
							<DT><A HREF="https://futhark-lang.org/">Why Futhark?</A>
							<DT><A HREF="https://www.youtube.com/watch?v=q38V66bqhfU">EfficientML.ai lecture - YouTube</A>
							<DT><A HREF="https://github.com/coreylowman/cudarc">coreylowman/cudarc: Safe rust wrapper around CUDA toolkit</A>
							<DT><A HREF="https://research.colfax-intl.com/adding-fp8-to-flashattention/">Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2 ‚Äì Colfax Research</A>
							<DT><A HREF="https://forums.fast.ai/t/clearing-gpu-memory-pytorch/14637">Clearing GPU Memory</A>
							<DT><A HREF="https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/core/providers/cuda">onnxruntime/onnxruntime/core/providers/cuda at main</A>
							<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/blob/df4a7534860137e060e18d2ebf019906120ea204/src/fastertransformer/kernels/matrix_transpose_kernels.cu#L4">FasterTransformer/src/fastertransformer/kernels/matrix_transpose_kernels.cu at df4a7534860137e060e18d2ebf019906120ea204 ¬∑ NVIDIA/FasterTransformer</A>
							<DT><A HREF="https://github.com/HigherOrderCO/HVM/blob/5de3e7ed8f1fcee6f267841a24119ffd569c714d/src/hvm.cu#L4">HVM/src/hvm.cu</A>
						</DL><p>
						<DT><H3 FOLDED>rocm</H3>
						<DL><p>
							<DT><H3 FOLDED>HIP</H3>
							<DL><p>
								<DT><A HREF="https://github.com/geohot/tinygrad/pull/750">HIP backend by nanamiwang ¬∑ Pull Request #750 ¬∑ geohot/tinygrad</A>
								<DT><A HREF="https://www.semianalysis.com/p/nvidiaopenaitritonpytorch">TorchInductor &amp; Triton</A>
								<DT><A HREF="https://github.com/openai/triton/issues/46">Support for HIP backend / AMD GPUs ¬∑ Issue #46 ¬∑ openai/triton</A>
								<DT><A HREF="https://github.com/openai/triton/blob/a2433f3135c312c05fbbcd98083896c93bf0c504/python/triton/runtime/driver.py#L111">triton/driver.py</A>
								<DT><A HREF="https://nv-adlr.github.io/MegatronLM">MegatronLM: NCCL</A>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/mpi.html">NCCL and MPI ‚Äî NCCL 2.18.1 documentation</A>
								<DT><A HREF="https://pytorch.org/docs/stable/notes/hip.html">HIP (ROCm) semantics ‚Äî PyTorch 2.0 documentation</A>
								<DT><A HREF="https://github.com/ROCm-Developer-Tools/HIP">ROCm-Developer-Tools/HIP: HIP: C++ Heterogeneous-Compute Interface for Portability</A>
								<DT><A HREF="https://github.com/ROCm-Developer-Tools/HIPIFY">HIPIFY</A>
								<DT><A HREF="https://docs.python.org/3/library/ctypes.html">ctypes ‚Äî A foreign function library for Python ‚Äî Python 3.11.3 documentation</A>
								<DT><A HREF="https://gist.github.com/geohot/6232fb00527de161a5c8ce8a635dd4f3">Wrapper for HIP</A>
								<DT><A HREF="https://www.youtube.com/playlist?list=PLB1fSi1mbw6IKbZSPz9a2r2DbnHWnLbF-">AMD HIP Tutorial - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>RDNA3</H3>
							<DL><p>
								<DT><A HREF="https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf">Instruction Set Architecture</A>
								<DT><A HREF="https://github.com/geohot/tinygrad/blob/ed038ba12906bb1980956b1e31e02e77ec0524ee/extra/rocm/rdna3/asm.py">tinygrad/asm.py</A>
							</DL><p>
							<DT><H3 FOLDED>rocm-llvm</H3>
							<DL><p>
								<DT><A HREF="https://llvm.org/docs/CommandGuide/llvm-mc.html">llvm-mc - LLVM Machine Code Playground ‚Äî LLVM 17.0.0git documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=avRWPe1MXPk">LLVM Isn't Always The Best Choice For Compilers. - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>rocm-lumi</H3>
							<DL><p>
								<DT><H3 FOLDED>user information</H3>
								<DL><p>
									<DT><A HREF="https://docs.csc.fi/accounts/how-to-manage-user-information/#how-to-link-your-csc-user-account-to-external-authentication-sources">Managing user information - Docs CSC</A>
									<DT><A HREF="https://my.csc.fi/profile">My CSC</A>
									<DT><A HREF="https://mms.myaccessid.org/fed-apps/profile/#settings_sshkeys">Perun - User profile</A>
									<DT><A HREF="https://puhuri-portal.neic.no/login/">Puhuri Portal |</A>
									<DT><A HREF="https://md.sigma2.no/lumi-general-course-oct23?both#DAY-1-%E2%80%93-Tuesday-3102023">LUMI General Course - HedgeDoc</A>
									<DT><A HREF="https://lumi-supercomputer.github.io/LUMI-training-materials/4day-20230530/extra_1_05_Compilers_and_Parallel_Programming_Models/">Compilers and Parallel Programming Models - LUMI training materials</A>
								</DL><p>
								<DT><A HREF="https://openfam.github.io/">OpenFAM: A library for programming Fabric-Attached Memory</A>
								<DT><A HREF="https://cpe.ext.hpe.com/docs/performance-tools/perftools-lite.html#description">perftools-lite ‚Äî HPE Cray Programming Environment 10.1.0 documentation</A>
							</DL><p>
							<DT><H3 FOLDED>rocm-instinct</H3>
							<DL><p>
								<DT><H3 FOLDED>rocm-amd-mi300x</H3>
								<DL><p>
									<DT><A HREF="https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance">AMD MI300 ‚Äì Taming The Hype ‚Äì AI Performance, Volume Ramp, Customers, Cost, IO, Networking, Software</A>
								</DL><p>
								<DT><H3 FOLDED>rocm-mi250x</H3>
								<DL><p>
									<DT><A HREF="https://www.lumi-supercomputer.eu/lumis-full-system-architecture-revealed/">LUMI‚Äôs full system architecture revealed - LUMI</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://rocm.github.io/rocncloc.html">ROCm, A New Era in Open GPU Computing</A>
							<DT><A HREF="https://github.com/ROCm-Developer-Tools/rocprofiler">ROCm-Developer-Tools/rocprofiler: ROC profiler library. Profiling with perf-counters and derived metrics.</A>
							<DT><A HREF="https://github.com/ROCmSoftwarePlatform/hip-python">ROCmSoftwarePlatform/hip-python: HIP Python Low-level Bindings</A>
							<DT><A HREF="https://www.youtube.com/watch?v=lnVQsJJFcdg&list=LL&index=296&t=1336s">George Hotz | Programming | writing a Qualcomm GPU driver | Freedreno | Mesa for compute | part 2 - YouTube</A>
							<DT><A HREF="https://github.com/tinygrad/remu?tab=readme-ov-file">tinygrad/remu: RDNA3 emulator</A>
							<DT><A HREF="https://docs.amd.com/">AMD Documentation - Portal</A>
							<DT><A HREF="https://pytorch.org/docs/stable/elastic/run.html#launcher-api">torchrun (Elastic Launch) ‚Äî PyTorch 2.0 documentation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=LG9G4aA28rU">GPU Programming Concepts (Part 1) - YouTube</A>
							<DT><A HREF="https://lambdalabs.com/blog/multi-node-pytorch-distributed-training-guide#distributed-pytorch-underthehood">(MPI): Multi node PyTorch Distributed Training</A>
							<DT><A HREF="https://github.com/openai/triton/pull/1983">[ROCM] Core Functionality for AMD by micmelesse ¬∑ Pull Request #1983 ¬∑ openai/triton</A>
							<DT><A HREF="https://www.mosaicml.com/blog/amd-mi250">Training LLMs with AMD MI250 GPUs and MosaicML</A>
							<DT><A HREF="https://tenstorrent.com/category/research/">Category: Research - Tenstorrent</A>
						</DL><p>
						<DT><H3 FOLDED>Kernels</H3>
						<DL><p>
							<DT><H3 FOLDED>kernels-theory</H3>
							<DL><p>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-05-12-quick-tk">ThunderKittens: A Simple Embedded DSL for AI kernels ¬∑ Hazy Research</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">GPUs Go Brrr ¬∑ Hazy Research</A>
								<DT><A HREF="https://twitter.com/karpathy/status/1789666350878601581">(1) Andrej Karpathy en X: "@Kartikayb77 I read this book and then I was surprised that I still understood so little of the kernels that started to appear as llm.c contributions, beating mine. It's a pretty good 101 intro. Learning CUDA is like that horse meme, all the learning resources you can find on the left, then... https://t.co/C0k1WZqkQM" / X</A>
								<DT><A HREF="https://siboehm.com/articles/22/CUDA-MMM">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index">cuda-mode/triton-index: Cataloging released Triton kernels.</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index/blob/main/kernel_overview.md">triton-index/kernel_overview.md</A>
								<DT><A HREF="https://github.com/mgmalek/efficient_cross_entropy/blob/main/modules.py#L67">efficient_cross_entropy/modules.py at main ¬∑ mgmalek/efficient_cross_entropy</A>
								<DT><A HREF="https://mp.weixin.qq.com/s/G_XvnB4CeEBWTLNefi0Riw">„ÄêBBufÁöÑCUDAÁ¨îËÆ∞„ÄëÂçÅ‰∫åÔºåLayerNorm/RMSNormÁöÑÈáçËÆ°ÁÆóÂÆûÁé∞</A>
								<DT><A HREF="http://giantpandacv.com/project/CUDA/%E3%80%90BBuf%E7%9A%84CUDA%E7%AC%94%E8%AE%B0%E3%80%91%E4%B8%80%EF%BC%8C%E8%A7%A3%E6%9E%90OneFlow%20Element-Wise%20%E7%AE%97%E5%AD%90%E5%AE%9E%E7%8E%B0/">„ÄêBBufÁöÑCUDAÁ¨îËÆ∞„Äë‰∏ÄÔºåËß£ÊûêOneFlow Element-Wise ÁÆóÂ≠êÂÆûÁé∞ - GiantPandaCV</A>
							</DL><p>
							<DT><H3 FOLDED>Marlin</H3>
							<DL><p>
								<DT><A HREF="https://github.com/IST-DASLab/marlin">IST-DASLab/marlin: FP16xINT4 LLM inference kernel that can achieve near-ideal ~4x speedups up to medium batchsizes of 16-32 tokens.</A>
								<DT><A HREF="https://github.com/flashinfer-ai/flashinfer">flashinfer-ai/flashinfer: FlashInfer: Kernel Library for LLM Serving</A>
								<DT><A HREF="https://github.com/IST-DASLab/Sparse-Marlin">IST-DASLab/Sparse-Marlin</A>
							</DL><p>
							<DT><H3 FOLDED>kernel fusion</H3>
							<DL><p>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM">NVIDIA/TensorRT-LLM: TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.</A>
								<DT><A HREF="https://nvidia.github.io/TensorRT-LLM/overview.html#about-tensorrt-llm">Overview ‚Äî tensorrt_llm documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=m6BSREnQ84U">Fusing Kernels - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>microxcaling</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/microxcaling">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024">deepspeed-fp6</A>
							</DL><p>
							<DT><H3 FOLDED>XQA-kernel</H3>
							<DL><p>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/XQA-kernel.md">TensorRT-LLM/docs/source/blogs/XQA-kernel.md</A>
							</DL><p>
							<DT><H3 FOLDED>split-k-gemm</H3>
							<DL><p>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/examples/06_splitK_gemm/splitk_gemm.cu">cutlass/examples/06_splitK_gemm/splitk_gemm.cu</A>
								<DT><A HREF="https://arxiv.org/abs/2402.00025">[2402.00025] Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK work decomposition</A>
							</DL><p>
							<DT><H3 FOLDED>Flash-Attention</H3>
							<DL><p>
								<DT><H3 FOLDED>flash-attention-issues</H3>
								<DL><p>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/253">ModuleNotFoundError: No module named 'torch' ¬∑ Issue #253 ¬∑ HazyResearch/flash-attention</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/246">No Module Named 'torch' ¬∑ Issue #246 ¬∑ HazyResearch/flash-attention</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/131">installing dropout_layer_norm ¬∑ Issue #131 ¬∑ HazyResearch/flash-attention</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/250">ModuleNotFoundError: No module named 'dropout_layer_norm' when trying to import flash_attn.ops.layer_norm ¬∑ Issue #250 ¬∑ HazyResearch/flash-attention</A>
								</DL><p>
								<DT><A HREF="https://github.com/HazyResearch/flash-attention">HazyResearch/flash-attention: Fast and memory-efficient exact attention</A>
								<DT><A HREF="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA) ‚Äî PyTorch Tutorials 2.1.0+cu121 documentation</A>
								<DT><A HREF="https://gist.github.com/Chillee/41baf11aac8036d25d637321c48dad20">You Could Have Invented Flash-Attention!</A>
								<DT><A HREF="https://github.com/tspeterkim/flash-attention-minimal">tspeterkim/flash-attention-minimal: Flash Attention in ~100 lines of CUDA (forward pass only)</A>
								<DT><A HREF="https://github.com/NVIDIA/online-softmax">NVIDIA/online-softmax: Benchmark code for the "Online normalizer calculation for softmax" paper</A>
								<DT><A HREF="https://github.com/lucidrains/flash-cosine-sim-attention">lucidrains/flash-cosine-sim-attention: Implementation of fused cosine similarity attention in the same style as Flash Attention</A>
								<DT><A HREF="https://github.com/jundaf2/INT8-Flash-Attention-FMHA-Quantization">jundaf2/INT8-Flash-Attention-FMHA-Quantization</A>
								<DT><A HREF="https://mp.weixin.qq.com/s/5K6yNj23NmNLcAQofHcT4Q">Flash Attention v2</A>
								<DT><A HREF="http://giantpandacv.com/project/CUDA/%E3%80%90BBuf%E7%9A%84CUDA%E7%AC%94%E8%AE%B0%E3%80%91%E5%8D%81%E4%BA%94%EF%BC%8COpenAI%20Triton%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%E4%B8%89%20FusedAttention/">FlashAttention V2 (chinese blog)</A>
							</DL><p>
							<DT><H3 FOLDED>FlashFFTConv</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/realDanFu/status/1724127071902011611">FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores</A>
							</DL><p>
							<DT><H3 FOLDED>kernels-quantization</H3>
							<DL><p>
								<DT><H3 FOLDED>gptq</H3>
								<DL><p>
									<DT><A HREF="https://github.com/IST-DASLab/gptq">IST-DASLab/gptq: Code for the ICLR 2023 paper "GPTQ: Accurate Post-training Quantization of Generative Pretrained Transformers".</A>
								</DL><p>
								<DT><H3 FOLDED>awq</H3>
								<DL><p>
									<DT><A HREF="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq: [MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</A>
								</DL><p>
								<DT><A HREF="https://github.com/hahnyuan/RPTQ4LLM">hahnyuan/RPTQ4LLM: Reorder-based post-training quantization for large language model</A>
								<DT><A HREF="https://github.com/IST-DASLab/QUIK">IST-DASLab/QUIK: Repository for the QUIK project, enabling the use of 4bit kernels for generative inference</A>
							</DL><p>
							<DT><H3 FOLDED>kernels-sparsity</H3>
							<DL><p>
								<DT><H3 FOLDED>dynamic sparse training</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/blog/accelerating-neural-network-training/?utm_content=297933946&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity | PyTorch</A>
									<DT><A HREF="https://arxiv.org/pdf/1903.05662">UNDERSTANDING STRAIGHT-THROUGH ESTIMATOR IN TRAINING ACTIVATION QUANTIZED NEURAL NETS</A>
									<DT><A HREF="https://arxiv.org/pdf/2310.06927">Sparse Fine-tuning for Inference Acceleration of Large Language Models</A>
								</DL><p>
								<DT><H3 FOLDED>V:N:M</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2310.02065">VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores</A>
								</DL><p>
								<DT><A HREF="https://pytorch.org/blog/accelerating-neural-network-training/?utm_content=297933946&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch/ao/tree/main/torchao/sparsity/training#benchmarking">ao/torchao/sparsity/training at main ¬∑ pytorch/ao</A>
								<DT><A HREF="https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/">Structured Sparsity in the NVIDIA Ampere Architecture and Applications in Search Engines | NVIDIA Technical Blog</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cusparselt/index.html">cuSPARSELt: A High-Performance CUDA Library for Sparse Matrix-Matrix Multiplication ‚Äî NVIDIA cuSPARSELt 0.6.1 documentation</A>
								<DT><A HREF="https://arxiv.org/pdf/2104.08378">Accelerating Sparse Deep Neural Networks</A>
								<DT><A HREF="https://pytorch.org/tutorials/advanced/semi_structured_sparse.html?highlight=beta">(beta) Accelerating BERT with semi-structured (2:4) sparsity ‚Äî PyTorch Tutorials 2.3.0+cu121 documentation</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/pull/122350">[sparse] Add fast semi-structured spasification kernels by jcaip ¬∑ Pull Request #122350 ¬∑ pytorch/pytorch</A>
								<DT><A HREF="https://github.com/pytorch/ao/commit/d97ae74f46bb92bc26d01b2b5aed11197c275dd9">training acceleration via runtime semi-structured sparsity (#184) ¬∑ pytorch/ao@d97ae74</A>
								<DT><A HREF="https://github.com/IST-DASLab/Sparse-Marlin">IST-DASLab/Sparse-Marlin</A>
							</DL><p>
							<DT><H3 FOLDED>Warp Divergence</H3>
							<DL><p>
								<DT><A HREF="https://www.reddit.com/r/CUDA/comments/gkpjxe/what_is_warp_divergence/#">What is Warp Divergence ? : r/CUDA</A>
								<DT><A HREF="https://pytorch.org/blog/accelerating-neural-network-training/?utm_content=297933946&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity | PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>arithmetic-intensity</H3>
							<DL><p>
								<DT><A HREF="https://zhuanlan.zhihu.com/p/638468472">LLMÔºàÂçÅ‰∏ÉÔºâÔºö‰ªé FlashAttention Âà∞ PagedAttention, Â¶Ç‰ΩïËøõ‰∏ÄÊ≠•‰ºòÂåñ Attention ÊÄßËÉΩ - Áü•‰πé</A>
							</DL><p>
							<DT><A HREF="https://github.com/cuda-mode/triton-index">cuda-mode/triton-index: Cataloging released Triton kernels.</A>
							<DT><A HREF="https://openai.com/research/block-sparse-gpu-kernels">OpenAI: Block-sparse GPU kernels</A>
							<DT><A HREF="https://github.com/IST-DASLab/marlin">IST-DASLab/marlin: FP16xINT4 LLM inference kernel that can achieve near-ideal ~4x speedups up to medium batchsizes of 16-32 tokens.</A>
							<DT><A HREF="https://hta.readthedocs.io/en/latest/source/features/kernel_breakdown.html">Kernel Breakdown ‚Äî Holistic Trace Analysis 0.2.0 documentation</A>
							<DT><A HREF="https://github.com/efeslab/Atom">efeslab/Atom: [MLSys'24] Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</A>
							<DT><A HREF="https://github.com/huggingface/candle-paged-attention">huggingface/candle-paged-attention</A>
							<DT><A HREF="https://github.com/microsoft/onnxscript">microsoft/onnxscript: ONNX Script enables developers to naturally author ONNX functions and models using a subset of Python.</A>
							<DT><A HREF="https://github.com/flashinfer-ai/flashinfer">flashinfer-ai/flashinfer: FlashInfer: Kernel Library for LLM Serving</A>
							<DT><A HREF="https://github.com/microsoft/onnxscript">microsoft/onnxscript: programm ONNX functions and models using a subset of Python.</A>
							<DT><A HREF="https://ai.meta.com/research/publications/accelerating-a-triton-fused-kernel-for-w4a16-quantized-inference-with-splitk-work-decomposition/">Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK Work Decomposition</A>
							<DT><A HREF="https://github.com/turboderp/exllama">turboderp/exllama: A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.</A>
							<DT><A HREF="https://github.com/turboderp/exllamav2">turboderp/exllamav2: LLMs locally on modern consumer-class GPUs</A>
							<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/de17730a993b1d2cce4fd09e3654b5f79fd23c96/kernels/triton/inference/gptq/a100_qlinear.py#L109">applied-ai: Triton GPTQ a100_qlinear.py (print perf stats)</A>
							<DT><A HREF="https://github.com/BearNinja123/channels-last-groupnorm">BearNinja123/channels-last-groupnorm: A CUDA kernel for NHWC GroupNorm for PyTorch</A>
							<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/66ef1df492f7bc9c8eeb01d7e14db01838e3f0bd/cpp/tensorrt_llm/kernels">TensorRT-LLM/cpp/tensorrt_llm/kernels</A>
							<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/66ef1df492f7bc9c8eeb01d7e14db01838e3f0bd/cpp/tensorrt_llm/kernels/cutlass_kernels">TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels</A>
							<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
							<DT><A HREF="https://pytorch.org/blog/accelerating-moe-model/#30-work-decomposition---splitk">Accelerating MoE model inference with Locality-Aware Kernel Design | PyTorch</A>
							<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/main/kernels/triton/inference/gptq/splitk_dequant_gemm.py">applied-ai/kernels/triton/inference/gptq/splitk_dequant_gemm.py at main ¬∑ pytorch-labs/applied-ai</A>
							<DT><A HREF="https://github.com/vedantroy/gpu_kernels/">vedantroy/gpu_kernels</A>
							<DT><A HREF="https://github.com/BBuf/how-to-optim-algorithm-in-cuda">BBuf/how-to-optim-algorithm-in-cuda: how to optimize some algorithm in cuda.</A>
							<DT><A HREF="https://github.com/mgmalek/efficient_cross_entropy">mgmalek/efficient_cross_entropy</A>
						</DL><p>
						<DT><H3 FOLDED>computational graph explorer</H3>
						<DL><p>
							<DT><A HREF="https://research.google/blog/model-explorer/">Model Explorer: Graph visualization for large model development</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/45e7400e3c86bc147cd1ab7b1b068068bdfe0cb2/docs-legacy/env_vars.md">tinygrad: env var GRAPH create a graph of all operations</A>
							<DT><A HREF="https://github.com/ezyang/torchdbg">ezyang/torchdbg: PyTorch centric eager mode debugger</A>
							<DT><A HREF="https://bbycroft.net/llm">LLM Visualization</A>
							<DT><A HREF="https://pytorch.org/blog/inside-the-matrix/">Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond | PyTorch</A>
							<DT><A HREF="https://ai.google.dev/edge/model-explorer#two_ways_to_use_model_explorer">Model Explorer ¬†|¬† Edge ¬†|¬† Google for Developers</A>
							<DT><A HREF="https://github.com/google-ai-edge/model-explorer">google-ai-edge/model-explorer: A modern model graph visualizer and debugger</A>
							<DT><A HREF="https://ieeexplore.ieee.org/document/8019861">Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow | IEEE Journals &amp; Magazine | IEEE Xplore</A>
							<DT><A HREF="https://idl.cs.washington.edu/files/2018-TensorFlowGraph-VAST.pdf">Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow</A>
						</DL><p>
						<DT><H3 FOLDED>PyTorch</H3>
						<DL><p>
							<DT><H3 FOLDED>torch-installation</H3>
							<DL><p>
								<DT><H3 FOLDED>pytorch-from-source</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/issues/77867">ccmake list</A>
									<DT><A HREF="https://gist.github.com/mhubii/1c1049fb5043b8be262259efac4b89d5">A guide to install and use the PyTorch C++ API with Anaconda.md</A>
								</DL><p>
								<DT><A HREF="https://chrisdare.medium.com/running-pytorch-on-apple-silicon-m1-gpus-a8bb6f680b02">Installing and running pytorch on M1 GPUs (Apple metal/MPS)</A>
								<DT><A HREF="https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/">Introducing Accelerated PyTorch Training on Mac | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/tree/main/.devcontainer">pytorch/.devcontainer at main ¬∑ pytorch/pytorch</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/89ba1b1a67d570e41b03da87e5518eaff0d31fbf/docker/common/install_pytorch.sh">TensorRT-LLM/docker/common/install_pytorch.sh at 89ba1b1a67d570e41b03da87e5518eaff0d31fbf ¬∑ NVIDIA/TensorRT-LLM</A>
							</DL><p>
							<DT><H3 FOLDED>pytorch-lightning</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Lightning-AI/pytorch-lightning">Lightning-AI/pytorch-lightning: Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes.</A>
								<DT><A HREF="https://github.com/Lightning-AI/litgpt">Lightning-AI/litgpt: Load, pretrain, finetune, deploy 20+ LLMs on your own data. Uses state-of-the-art techniques: flash attention, FSDP, 4-bit, LoRA, and more.</A>
								<DT><A HREF="https://github.com/Lightning-AI/LitServe">Lightning-AI/LitServe: Deploy AI models at scale. High-throughput serving engine for AI/ML models that uses the latest state-of-the-art model deployment techniques.</A>
							</DL><p>
							<DT><H3 FOLDED>torch-nightly</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/karpathy/status/1779354343013269929">"compound" F.sdpa (scaled dot product attention)</A>
								<DT><A HREF="https://github.com/Chillee/llm.c?tab=readme-ov-file#some-benchmark-numbers-with-newer-version-of-pytorch">PyTorch nightly + F.sdpa + coordinate descent tuning vs llm.c</A>
							</DL><p>
							<DT><H3 FOLDED>torch-docs</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py">Tensors ‚Äî PyTorch Tutorials 1.11.0+cu102 documentation</A>
								<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss ‚Äî PyTorch 1.11.0 documentation</A>
								<DT><A HREF="https://pytorch.org/docs/stable/optim.html">torch.optim ‚Äî PyTorch 1.11.0 documentation</A>
								<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.Tensor.to.html">torch.Tensor.to ‚Äî PyTorch 1.12 documentation</A>
								<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>torch-release-notes</H3>
							<DL><p>
								<DT><H3 FOLDED>PyTorch 2.0</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>PyTorch 2.1</H3>
								<DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/pytorch-2-1-automatic-dynamic-shape-compilation-torch-distributed-checkpoint-torch-compile-numpy-torch-export-prototype-and-more/1548">PyTorch 2.1: automatic dynamic shape compilation, torch.distributed.checkpoint, torch.compile + NumPy, torch.export prototype, and more! - Release Announcements - PyTorch Dev Discussions</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/releases/tag/v2.1.0">Release PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing ¬∑ pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>PyTorch 2.2</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/releases/tag/v2.2.0">PyTorch 2.2: FlashAttention-v2, AOTInductor</A>
								</DL><p>
								<DT><H3 FOLDED>PyTorch 2.3</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/blog/pytorch2-3/">PyTorch 2.3 Release Blog | PyTorch</A>
									<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile ‚Äî PyTorch Tutorials 2.3.0+cu121 documentation</A>
									<DT><A HREF="https://cloud.google.com/blog/products/ai-machine-learning/introducing-pytorch-xla-2-3">Introducing PyTorch/XLA 2.3 | Google Cloud Blog</A>
								</DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/pytorch-2-1-automatic-dynamic-shape-compilation-torch-distributed-checkpoint-torch-compile-numpy-torch-export-prototype-and-more/1548">PyTorch 2.1: automatic dynamic shape compilation, torch.distributed.checkpoint, torch.compile + NumPy, torch.export prototype, and more! - Release Announcements - PyTorch Dev Discussions</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/releases/tag/v2.1.0">Release PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing ¬∑ pytorch/pytorch</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/releases/tag/v2.2.0">PyTorch 2.2: FlashAttention-v2, AOTInductor</A>
							</DL><p>
							<DT><H3 FOLDED>torch-dev</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-composability</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/document/d/1QTR3t3KdRu5JT1lvAuJLsPd3LruCfv0LecpsgR8eWhg/edit">Composability meeting notes - Google Docs</A>
								</DL><p>
								<DT><A HREF="https://github.com/albanD/pytorch_dev_env_setup">albanD/pytorch_dev_env_setup</A>
								<DT><A HREF="https://direnv.net/">direnv ‚Äì unclutter your .profile | direnv</A>
								<DT><A HREF="http://giantpandacv.com/project/PyTorch/%E3%80%8APytorchConference2023%20%E7%BF%BB%E8%AF%91%E7%B3%BB%E5%88%97%E3%80%8B2-PyTorch%E5%BC%80%E5%8F%91%E8%80%85%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/">PyTorch 2.0 SW infra</A>
								<DT><A HREF="https://docs.google.com/document/d/1QTR3t3KdRu5JT1lvAuJLsPd3LruCfv0LecpsgR8eWhg/edit">Composability meeting notes - Google Docs</A>
								<DT><A HREF="https://github.com/pytorch/builder">pytorch/builder: Continuous builder and binary build scripts for pytorch</A>
							</DL><p>
							<DT><H3 FOLDED>torch-internals</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-compiler</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-compiler-debug</H3>
									<DL><p>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/torch-compile-can-be-debugged-now/1595">Torch.compile can be debugged now! - compiler - PyTorch Dev Discussions</A>
										<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">Lightning Talk: PT2 Export - A Sound Full Graph Capture Mechanism for PyTorch - Avik Chaudhuri, Meta - YouTube</A>
										<DT><A HREF="https://github.com/youkaichao/hello_frame_eval">youkaichao/hello_frame_eval: A hello-world usage for frame eval api.</A>
										<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst">pytorch/docs/source/torch.compiler_troubleshooting.rst at main ¬∑ pytorch/pytorch</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/torch-compile-can-be-debugged-now/1595">Torch.compile can be debugged now</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst">pytorch/docs/source/torch.compiler_troubleshooting.rst</A>
									</DL><p>
									<DT><H3 FOLDED>Inductor</H3>
									<DL><p>
										<DT><H3 FOLDED>TORCHINDUCTOR-env-vars</H3>
										<DL><p>
											<DT><A HREF="https://twitter.com/cHHillee/status/1777825367954432114/photo/1">TORCH_LOGS="output_code" python t.py</A>
											<DT><A HREF="https://twitter.com/karpathy/status/1779354343013269929">TORCHINDUCTOR_COORDINATE_DESCENT_TUNING=1</A>
										</DL><p>
										<DT><H3 FOLDED>inductor-codegen</H3>
										<DL><p>
											<DT><A HREF="https://twitter.com/cHHillee/status/1777825367954432114/photo/1">TORCH_LOGS="output_code" python t.py</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/wiki/Codegen-and-Structured-Kernels">Codegen and Structured Kernels ¬∑ pytorch/pytorch Wiki</A>
											<DT><A HREF="https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation</A>
											<DT><A HREF="https://twitter.com/marksaroufim/status/1746307482904076374">torch.utils.cpp_extension import load_inline</A>
											<DT><A HREF="https://twitter.com/johnowhitaker/status/1746275479806742664/photo/1">IR Triton code</A>
											<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch</A>
											<DT><A HREF="https://twitter.com/cHHillee/status/1777825367954432114">Horace He: see what kernels are being executed under torch.compile</A>
											<DT><A HREF="https://github.com/Chillee/llm.c/blob/master/inductor_gpt2.cpp">torch compile can also generate and emit C++ code (llm.c)</A>
											<DT><A HREF="https://pytorch.org/tutorials/prototype/inductor_cpp_wrapper_tutorial.html">Inductor C++ Wrapper Tutorial</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/fea1b99d89204989db64d0d63f5e46fce60d1962/torch/_inductor/config.py#L34">TORCHINDUCTOR_CPP_WRAPPER</A>
										</DL><p>
										<DT><H3 FOLDED>inductor-mode</H3>
										<DL><p>
											<DT><A HREF="https://pytorch.org/get-started/pytorch-2.0/">max-autotune</A>
											<DT><A HREF="https://twitter.com/jxmnop/status/1778835034079691008">max-autotune description</A>
										</DL><p>
										<DT><H3 FOLDED>inductor-caching</H3>
										<DL><p>
											<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-bring-compile-time-down-to-zero-our-plans-and-direction-may-14th-edition/2089">How To Bring Compile Time Down to Zero: Our Plans and Direction (May 14th Edition) - compiler - PyTorch Developer Mailing List</A>
										</DL><p>
										<DT><A HREF="https://github.com/thuml/depyf">thuml/depyf: understand and adapt to PyTorch compiler</A>
										<DT><A HREF="https://pytorch.org/docs/main/torch.compiler_aot_inductor.html">AHEAD-OF-TIME COMPILATION FOR TORCH.EXPORT</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/the-future-of-c-model-deployment/1282">The future of C++ model deployment</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codegen/wrapper.py">codegen/wrapper.py</A>
										<DT><A HREF="https://www.youtube.com/watch?v=w7d4oWzwZ0c">AOTInductor: Ahead-of-Time Compilation for PT2 Exported Models</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747">TorchInductor: a PyTorch-native Compiler with Define-by-Run IR and Symbolic Shapes</A>
										<DT><A HREF="https://twitter.com/mathemakitten/status/1744861826528211323">inductor + triton = kernels &gt; handcoded FlashAttention 2</A>
										<DT><A HREF="https://www.youtube.com/watch?v=682pQYiS4cQ">Fixing an PyTorch Inductor bug: Views, Buffers, Realize - YouTube</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/pull/111434">[Inductor] Support user defined triton kernels in inductor by oulgen ¬∑ Pull Request #111434 ¬∑ pytorch/pytorch</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/inductor-file-structure-explanation/1860/2">Inductor file structure explanation - PyTorch Dev Discussions</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/pull/107802">[Inductor CUTLASS backend] Step 1: Inductor config for cuda / cutlass, util functions. by ipiszy ¬∑ Pull Request #107802 ¬∑ pytorch/pytorch</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/e91c37c1c39734359637e3fd999ba20e26b6bfa7/torch/_inductor/config.py">pytorch/torch/_inductor/config.py</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/pull/111434">[Inductor] Support user defined triton kernels in inductor</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/inductor-file-structure-explanation/1860/2">Inductor file structure explanation</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/pull/107802">[Inductor CUTLASS backend] Step 1: Inductor config for cuda / cutlass, util functions</A>
										<DT><A HREF="https://github.com/ngimel/inductor_generated">ngimel/inductor_generated</A>
										<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common deep learning models</A>
										<DT><A HREF="https://twitter.com/cHHillee/status/1777825367954432114/photo/1">TORCH_LOGS="output_code" python t.py</A>
										<DT><A HREF="https://gist.github.com/Chillee/f86675147366a7a0c6e244eaa78660f7">1-pw_op_fusion.py</A>
										<DT><A HREF="https://pytorch.org/blog/introducing-depyf/">Introducing depyf: mastering torch.compile with ease | PyTorch</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/when-does-the-inductor-code-run/2088/3">When does the inductor code run? - compiler - PyTorch Developer Mailing List</A>
									</DL><p>
									<DT><H3 FOLDED>Dynamo</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=egZB5Uxki0I">torchdynamo deep dive - YouTube</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/torchdynamo-update-4-lazytensor-nvfuser-experiments/496">TorchDynamo Update 4: LazyTensor &amp; nvFuser Experiments - compiler - PyTorch Developer Mailing List</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/torchdynamo-update-7-inference-with-fx2trt/576">TorchDynamo Update 7: Inference with FX2TRT - compiler - PyTorch Developer Mailing List</A>
										<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">Lightning Talk: PT2 Export - A Sound Full Graph Capture Mechanism for PyTorch - Avik Chaudhuri, Meta - YouTube</A>
										<DT><A HREF="https://github.com/Skylion007/pytorch/blob/ad8448497290394b50f84a2932fcb4f078793b3f/test/dynamo/test_functions.py#L1603">pytorch/test/dynamo/test_functions.py</A>
										<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common deep learning models</A>
										<DT><A HREF="https://github.com/thuml/depyf">thuml/depyf: understand and adapt to PyTorch compiler</A>
										<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">Export - A Sound Full Graph Capture Mechanism for PyTorch</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/a-torchdynamo-trace-time-ablation-study/1961">A TorchDynamo trace time ablation study - compiler - PyTorch Developer Mailing List</A>
									</DL><p>
									<DT><H3 FOLDED>torch-compiler-benchmark</H3>
									<DL><p>
										<DT><A HREF="https://hud.pytorch.org/benchmark/compilers">compilers</A>
									</DL><p>
									<DT><H3 FOLDED>pytorch-internals-symbolic-shapes</H3>
									<DL><p>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/state-of-symbolic-shapes-branch/777">State of Symbolic Shapes</A>
										<DT><A HREF="https://www.youtube.com/watch?v=R-AVYgBIZRY">Dynamic Shapes</A>
										<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes ‚Äî Good Coding Style | by Noam Shazeer | Feb, 2024 | Medium</A>
										<DT><A HREF="https://github.com/ezyang/data-dependent-shape-puzzles">ezyang/data-dependent-shape-puzzles: Puzzlers regarding data-dependent shapes in PT2</A>
										<DT><A HREF="https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit#heading=h.44gwi83jepaj">Dealing with GuardOnDataDependentSymNode errors - Google Docs</A>
										<DT><A HREF="https://colab.research.google.com/github/ezyang/data-dependent-shape-puzzles/blob/main/data-dependent-shape-puzzles.ipynb#scrollTo=65eb2hyriPM0">data-dependent-shape-puzzles.ipynb - Colab</A>
									</DL><p>
									<DT><H3 FOLDED>torch-compiler-caching</H3>
									<DL><p>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-bring-compile-time-down-to-zero-our-plans-and-direction-may-14th-edition/2089">How To Bring Compile Time Down to Zero: Our Plans and Direction (May 14th Edition) - compiler - PyTorch Developer Mailing List</A>
										<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html">Compile Time Caching in torch.compile ‚Äî PyTorch Tutorials 2.3.0+cu121 documentation</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/remote-compilation-caching-system-testing/2179">Remote compilation caching system testing - PyTorch Developer Mailing List</A>
									</DL><p>
									<DT><H3 FOLDED>fx-graph</H3>
									<DL><p>
										<DT><A HREF="http://giantpandacv.com/project/PyTorch/%E7%94%A8%E6%B2%90%E7%A5%9E%E7%9A%84%E6%96%B9%E6%B3%95%E9%98%85%E8%AF%BBPyTorch%20FX%E8%AE%BA%E6%96%87/">Reading PyTorch FX Papers Using Mushen‚Äôs Method</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/different-points-at-which-fusion-occurs/2099">Different points at which fusion occurs? - compiler / FX - PyTorch Developer Mailing List</A>
									</DL><p>
									<DT><H3 FOLDED>torch-compiler-fusion</H3>
									<DL><p>
										<DT><A HREF="https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">Operator Fusion &amp; Compilation</A>
									</DL><p>
									<DT><H3 FOLDED>torch-compiler-examples</H3>
									<DL><p>
										<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common DNN</A>
									</DL><p>
									<DT><H3 FOLDED>torch-compiler-export</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">PT2 Export - A Sound Full Graph Capture Mechanism</A>
										<DT><A HREF="https://www.youtube.com/watch?v=krwpwbH7NJc">PyTorch's Computational Graph</A>
										<DT><A HREF="https://github.com/microsoft/Olive">microsoft/Olive: Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation.</A>
									</DL><p>
									<DT><H3 FOLDED>torch-compiler-depyf</H3>
									<DL><p>
										<DT><A HREF="https://github.com/thuml/depyf">thuml/depyf: depyf is a tool to help you understand and adapt to PyTorch compiler torch.compile.</A>
										<DT><A HREF="https://pytorch.org/blog/introducing-depyf/">Introducing depyf: mastering torch.compile with ease | PyTorch</A>
										<DT><A HREF="https://github.com/thuml/depyf/blob/master/docs/walk_through.rst">depyf/docs/walk_through.rst at master ¬∑ thuml/depyf</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/interactively-explore-what-torch-compile-does-to-your-code/1576">Interactively explore what torch.compile does to your code! - compiler</A>
									</DL><p>
									<DT><H3 FOLDED>torch-compiler-kernels</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile ‚Äî PyTorch Tutorials 2.3.0+cu121 documentation</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/user-defined-kernels-vs-torch-library-custom-op/2113">User-defined Kernels vs. `torch.library` custom op - compiler - PyTorch Developer Mailing List</A>
									</DL><p>
									<DT><A HREF="https://github.com/thuml/depyf/blob/master/docs/walk_through.rst">depyf/docs/walk_through.rst at master ¬∑ thuml/depyf</A>
									<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common deep learning models, can be used as a learning resource for torch.compile</A>
									<DT><A HREF="https://www.youtube.com/watch?v=kdZwgRghy3M">XLA Internal</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965">Functionalization in PyTorch: Everything You Wanted To Know - compiler - PyTorch Dev Discussions</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467">Min-cut optimal(*) recomputation (i.e. activation checkpointing) with AOTAutograd - compiler - PyTorch Dev Discussions</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/interactively-explore-what-torch-compile-does-to-your-code/1576">Interactively explore what torch.compile does to your code! - compiler</A>
									<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler">torch.compiler ‚Äî PyTorch 2.3 documentation</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747">TorchInductor: a PyTorch-native Compiler with Define-by-Run IR and Symbolic Shapes</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/i-build-a-decompiler-to-convert-bytecode-generated-by-dynamo-into-readable-source-code/1471/4">I build a decompiler to convert bytecode generated by dynamo into readable source code</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-capture-nccl-communication-ops-in-faketensormode/1410">How to capture NCCL communication ops in FakeTensorMode</A>
									<DT><A HREF="https://pytorch.org/docs/stable/fx.html">torch.fx ‚Äî PyTorch 2.1 documentation</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/torch-compile-tech-talks-at-ptc23/1625">Torch.compile() tech talks at PTC'23</A>
									<DT><A HREF="https://www.youtube.com/watch?v=krwpwbH7NJc">PyTorch's Computational Graph</A>
									<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">PT2 Export - A Sound Full Graph Capture Mechanism</A>
									<DT><A HREF="https://peps.python.org/pep-0523/">PEP 523 ‚Äì Adding a frame evaluation API to CPython | peps.python.org</A>
									<DT><A HREF="https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha/blob/main/app.py">HF Transformer simple example</A>
									<DT><A HREF="https://github.com/thuml/depyf">thuml/depyf: understand and adapt to PyTorch compiler</A>
									<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common deep learning models</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965">Functionalization in PyTorch: Everything You Wanted To Know</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467">Min-cut optimal(*) recomputation (i.e. activation checkpointing) with AOTAutograd - compiler</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/interactively-explore-what-torch-compile-does-to-your-code/1576">Interactively explore what torch.compile does to your code</A>
									<DT><A HREF="https://peps.python.org/pep-0523/">PEP 523 ‚Äì Adding a frame evaluation API to CPython</A>
									<DT><A HREF="https://www.youtube.com/watch?v=mEYzE1iIEDI">Composability sync - Hierarchical compilation, torchrec PT2, NF4, accuracy - YouTube</A>
									<DT><A HREF="https://en.cppreference.com/w/c/numeric/math/fma">fma, fmaf, fmal - cppreference.com</A>
									<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile ‚Äî PyTorch Tutorials 2.3.0+cu121 documentation</A>
									<DT><A HREF="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">Introduction to torch.compile ‚Äî PyTorch Tutorials 2.3.0+cu121 documentation</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-bring-compile-time-down-to-zero-our-plans-and-direction-may-14th-edition/2089">How To Bring Compile Time Down to Zero: Our Plans and Direction (May 14th Edition) - compiler - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/how-can-i-dump-the-prims-ir-triton-code-and-ptx-code-when-using-torch-compile/2057">How can I dump the prims IR, triton code, and ptx code when using torch.compile() - compiler - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://github.com/Lightning-AI/lightning-thunder">Lightning-AI/lightning-thunder: Make PyTorch models up to 40% faster! Thunder is a source to source compiler for PyTorch. It enables using different hardware executors at once; across one or thousands of GPUs.</A>
								</DL><p>
								<DT><H3 FOLDED>torch-device</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/pull/4508">add available memory check to accelerators by jeffra ¬∑ Pull Request #4508 ¬∑ microsoft/DeepSpeed</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/pull/4508">add available memory check to accelerators</A>
								</DL><p>
								<DT><H3 FOLDED>torch-internals-tensor</H3>
								<DL><p>
									<DT><A HREF="http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/">PyTorch dispatcher</A>
									<DT><A HREF="https://ezyang.github.io/stride-visualizer/index.html">STRIDE Visualizer</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/c10/core/TensorImpl.h">TensorImpl.h at main</A>
									<DT><A HREF="http://blog.ezyang.com/2020/05/a-brief-taxonomy-of-pytorch-operators-by-shape-behavior/">A brief taxonomy of PyTorch operators by shape behavior : ezyang‚Äôs blog</A>
									<DT><A HREF="https://theaisummer.com/einsum-attention/">Einsum: Transformer example</A>
									<DT><A HREF="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch internals : ezyang‚Äôs blog</A>
								</DL><p>
								<DT><H3 FOLDED>torch-backends</H3>
								<DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/weight-sharing-on-cuda/701">Weight sharing on cuda</A>
									<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile</A>
								</DL><p>
								<DT><H3 FOLDED>torch-hooks</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=syLFCVYua6Q">PyTorch Hooks Explained</A>
								</DL><p>
								<DT><H3 FOLDED>ATen</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-AutoGrad</H3>
									<DL><p>
										<DT><A HREF="https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation">Automatic differentiation - Wikipedia</A>
										<DT><A HREF="https://www.youtube.com/watch?v=dEnUP6_kpeo">04 PyTorch tutorial - How do computational graphs and autograd in PyTorch work - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=MswxJw-8PvE">PyTorch Autograd Explained - In-depth Tutorial - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=ne99laPUxN4">The Simple Essence of Automatic Differentiation - Conal Elliott - YouTube</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-read-the-autograd-codebase/383">How to read the autograd codebase - frontend API - PyTorch Dev Discussions</A>
										<DT><A HREF="https://x.com/i/bookmarks?post_id=1803963383018066272">94 lines of code are everything that is needed</A>
									</DL><p>
									<DT><H3 FOLDED>torch-tensor</H3>
									<DL><p>
										<DT><H3 FOLDED>torch-dtypes</H3>
										<DL><p>
											<DT><H3 FOLDED>torch-fp8</H3>
											<DL><p>
												<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
												<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization ‚Äî PyTorch 2.0 documentation</A>
												<DT><A HREF="https://github.com/pytorch/pytorch/pull/109168">Basic fp8 support in Inductor</A>
											</DL><p>
											<DT><H3 FOLDED>torch-bfloat16</H3>
											<DL><p>
												<DT><A HREF="https://dev-discuss.pytorch.org/t/summation-bfloat16/390">Summation bfloat16 - documentation - PyTorch Developer Mailing List</A>
												<DT><A HREF="https://www.youtube.com/watch?v=B1eFGn5nN84">Are Numerical Linear Algebra Algorithms Accurate at Extreme Scale and at Low Precisions?</A>
											</DL><p>
											<DT><A HREF="https://pytorch.org/docs/stable/tensors.html">torch.Tensor ‚Äî PyTorch 2.2 documentation</A>
											<DT><A HREF="https://dev-discuss.pytorch.org/t/summation-bfloat16/390/2">Summation bfloat16 - documentation - PyTorch Developer Mailing List</A>
										</DL><p>
										<DT><A HREF="http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/">PyTorch dispatcher</A>
										<DT><A HREF="https://ezyang.github.io/stride-visualizer/index.html">STRIDE Visualizer</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/c10/core/TensorImpl.h">TensorImpl.h at main</A>
										<DT><A HREF="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch internals : ezyang‚Äôs blog</A>
										<DT><A HREF="http://blog.ezyang.com/2020/05/a-brief-taxonomy-of-pytorch-operators-by-shape-behavior/">A brief taxonomy of PyTorch operators by shape behavior : ezyang‚Äôs blog</A>
										<DT><A HREF="https://theaisummer.com/einsum-attention/">Einsum: Transformer example</A>
									</DL><p>
									<DT><H3 FOLDED>custom OPS</H3>
									<DL><p>
										<DT><A HREF="https://github.com/drisspg/driss_torch">drisspg/driss_torch: Cuda extensions for PyTorch</A>
									</DL><p>
									<DT><A HREF="https://x.com/karpathy/status/1803963383018066272">(1) Andrej Karpathy en X: "These 94 lines of code are everything that is needed to train a neural network. Everything else is just efficiency. This is my earlier project Micrograd. It implements a scalar-valued auto-grad engine. You start with some numbers at the leafs (usually the input data and the https://t.co/2zVJP3cNJ0" / X</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-read-the-autograd-codebase/383">How to read the autograd codebase - frontend API - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC?usp=sharing">Simple Grad - Colab</A>
									<DT><A HREF="https://pytorch.org/docs/main/torch.compiler_ir.html#prims-ir">IRs ‚Äî PyTorch main documentation</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/tree/main/aten/src#the-c-interface">low-level tensor libraries for PyTorch, as well as the new ATen C++ bindings</A>
								</DL><p>
								<DT><H3 FOLDED>torch-nn</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/d04957c0c682d766987cad07dce20986ca4a5b78/torch/_refs/nn/functional/__init__.py#L304">pytorch/torch/_refs/nn/functional/__init__.py OPS</A>
								</DL><p>
								<DT><H3 FOLDED>torch-internals-logs</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ezyang/tlparse">ezyang/tlparse: TORCH_LOG parser for PT2</A>
									<DT><A HREF="https://twitter.com/johnowhitaker/status/1746275479806742664/photo/1">TORCH_LOGS="output_code" python compile_square.py</A>
									<DT><A HREF="https://github.com/ezyang/tlparse">ezyang/tlparse: TORCH_LOGS parser for PT2</A>
								</DL><p>
								<DT><H3 FOLDED>torch-profiling</H3>
								<DL><p>
									<DT><H3 FOLDED>Host-Device Synchronization</H3>
									<DL><p>
										<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/15">Proper benchmarkign with CUDA synchronization NVIDIA/TransformerEngine</A>
										<DT><A HREF="https://discuss.pytorch.org/t/torch-gets-slower-when-upgrading-the-version/186525/4">torch.cuda.symnchronize() before st and stop host timers (et)</A>
										<DT><A HREF="https://discuss.pytorch.org/t/torch-gets-slower-when-upgrading-the-version/186525">Torch gets slower when upgrading the version (good stats)</A>
										<DT><A HREF="https://github.com/Chillee/llm.c/blob/3f232c12f688233ae7949add457fd50192bba867/train_gpt2.py#L394C3-L394C4">llm.c: train_gpt2.py#L394C3-L394C4</A>
									</DL><p>
									<DT><H3 FOLDED>torch-profilling-cuda-events</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#">Event ‚Äî PyTorch 2.2 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>Warm-Up Steps</H3>
									<DL><p>
										<DT><A HREF="https://github.com/horseee/DeepCache/blob/97813f6406ab71e236bfeb2f8a0e58c6a25b7397/stable_diffusion.py#L29">DeepCache: stable_diffusion.py#L29 (benchmark)</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/78f87d5a0c7d82911a639c397577284868a53c42/server/text_generation_server/models/flash_causal_lm.py#L690">flash_causal_lm.py#L690</A>
									</DL><p>
									<DT><H3 FOLDED>Fixed Clocks</H3>
									<DL><p>
										<DT><A HREF="https://blog.speechmatics.com/cuda-timings">How to Accurately Time CUDA Kernels in Pytorch</A>
										<DT><A HREF="https://github.com/google-deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/run_gpu_benchmark.py#L60">alphatensor/benchmarking/run_gpu_benchmark.py --lock-gpu-clocks</A>
										<DT><A HREF="https://github.com/openai/triton/blob/8e0c7b425ac149c43183de966ffa423fd46e4762/python/triton/testing.py#L441">Triton testing.py#L441 set_gpu_clock</A>
										<DT><A HREF="https://github.com/openai/triton/blob/8e0c7b425ac149c43183de966ffa423fd46e4762/python/test/regression/test_performance.py#L25">Triton test_performance.py#L25</A>
										<DT><A HREF="https://twitter.com/mike64_t/status/1763239211254030365">RTX 4090 Core and Mem overclock</A>
										<DT><A HREF="https://www.thonking.ai/p/strangely-matrix-multiplications">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</A>
									</DL><p>
									<DT><H3 FOLDED>Cache Flush</H3>
									<DL><p>
										<DT><A HREF="https://github.com/openai/triton/blob/8e0c7b425ac149c43183de966ffa423fd46e4762/python/triton/testing.py#L141">A100 L2 cache 40 MB (Triton testing.py#L141)</A>
									</DL><p>
									<DT><H3 FOLDED>torch-profilling-cuda-graphs</H3>
									<DL><p>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/78f87d5a0c7d82911a639c397577284868a53c42/server/text_generation_server/models/flash_causal_lm.py#L690">flash_causal_lm.py#L690</A>
									</DL><p>
									<DT><A HREF="https://blog.speechmatics.com/cuda-timings">How to Accurately Time CUDA Kernels in Pytorch</A>
									<DT><A HREF="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles</A>
									<DT><A HREF="https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics">CUDA semantics ‚Äî PyTorch 2.0 documentation</A>
									<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch - YouTube</A>
									<DT><A HREF="https://github.com/quentinf00/article-memory-log?tab=readme-ov-file">quentinf00/article-memory-log</A>
									<DT><A HREF="https://docs.google.com/presentation/d/110dnMW94LX1ySWxu9La17AVUxjgSaQDLOotFC3BZZD4/edit#slide=id.p">Lecture 1 How to profile CUDA kernels in PyTorch</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#codebase-structure">Lecture 1 How to profile CUDA kernels in PyTorch</A>
									<DT><A HREF="https://github.com/pytorch/kineto">pytorch/kineto: A CPU+GPU Profiling library that provides access to timeline traces and hardware performance counters.</A>
									<DT><A HREF="https://www.speechmatics.com/company/articles-and-news/timing-operations-in-pytorch">How to Accurately Time CUDA Kernels in Pytorch</A>
									<DT><A HREF="https://github.com/cloneofsimo/reverse_eng_deepspeed_study/blob/main/day3/run.py">reverse_eng_deepspeed_study/day3/run.py at main</A>
									<DT><A HREF="https://github.com/pytorch/kineto?tab=readme-ov-file">pytorch/kineto: A CPU+GPU Profiling library that provides access to timeline traces and hardware performance counters.</A>
									<DT><A HREF="https://discuss.pytorch.org/t/torch-gets-slower-when-upgrading-the-version/186525/4">torch.cuda.symnchronize() before st and stop host timers (et)</A>
									<DT><A HREF="https://discuss.pytorch.org/t/torch-gets-slower-when-upgrading-the-version/186525">Torch gets slower when upgrading the version (good stats)</A>
									<DT><A HREF="https://twitter.com/neurosp1ke/status/1784239369949159740">CUDA-MODE 16: Profiling</A>
									<DT><A HREF="https://www.youtube.com/watch?v=SKV6kDk1s94">Lecture 16: On Hands Profiling - YouTube</A>
									<DT><A HREF="https://gist.github.com/Chillee/41baf11aac8036d25d637321c48dad20">You Could Have Invented Flash-Attention!</A>
									<DT><A HREF="https://gist.github.com/Chillee/07b36672a0ca2d1280e42b8d10f23174">Compute Flop Utilization in PyTorch</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
								</DL><p>
								<DT><H3 FOLDED>torch-amp</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/docs/stable/amp.html">Automatic Mixed Precision package - torch.amp ‚Äî PyTorch 2.3 documentation</A>
									<DT><A HREF="https://tspeterkim.github.io/posts/mixed-precision-from-scratch">Mixed Precision Training from Scratch | Taeksang Peter Kim</A>
									<DT><A HREF="https://github.com/tspeterkim/mixed-precision-from-scratch">tspeterkim/mixed-precision-from-scratch: Mixed precision training from scratch with Tensors and CUDA</A>
								</DL><p>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml">native_functions.yaml</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/wiki/Codegen-and-Structured-Kernels">Codegen and Structured Kernels ¬∑ pytorch/pytorch Wiki</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#codebase-structure">Onboarding to PyTorch Internals (WIKI): Codebase structure</A>
								<DT><A HREF="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch internals : ezyang‚Äôs blog</A>
								<DT><A HREF="http://blog.ezyang.com/2020/01/vmap-in-haskell/">vmap in Haskell : ezyang‚Äôs blog</A>
								<DT><A HREF="https://docs.google.com/spreadsheets/d/e/2PACX-1vQQFW0T_bucT5KZn0BHYTC1KYhkL6ZMG5ZxQWc6UmAkHUDYpqkpzXnsb59uv2TB0Jgc1Q6qO63bx6WQ/pubhtml">Copy of The PyTorch Operator Spreadsheet - Google Drive</A>
								<DT><A HREF="https://docs.google.com/document/d/1tlgPcR2YmC3PcQuYDPUORFmEaBPQEmo8dsh4eUjnlyI/edit#heading=h.unax8xdp403v">PT2 Manifesto - Google Docs</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-find-the-c-cuda-implementation-of-specific-operators-in-pytorch-source-code/1551/3">How to find the c++/cuda implementation of specific operators in pytorch source code - PyTorch Dev Discussions</A>
								<DT><A HREF="https://github.com/ToluClassics/candle-tutorial">Convert PyTorch Models to Candle</A>
								<DT><A HREF="https://github.com/albanD/pytorch_dev_env_setup">albanD/pytorch_dev_env_setup</A>
								<DT><A HREF="https://github.com/albanD/pytorchviz">albanD/pytorchviz: A small package to create visualizations of PyTorch execution graphs</A>
								<DT><A HREF="https://pytorch-dev-podcast.simplecast.com/episodes">Episodes | PyTorch Developer Podcast</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md">pytorch/torch/csrc/jit/docs/serialization.md at main ¬∑ pytorch/pytorch</A>
								<DT><A HREF="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles | PyTorch</A>
								<DT><A HREF="https://www.youtube.com/watch?v=asWINANITgg">PyTorch composability sync: Full step capture, sub-byte dtypes - YouTube</A>
								<DT><A HREF="https://drive.google.com/file/d/1XBox0G3FI-71efQQjmqGh0-VkCd-AHPL/view">pytorch2_internals.pdf - Google Drive</A>
								<DT><A HREF="https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html">A guide to PyTorch‚Äôs CUDA Caching Allocator | Zach‚Äôs Blog</A>
								<DT><A HREF="https://www.youtube.com/watch?v=kSOmyARCbyM">PyTorch composability sync</A>
								<DT><A HREF="https://pytorch.org/blog/pytorch-2-paper-tutorial/?utm_content=282093849&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">PyTorch 2 paper and tutorial @ ASPLOS 2024 | PyTorch</A>
								<DT><A HREF="https://pytorch.org/assets/pytorch_2.pdf">PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation</A>
								<DT><A HREF="https://drive.google.com/file/d/1XBox0G3FI-71efQQjmqGh0-VkCd-AHPL/view">pytorch2_internals.pdf</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-find-the-c-cuda-implementation-of-specific-operators-in-pytorch-source-code/1551/3">How to find the c++/cuda implementation of specific operators in pytorch source code</A>
								<DT><A HREF="https://www.youtube.com/watch?v=asWINANITgg">PyTorch composability sync: Full step capture, sub-byte dtypes</A>
								<DT><A HREF="https://pytorch.org/blog/accelerating-generative-ai/">Accelerating Generative AI with PyTorch: Segment Anything, Fast</A>
								<DT><A HREF="https://docs.google.com/document/d/1QTR3t3KdRu5JT1lvAuJLsPd3LruCfv0LecpsgR8eWhg/edit">Composability meeting notes - Google Docs</A>
								<DT><A HREF="https://github.com/lernapparat/torchhacks/blob/main/test/test_lazyload.py">torchhacks/test/test_lazyload.py</A>
							</DL><p>
							<DT><H3 FOLDED>torch-research</H3>
							<DL><p>
								<DT><H3 FOLDED>PyTorch Labs</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-fp8</H3>
									<DL><p>
										<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
										<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization ‚Äî PyTorch 2.0 documentation</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/pull/109168">Basic fp8 support in Inductor by ipiszy ¬∑ Pull Request #109168 ¬∑ pytorch/pytorch</A>
										<DT><A HREF="https://twitter.com/MSFTDeepSpeed/status/1765923648773525795">DeepSpeed-FP6</A>
										<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024">DeepSpeed/blogs/deepspeed-fp6/03-05-2024 at master ¬∑ microsoft/DeepSpeed</A>
										<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
									</DL><p>
									<DT><H3 FOLDED>torch-ao</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch-labs/ao/issues/47">[RFC] Plans for torchao ¬∑ Issue #47 ¬∑ pytorch-labs/ao</A>
										<DT><A HREF="https://github.com/pytorch-labs/ao">pytorch-labs/ao: api's and workflows for quantization and pruning gpu models.</A>
										<DT><A HREF="https://github.com/pytorch-labs/applied-ai">pytorch-labs/applied-ai: Applied AI experiments and examples for PyTorch</A>
										<DT><A HREF="https://pytorch.org/blog/accelerating-generative-ai-2/">Accelerating Generative AI with PyTorch II: GPT, Fast | PyTorch</A>
										<DT><A HREF="https://github.com/pytorch/ao">pytorch/ao: Create and integrate custom data types, layouts and kernels with up to 2x speedups with 65% less VRAM for inference and training</A>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch-labs">PyTorch Labs</A>
									<DT><A HREF="https://github.com/pytorch-labs/gpt-fast">pytorch-labs/gpt-fast: Simple and efficient pytorch-native transformer text generation in &lt;1000 LOC of python.</A>
									<DT><A HREF="https://github.com/pytorch-labs/float8_experimental">pytorch-labs/float8_experimental</A>
									<DT><A HREF="https://github.com/pytorch-labs/ao/issues/47">[RFC] Plans for torchao ¬∑ Issue #47 ¬∑ pytorch-labs/ao</A>
									<DT><A HREF="https://github.com/pytorch-labs/segment-anything-fast">pytorch-labs/segment-anything-fast: A batched offline inference oriented version of SAM</A>
									<DT><A HREF="https://github.com/pytorch-labs/ao">pytorch-labs/ao: api's and workflows for quantization and pruning gpu models.</A>
									<DT><A HREF="https://github.com/Chillee/lit-llama">Chillee/lit-llama: Simple transformer inference in PyTorch with torch.compile + lit-llama code</A>
									<DT><A HREF="https://github.com/pytorch-labs/torchfix">pytorch-labs/torchfix: TorchFix - a linter for PyTorch-using code with autofix support</A>
									<DT><A HREF="https://github.com/pytorch-labs/applied-ai">pytorch-labs/applied-ai: Applied AI experiments and examples for PyTorch</A>
									<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/de17730a993b1d2cce4fd09e3654b5f79fd23c96/kernels/triton/inference/gptq/a100_qlinear.py#L109">applied-ai: Triton GPTQ a100_qlinear.py (print perf stats)</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-generative-ai-2/">Accelerating Generative AI with PyTorch II: GPT, Fast | PyTorch</A>
								</DL><p>
								<DT><H3 FOLDED>torch-research-facebookexperimental</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
									<DT><A HREF="https://github.com/youkaichao/TRUMPY">youkaichao/TRUMPY: Analyze backward memory usage in pytorch!</A>
									<DT><A HREF="https://github.com/lucidrains/pytorch-custom-utils">lucidrains/pytorch-custom-utils: Just some miscellaneous utility functions / decorators / modules related to Pytorch and Accelerate to help speed up implementation of new AI research</A>
									<DT><A HREF="https://github.com/pytorch-labs/ao">pytorch-labs/ao: The torchao repository contains api's and workflows for quantization and pruning gpu models.</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-triton/?utm_content=278887799&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Triton Dequantization Kernels for GPTQ | PyTorch</A>
									<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
									<DT><A HREF="https://github.com/facebookincubator">Meta Incubator</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
								</DL><p>
								<DT><A HREF="https://github.com/pytorch/tensordict">pytorch/tensordict: TensorDict is a pytorch dedicated tensor container.</A>
								<DT><A HREF="https://github.com/BobMcDear/attorch">BobMcDear/attorch: A subset of PyTorch's neural network modules, written in Python using OpenAI's Triton.</A>
								<DT><A HREF="https://github.com/lernapparat/torchhacks">lernapparat/torchhacks: Hacks for PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>torch-debug</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-debug-tracing</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/blog/trace-analysis-for-masses/">PyTorch Trace Analysis for the Masses | PyTorch</A>
									<DT><A HREF="https://pytorch.org/blog/trace-analysis-for-masses/">PyTorch Trace Analysis for the Masses</A>
									<DT><A HREF="https://github.com/facebookincubator/dynolog">facebookincubator/dynolog: Dynolog is a telemetry daemon for performance monitoring and tracing. It exports metrics from different components in the system like the linux kernel, CPU, disks, Intel PT, GPUs etc. Dynolog also integrates with pytorch and can trigger traces for distributed training applications.</A>
									<DT><A HREF="https://pytorch-dev-podcast.simplecast.com/episodes/torch-trace-and-tlparse">TORCH_TRACE and tlparse | PyTorch Developer Podcast</A>
									<DT><A HREF="https://twitter.com/ezyang/status/1777475405642907887">(1) Edward Z. Yang en X: "I spent my airplane ride home polishing up torchdbg on a trace of maskrcnn. Here's the result (no need to collect a trace yourself, this downloads one I pre-baked for you): https://t.co/1E3pVnxO9g" / X</A>
								</DL><p>
								<DT><H3 FOLDED>torch-debug-memory</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles | PyTorch</A>
									<DT><A HREF="https://zdevito.github.io/2022/12/09/memory-traces.html">Visualizing PyTorch memory usage over time | Zach‚Äôs Blog</A>
									<DT><A HREF="https://pytorch.org/docs/stable/torch_cuda_memory.html">Understanding CUDA Memory Usage ‚Äî PyTorch 2.2 documentation</A>
									<DT><A HREF="https://pytorch.org/memory_viz">https://pytorch.org/memory_viz</A>
									<DT><A HREF="https://zdevito.github.io/2022/08/16/memory-snapshots.html">Debugging PyTorch memory use with snapshots | Zach‚Äôs Blog</A>
									<DT><A HREF="https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html">A guide to PyTorch‚Äôs CUDA Caching Allocator | Zach‚Äôs Blog</A>
									<DT><A HREF="https://github.com/google/zerocopy">google/zerocopy</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">1. Overview ‚Äî cuda-binary-utilities (SaSS)</A>
								</DL><p>
								<DT><H3 FOLDED>torch-metrics</H3>
								<DL><p>
									<DT><A HREF="https://github.com/Lightning-AI/torchmetrics">Lightning-AI/torchmetrics: Torchmetrics - Machine learning metrics for distributed, scalable PyTorch applications.</A>
								</DL><p>
								<DT><A HREF="https://github.com/ezyang/torchdbg">ezyang/torchdbg: PyTorch centric eager mode debugger</A>
								<DT><A HREF="https://www.youtube.com/watch?v=iSCF-VOJtEw">PyTorch: Debugging session - reference cycle - YouTube</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/torch-compile-can-be-debugged-now/1595">Torch.compile can be debugged now! - compiler - PyTorch Dev Discussions</A>
								<DT><A HREF="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles | PyTorch</A>
								<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch - YouTube</A>
								<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes ‚Äî Good Coding Style | by Noam Shazeer | Feb, 2024 | Medium</A>
								<DT><A HREF="https://github.com/facebookincubator/dynolog">facebookincubator/dynolog: Dynolog is a telemetry daemon for performance monitoring and tracing. It exports metrics from different components in the system like the linux kernel, CPU, disks, Intel PT, GPUs etc. Dynolog also integrates with pytorch and can trigger traces for distributed training applications.</A>
								<DT><A HREF="https://github.com/graphcore-research/pytorch-tensor-tracker">graphcore-research/pytorch-tensor-tracker: Flexibly track outputs and grad-outputs of torch.nn.Module.</A>
							</DL><p>
							<DT><H3 FOLDED>torch-inference</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/blog/accelerating-generative-ai/">Accelerating Generative AI with PyTorch: Segment Anything, Fast | PyTorch</A>
								<DT><A HREF="https://twitter.com/PyTorch/status/1725242585667584453">(1) PyTorch en X: "New blog series: Accelerating Generative AI using native PyTorch. üî• In this post we talk through new PyTorch performance features from the conference and how they can be used to produce an 8x faster, entirely PyTorch implementation of Segment Anything. https://t.co/o4kc037DN8 https://t.co/QaOs0dIDB9" / X</A>
								<DT><A HREF="https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/">PyTorch compile to speed up inference on Llama 2 | PyTorch</A>
								<DT><A HREF="https://pytorch.org/blog/high-performance-llama-2/?utm_content=270816312&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs | PyTorch</A>
								<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/pull/114001">Introduce 3 low-latency, intra-node allreduce algorithms for small messages to PyTorch by yifuwang ¬∑ Pull Request #114001 ¬∑ pytorch/pytorch</A>
							</DL><p>
							<DT><H3 FOLDED>torch-deployment</H3>
							<DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/the-future-of-c-model-deployment/1282">The future of C++ model deployment</A>
							</DL><p>
							<DT><H3 FOLDED>torch-distributed</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-NCCL</H3>
								<DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-capture-nccl-communication-ops-in-faketensormode/1410">How to capture NCCL communication ops in FakeTensorMode</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/pull/114001">Introduce 3 low-latency, intra-node allreduce algorithms for small messages to PyTorch by yifuwang ¬∑ Pull Request #114001 ¬∑ pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>DTensor</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md">pytorch/torch/distributed/_tensor/README.md at main</A>
									<DT><A HREF="https://pytorch.org/blog/training-moes/?utm_content=298456196&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Training MoEs at Scale with PyTorch | PyTorch</A>
								</DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019">Rethinking Fully Sharded Data Parallel (FSDP) from First Principles</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486">FSDP &amp; CUDACachingAllocator: an outsider newb perspective</A>
								<DT><A HREF="https://github.com/facebookresearch/fairring">facebookresearch/fairring: Fairring (FAIR + Herring) is a plug-in for PyTorch that provides a process group for distributed training that outperforms NCCL at large scales</A>
								<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel: gradient_as_bucket_view</A>
								<DT><A HREF="https://github.com/pkusys/TGS/blob/main/worker.py">TGS/worker.py at main ¬∑ pkusys/TGS</A>
							</DL><p>
							<DT><H3 FOLDED>torch-jax</H3>
							<DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/help-pytorch-brain-understand-jax-flax-code/1554">Help PyTorch brain understand JAX/FLAX code - frontend API / autodiff - PyTorch Dev Discussions</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/help-pytorch-brain-understand-jax-flax-code/1554">Help PyTorch brain understand JAX/FLAX code</A>
								<DT><A HREF="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html">Tutorial 2 (JAX): Introduction to JAX+Flax</A>
							</DL><p>
							<DT><H3 FOLDED>torch-people</H3>
							<DL><p>
								<DT><A HREF="http://blog.ezyang.com/about/">Edward Z. Yang (BLOG)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=_qB2Ho1O3u4&t=7s">Edward Z. Yang (PyTorch Developer Podcast)</A>
								<DT><A HREF="https://pytorch.org/docs/stable/community/persons_of_interest.html">PyTorch Governance | Maintainers ‚Äî PyTorch 2.1 documentation</A>
								<DT><A HREF="https://github.com/albanD">albanD</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/u/penguinwu/summary">penguinwu</A>
								<DT><A HREF="https://www.linkedin.com/in/peng--wu/">Peng Wu</A>
								<DT><A HREF="https://www.youtube.com/@edwardzyang/videos">Edward Z. Yang's PyTorch and PL - YouTube</A>
								<DT><A HREF="https://jasonansel.com/">jasonansel.com</A>
								<DT><A HREF="https://github.com/Chillee">Horace He (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/natalia-gimelshein-8347a480/">Natalia Gimelshein (OpenAI)</A>
								<DT><A HREF="https://www.linkedin.com/in/animesh-jain-39244417/">Animesh Jain (Meta)</A>
								<DT><A HREF="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiL3s_lt6iEAxXOU6QEHdc8D_gQFnoECBQQAQ&url=https%3A%2F%2Frocketreach.co%2Fmichael-voznesensky-email_1979109&usg=AOvVaw0-C9Hymj48aNDsG0DgWsG3&opi=89978449">Michael Voznesensky (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/bin-bao-9b095812/">Bin Bao (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/david-berard-2a0531176/">David Berard (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/geetachauhan/">Geeta Chauhan (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/anjali-chourdia/">Anjali Chourdia (Meta)</A>
								<DT><A HREF="https://github.com/wconstab">Will Constable (Meta)</A>
								<DT><A HREF="https://cs.stanford.edu/~zdevito/">Zachary DeVito (Meta)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Lg8F4F_qZxk">Elias Ellison (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/yfengus/">Will Feng (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/jiong-gong-8504944/?originalSubdomain=cn">Jiong Gong (Intel)</A>
								<DT><A HREF="https://www.linkedin.com/in/philippe-tillet-809b5536/">Phil Tillet (OpenAI)</A>
								<DT><A HREF="https://www.jokeren.tech/">Keren Zhou (OpenAI)</A>
								<DT><A HREF="https://github.com/msaroufim">Mark Saroufim (PyTorch Labs)</A>
								<DT><A HREF="https://github.com/yzh119">Zihao Ye (flashinfer)</A>
								<DT><A HREF="https://github.com/efrantar">Elias Frantar (marlinn kernels)</A>
								<DT><A HREF="https://x.com/isidentical">isidentical: building the most efficient inference engine for diffusion models</A>
							</DL><p>
							<DT><H3 FOLDED>torch-rfcs</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch/rfcs/">pytorch/rfcs: PyTorch RFCs (experimental)</A>
								<DT><A HREF="https://github.com/pytorch/rfcs/pull/59">RFC-0033-GDS-checkpointing by antferdom ¬∑ Pull Request #59 ¬∑ pytorch/rfcs</A>
							</DL><p>
							<DT><H3 FOLDED>torch-cuda</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics">CUDA semantics ‚Äî PyTorch 2.0 documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch - YouTube</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/custom_kernels/setup.py">text-generation-inference/server/custom_kernels/setup.py at main ¬∑ huggingface/text-generation-inference</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/custom-cuda-extension-support-in-inductor/1924">Custom cuda extension support in Inductor - compiler - PyTorch Developer Mailing List</A>
							</DL><p>
							<DT><H3 FOLDED>pybind</H3>
							<DL><p>
								<DT><H3 FOLDED>cpp extension</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/francoisfleuret/status/1741481952618676698">(1) Fran√ßois Fleuret en X: "Wow, inline C++ extensions for @pytorch are that simple?</A>
									<DT><A HREF="https://pytorch.org/tutorials/advanced/cpp_extension.html">Custom C++ and CUDA Extensions ‚Äî PyTorch Tutorials 2.3.0+cu121 documentation</A>
									<DT><A HREF="https://research.colfax-intl.com/tutorial-python-binding-for-cuda-libraries-in-pytorch/">Tutorial: Python bindings for CUDA libraries in PyTorch ‚Äì Colfax Research</A>
								</DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/custom-cuda-extension-support-in-inductor/1924">Custom cuda extension support in Inductor - compiler - PyTorch Developer Mailing List</A>
								<DT><A HREF="https://github.com/torstem/demo-cuda-pybind11">torstem/demo-cuda-pybind11: How to use CUDA with Python numpy</A>
								<DT><A HREF="https://github.com/youkaichao/compare_pass_data/blob/main/example.cpp">compare_pass_data/example.cpp at main ¬∑ youkaichao/compare_pass_data</A>
								<DT><A HREF="https://github.com/openai/triton/blob/main/third_party/amd/python/triton_amd.cc">triton/third_party/amd/python/triton_amd.cc at main ¬∑ openai/triton</A>
								<DT><A HREF="https://github.com/rapidsai/cudf/tree/branch-0.7/python/cudf/bindings">cudf/python/cudf/bindings pxd &amp; pyx cython</A>
								<DT><A HREF="https://pybind11.readthedocs.io/en/stable/">pybind11 documentation</A>
								<DT><A HREF="https://twitter.com/SkyLi0n">Aaron Gokaslan (@SkyLi0n) / X</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-XLSyaJ6m3o&t=902s">WTF is Build.Zig? by Ed Yu - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MUISz2qA640&t=19s">Will Ada Replace C/C++? - YouTube</A>
								<DT><A HREF="https://research.colfax-intl.com/tutorial-python-binding-for-cuda-libraries-in-pytorch/">Tutorial: Python bindings for CUDA libraries in PyTorch ‚Äì Colfax Research</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/custom_kernels/setup.py">text-generation-inference/server/custom_kernels/setup.py at main ¬∑ huggingface/text-generation-inference</A>
								<DT><A HREF="https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary">flash-attention/csrc/rotary at main ¬∑ Dao-AILab/flash-attention</A>
								<DT><A HREF="https://github.com/hyhieu/easy_pybind">hyhieu/easy_pybind</A>
								<DT><A HREF="https://github.com/mlecauchois/micrograd-cuda/blob/main/micrograd_cuda/operations.py">micrograd-cuda/micrograd_cuda/operations.py at main ¬∑ mlecauchois/micrograd-cuda</A>
							</DL><p>
							<DT><H3 FOLDED>torch-dataloader</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=Sk35MKtCXfQ&t=1748s">celeba_iterate</A>
							</DL><p>
							<DT><H3 FOLDED>torch-optim</H3>
							<DL><p>
								<DT><H3 FOLDED>cross-entropy</H3>
								<DL><p>
									<DT><A HREF="https://github.com/mgmalek/efficient_cross_entropy">mgmalek/efficient_cross_entropy</A>
								</DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/performance-comparison-between-torch-compile-and-apex-optimizers/2023">Performance Comparison between Torch.Compile and APEX optimizers - compiler - PyTorch Developer Mailing List</A>
							</DL><p>
							<DT><A HREF="https://github.com/chu-tianxiang/llama-cpp-torch">chu-tianxiang/llama-cpp-torch: llama.cpp to PyTorch Converter</A>
							<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
							<DT><A HREF="https://pytorch.org/torchx/latest/">torchx: universal job launcher for PyTorch</A>
							<DT><A HREF="https://dev-discuss.pytorch.org/latest">Latest topics - PyTorch Dev Discussions</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/c22e77abfdb55f8248db852126737fbecfd52a7b/tinygrad/tensor.py">tinygrad/tinygrad/tensor.py at c22e77abfdb55f8248db852126737fbecfd52a7b ¬∑ tinygrad/tinygrad</A>
							<DT><A HREF="https://gist.github.com/thecharlieblake/82f1b54bbf608d8d339043ed8852cf91">Given a numpy function, prints equivalent PyTorch code (as canonical ATen ops) and returns it as a new function.</A>
						</DL><p>
						<DT><H3 FOLDED>Compilers Fundamentals</H3>
						<DL><p>
							<DT><H3 FOLDED>compiler-grammar</H3>
							<DL><p>
								<DT><H3 FOLDED>BNF</H3>
								<DL><p>
									<DT><A HREF="https://www.icosaedro.it/bnf_chk/">BNF Syntax Checker</A>
									<DT><A HREF="https://bnfplayground.pauliankline.com/">BNF Playground</A>
									<DT><A HREF="http://arran.fi.muni.cz/bnfparser2/bnfweb.php?sf=1&tf=">The BNF Verification Service</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form">Backus‚ÄìNaur form - Wikipedia</A>
									<DT><A HREF="https://docs.python.org/3/reference/grammar.html">10. Full Grammar specification ‚Äî Python 3.10.4 documentation</A>
									<DT><A HREF="https://inst.eecs.berkeley.edu/~cs164/sp18/python-grammar.html">Official Python Grammar (Python 2.5)</A>
									<DT><A HREF="https://github.com/python/cpython/blob/3.10/Grammar/python.gram">cpython/python.gram at 3.10 ¬∑ python/cpython</A>
									<DT><A HREF="https://www.iso.org/standard/26153.html">ISO - ISO/IEC 14977:1996 - Extended BNF</A>
								</DL><p>
								<DT><H3 FOLDED>ANTLR</H3>
								<DL><p>
									<DT><A HREF="https://datacadamia.com/antlr/start">Antlr (ANother Tool for Language Recognition</A>
									<DT><A HREF="https://datacadamia.com/code/compiler/compiler-compiler">Language - Compiler compilers or (lexer|parser) generators</A>
									<DT><A HREF="https://datacadamia.com/antlr/grammar">Antlr - (Grammar|Lexicon) (g4)</A>
									<DT><A HREF="https://github.com/antlr/antlr4/blob/master/doc/grammars.md">Grammar Structure</A>
									<DT><A HREF="https://github.com/antlr/antlr4/blob/master/doc/lexicon.md">Grammar Lexicon</A>
									<DT><A HREF="http://bearcave.com/software/antlr/antlr_examples.html">ANTLR Examples</A>
									<DT><A HREF="https://github.com/antlr/grammars-v4">Grammars written for ANTLR4</A>
									<DT><H3 FOLDED>Indentation</H3>
									<DL><p>
										<DT><A HREF="https://github.com/yshavit/antlr-denter">Helper class for generating python-like INDENT/DEDENT tokens with antlr4.</A>
										<DT><A HREF="https://groups.google.com/g/antlr-discussion/c/Let2Q5gOvGo/m/-ieYBGv9CwAJ">Indentation/whitespace in actions with Python target</A>
										<DT><A HREF="https://github.com/wevre/wry/blob/master/grammars/DentLexer.g4">DentLexer.g4</A>
										<DT><A HREF="http://blog.yuvalshavit.com/2014/02/python-like-indentation-using-antlr4.html">Yuval Shavit: Python-like indentation using Antlr4</A>
										<DT><A HREF="https://docs.python.org/3/reference/lexical_analysis.html#indentation">2. Lexical analysis ‚Äî Python 3.10.4 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>Functional parsing</H3>
									<DL><p>
										<DT><A HREF="https://github.com/cronburg/antlr-haskell">cronburg/antlr-haskell: A language parsing quasiquoter for Haskell based heavily on ANTLR4.</A>
									</DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/8642154/antlr-what-is-simpliest-way-to-realize-python-like-indent-depending-grammar">lexer - ANTLR What is simpliest way to realize python like indent-depending grammar? - Stack Overflow</A>
								</DL><p>
								<DT><H3 FOLDED>PEG</H3>
								<DL><p>
									<DT><A HREF="https://en.wikipedia.org/wiki/Parsing_expression_grammar">Parsing expression grammar (PEG)</A>
									<DT><A HREF="https://nim-lang.org/docs/pegs.html">PEG syntax and semantics (draft)</A>
								</DL><p>
								<DT><H3 FOLDED>grammar-conversions</H3>
								<DL><p>
									<DT><H3 FOLDED>EBNF to PEG</H3>
									<DL><p>
										<DT><A HREF="http://ceur-ws.org/Vol-928/0324.pdf">‚Äéceur-ws.org/Vol-928/0324.pdf</A>
										<DT><A HREF="https://stackoverflow.com/questions/53053899/convert-ebnf-grammar-to-peg">php - Convert EBNF grammar to PEG - Stack Overflow</A>
										<DT><A HREF="https://issueantenna.com/repo/dryruby/ebnf">gem ebnf</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://github.com/prql/prql/blob/main/prql-compiler/src/prql.pest">prql/prql.pest at main ¬∑ prql/prql</A>
								<DT><A HREF="http://essay.utwente.nl/85728/1/vanderWal_BA_FMT.pdf">Rosetta ANTLR: Ultimate Grammar Extractor</A>
								<DT><A HREF="https://stackoverflow.com/questions/8816759/ll-versus-peg-parsers-what-is-the-difference">parsing - LL(*) versus PEG parsers : what is the difference?</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Left_recursion#Removing_left_recursion">Left recursion - Wikipedia</A>
								<DT><A HREF="https://github.com/rust-lang/wg-grammar/tree/master/testdata">test grammar</A>
								<DT><A HREF="https://www.libtrends.info/npm-compare/antlr4-vs-chevrotain-vs-peg-parser-vs-pegjs">antlr4 vs chevrotain vs peg parser vs pegjs comparison - LibTrends</A>
								<DT><A HREF="https://www.boost.org/doc/libs/1_47_0/libs/spirit/doc/html/spirit/abstracts/syntax_diagram.html">Terminal vs non-terminal</A>
								<DT><A HREF="https://news.ycombinator.com/item?id=2644458">Language.js - A fast PEG parser written in JavaScript | Hacker News</A>
								<DT><A HREF="https://www.w3.org/2000/10/swap/grammar/ebnf2turtle.py">Motivation</A>
								<DT><A HREF="https://www.reddit.com/r/rust/comments/cobadh/antlr_grammars_in_rust/">(1) Antlr grammars in Rust : rust</A>
								<DT><A HREF="https://github.com/kaby76/Domemtech.Trash">kaby76/Domemtech.Trash: Toolkit for grammars</A>
							</DL><p>
							<DT><H3 FOLDED>compilers-template-engine</H3>
							<DL><p>
								<DT><H3 FOLDED>Jinja2</H3>
								<DL><p>
									<DT><A HREF="https://sites.google.com/a/chromium.org/dev/developers/jinja">Jinja - The Chromium Projects</A>
									<DT><A HREF="https://jinja.palletsprojects.com/en/3.1.x/">Jinja ‚Äî Jinja Documentation (3.1.x)</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Template_processor">Template processor - Wikipedia</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>compilers-courses</H3>
							<DL><p>
								<DT><A HREF="https://suif.stanford.edu/~courses/cs243/">CS243 - Advanced Compilers | Winter 2021</A>
								<DT><A HREF="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/pages/lecture-slides/">Lecture Slides | Performance Engineering of Software Systems</A>
							</DL><p>
							<DT><A HREF="https://godbolt.org/">Compiler Explorer</A>
							<DT><A HREF="https://www.kirupa.com/hodgepodge/compiling_transpiling.htm">Compiling (and Transpiling) Explained</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Source-to-source_compiler">Source-to-source compiler - Wikipedia</A>
							<DT><A HREF="https://datacadamia.com/code/compiler/compiler">Translation</A>
							<DT><A HREF="https://datacadamia.com/code/compiler/lexer">Lexical Analysis - Lexer (Tokenizer)</A>
							<DT><A HREF="https://www.youtube.com/watch?v=k6ZsRiLSyvY">Compilation, Libraries and the pesky "unresolved external symbol" error - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=zX-kazAtX0c&t=47s">Calling Functions Across Languages ‚Äî Richard Feldman - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ZI198eFghJk">Modernizing Compiler Design for Carbon Toolchain - Chandler Carruth - CppNow 2023 - YouTube</A>
							<DT><A HREF="https://twitter.com/chandlerc1024/status/1549411352657133568?lang=en">(1) Chandler Carruth on X: "Really excited we've been able to start sharing our experimental work on #CarbonLang with the wider C++ community. That said, I'd suggest folks read up on the docs and maaaybe wait for our announcement keynote and Q&amp;amp;A from #CppNorth before leaping to too many assumptions. =]" / X</A>
							<DT><A HREF="https://www.youtube.com/watch?v=L6f--pEHJMo">Writing a Programming Language (in Rust) 23: Laurel: Continuing a bash script port</A>
							<DT><A HREF="https://github.com/compiler-explorer/compiler-explorer">compiler-explorer/compiler-explorer: Run compilers interactively from your web browser and interact with the assembly</A>
							<DT><A HREF="https://www.youtube.com/watch?v=MC7qoiJ5uPc">Why is C is so important for compiler developers? - YouTube</A>
							<DT><A HREF="https://github.com/gergo-/missed-optimizations">gergo-/missed-optimizations: Missed optimizations in C compilers</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M6NoMv69sgU">Writing a Compiler and Interpreter in Rust - Part 1 - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>JAX</H3>
						<DL><p>
							<DT><H3 FOLDED>jax-profilling</H3>
							<DL><p>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/profiling.html">profiling Jax programs</A>
							</DL><p>
							<DT><H3 FOLDED>Pallas</H3>
							<DL><p>
								<DT><A HREF="https://github.com/google/jax/pull/17328">feat(pallas): Optimize Pallas Attention + Benchmark by jon-chuang ¬∑ Pull Request #17328 ¬∑ google/jax</A>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/_images/pallas_flow.png">pallas_flow.png 908√ó832 pixels</A>
								<DT><A HREF="https://bnikolic.co.uk/blog/python/jax/2020/10/20/jax-outputgraph.html">Jax: Visualising the computational graph of a jax program | B. Nikolic Software and Computing Blog</A>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/design.html">Pallas Design ‚Äî JAX documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5ilr4gcenaA">(Google) JAX:¬†Low-level control with shard_map and Pallas - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NFKubflDb1A">Pallas + Mosaic by Example (MAIN)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NFKubflDb1A">TPU Memory Model</A>
								<DT><A HREF="https://github.com/google/jax/pull/20462/">Add MegaBlox grouped matrix multiplication kernels for TPU. by copybara-service[bot] ¬∑ Pull Request #20462 ¬∑ google/jax</A>
							</DL><p>
							<DT><H3 FOLDED>jax-triton</H3>
							<DL><p>
								<DT><A HREF="https://github.com/jax-ml/jax-triton">jax-ml/jax-triton: integrations between JAX and OpenAI Triton</A>
							</DL><p>
							<DT><H3 FOLDED>pjit</H3>
							<DL><p>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html?ref=blog.salesforceairesearch.com">Introduction to pjit ‚Äî JAX documentation</A>
								<DT><A HREF="https://github.com/ayaka14732/einshard?tab=readme-ov-file">ayaka14732/einshard: High-level array sharding API for JAX</A>
							</DL><p>
							<DT><H3 FOLDED>Flax</H3>
							<DL><p>
								<DT><A HREF="https://github.com/skye/flax_bert">skye/flax_bert</A>
							</DL><p>
							<DT><H3 FOLDED>Haliax</H3>
							<DL><p>
								<DT><A HREF="https://github.com/stanford-crfm/haliax">stanford-crfm/haliax: Named Tensors for Legible Deep Learning in JAX</A>
							</DL><p>
							<DT><H3 FOLDED>jax-transformer</H3>
							<DL><p>
								<DT><A HREF="https://github.com/joschu/jax-exp/blob/master/jax_transformer.py#L96">John Schulman: jax-exp/jax_transformer.py at master</A>
								<DT><A HREF="https://github.com/xjdr-alt/simple_transformer/blob/main/simple_transformer.py">simple_transformer/simple_transformer.py at main ¬∑ xjdr-alt/simple_transformer</A>
								<DT><A HREF="https://github.com/awf/functional-transformer">functional-transformer: A pure-functional implementation of a machine learning transformer model in Python/JAX</A>
								<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x</A>
								<DT><A HREF="https://twitter.com/LiamFedus/status/1536791574612303872">Switch Transformer models in T5X/JAX (1.6T param)</A>
								<DT><A HREF="https://github.com/yixiaoer/mistral-v0.2-jax">yixiaoer/mistral-v0.2-jax: JAX implementation of the Mistral 7b v0.2 model</A>
							</DL><p>
							<DT><H3 FOLDED>jax-graphs</H3>
							<DL><p>
								<DT><A HREF="https://github.com/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb">educational/intro_to_graph_nets_tutorial_with_jraph.ipynb at master ¬∑ deepmind/educational</A>
								<DT><A HREF="https://github.com/deepmind/jraph">deepmind/jraph: A Graph Neural Network Library in Jax</A>
							</DL><p>
							<DT><H3 FOLDED>jax-torch</H3>
							<DL><p>
								<DT><A HREF="https://sjmielke.com/jax-purify.htm">From PyTorch to JAX: towards neural net frameworks that purify stateful code</A>
								<DT><A HREF="https://sjmielke.com/jax-purify.htm">From PyTorch to JAX</A>
							</DL><p>
							<DT><H3 FOLDED>jax-lectures</H3>
							<DL><p>
								<DT><A HREF="https://afmck.in/posts/2023-05-22-jax-post/">On Learning JAX</A>
							</DL><p>
							<DT><H3 FOLDED>jax-lax-scan</H3>
							<DL><p>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html">jax.lax.scan ‚Äî JAX documentation</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=e85Ceq2g5z0&t=1s">(Day 1 - Breakout Session) StableHLO &amp; PJRT - YouTube</A>
							<DT><A HREF="https://github.com/jax-ml/jax-triton">jax-ml/jax-triton: jax-triton contains integrations between JAX and OpenAI Triton</A>
							<DT><A HREF="https://github.com/n2cholas/awesome-jax">n2cholas/awesome-jax: JAX - A curated list of resources</A>
							<DT><A HREF="https://github.com/skye/gpu_jaxlib_docker_image">skye/gpu_jaxlib_docker_image</A>
							<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox">NVIDIA/JAX-Toolbox: JAX-Toolbox</A>
							<DT><A HREF="https://www.youtube.com/watch?v=NlQ1N3W3Wms&t=40s">JAX.lax.scan tutorial (for autoregressive rollout) - YouTube</A>
							<DT><A HREF="https://jax.readthedocs.io/en/latest/autodidax.html">Autodidax: JAX core from scratch ‚Äî JAX documentation</A>
							<DT><A HREF="https://research.google/pubs/pub46196/">A Computational Model for TensorFlow (An Introduction)</A>
							<DT><A HREF="https://github.com/google/jax/blob/967c38d53d632be713cde1f4caadb3e388b51f37/jax/_src/nn/functions.py#L559">jax/jax/_src/nn/functions.py OPS</A>
							<DT><A HREF="https://x.com/_xjdr/status/1801063628298412239">Megablox, splash attention, pallas and automatically sharded named axis come free and built in with Jax and y'all are still using pytorch in production?!?!" / X</A>
						</DL><p>
						<DT><H3 FOLDED>Triton</H3>
						<DL><p>
							<DT><H3 FOLDED>triton-examples</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ELS-RD/kernl/blob/main/test/debugger/test_debugger.py">Example: addition</A>
								<DT><A HREF="https://github.com/openai/triton/blob/main/python/triton/ops/blocksparse/matmul.py">Flash-Attention/blocksparse/matmul.py</A>
								<DT><A HREF="https://github.com/lucidrains/triton-transformer">lucidrains/triton-transformer: Implementation of a Transformer, but completely in Triton</A>
								<DT><A HREF="https://kushajveersingh.com/blog/writing-custom-cuda-kernels-with-triton">Writing custom CUDA kernels with Triton</A>
								<DT><A HREF="https://github.com/ELS-RD/kernl/blob/91e2cd92db44d503874d39a9f6dec42c9f481a8e/src/kernl/model_optimization.py#L27">kernl/src/kernl/model_optimization.py</A>
								<DT><A HREF="https://github.com/srush/Triton-Puzzles">srush/Triton-Puzzles: Puzzles for learning Triton</A>
								<DT><A HREF="https://github.com/nikitaved/Intro_to_Triton/blob/main/Intro%20to%20Triton.ipynb">Intro_to_Triton/Intro to Triton.ipynb at main ¬∑ nikitaved/Intro_to_Triton</A>
								<DT><A HREF="https://x.com/pommedeterre33/status/1681935636129873920">(1) Micha√´l Benesty en X: "Boosted Llama V2 inference speed by 1.8x using @OpenAI's Triton (one key tech behind GPT-4) at batch=1 FP16 on a 3090 GPU, no quality compromises (-&amp;gt; no quant, etc.) Triton allows efficient custom GPU kernels, letting us merge operations together. Here's how we did + how it works" / X</A>
								<DT><A HREF="https://github.com/ELS-RD/kernl/tree/main/experimental/llama-v2">kernl/experimental/llama-v2 at main ¬∑ ELS-RD/kernl</A>
								<DT><A HREF="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/OpenAITriton%20MLIR%20%E7%AC%AC%E4%BA%8C%E7%AB%A0%20Batch%20GEMM%20benchmark/#giantpandacv">OpenAITriton MLIR Á¨¨‰∫åÁ´† Batch GEMM benchmark - GiantPandaCV</A>
								<DT><A HREF="http://giantpandacv.com/project/CUDA/%E3%80%90BBuf%E7%9A%84CUDA%E7%AC%94%E8%AE%B0%E3%80%91%E5%8D%81%E4%BA%94%EF%BC%8COpenAI%20Triton%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%E4%B8%89%20FusedAttention/">FlashAttention V2 (chinese blog)</A>
							</DL><p>
							<DT><H3 FOLDED>TTIR</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>triton-programming-model</H3>
							<DL><p>
								<DT><A HREF="https://github.com/kshama-msft/triton/blob/f0cf3a2e7a35260a013e639aea558fe1b7befa7b/docs/programming-guide/chapter-1/introduction.rst">triton/docs/programming-guide/chapter-1/introduction.rst</A>
								<DT><A HREF="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/OpenAITriton%20MLIR%20%E7%AC%AC%E4%BA%8C%E7%AB%A0%20Batch%20GEMM%20benchmark/#giantpandacv">OpenAITriton MLIR Á¨¨‰∫åÁ´† Batch GEMM benchmark - GiantPandaCV</A>
								<DT><A HREF="https://github.com/kimbochen/md-blogs/tree/main/what-triton-does-in-a-matmul">md-blogs/what-triton-does-in-a-matmul at main</A>
							</DL><p>
							<DT><H3 FOLDED>triton-kernels</H3>
							<DL><p>
								<DT><A HREF="https://github.com/cuda-mode/triton-index">cuda-mode/triton-index: Cataloging released Triton kernels.</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index/blob/main/kernel_overview.md">triton-index/kernel_overview.md</A>
								<DT><A HREF="https://github.com/ELS-RD/kernl/blob/main/test/debugger/test_debugger.py">Example: addition</A>
								<DT><A HREF="https://github.com/openai/triton/blob/main/python/triton/ops/blocksparse/matmul.py">Flash-Attention/blocksparse/matmul.py</A>
							</DL><p>
							<DT><H3 FOLDED>triton-compiler</H3>
							<DL><p>
								<DT><A HREF="https://github.com/triton-lang/triton/blob/main/python/triton/tools/compile.py">triton/python/triton/tools/compile.py</A>
							</DL><p>
							<DT><H3 FOLDED>triton-backends</H3>
							<DL><p>
								<DT><H3 FOLDED>triton-backends-llvm</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ptillet/triton-llvm-releases">ptillet/triton-llvm-releases</A>
								</DL><p>
								<DT><H3 FOLDED>triton-backends-ptx</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/cHHillee/status/1779141387876962469">Triton kernels can be precompiled into .cubin files</A>
								</DL><p>
								<DT><H3 FOLDED>triton-backends-hip</H3>
								<DL><p>
									<DT><A HREF="https://github.com/openai/triton/pull/1983">[ROCM] Core Functionality for AMD by micmelesse ¬∑ Pull Request #1983 ¬∑ openai/triton</A>
									<DT><A HREF="https://gemini.google.com/app/ba25862900744324">HIP: clang2py</A>
								</DL><p>
								<DT><H3 FOLDED>triton-cpu</H3>
								<DL><p>
									<DT><A HREF="https://github.com/triton-lang/triton-cpu">triton-lang/triton-cpu: An experimental CPU backend for Triton</A>
									<DT><A HREF="https://github.com/kshama-msft/triton/blob/f0cf3a2e7a35260a013e639aea558fe1b7befa7b/docs/meetups/05-07-2024/notes.md">triton/docs/meetups/05-07-2024/notes</A>
									<DT><A HREF="https://www.youtube.com/watch?v=hgINpebZ7n0">Triton May Community meetup 20240507 - YouTube</A>
									<DT><A HREF="https://drive.google.com/drive/folders/1xPnRO5P59aMVJnXz_o9ASTUgTXK1lhHW">May 2024 meetup - Google Drive</A>
									<DT><A HREF="https://github.com/pytorch-labs/triton-cpu">pytorch-labs/triton-cpu: An experimental CPU backend</A>
									<DT><A HREF="https://siboehm.com/articles/22/Fast-MMM-on-CPU">Fast Multidimensional Matrix Multiplication on CPU from Scratch</A>
								</DL><p>
								<DT><H3 FOLDED>triton-backends-xpu</H3>
								<DL><p>
									<DT><A HREF="https://github.com/intel/intel-xpu-backend-for-triton">intel/intel-xpu-backend-for-triton: OpenAI Triton backend for Intel¬Æ GPUs</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>triton-profiler</H3>
							<DL><p>
								<DT><H3 FOLDED>proton</H3>
								<DL><p>
									<DT><H3 FOLDED>Hatchet</H3>
									<DL><p>
										<DT><A HREF="https://hatchet.readthedocs.io/en/latest/">Hatchet ‚Äî hatchet 1.4.0 documentation</A>
										<DT><A HREF="https://github.com/hatchet/hatchet">hatchet/hatchet: Analyze graph/hierarchical performance data using pandas dataframes</A>
									</DL><p>
									<DT><A HREF="https://github.com/kshama-msft/triton/blob/f0cf3a2e7a35260a013e639aea558fe1b7befa7b/docs/meetups/02-20-2024/Proton.pdf">triton/docs/meetups/02-20-2024/Proton.pdf</A>
									<DT><A HREF="https://github.com/openai/triton/tree/main/third_party/proton">triton/third_party/proton</A>
									<DT><A HREF="https://github.com/openai/triton/blob/main/third_party/proton/tutorials/matmul.py">triton/third_party/proton/tutorials/matmul.py at main ¬∑ openai/triton</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>triton-jax</H3>
							<DL><p>
								<DT><A HREF="https://github.com/jax-ml/jax-triton">jax-ml/jax-triton: jax-triton contains integrations between JAX and OpenAI Triton</A>
							</DL><p>
							<DT><H3 FOLDED>triton-community-meetup</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=KZAzpKx1ebI">Triton October Community meetup 20231025 - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>triton-people</H3>
							<DL><p>
								<DT><A HREF="https://github.com/manbearian">ian Bearman (MS)</A>
								<DT><A HREF="https://twitter.com/Si_Boehm">(1) Simon Boehm (@Si_Boehm) / X</A>
								<DT><A HREF="https://x.com/nadavrot">(Nadav Rotem: Engineering director at Facebook. Interested in systems, compilers, ML, performance, and other stuff</A>
							</DL><p>
							<DT><H3 FOLDED>triton-debug</H3>
							<DL><p>
								<DT><H3 FOLDED>triton-debug-perf</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/main_horse/status/1742013125090795531">NVIDIA non-industrial: triton.ops.matmul</A>
									<DT><A HREF="https://twitter.com/cis_female/status/1660386176761724928">do_bench</A>
									<DT><A HREF="https://gist.github.com/Chillee/41baf11aac8036d25d637321c48dad20">You Could Have Invented Flash-Attention!</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
								</DL><p>
								<DT><H3 FOLDED>triton-debug-interpreter</H3>
								<DL><p>
									<DT><A HREF="https://drive.google.com/drive/folders/1bKpvz1NiBL_fHrGhMoZPvQfXCeetV2iY">Triton Interpreter Update</A>
								</DL><p>
								<DT><H3 FOLDED>triton-debug-sass</H3>
								<DL><p>
									<DT><A HREF="https://github.com/compiler-explorer/compiler-explorer/pull/5531">Add Triton language by siboehm ¬∑ Pull Request #5531 ¬∑ compiler-explorer/compiler-explorer</A>
									<DT><A HREF="https://twitter.com/Si_Boehm/status/1708233662305910785">(1) Simon Boehm en X: "I got Triton working on (local) Godbolt instances, including Python &amp;lt;-&amp;gt; SASS line correspondence. Pretty nifty. Still got some infra issues to figure out, meanwhile PR is here if anyone wants to try it: https://t.co/FPJjgOo3tl https://t.co/U2ie1evR9o" / X</A>
								</DL><p>
								<DT><A HREF="https://github.com/openai/triton/issues/517#issuecomment-2028547176">How to debug kernels ¬∑ Issue #517 TRITON_INTERPRET=1</A>
								<DT><A HREF="https://github.com/openai/triton/blob/8e0c7b425ac149c43183de966ffa423fd46e4762/python/triton/testing.py#L441">triton/python/triton/testing.py (main)</A>
								<DT><A HREF="https://github.com/Deep-Learning-Profiling-Tools/triton-viz">Deep-Learning-Profiling-Tools/triton-viz</A>
								<DT><A HREF="https://chat.openai.com/c/74ad9a19-e6f0-4ba3-a0d7-0d7a0b6c5c61">Performance Logging &amp; Saving Kernel ASM &amp; IR</A>
								<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/de17730a993b1d2cce4fd09e3654b5f79fd23c96/kernels/triton/inference/gptq/a100_qlinear.py#L8">applied-ai/kernels/triton/inference/gptq/a100_qlinear.py (ASM &amp; IR)</A>
							</DL><p>
							<DT><H3 FOLDED>triton-torch-inductor</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch/pytorch/pull/111434">[Inductor] Support user defined triton kernels in inductor</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ACR1WnRScCc">Composability Sync - User defined Triton vs custom ops / C++</A>
								<DT><A HREF="https://github.com/BobMcDear/attorch">BobMcDear/attorch: A subset of PyTorch's neural network modules, written in Python using OpenAI's Triton.</A>
								<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile ‚Äî PyTorch Tutorials 2.3.0+cu121 documentation</A>
							</DL><p>
							<DT><H3 FOLDED>ThunderKittens</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/bfspector/status/1789749117104894179">(1) Benjamin F Spector en X: "(1/7) Happy mother‚Äôs day! We think what the mothers of America really want is a Flash Attention implementation that‚Äôs just 100 lines of code and 30% faster, and we‚Äôre happy to provide. We're excited to introduce ThunderKittens (TK), a simple DSL embedded within CUDA that makes... https://t.co/7Nupt8B4hq" / X</A>
								<DT><A HREF="https://github.com/HazyResearch/ThunderKittens">HazyResearch/ThunderKittens: Tile primitives for speedy kernels</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-05-12-quick-tk">ThunderKittens: A Simple Embedded DSL for AI kernels ¬∑ Hazy Research</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">GPUs Go Brrr ¬∑ Hazy Research</A>
								<DT><A HREF="https://github.com/Narsil/zandle/tree/main/src">zandle/src at main ¬∑ Narsil/zandle</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=yCyZEJrlrfY&t=126s">Lightning Talk: Harnessing NVIDIA Tensor Cores: An Exploration of CUTLASS &amp; OpenAI</A>
							<DT><A HREF="https://openai.com/research/triton">Introducing Triton: Open-source GPU programming for neural networks</A>
							<DT><A HREF="https://github.com/microsoft/triton-shared">microsoft/triton-shared: Shared Middle-Layer for Triton Compilation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GHQ1M3VDOmU&t=8s">Intro to Triton: A Parallel Programming Compiler and Language (esp for AI acceleration) - YouTube</A>
							<DT><A HREF="https://github.com/ptillet/triton-llvm-releases">ptillet/triton-llvm-releases</A>
							<DT><A HREF="https://pytorch.org/blog/accelerating-triton/?utm_content=278887799&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Triton Dequantization Kernels for GPTQ | PyTorch</A>
							<DT><A HREF="https://github.com/pytorch/pytorch/pull/111434">[Inductor] Support user defined triton kernels in inductor by oulgen ¬∑ Pull Request #111434 ¬∑ pytorch/pytorch</A>
							<DT><A HREF="https://github.com/kakaobrain/trident">kakaobrain/trident: A performance library for machine learning applications.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=yCyZEJrlrfY&t=126s">NVIDIA Tensor Cores: An Exploration of CUTLASS &amp; OpenAI</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GHQ1M3VDOmU&t=8s">Intro to Triton: A Parallel Programming Compiler and Language</A>
							<DT><A HREF="https://github.com/kakaobrain/trident">kakaobrain/trident: A performance library for machine learning</A>
							<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/">GEMM kernels Hopper</A>
							<DT><A HREF="https://github.com/srush/Triton-Puzzles">srush/Triton-Puzzles: Puzzles for learning Triton</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GHQ1M3VDOmU&t=8s">Intro to Triton: A Parallel Programming Compiler and Language (esp for AI acceleration)</A>
						</DL><p>
						<DT><H3 FOLDED>MLIR</H3>
						<DL><p>
							<DT><H3 FOLDED>StableHLO</H3>
							<DL><p>
								<DT><A HREF="https://github.com/openxla/stablehlo/blob/main/docs/spec.md">stablehlo/docs/spec.md at main ¬∑ openxla/stablehlo</A>
							</DL><p>
							<DT><H3 FOLDED>NVGPU Dialect</H3>
							<DL><p>
								<DT><A HREF="https://grypp.github.io/papers/nvdsl.pdf">Programming
Nvidia Hopper with MLIR‚Äôs NVGPU Dialect</A>
							</DL><p>
							<DT><A HREF="https://mlir.llvm.org/docs/LangRef/">MLIR Language Reference - MLIR</A>
							<DT><A HREF="https://www.youtube.com/watch?v=LPlRLt9w4b0">2023 EuroLLVM - What's new in MLIR? - YouTube</A>
							<DT><A HREF="https://github.com/openxla/iree">openxla/iree: A retargetable MLIR-based machine learning compiler and runtime toolkit.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=SEwTjZvy8vw">Mojo: A system programming language for heterogenous computing</A>
						</DL><p>
						<DT><H3 FOLDED>AITemplate</H3>
						<DL><p>
							<DT><A HREF="https://github.com/facebookincubator/AITemplate">facebookincubator/AITemplate: AITemplate is a Python framework which renders neural network into high performance CUDA/HIP C++ code. Specialized for FP16 TensorCore (NVIDIA GPU) and MatrixCore (AMD GPU) inference.</A>
							<DT><A HREF="https://github.com/facebookincubator/AITemplate/blob/992e1a08e363f0d301bc269ca092f6d999abcca8/tests/unittest/ops/test_gemm_bias.py#L39">AITemplate/tests/unittest/ops/test_gemm_bias.py</A>
							<DT><A HREF="https://facebookincubator.github.io/AITemplate/tutorial/how_to_infer_pt.html">AIT module is a container to build a graph, while PyTorch module is a container to store parameters for eager</A>
						</DL><p>
						<DT><H3 FOLDED>DeepSpeed</H3>
						<DL><p>
							<DT><H3 FOLDED>deepspeed-kernels</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/csrc/transformer/inference/csrc/gelu.cu#L656">DeepSpeed/csrc/transformer/inference/csrc/gelu.cu at master ¬∑ microsoft/DeepSpeed</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>XLA</H3>
						<DL><p>
							<DT><H3 FOLDED>torch-xla-auto-sharding</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=L1PSUhGtZVc">Optimizing PyTorch Auto sharding For Your Hardware 2024 04 25 09 40 GMT 7 - YouTube</A>
								<DT><A HREF="https://pytorch.org/blog/pytorch-xla-spmd/">PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization | PyTorch</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/pytorch-xla-2-3-dev-update/2124">PyTorch/XLA 2.3 dev update - compiler - PyTorch Developer Mailing List</A>
							</DL><p>
							<DT><H3 FOLDED>xla-videos</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=e85Ceq2g5z0&t=1s">(Day 1 - Breakout Session) StableHLO &amp; PJRT - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5ilr4gcenaA">(Google) JAX:¬†Low-level control with shard_map and Pallas - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NFKubflDb1A">(Day 1 - Breakout Session) JAX: Pallas and Shard Map - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5i7xrBUCD38">OpenXLA Lightning Talks - April 27, 2023 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=zS7NTHYNiF4">OpenXLA Roadmap Presentations - April 27, 2023 - YouTube</A>
							</DL><p>
							<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox">NVIDIA/JAX-Toolbox: JAX-Toolbox</A>
							<DT><A HREF="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">Mixed-input matrix multiplication performance optimizations ‚Äì Google Research Blog</A>
							<DT><A HREF="https://github.com/google/jax/pull/20462/">Add MegaBlox grouped matrix multiplication kernels for TPU. by copybara-service[bot] ¬∑ Pull Request #20462 ¬∑ google/jax</A>
							<DT><A HREF="https://cloud.google.com/blog/products/ai-machine-learning/introducing-pytorch-xla-2-3">Introducing PyTorch/XLA 2.3 | Google Cloud Blog</A>
							<DT><A HREF="https://github.com/pytorch/xla">pytorch/xla: Enabling PyTorch on XLA Devices (e.g. Google TPU)</A>
						</DL><p>
						<DT><H3 FOLDED>ai-compilers-people</H3>
						<DL><p>
							<DT><A HREF="https://github.com/skye?tab=stars">skye (Skye Wanderman-Milne) / Starred</A>
							<DT><A HREF="https://pytorch.org/docs/stable/community/persons_of_interest.html">PyTorch Governance | Maintainers ‚Äî PyTorch 2.1 documentation</A>
							<DT><A HREF="https://github.com/albanD">albanD</A>
							<DT><A HREF="https://github.com/ptillet">ptillet (Philippe Tillet) OpenAI Triton</A>
							<DT><A HREF="https://twitter.com/main_horse/status/1742013125090795531">main_horse</A>
							<DT><A HREF="https://github.com/philipturner/metal-flash-attention">Philip Turner: metal-flash-attention</A>
							<DT><A HREF="https://twitter.com/fluffykittnmeow">fluffy (Triton &amp; Tinygrad)</A>
							<DT><A HREF="https://github.com/Bruce-Lee-LY">Bruce-Lee-LY (Bruce-Lee-LY)</A>
							<DT><A HREF="https://twitter.com/MimeeXu/status/1736045781486846184">ML For Systems: Bill Dally (NVIDIA)</A>
							<DT><A HREF="https://github.com/scott-gray">scott-gray (Scott Gray) OpenAI</A>
							<DT><A HREF="https://github.com/sjfeng1999">sjfeng1999 (Feng Shijie)</A>
							<DT><A HREF="https://github.com/daadaada">daadaada (Da Yan)</A>
							<DT><A HREF="https://www.linkedin.com/in/rawn-henry/">Rawn Henry (NVIDIA)</A>
							<DT><A HREF="https://www.linkedin.com/in/quentincolombet/?originalSubdomain=ch">Quentin Colombet ML compiler (Google)</A>
							<DT><A HREF="https://www.linkedin.com/in/pradeep-ramani/">Pradeep Ramani (NVIDIA)</A>
							<DT><A HREF="https://github.com/thakkarV">Vijay Thakkar (NVIDIA)</A>
							<DT><A HREF="https://github.com/VictorTaelin">VictorTaelin (Victor Taelin)</A>
						</DL><p>
						<DT><H3 FOLDED>Tinygrad</H3>
						<DL><p>
							<DT><H3 FOLDED>tinygrad-backends</H3>
							<DL><p>
								<DT><H3 FOLDED>tinygrad-backends-triton</H3>
								<DL><p>
									<DT><A HREF="https://github.com/geohot/tinygrad/pull/470">A Triton backend for tinygrad by geohot</A>
								</DL><p>
								<DT><H3 FOLDED>tinygrad-backends-llvm</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=bMdzsQuQxKs">George Hotz | Programming | tinygrad: LLVM backend</A>
								</DL><p>
								<DT><H3 FOLDED>tinygrad-backends-hip</H3>
								<DL><p>
									<DT><A HREF="https://github.com/geohot/tinygrad/pull/750">HIP backend by nanamiwang ¬∑ Pull Request #750 ¬∑ geohot/tinygrad</A>
								</DL><p>
								<DT><H3 FOLDED>tinygrad-backends-llm.c</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/karpathy/status/1783527854741114981">Andrej Karpathy: 1) write a better compiler 2) write a better assembly-level program</A>
									<DT><A HREF="https://gist.github.com/geohot/7c9f10f5770f058a1de6ef0598e4c9d8">Outputted llm.c from tinygrad</A>
								</DL><p>
								<DT><A HREF="https://gemini.google.com/app/ba25862900744324">Gemini</A>
								<DT><A HREF="https://gist.github.com/fxkamd/ffd02d66a2863e444ec208ea4f3adc48">Observations about HSA and KFD backends in TinyGrad</A>
							</DL><p>
							<DT><H3 FOLDED>tinygrad-people</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Qazalin?tab=repositories">Qazalin (Qazalin) / Repositories</A>
							</DL><p>
							<DT><H3 FOLDED>tinygrad-docs</H3>
							<DL><p>
								<DT><A HREF="https://mesozoic-egg.github.io/tinygrad-notes/">Tutorials on Tinygrad | tinygrad-notes</A>
								<DT><A HREF="https://www.youtube.com/watch?v=I_c9cdNAkH4&t=3146s">George Hotz (52:00) | Tinygrad layers of abstraction, Triton</A>
								<DT><A HREF="https://www.youtube.com/watch?v=fq__NqceKVs">George Hotz explains tinygrad's approach to winning at deep learning - YouTube</A>
								<DT><A HREF="https://mesozoic-egg.github.io/tinygrad-notes/shapetracker.html">How ShapeTracker works | tinygrad-notes</A>
							</DL><p>
							<DT><H3 FOLDED>tinygrad-hand-optimization</H3>
							<DL><p>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/c4fdb9c725924fd1bc8a89ca07a1f405953b4d54/examples/handcode_resnet50_opt.py">tinygrad/examples/handcode_resnet50_opt.py</A>
							</DL><p>
							<DT><H3 FOLDED>tinygrad-checkpointing</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=2QO3vzwHXhg&t=3551s">converting to float16 slowing down</A>
							</DL><p>
							<DT><A HREF="https://docs.google.com/document/d/1q0VulPvi1awazH4EAsScXw3kqWHM-GAbBGY6IQzSV70/edit">tiny corp master plan - Google Docs</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/tinygrad/tensor.py">tinygrad/tinygrad/tensor.py at master ¬∑ tinygrad/tinygrad</A>
							<DT><A HREF="https://github.com/tinygrad/gpuctypes">tinygrad/gpuctypes: ctypes wrappers for HIP, CUDA, and OpenCL</A>
							<DT><A HREF="https://github.com/geohot/ctypeslib">geohot/ctypeslib: Generate python ctypes classes from C headers. Requires LLVM clang</A>
							<DT><A HREF="https://github.com/tinygrad/teenygrad">tinygrad/teenygrad: If tinygrad wasn't small enough for you...</A>
							<DT><A HREF="https://twitter.com/__tinygrad__/status/1729596377028567137">Tinybox FP16 TFLOPs JAX</A>
							<DT><A HREF="https://www.youtube.com/watch?v=-iH5wvFnsKs">George Hotz | Programming | ripping out all of AMD's userspace, AMDGPU ioctls | GPU memory | HSA KFD - YouTube</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/examples/compile_efficientnet.py">tinygrad/examples/compile_efficientnet.py C codegen</A>
							<DT><A HREF="https://github.com/karpathy/llm.c">karpathy/llm.c: LLM training in simple, raw C/CUDA</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/discussions/3342">Do we have a Model Summary Feature in Tinygrad? ¬∑ tinygrad/tinygrad ¬∑ Discussion #3342</A>
							<DT><A HREF="https://developer.nvidia.com/blog/boosting-dynamic-programming-performance-using-nvidia-hopper-gpu-dpx-instructions/">Boosting Dynamic Programming Performance Using NVIDIA Hopper GPU DPX Instructions | NVIDIA Technical Blog</A>
							<DT><A HREF="https://mesozoic-egg.github.io/tinygrad-notes/">Tutorials on Tinygrad | tinygrad-notes</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/examples/llm.c/train_gpt2.py">tinygrad/examples/llm.c/train_gpt2.py</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/de832d26c64a9ec575e47aeb58efe27a0ccf4e0b/autogen_stubs.sh">tinygrad/autogen_stubs.sh (runtime.autogen) clang2py</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Sk35MKtCXfQ&t=4743s">matrix multiplication, a@b, cube</A>
							<DT><A HREF="https://x.com/__tinygrad__/status/1742365883048284421">(1) the tiny corp en X: "tinygrad's multiGPU tensor sharding is merged. experimental, but is that not the simplest API you have ever seen for data/model parallel? (hint: it's just what axis you shard) https://t.co/HDqtRVAHUy" / X</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/docs/tinygrad_intro.pdf">tinygrad/docs/tinygrad_intro.pdf at master ¬∑ tinygrad/tinygrad</A>
							<DT><A HREF="https://github.com/geohot/cuda_ioctl_sniffer">geohot/cuda_ioctl_sniffer: Sniff CUDA ioctls</A>
						</DL><p>
						<DT><H3 FOLDED>Tensors</H3>
						<DL><p>
							<DT><H3 FOLDED>tensors-examples</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ezyang/data-dependent-shape-puzzles">ezyang/data-dependent-shape-puzzles: Puzzlers regarding data-dependent shapes in PT2</A>
								<DT><A HREF="https://github.com/srush/Tensor-Puzzles">srush/Tensor-Puzzles: Solve puzzles. Improve your pytorch.</A>
							</DL><p>
							<DT><H3 FOLDED>tensors-debug</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ezyang/torchdbg">ezyang/torchdbg: PyTorch centric eager mode debugger</A>
							</DL><p>
							<DT><H3 FOLDED>named-tensors</H3>
							<DL><p>
								<DT><H3 FOLDED>Haliax</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/dlwh/status/1716900734120464834/photo/1">Named tensor library</A>
									<DT><A HREF="https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html">Stanford CRFM</A>
									<DT><A HREF="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</A>
								</DL><p>
								<DT><A HREF="https://nlp.seas.harvard.edu/NamedTensor">Tensor Considered Harmful</A>
								<DT><A HREF="https://pytorch.org/docs/stable/named_tensor.html">Named Tensors ‚Äî PyTorch 2.3 documentation</A>
								<DT><A HREF="https://github.com/google-research/dex-lang">google-research/dex-lang: Research language for array processing in the Haskell/ML family</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md">Named Tensors using First-class Dimensions in PyTorch (pytorch/functorch/dim/README.md)</A>
							</DL><p>
							<DT><H3 FOLDED>Numpy</H3>
							<DL><p>
								<DT><A HREF="https://numpy.org/doc/stable/reference/generated/numpy.reshape.html">numpy.reshape ‚Äî NumPy v1.22 Manual</A>
								<DT><A HREF="https://twitter.com/awnihannun/status/1779133564619284894">advance indexing</A>
							</DL><p>
							<DT><H3 FOLDED>matmul</H3>
							<DL><p>
								<DT><H3 FOLDED>matmul-gemm</H3>
								<DL><p>
									<DT><A HREF="https://github.com/openai/openai-gemm">openai/openai-gemm: Open single and half precision gemm implementations</A>
									<DT><A HREF="https://engineering.fb.com/2018/11/07/ml-applications/fbgemm/">Open-sourcing FBGEMM for server-side inference - Engineering at Meta</A>
									<DT><A HREF="https://github.com/pytorch/FBGEMM">pytorch/FBGEMM: FB (Facebook) + GEMM (General Matrix-Matrix Multiplication) - https://code.fb.com/ml-applications/fbgemm/</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/3930f709ce01ada61b7d7a57935b5503ab72f1ed/media/docs/cute/0x_gemm_tutorial.md">cutlass/media/docs/cute/0x_gemm_tutorial.md at 3930f709ce01ada61b7d7a57935b5503ab72f1ed ¬∑ NVIDIA/cutlass</A>
									<DT><A HREF="https://mail.google.com/mail/u/0/#inbox?compose=DmwnWrRsqXmrkbzBZsQFbfJVfZvlwXCjzDchFHgDdHfCXVzFPPXgdvpvTmLnPxcltZHbpLcKZTKL">Inbox (3) - antonio.jfdominguez@gmail.com - Gmail</A>
									<DT><A HREF="https://arxiv.org/abs/1804.06826">[1804.06826] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking</A>
									<DT><A HREF="https://scholar.google.com/citations?hl=en&user=8d0x03EAAAAJ&view_op=list_works&sortby=pubdate">‚Ä™Zhe Jia‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
									<DT><A HREF="https://arxiv.org/pdf/1912.03413.pdf">https://arxiv.org/pdf/1912.03413.pdf</A>
									<DT><A HREF="https://arxiv.org/pdf/1804.06826.pdf%5B/url%5D">https://arxiv.org/pdf/1804.06826.pdf%5B/url%5D</A>
									<DT><A HREF="https://www.graphcore.ai/hubfs/assets/pdf/Citadel%20Securities%20Technical%20Report%20-%20Dissecting%20the%20Graphcore%20IPU%20Architecture%20via%20Microbenchmarking%20Dec%202019.pdf">https://www.graphcore.ai/hubfs/assets/pdf/Citadel%20Securities%20Technical%20Report%20-%20Dissecting%20the%20Graphcore%20IPU%20Architecture%20via%20Microbenchmarking%20Dec%202019.pdf</A>
									<DT><A HREF="https://github.com/vdumoulin/conv_arithmetic">vdumoulin/conv_arithmetic: A technical report on convolution arithmetic in the context of deep learning</A>
									<DT><A HREF="https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/enums.py">FBGEMM/fbgemm_gpu/fbgemm_gpu/enums.py at main ¬∑ pytorch/FBGEMM</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/native/cuda">pytorch/aten/src/ATen/native/cuda at main ¬∑ pytorch/pytorch</A>
									<DT><A HREF="https://github.com/sjfeng1999/gpu-arch-microbenchmark">sjfeng1999/gpu-arch-microbenchmark: Dissecting NVIDIA GPU Architecture</A>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/extra/gemm/torch_gemm.py">tinygrad/extra/gemm/torch_gemm.py at master ¬∑ tinygrad/tinygrad</A>
									<DT><A HREF="https://github.com/LaurentMazare/gemm">LaurentMazare/gemm</A>
									<DT><A HREF="https://github.com/Bruce-Lee-LY/cuda_hgemm">Bruce-Lee-LY/cuda_hgemm: Several optimization methods of half-precision general matrix multiplication (HGEMM) using tensor core with WMMA API and MMA PTX instruction.</A>
									<DT><A HREF="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">Mixed-input matrix multiplication performance optimizations ‚Äì Google Research Blog</A>
									<DT><A HREF="https://lwn.net/Articles/255364/">Memory part 5: What programmers can do [LWN.net]</A>
									<DT><A HREF="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications?utm_source=post-email-title&publication_id=1781836&post_id=142904770&utm_campaign=email-post-title&isFreemail=true&r=1mqy6n&triedRedirect=true&utm_medium=email">What Shapes Do Matrix Multiplications Like? [medium]</A>
								</DL><p>
								<DT><H3 FOLDED>matmul-sparsity</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/cHHillee/status/1785021827871596581">(1) Horace He en X: "Many don't know that GPUs automatically leverage ternary and fine-grained sparsity to accelerate your matmuls! e.g. A matmul with ternary + 90% sparsity results in 33% more FLOPs in my benchmark. (not joking) I explore this "optimization" here: https://t.co/YD3CTq1i7J (1/3) https://t.co/QQ7x8fmbfZ" / X</A>
									<DT><A HREF="https://www.thonking.ai/p/strangely-matrix-multiplications">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</A>
									<DT><A HREF="https://gist.github.com/Chillee/42e4635c59760a74cb3b4ba7ea5ad9f8#file-mm_weird-py">Strangely, Matrix Multiplications Run Faster When Given "Predictable" Data!</A>
								</DL><p>
								<DT><A HREF="https://github.com/KnowingNothing/MatmulTutorial">KnowingNothing/MatmulTutorial: A Easy-to-understand TensorOp Matmul Tutorial</A>
								<DT><A HREF="https://github.com/openai/openai-gemm">openai/openai-gemm: Open single and half precision gemm implementations</A>
								<DT><A HREF="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">Mixed-input matrix multiplication performance optimizations ‚Äì Google Research Blog</A>
								<DT><A HREF="https://arxiv.org/pdf/2006.16668.pdf">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma">matrix-multiply-accumulate (mma)</A>
								<DT><A HREF="https://gist.github.com/nadavrot/5b35d44e8ba3dd718e595e40184d03f0">Efficient matrix multiplication</A>
								<DT><A HREF="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications?utm_source=post-email-title&publication_id=1781836&post_id=142904770&utm_campaign=email-post-title&isFreemail=true&r=1mqy6n&triedRedirect=true&utm_medium=email">What Shapes Do Matrix Multiplications Like? [medium]</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Sk35MKtCXfQ&t=4743s">tinygrad: matrix multiplication, a@b, cube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=VgSQ1GOC86s&t=19510s">George Hotz | Programming | can you multiply a matrix? (noob lesson) | geohot/tinygrad/tree/gemm - YouTube</A>
								<DT><A HREF="https://github.com/matiaslindgren/cuda-memory-access-recorder/tree/master/examples">cuda-memory-access-recorder/examples at master ¬∑ matiaslindgren/cuda-memory-access-recorder</A>
								<DT><A HREF="https://github.com/BearNinja123/matmul">BearNinja123/matmul: A collection of matrix multiplication techniques in C, from naive to BLAS</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/examples/llm.c/ubench/matmul.c">tinygrad/examples/llm.c/ubench/matmul.c</A>
								<DT><A HREF="https://gist.github.com/geohot/0cad05378fcbaeb0dceec3e89e0d4d7b">A 1024x1024x1024 matmul with a 2x2x2 core in OpenCL</A>
								<DT><A HREF="https://twitter.com/cis_female/status/1771746532892586388">arithmetic intensity: easy appro min(m,n,k)</A>
								<DT><A HREF="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications">What Shapes Do Matrix Multiplications Like? [medium]</A>
								<DT><A HREF="https://gist.github.com/Chillee/abc38703f88fcb64683b6ccb0ae9d8ba">What Shapes Do Matrix Multiplications Like?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=VgSQ1GOC86s&t=19510s">George Hotz | Programming | can you multiply a matrix? (noob lesson)</A>
								<DT><A HREF="https://pytorch.org/blog/inside-the-matrix/">Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond | PyTorch</A>
								<DT><A HREF="https://github.com/ridgerchu/matmulfreellm">ridgerchu/matmulfreellm: Implementation for MatMul-free LM.</A>
								<DT><A HREF="https://github.com/tspeterkim/cuda-matmult">tspeterkim/cuda-matmult</A>
								<DT><A HREF="https://github.com/yester31/Matrix_Multiplication">yester31/Matrix_Multiplication: Diverse Matrix multiplication algorithms</A>
							</DL><p>
							<DT><H3 FOLDED>GEMM</H3>
							<DL><p>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/tree/c4fdb9c725924fd1bc8a89ca07a1f405953b4d54/extra/gemm">tinygrad/extra/gemm</A>
								<DT><A HREF="http://giantpandacv.com/project/CUDA/%E5%8F%AF%E8%83%BD%E6%98%AF%E8%AE%B2%E5%BE%97%E6%9C%80%E6%B8%85%E6%A5%9A%E7%9A%84WeightOnlyGEMM/">ÂèØËÉΩÊòØËÆ≤ÂæóÊúÄÊ∏ÖÊ•öÁöÑWeightOnlyGEMM - GiantPandaCV</A>
								<DT><A HREF="https://engineering.fb.com/2018/11/07/ml-applications/fbgemm/">Open-sourcing FBGEMM for server-side inference - Engineering at Meta</A>
								<DT><A HREF="https://github.com/pytorch/FBGEMM">pytorch/FBGEMM: FB (Facebook) + GEMM (General Matrix-Matrix Multiplication) - https://code.fb.com/ml-applications/fbgemm/</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/3930f709ce01ada61b7d7a57935b5503ab72f1ed/media/docs/cute/0x_gemm_tutorial.md">cutlass/media/docs/cute/0x_gemm_tutorial.md at 3930f709ce01ada61b7d7a57935b5503ab72f1ed ¬∑ NVIDIA/cutlass</A>
								<DT><A HREF="https://arxiv.org/abs/1804.06826">[1804.06826] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking</A>
								<DT><A HREF="https://scholar.google.com/citations?hl=en&user=8d0x03EAAAAJ&view_op=list_works&sortby=pubdate">‚Ä™Zhe Jia‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
								<DT><A HREF="https://arxiv.org/pdf/1912.03413.pdf">https://arxiv.org/pdf/1912.03413.pdf</A>
								<DT><A HREF="https://arxiv.org/pdf/1804.06826.pdf%5B/url%5D">https://arxiv.org/pdf/1804.06826.pdf%5B/url%5D</A>
								<DT><A HREF="https://www.graphcore.ai/hubfs/assets/pdf/Citadel%20Securities%20Technical%20Report%20-%20Dissecting%20the%20Graphcore%20IPU%20Architecture%20via%20Microbenchmarking%20Dec%202019.pdf">https://www.graphcore.ai/hubfs/assets/pdf/Citadel%20Securities%20Technical%20Report%20-%20Dissecting%20the%20Graphcore%20IPU%20Architecture%20via%20Microbenchmarking%20Dec%202019.pdf</A>
								<DT><A HREF="https://github.com/vdumoulin/conv_arithmetic">vdumoulin/conv_arithmetic: A technical report on convolution arithmetic in the context of deep learning</A>
								<DT><A HREF="https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/enums.py">FBGEMM/fbgemm_gpu/fbgemm_gpu/enums.py at main ¬∑ pytorch/FBGEMM</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/native/cuda">pytorch/aten/src/ATen/native/cuda at main ¬∑ pytorch/pytorch</A>
								<DT><A HREF="https://github.com/sjfeng1999/gpu-arch-microbenchmark">sjfeng1999/gpu-arch-microbenchmark: Dissecting NVIDIA GPU Architecture</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/extra/gemm/torch_gemm.py">tinygrad/extra/gemm/torch_gemm.py at master ¬∑ tinygrad/tinygrad</A>
								<DT><A HREF="https://github.com/LaurentMazare/gemm">LaurentMazare/gemm</A>
								<DT><A HREF="https://github.com/Bruce-Lee-LY/cuda_hgemm">Bruce-Lee-LY/cuda_hgemm: Several optimization methods of half-precision general matrix multiplication (HGEMM) using tensor core with WMMA API and MMA PTX instruction.</A>
								<DT><A HREF="https://github.com/yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs">yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs: Optimizing SGEMM kernel functions on NVIDIA GPUs to a close-to-cuBLAS performance.</A>
								<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/">GEMM kernels Hopper</A>
								<DT><A HREF="https://research.colfax-intl.com/adding-fp8-to-flashattention/">Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2 ‚Äì Colfax Research</A>
								<DT><A HREF="https://github.com/google/jax/pull/20462/">Add MegaBlox grouped matrix multiplication kernels for TPU. by copybara-service[bot] ¬∑ Pull Request #20462 ¬∑ google/jax</A>
								<DT><A HREF="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications?utm_source=post-email-title&publication_id=1781836&post_id=142904770&utm_campaign=email-post-title&isFreemail=true&r=1mqy6n&triedRedirect=true&utm_medium=email">What Shapes Do Matrix Multiplications Like? [medium]</A>
								<DT><A HREF="https://twitter.com/Si_Boehm/status/1610335205767933952">Iterative CUDA matrix multiply optimization (80% cuBLAS perf)</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/examples/00_basic_gemm/basic_gemm.cu">cutlass/examples/00_basic_gemm/basic_gemm.cu</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/66ef1df492f7bc9c8eeb01d7e14db01838e3f0bd/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py#L69">generate_kernels.py#L69</A>
								<DT><A HREF="https://github.com/wangzyon/NVIDIA_SGEMM_PRACTICE">wangzyon/NVIDIA_SGEMM_PRACTICE: Step-by-step optimization of CUDA SGEMM</A>
								<DT><A HREF="https://github.com/microsoft/microxcaling">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
								<DT><A HREF="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/sgemm.cpp">llamafile/llamafile/sgemm.cpp at main ¬∑ Mozilla-Ocho/llamafile</A>
								<DT><A HREF="https://github.com/flame/how-to-optimize-gemm">flame/how-to-optimize-gemm</A>
								<DT><A HREF="https://github.com/flame/how-to-optimize-gemm/wiki#step-by-step-optimizations">Approach to Optimizing Matrix-Matrix Multiplication - Step-by-Step</A>
								<DT><A HREF="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS: Fast Linear Algebra in CUDA C++ | NVIDIA Technical Blog</A>
								<DT><A HREF="https://leimao.github.io/article/CUDA-Matrix-Multiplication-Optimization/">CUDA Matrix Multiplication Optimization - Lei Mao's Log Book</A>
								<DT><A HREF="https://github.com/leimao/CUDA-GEMM-Optimization">leimao/CUDA-GEMM-Optimization: CUDA Matrix Multiplication Optimization</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/5c447dd84f8ae0e1d48ff9a2eae26ce8c4958101/python/cutlass/op/gemm.py#L137">cutlass/python/cutlass/op/gemm.py</A>
								<DT><A HREF="https://gist.github.com/Chillee/abc38703f88fcb64683b6ccb0ae9d8ba">What Shapes Do Matrix Multiplications Like?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Sk35MKtCXfQ&t=4743s">matrix multiplication, a@b, cube</A>
								<DT><A HREF="https://pytorch.org/blog/inside-the-matrix/">Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch/ao/blob/cb3bd8c674f2123af232a0231b5e38ddafa756a8/torchao/dtypes/aqt.py#L526">ao/torchao/dtypes/aqt.py</A>
							</DL><p>
							<DT><H3 FOLDED>linalg: standard Linear Algebra</H3>
							<DL><p>
								<DT><H3 FOLDED>BLAS</H3>
								<DL><p>
									<DT><H3 FOLDED>colmajor-rowmajor</H3>
									<DL><p>
										<DT><A HREF="https://www.netlib.org/blas/blast-forum/blas-report.pdf">Conventional Storage</A>
										<DT><A HREF="https://leimao.github.io/blog/Row-Major-VS-Column-Major/">Row-Major VS Column-Major - Lei Mao's Log Book</A>
									</DL><p>
									<DT><H3 FOLDED>BLAS-level-1</H3>
									<DL><p>
										<DT><A HREF="https://www.netlib.org/blas/snrm2.f90">SNRM2</A>
										<DT><A HREF="https://netlib.org/lapack/explore-html/d6/d12/snrm2_8f90_source.html">LAPACK: BLAS/SRC/snrm2.f90 Source File</A>
									</DL><p>
									<DT><A HREF="https://www.netlib.org/blas/">BLAS (Basic Linear Algebra Subprograms)</A>
									<DT><A HREF="https://www.netlib.org/blas/blast-forum/blas-report.pdf">Basic Linear Algebra Subprograms Technial (BLAST)</A>
									<DT><A HREF="https://github.com/flame/blis/">flame/blis: BLAS-like Library Instantiation Software Framework (main)</A>
									<DT><A HREF="https://github.com/flame">flame</A>
									<DT><A HREF="https://github.com/flame/blislab?tab=readme-ov-file">flame/blislab: BLISlab: A Sandbox for Optimizing GEMM</A>
									<DT><A HREF="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS: Fast Linear Algebra in CUDA C++ | NVIDIA Technical Blog</A>
									<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html">Matrix Multiplication Background User's Guide - NVIDIA Docs</A>
									<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/blob/df4a7534860137e060e18d2ebf019906120ea204/src/fastertransformer/kernels/matrix_transpose_kernels.cu#L4">FasterTransformer/src/fastertransformer/kernels/matrix_transpose_kernels.cu</A>
									<DT><A HREF="https://github.com/microsoft/BitBLAS">microsoft/BitBLAS: BitBLAS is a library to support mixed-precision matrix multiplications, especially for quantized LLM deployment.</A>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/fd27f19e9231160939da8a3a23434b4ae8ce51ee/docs/tensor/ops.md?plain=1">tinygrad/docs/tensor/ops.md</A>
								</DL><p>
								<DT><A HREF="https://www.netlib.org/blas/blast-forum/blas-report.pdf">Basic Linear Algebra Subprograms Technial (BLAST)</A>
								<DT><A HREF="https://en.cppreference.com/w/cpp/numeric/linalg">Basic linear algebra algorithms (since C++26) - cppreference.com</A>
								<DT><A HREF="https://numpy.org/doc/stable/reference/routines.linalg.html">Linear algebra (numpy.linalg) ‚Äî NumPy v1.26 Manual</A>
								<DT><A HREF="https://github.com/bytedance/byteir/blob/main/talks/c4ml23_poster.pdf">Linalg is All You Need to Optimize Attention</A>
								<DT><A HREF="https://www.openblas.net/">OpenBLAS : An optimized BLAS library</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cublas/">cuBLAS</A>
								<DT><A HREF="https://github.com/flame/blislab?tab=readme-ov-file">flame/blislab: BLISlab: A Sandbox for Optimizing GEMM</A>
								<DT><A HREF="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS: Fast Linear Algebra in CUDA C++ | NVIDIA Technical Blog</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/c4fdb9c725924fd1bc8a89ca07a1f405953b4d54/docs/tensor/ops.md?plain=1#L2">tinygrad/docs/tensor/ops.md</A>
							</DL><p>
							<DT><H3 FOLDED>Shape Suffixes</H3>
							<DL><p>
								<DT><A HREF="https://github.com/patrick-kidger/jaxtyping">patrick-kidger/jaxtyping: Type annotations and runtime checking for shape and dtype of JAX/NumPy/PyTorch/etc. arrays. https://docs.kidger.site/jaxtyping/</A>
								<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes ‚Äî Good Coding Style | by Noam Shazeer | Medium</A>
								<DT><A HREF="https://x.com/NoamShazeer/status/1762733550892401030">Noam Shazeer: Shape Suffixes ‚Äî Good Coding Style</A>
								<DT><A HREF="https://kidger.site/thoughts/jaxtyping/">No more shape errors! Type annotations for the shape+dtype of tensors/arrays</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md">Named Tensors using First-class Dimensions in PyTorch (pytorch/functorch/dim/README.md)</A>
								<DT><A HREF="https://github.com/joschu/jax-exp/blob/master/jax_transformer.py#L96">jax-exp/jax_transformer.py at master ¬∑ joschu/jax-exp</A>
								<DT><A HREF="https://github.com/xjdr-alt/simple_transformer">xjdr-alt/simple_transformer: Simple Transformer in Jax</A>
							</DL><p>
							<DT><H3 FOLDED>einsum</H3>
							<DL><p>
								<DT><A HREF="https://sankalp.bearblog.dev/einsum-new/">Shape Rotation 101: An Intro to Einsum and Jax Transformers | sankalp's blog</A>
								<DT><A HREF="https://x.com/dejavucoder/status/1804379391910383784">(1) sankalp en X: "new post is up. it's split up in two parts. in the first part, i talk about the einsum operation in detail. second part is all about understanding @_xjdr 's JAX transformer implementation (that has lots of einsums). https://t.co/5fJhorgoul https://t.co/JiZcg5cknC" / X</A>
								<DT><A HREF="https://jimypbr.github.io/2020/02/fast-ai-lesson-8-notes-backprop-from-the-foundations">go-seq | James Briggs' Blog</A>
								<DT><A HREF="https://nlp.seas.harvard.edu/NamedTensor">Tensor Considered Harmful</A>
								<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.einsum.html">torch.einsum ‚Äî PyTorch 2.3 documentation</A>
								<DT><A HREF="https://zhuanlan.zhihu.com/p/361209187">‰∏ÄÊñáÂ≠¶‰ºö Pytorch ‰∏≠ÁöÑ einsum - Áü•‰πé</A>
								<DT><A HREF="https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&mid=2247493772&idx=1&sn=4eea0e68f2e813fb1e474bf74f3c3f6e&chksm=9f83521aa8f4db0c2d2221d259b1ff7b8fca53024fdd4be9def3d1f439b0eebd494501223079&token=650657988&lang=zh_CN#rd">A literature on einsum in Pytorch</A>
							</DL><p>
							<DT><A HREF="https://developer.nvidia.com/blog/nvidia-research-tensors-are-the-future-of-deep-learning/">NVIDIA Research: Tensors Are the Future of Deep Learning</A>
							<DT><A HREF="https://stackoverflow.com/questions/26089893/understanding-numpys-einsum/47966452#47966452">einsum</A>
							<DT><A HREF="http://www.continuummechanics.org/tensornotationbasic.html">Tensor Notation (Basics)</A>
							<DT><A HREF="https://twitter.com/ayaka14732/media">Tweets con contenido multimedia de Ayaka (@ayaka14732) / Twitter</A>
							<DT><A HREF="https://link.springer.com/chapter/10.1007/978-3-030-74386-4_1">Tensor Computation | SpringerLink</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Hafo7hIl8MU">Tensor Puzzles: Let's Play - YouTube</A>
							<DT><A HREF="https://github.com/Bruce-Lee-LY/cuda_hgemm">Bruce-Lee-LY/cuda_hgemm: Several optimization methods of half-precision general matrix multiplication (HGEMM) using tensor core with WMMA API and MMA PTX instruction.</A>
							<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
							<DT><A HREF="https://theaisummer.com/einsum-attention/">Einsum: Transformer example</A>
							<DT><A HREF="https://github.com/xl0/lovely-tensors">xl0/lovely-tensors: Tensors, ready for human consumption</A>
							<DT><A HREF="https://github.com/snap-research/BitsFusion">snap-research/BitsFusion</A>
						</DL><p>
						<DT><H3 FOLDED>Assembler &amp; disassemblers</H3>
						<DL><p>
							<DT><A HREF="https://github.com/daadaada/turingas">daadaada/turingas: Assembler for NVIDIA Volta and Turing GPUs</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/tree/957e9800f15bb3b8727f56b9298433432f703d9f/disassemblers">tinygrad/disassemblers/adreno/disasm-a3xx.c</A>
							<DT><A HREF="https://github.com/cloudcores/CuAssembler">cloudcores/CuAssembler: An unofficial cuda assembler, for all generations of SASS, hopefully ÔºöÔºâ</A>
							<DT><A HREF="https://github.com/vosen/ZLUDA">vosen/ZLUDA: CUDA on AMD GPUs</A>
							<DT><A HREF="https://justine.lol/matmul/">disassembly for the C++ code I'm working on will pop up on the screen in a few milliseconds (llamafile)</A>
						</DL><p>
						<DT><H3 FOLDED>xformers</H3>
						<DL><p>
							<DT><A HREF="https://github.com/facebookresearch/xformers/blob/ad986981b141a218bf07bf968e920051ff2c7b41/xformers/benchmarks/benchmark_mem_eff_attention.py#L85">xformers/xformers/benchmarks/benchmark_mem_eff_attention.py at ad986981b141a218bf07bf968e920051ff2c7b41 ¬∑ facebookresearch/xformers</A>
						</DL><p>
						<DT><H3 FOLDED>LLVM</H3>
						<DL><p>
							<DT><H3 FOLDED>llvm-compiler-optimizations</H3>
							<DL><p>
								<DT><A HREF="https://github.com/google/ml-compiler-opt">google/ml-compiler-opt: Infrastructure for Machine Learning Guided Optimization (MLGO) in LLVM.</A>
								<DT><A HREF="https://lists.llvm.org/pipermail/llvm-dev/2020-April/140763.html">[llvm-dev] RFC: a practical mechanism for applying Machine Learning for optimization policies in LLVM</A>
							</DL><p>
							<DT><A HREF="https://github.com/banach-space/llvm-tutor">banach-space/llvm-tutor: A collection of out-of-tree LLVM passes for teaching and learning</A>
							<DT><A HREF="https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/?utm_source=twitter&utm_medium=organic_social&utm_content=link&utm_campaign=fair">Meta Large Language Model Compiler: Foundation Models of Compiler Optimization | Research - AI at Meta</A>
							<DT><A HREF="https://compilergym.com/">Indices and tables ‚Äî CompilerGym 0.2.5 documentation</A>
							<DT><A HREF="https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/?utm_source=linkedin&utm_medium=organic_social&utm_content=image&utm_campaign=fair">Meta Large Language Model Compiler: Foundation Models of Compiler Optimization | Research - AI at Meta</A>
						</DL><p>
						<DT><H3 FOLDED>WASM</H3>
						<DL><p>
							<DT><H3 FOLDED>wasm-internals</H3>
							<DL><p>
								<DT><A HREF="https://developer.mozilla.org/en-US/docs/WebAssembly/Understanding_the_text_format">Understanding WebAssembly text format - WebAssembly | MDN</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ojYEfRye6aE&t=13s">HELLO WEBASSEMBLY - wat</A>
								<DT><A HREF="https://developer.mozilla.org/en-US/docs/WebAssembly/Text_format_to_wasm">Converting WebAssembly text format to wasm - WebAssembly | MDN</A>
								<DT><A HREF="https://rustwasm.github.io/docs.html">Rust and WebAssembly Documentation | Rust and WebAssembly</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-C/C++</H3>
							<DL><p>
								<DT><H3 FOLDED>Emscripten</H3>
								<DL><p>
									<DT><A HREF="https://emscripten.org/">Main ‚Äî Emscripten 3.0.1-git (dev) documentation</A>
									<DT><H3 FOLDED>installation</H3>
									<DL><p>
										<DT><A HREF="https://emscripten.org/docs/building_from_source/toolchain_what_is_needed.html#toolchain-what-you-need">Emscripten Toolchain Requirements ‚Äî Emscripten 3.1.9</A>
										<DT><A HREF="https://formulae.brew.sh/formula/emscripten">emscripten ‚Äî Homebrew Formulae</A>
										<DT><A HREF="https://formulae.brew.sh/formula/llvm#default">llvm ‚Äî Homebrew Formulae</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://medium.com/@tdeniffel/pragmatic-compiling-from-c-to-webassembly-a-guide-a496cc5954b8">Pragmatic compiling of C++ to WebAssembly. A Guide. | by Thomas Deniffel | Medium</A>
								<DT><A HREF="https://web.dev/loading-wasm/">Loading WebAssembly modules efficiently</A>
								<DT><A HREF="https://nodejs.dev/learn/nodejs-with-webassembly">Node.js with WebAssembly</A>
								<DT><A HREF="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/Interacting-with-code.html">Interacting with code ‚Äî Emscripten 3.1.9-git (dev) documentation</A>
								<DT><A HREF="https://web.dev/emscripten-npm/">Emscripten and npm</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-wabt</H3>
							<DL><p>
								<DT><A HREF="https://github.com/WebAssembly/wabt">WebAssembly/wabt: The WebAssembly Binary Toolkit</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-wasi</H3>
							<DL><p>
								<DT><A HREF="https://github.com/WebAssembly/WASI">WebAssembly/WASI: WebAssembly System Interface</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-containers</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=ulZGjeFZirU&t=21s">WASM + Kubernetes: Beyond Containers - YouTube</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=VhCgep06-I8">Browserless app runtime in Rust - Demo app in Zig - Wasm/WebGPU - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=b5HHyb1d4Ys">Evolution of Wasm: Past, Present, Future - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ulZGjeFZirU&t=21s">WASM + Kubernetes: Beyond Containers - YouTube</A>
							<DT><A HREF="https://wasmcloud.dev/">wasmCloud Documentation</A>
							<DT><A HREF="https://pragprog.com/titles/khrust/programming-webassembly-with-rust/">Programming WebAssembly with Rust</A>
							<DT><A HREF="https://pspdfkit.com/blog/2017/webassembly-a-new-hope/">WebAssembly: A New Hope | PSPDFKit</A>
							<DT><A HREF="https://www.youtube.com/watch?v=fh9WXPu0hw8">Bringing WebAssembly outside the web with WASI by Lin Clark - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>GGML</H3>
						<DL><p>
							<DT><H3 FOLDED>llamafile</H3>
							<DL><p>
								<DT><A HREF="https://justine.lol/matmul/">LLaMA Now Goes Faster on CPUs</A>
								<DT><A HREF="https://github.com/Mozilla-Ocho/llamafile">Mozilla-Ocho/llamafile: Distribute and run LLMs with a single file.</A>
							</DL><p>
							<DT><H3 FOLDED>llama.cpp</H3>
							<DL><p>
								<DT><A HREF="https://github.com/abetlen/llama-cpp-python">abetlen/llama-cpp-python: Python bindings for llama.cpp</A>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/discussions/4508">Performance of llama.cpp on Apple Silicon A-series ¬∑ ggerganov/llama.cpp ¬∑ Discussion #4508</A>
							</DL><p>
							<DT><A HREF="https://github.com/marella/ctransformers">marella/ctransformers: Python bindings for the Transformer models implemented in C/C++ using GGML library.</A>
							<DT><A HREF="https://huggingface.co/crusoeai">crusoeai (Crusoe AI)</A>
							<DT><A HREF="https://github.com/antirez/gguf-tools">antirez/gguf-tools: GGUF implementation in C as a library and a tools CLI program</A>
							<DT><A HREF="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">ggml/docs/gguf.md at master ¬∑ ggerganov/ggml</A>
							<DT><A HREF="https://github.com/ggerganov/ggml/blob/master/src/ggml-quants.h">ggml/src/ggml-quants.h at master ¬∑ ggerganov/ggml</A>
						</DL><p>
						<DT><H3 FOLDED>ONNX</H3>
						<DL><p>
							<DT><H3 FOLDED>onnx-optimum</H3>
							<DL><p>
								<DT><A HREF="https://github.com/philschmid/optimum-static-quantization">philschmid/optimum-static-quantization</A>
							</DL><p>
							<DT><A HREF="https://github.com/microsoft/onnxscript">microsoft/onnxscript: ONNX Script enables author ONNX functions and models using a subset of Python.</A>
							<DT><A HREF="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization">ONNX Runtime</A>
							<DT><A HREF="https://onnx.ai/">ONNX | Home</A>
							<DT><A HREF="https://onnxruntime.ai/">ONNX Runtime | Home</A>
							<DT><A HREF="https://github.com/lutzroeder/netron">lutzroeder/netron: Visualizer for neural network, deep learning, and machine learning models</A>
							<DT><A HREF="https://github.com/lutzroeder/netron">lutzroeder/netron: Visualizer for neural networks</A>
							<DT><A HREF="https://onnxruntime.ai/docs/performance/olive.html">End to end optimization with Olive | onnxruntime</A>
							<DT><A HREF="https://github.com/microsoft/onnxscript/blob/main/docs/examples/04_plot_eager_mode_evaluation.py">onnxscript/docs/examples/04_plot_eager_mode_evaluation.py at main ¬∑ microsoft/onnxscript</A>
							<DT><A HREF="https://github.com/onnx/onnx/blob/main/docs/Operators.md">Operator Schemas</A>
							<DT><A HREF="https://github.com/onnx/onnx/blob/main/docs/Syntax.md">onnx/docs/Syntax.md at main ¬∑ onnx/onnx</A>
							<DT><A HREF="https://github.com/microsoft/Olive">microsoft/Olive: Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation.</A>
							<DT><A HREF="https://github.com/microsoft/onnxscript">microsoft/onnxscript: ONNX Script enables developers to naturally author ONNX functions and models using a subset of Python.</A>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html">Developer Guide :: NVIDIA Deep Learning TensorRT Documentation</A>
							<DT><A HREF="https://github.com/Ki6an/fastT5">Ki6an/fastT5: ‚ö° boost inference speed of T5 models by 5x &amp; reduce the model size by 3x.</A>
							<DT><A HREF="https://github.com/stars/pommedeterresautee/lists/quantization">pommedeterresautee's list / quantization</A>
						</DL><p>
						<DT><H3 FOLDED>Higher-order Virtual Machine 2</H3>
						<DL><p>
							<DT><A HREF="https://github.com/VictorTaelin">VictorTaelin (Victor Taelin)</A>
							<DT><A HREF="https://github.com/HigherOrderCO/HVM">HigherOrderCO/HVM: A massively parallel, optimal functional runtime in Rust</A>
							<DT><A HREF="https://github.com/HigherOrderCO/bend">HigherOrderCO/Bend</A>
						</DL><p>
						<DT><H3 FOLDED>Metal</H3>
						<DL><p>
							<DT><H3 FOLDED>metal-llama.cpp</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/discussions/4167">Performance of llama.cpp on Apple Silicon M-series ¬∑ ggerganov/llama.cpp ¬∑ Discussion #4167</A>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/discussions/4508">Performance of llama.cpp on Apple Silicon A-series ¬∑ ggerganov/llama.cpp ¬∑ Discussion #4508</A>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/discussions/4508#user-content-fn-4-533433ec2a70d995c2039ce1939985be">Performance of llama.cpp on Apple Silicon A-series ¬∑ ggerganov/llama.cpp ¬∑ Discussion #4508</A>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/tree/0e18b2e7d0b5c0a509ea40098def234b8d4a938a/examples/llama.swiftui">llama.cpp/examples/llama.swiftui</A>
							</DL><p>
							<DT><H3 FOLDED>metal-perf</H3>
							<DL><p>
								<DT><A HREF="https://github.com/tlkh/asitop">tlkh/asitop: Perf monitoring CLI tool for Apple Silicon</A>
							</DL><p>
							<DT><H3 FOLDED>metal-mlx</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ml-explore/mlx">ml-explore/mlx: MLX: An array framework for Apple silicon</A>
								<DT><A HREF="https://github.com/ml-explore/mlx-examples">ml-explore/mlx-examples: Examples in the MLX framework</A>
								<DT><A HREF="https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e">GPT from Scratch with MLX. Define and train GPT-2 on your MacBook | by Pranav Jadhav | Jun, 2024 | Towards Data Science</A>
							</DL><p>
							<DT><H3 FOLDED>metal-corenet</H3>
							<DL><p>
								<DT><A HREF="https://github.com/apple/corenet">apple/corenet: CoreNet: A library for training deep neural networks</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/atiorh/status/1737912777153609918">(Apple) Atila en X: "My takeaways from Apple's ‚ÄúLLM in a flash" (1/n)" / X</A>
						</DL><p>
						<DT><H3 FOLDED>Tenstorrent</H3>
						<DL><p>
							<DT><A HREF="https://github.com/tenstorrent-metal/tt-metal">tenstorrent-metal/tt-metal: ttnn - a python API and OP library. TT-Metalium - a low level kernel programming model</A>
						</DL><p>
						<DT><H3 FOLDED>ai-compilers-tracing</H3>
						<DL><p>
							<DT><A HREF="https://github.com/facebookresearch/xformers/blob/ad986981b141a218bf07bf968e920051ff2c7b41/xformers/benchmarks/benchmark_mem_eff_attention.py#L85">xformers/xformers/benchmarks/benchmark_mem_eff_attention.py at ad986981b141a218bf07bf968e920051ff2c7b41 ¬∑ facebookresearch/xformers</A>
							<DT><A HREF="https://github.com/facebookresearch/xformers/blob/ad986981b141a218bf07bf968e920051ff2c7b41/xformers/benchmarks/benchmark_mem_eff_attention.py#L85">xformers/xformers/benchmarks/benchmark_mem_eff_attention.py</A>
						</DL><p>
						<DT><H3 FOLDED>Exo</H3>
						<DL><p>
							<DT><A HREF="https://exo-lang.dev/">The Exo Language | Exo is a low-level language (and exocompiler) designed to help performance engineers write, optimize, and target high-performance computing kernels onto new hardware accelerators.</A>
							<DT><A HREF="https://github.com/exo-lang/ExoBLAS">exo-lang/ExoBLAS: BLAS implementation using Exo</A>
							<DT><A HREF="https://github.com/exo-lang">exo-lang</A>
						</DL><p>
						<DT><H3 FOLDED>GEMMINI</H3>
						<DL><p>
							<DT><A HREF="https://github.com/ucb-bar/gemmini">ucb-bar/gemmini: Berkeley's Spatial Array Generator</A>
							<DT><A HREF="https://people.eecs.berkeley.edu/~ysshao/assets/papers/genc2021-dac.pdf">Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration</A>
							<DT><A HREF="https://github.com/ucb-bar/chipyard">ucb-bar/chipyard: An Agile RISC-V SoC Design Framework with in-order cores, out-of-order cores, accelerators, and more</A>
							<DT><A HREF="https://www.chisel-lang.org/">Home | Chisel</A>
							<DT><A HREF="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">Mixed-input matrix multiplication performance optimizations ‚Äì Google Research Blog</A>
							<DT><A HREF="https://research.colfax-intl.com/adding-fp8-to-flashattention/">Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2 ‚Äì Colfax Research</A>
						</DL><p>
						<DT><H3 FOLDED>TVM</H3>
						<DL><p>
							<DT><A HREF="https://mlc.ai/chapter_graph_optimization/index.html#prelude">7. Computational Graph Optimization ‚Äî Machine Learing Compilation 0.0.1 documentation</A>
						</DL><p>
						<DT><H3 FOLDED>Mojo</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=3FKSlhZNdL0">Mojo Community Meeting #2 - YouTube</A>
							<DT><A HREF="https://www.modular.com/blog/mojo-vs-rust-is-mojo-faster-than-rust">Modular: Mojo vs. Rust: is Mojo üî• faster than Rust ü¶Ä ?</A>
						</DL><p>
						<DT><H3 FOLDED>stable-fast</H3>
						<DL><p>
							<DT><H3 FOLDED>stable-fast-installation</H3>
							<DL><p>
								<DT><A HREF="https://github.com/DataCrunch-io/inferno/blob/5b0633c8b5a9d6c84b7215b660e5e510ef00315c/Dockerfiles/Dockerfile.lcm#L24">Download the wheel corresponding to your system: pip3 install &lt;wheel file&gt;</A>
							</DL><p>
							<DT><A HREF="https://github.com/chengzeyi/stable-fast">chengzeyi/stable-fast: Best inference performance optimization framework for HuggingFace Diffusers on NVIDIA GPUs.</A>
							<DT><A HREF="https://www.vrushankdes.ai/diffusion-inference-optimization">Diffusion Inference Optimization</A>
						</DL><p>
						<DT><H3 FOLDED>PL</H3>
						<DL><p>
							<DT><H3 FOLDED>Python</H3>
							<DL><p>
								<DT><H3 FOLDED>py-virtual-environment</H3>
								<DL><p>
									<DT><H3 FOLDED>pipenv</H3>
									<DL><p>
										<DT><A HREF="https://pipenv.pypa.io/en/stable/">Pipenv</A>
										<DT><A HREF="https://pipenv-fork.readthedocs.io/en/latest/basics.html">Basic Usage of Pipenv</A>
										<DT><A HREF="https://stackoverflow.com/questions/50161551/set-python-version-when-creating-virtualenv-using-pipenv">Set python version when creating virtualenv</A>
									</DL><p>
									<DT><H3 FOLDED>py-virtual-environment-vanilla</H3>
									<DL><p>
										<DT><A HREF="https://docs.python.org/3/tutorial/venv.html">12. Virtual Environments and Packages ‚Äî Python 3.9.6 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>conda</H3>
									<DL><p>
										<DT><H3 FOLDED>conda-cloud</H3>
										<DL><p>
											<DT><A HREF="https://docs.anaconda.com/anaconda/install/linux/">Installing on Linux ‚Äî Anaconda documentation</A>
											<DT><A HREF="https://repo.anaconda.com/archive/">Index of conda</A>
											<DT><A HREF="https://stackoverflow.com/questions/28852841/install-anaconda-on-ubuntu-or-linux-via-command-line">python - Install Anaconda on Ubuntu (or Linux) via command line - Stack Overflow</A>
											<DT><A HREF="https://medium.com/data-professor/how-to-install-conda-on-google-colab-e7bbf9036f76">How to install conda on Google Colab</A>
											<DT><A HREF="https://askubuntu.com/questions/1268833/error-command-path-to-env-bin-python3-7-im-ensurepip-upgrade">python - Error: Command '['/path/to/env/bin/python3.7', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1 - Ask Ubuntu</A>
										</DL><p>
										<DT><H3 FOLDED>conda-scripts</H3>
										<DL><p>
											<DT><A HREF="https://github.com/fastai/fastsetup/blob/master/setup-conda.sh">fastsetup/setup-conda.sh at master ¬∑ fastai/fastsetup</A>
										</DL><p>
										<DT><A HREF="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">Managing environments ‚Äî conda 4.12.0</A>
										<DT><A HREF="https://stackoverflow.com/questions/41060382/using-pip-to-install-packages-to-anaconda-environment">python - Using Pip to install packages to conda env</A>
										<DT><A HREF="https://stackoverflow.com/questions/51042589/conda-version-pip-install-r-requirements-txt-target-lib">environment.yml</A>
										<DT><A HREF="https://datumorphism.leima.is/til/programming/python/python-anaconda-install-requirements/">Installing a specific Python pip version within the virtual environment</A>
										<DT><A HREF="https://repo.anaconda.com/archive/">Index of conda</A>
										<DT><A HREF="https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#viewing-a-list-of-your-environments">Managing environments ‚Äî conda 4.14.0.post16+8b846957c documentation</A>
										<DT><A HREF="https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html">Managing Python ‚Äî conda 4.14.0</A>
										<DT><A HREF="https://whiteboxml.com/blog/the-definitive-guide-to-python-virtual-environments-with-conda">5.2 Packaging a conda environment with conda-pack</A>
										<DT><A HREF="https://stackoverflow.com/questions/50005949/python-3-2-in-anaconda">Python 3.2 in anaconda - Stack Overflow</A>
										<DT><A HREF="https://stackoverflow.com/questions/66607225/adding-python-3-7-to-anaconda">pip - Adding Python 3.7 to Anaconda - Stack Overflow</A>
										<DT><A HREF="https://stackoverflow.com/questions/70205633/cannot-install-python-3-7-on-osx-arm64">conda - Cannot install Python 3.7 on osx-arm64 - Stack Overflow</A>
									</DL><p>
									<DT><H3 FOLDED>virtualenv</H3>
									<DL><p>
										<DT><A HREF="https://gist.github.com/Geoyi/d9fab4f609e9f75941946be45000632b">virtualenv man</A>
									</DL><p>
									<DT><A HREF="https://towardsdatascience.com/virtual-environments-104c62d48c54">A Guide to Python‚Äôs Virtual Environments</A>
									<DT><A HREF="https://towardsdatascience.com/python-and-the-module-search-path-e71ae7a7e65f">Python and the Module Search Path</A>
									<DT><A HREF="https://towardsdatascience.com/python-the-system-path-and-how-conda-and-pyenv-manipulate-it-234f8e8bbc3e">PATH and conda</A>
								</DL><p>
								<DT><H3 FOLDED>py-package-manager</H3>
								<DL><p>
									<DT><H3 FOLDED>py-brew</H3>
									<DL><p>
										<DT><A HREF="https://docs.brew.sh/Homebrew-and-Python">Python ‚Äî Homebrew Documentation</A>
									</DL><p>
									<DT><H3 FOLDED>pip</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>package-manager-uv</H3>
									<DL><p>
										<DT><A HREF="https://pypi.org/project/uv/">uv ¬∑ PyPI</A>
									</DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/46375576/get-the-list-of-packages-installed-in-anaconda">Get the list of packages installed in Anaconda</A>
									<DT><A HREF="https://stackoverflow.com/questions/41060382/using-pip-to-install-packages-to-anaconda-environment">python - Using Pip to install packages to Anaconda Environment</A>
									<DT><A HREF="https://github.com/pdm-project/pdm">pdm-project/pdm: A modern Python package and dependency manager supporting the latest PEP standards</A>
								</DL><p>
								<DT><H3 FOLDED>py-visualization</H3>
								<DL><p>
									<DT><A HREF="https://matplotlib.org/stable/tutorials/introductory/pyplot.html#">Pyplot tutorial ‚Äî Matplotlib 3.5.2 documentation</A>
									<DT><A HREF="https://twitter.com/andfanilo/status/1530505914981179392/photo/2">Official cheatsheet</A>
								</DL><p>
								<DT><H3 FOLDED>py-idioms</H3>
								<DL><p>
									<DT><A HREF="https://note.nkmk.me/en/python-tuple-list-unpack/">Unpack a tuple / list in Python</A>
									<DT><A HREF="https://www.freecodecamp.org/news/list-comprehension-in-python/">List Comprehension in Python Explained for Beginners</A>
									<DT><A HREF="https://towardsdatascience.com/elegant-and-efficient-usage-of-if-else-clauses-d41d3e88fe07">Elegant And Efficient Usage of If-Else Clauses</A>
									<DT><A HREF="https://towardsdatascience.com/8-more-python-best-practices-for-writing-industry-standard-code-64d97f42da5e">8 More Python Best Practices for Writing Industry-Standard Code</A>
									<DT><A HREF="https://www.w3schools.com/python/ref_dictionary_items.asp">Python Dictionary items() Method</A>
									<DT><A HREF="https://realpython.com/python-type-checking/">Python Type Checking (Guide)</A>
									<DT><A HREF="https://github.com/openai/openai-quickstart-python/blob/master/app.py">String template and formatting</A>
									<DT><A HREF="https://docs.python.org/3/library/functools.html">functools ‚Äî Higher-order functions and operations on callable objects</A>
									<DT><A HREF="https://pyneng.readthedocs.io/en/latest/book/additional_info/naming_conventions/underscore_names.html">Underscore in names - Python for network engineers</A>
									<DT><A HREF="https://docs.python.org/3/library/pprint.html">pprint ‚Äî Data pretty printer ‚Äî Python 3.10.6 documentation</A>
									<DT><A HREF="https://stackoverflow.com/questions/65214482/make-list-of-dictionaries-overwriting-one-key-entry-from-a-list-using-iterators">make list of dictionaries overwriting one key entry from a list using iterators</A>
									<DT><A HREF="https://github.com/lovasoa/marshmallow_dataclass">Dataclasses</A>
									<DT><A HREF="https://stackoverflow.com/questions/6578986/how-to-convert-json-data-into-a-python-object">How to convert JSON data into a Python object? - Stack Overflow</A>
									<DT><A HREF="https://towardsdatascience.com/how-to-use-variable-number-of-arguments-in-python-functions-d3a49a9b7db6">*args &amp; **kwargs</A>
									<DT><A HREF="https://www.programiz.com/python-programming/property">@property</A>
									<DT><A HREF="https://stackoverflow.com/questions/6981717/pythonic-way-to-combine-for-loop-and-if-statement">Combine for-loop and if-statement</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/model/wrapper.py#L501">model_class (dynamic import)</A>
								</DL><p>
								<DT><H3 FOLDED>py-build-system</H3>
								<DL><p>
									<DT><A HREF="https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/">pyproject.toml - pip documentation v22.2</A>
									<DT><A HREF="https://stackoverflow.com/questions/62983756/what-is-pyproject-toml-file-for">What is pyproject.toml file for?</A>
									<DT><A HREF="https://docs.python.org/3/library/functions.html#compile">Built-in Functions ‚Äî Python 3.10.8 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>py-fmt</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/yapf">google/yapf: A formatter for Python files</A>
									<DT><A HREF="https://realpython.com/python-f-strings/#multiline-f-strings">f-strings</A>
									<DT><A HREF="https://google.github.io/styleguide/pyguide.html">styleguide | Style guides for Google-originated open-source projects</A>
								</DL><p>
								<DT><H3 FOLDED>py-functional-programming</H3>
								<DL><p>
									<DT><A HREF="https://towardsdatascience.com/if-you-can-write-functions-you-can-use-dask-bbb6d8b3a248">If You Can Write Functions, You Can Use Dask</A>
									<DT><A HREF="https://github.com/kykosic/pycats">kykosic/pycats: Functional Python with Typeclasses and Categories</A>
								</DL><p>
								<DT><H3 FOLDED>py-profiling</H3>
								<DL><p>
									<DT><A HREF="https://blog.codingconfessions.com/p/python-profilers-intro">Everything You Wanted to Know About Profilers in Python (main)</A>
									<DT><A HREF="https://github.com/jrfonseca/gprof2dot">jrfonseca/gprof2dot: Converts profiling output to a dot graph.</A>
									<DT><A HREF="https://github.com/bloomberg/memray">bloomberg/memray: Memray is a memory profiler for Python</A>
								</DL><p>
								<DT><H3 FOLDED>py-configuration</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/gin-config">Gin provides a lightweight configuration framework for Python</A>
									<DT><A HREF="https://www.bitecode.dev/p/python-as-a-configuration-language">Python as a configuration language - Bite code!</A>
									<DT><A HREF="https://www.bitecode.dev/">Bite code! | Substack</A>
								</DL><p>
								<DT><H3 FOLDED>py-debug</H3>
								<DL><p>
									<DT><A HREF="https://docs.python.org/3/library/pdb.html">pdb ‚Äî The Python Debugger ‚Äî Python 3.10.5 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>py-utils</H3>
								<DL><p>
									<DT><A HREF="https://docs.python.org/3/library/contextlib.html">contextlib ‚Äî Utilities for with-statement contexts ‚Äî Python 3.10.6 documentation</A>
									<DT><A HREF="https://book.pythontips.com/en/latest/context_managers.html">27. Context Managers ‚Äî Python Tips 0.1 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>py-built-in</H3>
								<DL><p>
									<DT><H3 FOLDED>py-iterators-generators</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=yadfyn6-TzE">20240104 Iterators, Generators</A>
										<DT><A HREF="https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do">Iterable &amp; Generators &amp; Yield</A>
									</DL><p>
									<DT><H3 FOLDED>hasattr</H3>
									<DL><p>
										<DT><A HREF="https://www.w3schools.com/python/ref_func_hasattr.asp">Python hasattr() Function</A>
										<DT><A HREF="https://www.geeksforgeeks.org/python-hasattr-method/">Python hasattr() method - GeeksforGeeks</A>
									</DL><p>
									<DT><H3 FOLDED>magic methods</H3>
									<DL><p>
										<DT><A HREF="https://rszalski.github.io/magicmethods/">A Guide to Python's Magic Methods</A>
									</DL><p>
									<DT><H3 FOLDED>py-string-formattng</H3>
									<DL><p>
										<DT><A HREF="https://docs.python.org/3/tutorial/inputoutput.html">String Formattng: 7. Input and Output ‚Äî Python 3.10.6 documentation</A>
										<DT><A HREF="https://www.bitecode.dev/p/string-manipulations-python-beginners">String manipulations Python beginners should know</A>
									</DL><p>
									<DT><H3 FOLDED>built-in-pattern-matching</H3>
									<DL><p>
										<DT><A HREF="https://peps.python.org/pep-0636/">PEP 636 ‚Äì Structural Pattern Matching: Tutorial | peps.python.org</A>
									</DL><p>
									<DT><A HREF="https://docs.python.org/3/library/functions.html#dir">dir(): With an argument, attempt to return a list of valid attributes for that object.</A>
									<DT><A HREF="https://docs.python.org/3/library/os.html#os.system">os.system()</A>
									<DT><A HREF="https://docs.python.org/3/whatsnew/3.8.html">Walrus Operator</A>
									<DT><A HREF="https://docs.python.org/3/tutorial/inputoutput.html">String Formattng: 7. Input and Output ‚Äî Python 3.10.6 documentation</A>
									<DT><A HREF="https://docs.python.org/3/tutorial/classes.html">9. Classes ‚Äî Python 3.10.6 documentation</A>
									<DT><A HREF="https://www.geeksforgeeks.org/classmethod-in-python/">classmethod() in Python - GeeksforGeeks</A>
									<DT><A HREF="https://devtut.github.io/python/property-objects.html">Python - Property Objects</A>
									<DT><A HREF="https://stackoverflow.com/questions/34439/finding-what-methods-a-python-object-has">dir() - Finding what methods a Python object</A>
									<DT><A HREF="https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do">Iterable &amp; Generators &amp; Yield</A>
									<DT><A HREF="https://rszalski.github.io/magicmethods/">A Guide to Python's Magic Methods</A>
									<DT><A HREF="https://docs.python.org/3/library/shelve.html">shelve ‚Äî Python object persistence ‚Äî Python 3.12.0 documentation</A>
									<DT><A HREF="https://wiki.python.org/moin/UsingSlots">UsingSlots - Python Wiki</A>
									<DT><A HREF="https://www.bitecode.dev/p/python-variables-references-and-mutability">Python variables, references and mutability - Bite code!</A>
									<DT><A HREF="https://www.browserstack.com/guide/assert-in-python">Assert in Python: What is it and How to use it | BrowserStack</A>
								</DL><p>
								<DT><H3 FOLDED>py-async</H3>
								<DL><p>
									<DT><A HREF="https://superfastpython.com/asyncio-gather/">asyncio.gather()</A>
								</DL><p>
								<DT><H3 FOLDED>py-compiler</H3>
								<DL><p>
									<DT><H3 FOLDED>cython</H3>
									<DL><p>
										<DT><H3 FOLDED>ctypes</H3>
										<DL><p>
											<DT><A HREF="https://gist.github.com/fxkamd/ffd02d66a2863e444ec208ea4f3adc48">Observations about HSA and KFD backends in TinyGrad  (fast prototyping)</A>
											<DT><A HREF="https://pybind11.readthedocs.io/en/stable/">pybind11 documentation</A>
										</DL><p>
										<DT><H3 FOLDED>pxd &amp; pyx</H3>
										<DL><p>
											<DT><A HREF="https://cython.readthedocs.io/en/latest/src/tutorial/pxd_files.html">pxd files ‚Äî Cython 3.1.0a0 documentation</A>
											<DT><A HREF="https://github.com/rapidsai/cudf/tree/branch-0.7/python/cudf/bindings">cudf/python/cudf/bindings</A>
										</DL><p>
										<DT><A HREF="https://github.com/explosion/cython-blis?tab=readme-ov-file">cython-blis?tab=readme-ov-file</A>
										<DT><A HREF="https://cython.org/">Cython: C-Extensions for Python</A>
										<DT><A HREF="https://github.com/facebookincubator/cinder">facebookincubator/cinder: Cinder is Meta's internal performance-oriented production version of CPython.</A>
										<DT><A HREF="https://cython.readthedocs.io/en/latest/src/tutorial/pxd_files.html">pxd files ‚Äî Cython 3.1.0a0 documentation</A>
										<DT><A HREF="https://www.youtube.com/watch?v=GXwYjI9cJd0">Lightning Talk: Write Valid C++ and Python in One File - Roth Michaels - CppCon 2023 - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=MUISz2qA640&t=19s">Will Ada Replace C/C++? - YouTube</A>
										<DT><A HREF="https://trycinder.com/">Cinder Explorer</A>
										<DT><A HREF="https://github.com/mypyc/mypyc">mypyc/mypyc: Compile type annotated Python to fast C extensions</A>
										<DT><A HREF="https://blog.codingconfessions.com/p/cpython-memory-management-internals?utm_source=profile&utm_medium=reader2">CPython Memory Management Internals - by Abhinav Upadhyay</A>
									</DL><p>
									<DT><H3 FOLDED>clang2py</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/extra/nv_gpu_driver/codegen.sh">tinygrad/extra/nv_gpu_driver/codegen.sh at master ¬∑ tinygrad/tinygrad</A>
										<DT><A HREF="https://github.com/trolldbois/ctypeslib/blob/master/ctypeslib/clang2py.py">ctypeslib/ctypeslib/clang2py.py at master ¬∑ trolldbois/ctypeslib</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/de832d26c64a9ec575e47aeb58efe27a0ccf4e0b/autogen_stubs.sh">tinygrad/autogen_stubs.sh  (runtimes)</A>
									</DL><p>
									<DT><H3 FOLDED>LPython</H3>
									<DL><p>
										<DT><A HREF="https://lpython.org/blog/2023/07/lpython-novel-fast-retargetable-python-compiler/">LPython: Novel, Fast, Retargetable Python Compiler -</A>
										<DT><A HREF="https://github.com/lcompilers/lpython">lcompilers/lpython: Python compiler</A>
										<DT><A HREF="https://dev.lpython.org/">LPython</A>
									</DL><p>
									<DT><A HREF="https://lpython.org/blog/2023/07/lpython-novel-fast-retargetable-python-compiler/">LPython: Novel, Fast, Retargetable Python Compiler -</A>
									<DT><A HREF="https://pybind11.readthedocs.io/en/stable/">pybind11 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>py-PEP</H3>
								<DL><p>
									<DT><A HREF="https://peps.python.org/pep-0604/">PEP 604 ‚Äì Allow writing union types as X | Y | peps.python.org</A>
									<DT><A HREF="https://www.youtube.com/watch?v=YaDYUQ5mD5Q">NEW generic / alias syntax for python 3.12 (PEP 695) (intermediate) anthony explains #561 - YouTube</A>
									<DT><A HREF="https://peps.python.org/pep-0523/">PEP 523 ‚Äì Adding a frame evaluation API to CPython | peps.python.org</A>
								</DL><p>
								<DT><H3 FOLDED>py-type-system</H3>
								<DL><p>
									<DT><H3 FOLDED>py-types</H3>
									<DL><p>
										<DT><H3 FOLDED>TypeVar</H3>
										<DL><p>
											<DT><A HREF="https://discuss.python.org/t/differences-in-bound-object-vs-bound-any/27052">Differences in bound=object vs bound=Any - Python Help - Discussions on Python.org</A>
											<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/bb2b2959a222594dd8a41b8bb18d7b6fe280730a/server/text_generation_server/models/model.py#L12">HF-TGI: model.py#L12 TypeVar("B", bound=Batch)</A>
											<DT><A HREF="https://stackoverflow.com/questions/59933946/difference-between-typevart-a-b-and-typevart-bound-uniona-b">python - Difference between TypeVar('T', A, B) and TypeVar('T', bound=Union[A, B]) - Stack Overflow</A>
										</DL><p>
										<DT><A HREF="http://mypy-lang.org/">mypy - Optional Static Typing for Python</A>
										<DT><A HREF="https://stackoverflow.com/questions/60459641/how-do-you-get-mypy-to-recognize-a-newer-version-of-python">How do you get mypy to recognize a newer version of python? - Stack Overflow</A>
										<DT><A HREF="https://kobzol.github.io/rust/python/2023/05/20/writing-python-like-its-rust.html">Writing Python like it‚Äôs Rust | Kobzol‚Äôs blog</A>
										<DT><A HREF="https://github.com/kykosic/pycats">kykosic/pycats: Functional Python with Typeclasses and Categories</A>
										<DT><A HREF="https://github.com/twtrubiks/python-notes/blob/master/MappingProxyType_tutorial.py">python-notes/MappingProxyType_tutorial.py</A>
									</DL><p>
									<DT><H3 FOLDED>py-interfaces-classes</H3>
									<DL><p>
										<DT><A HREF="https://github.com/google/seqio/blob/1e3a46e690f4c867e7acca7e836c425a7b0f32a7/seqio/dataset_providers.py#L244">typing.Protocol: dataset_providers.py#L244 (seqio)</A>
									</DL><p>
									<DT><H3 FOLDED>static analyzer</H3>
									<DL><p>
										<DT><H3 FOLDED>mypy</H3>
										<DL><p>
											<DT><A HREF="https://mypy-lang.org/">mypy - Optional Static Typing for Python</A>
											<DT><A HREF="http://mypy-lang.org/">mypy - Optional Static Typing for Python</A>
											<DT><A HREF="https://stackoverflow.com/questions/60459641/how-do-you-get-mypy-to-recognize-a-newer-version-of-python">How do you get mypy to recognize a newer version of python? - Stack Overflow</A>
											<DT><A HREF="https://github.com/python/mypy/issues/4440">Proper way to type circular dependency</A>
											<DT><A HREF="https://vickiboykis.com/2023/12/11/why-if-type_checking/">Why if TYPE_CHECKING?</A>
											<DT><A HREF="https://www.stefaanlippens.net/circular-imports-type-hints-python.html">Yet another solution to dig you out of a circular import hole in Python - Stefaan Lippens inserts content here</A>
											<DT><A HREF="https://peps.python.org/pep-0484/#forward-references">PEP 484 ‚Äì Type Hints | peps.python.org</A>
											<DT><A HREF="https://mypy.readthedocs.io/en/stable/getting_started.html#strict-mode-and-configuration">Getting started - mypy 1.10.0 documentation</A>
											<DT><A HREF="https://www.youtube.com/watch?v=tH3Nul6jDQM">typing the untype-able with mypy plugins (advanced) anthony explains #574 - YouTube</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>dataclasses</H3>
									<DL><p>
										<DT><A HREF="https://docs.python.org/3/library/dataclasses.html">dataclasses ‚Äî Data Classes ‚Äî Python 3.12.3 documentation</A>
									</DL><p>
									<DT><A HREF="https://docs.python.org/3/library/functions.html#float">Types ‚Äî Python 3.10.5 documentation</A>
									<DT><A HREF="https://peps.python.org/pep-0484/">PEP 484 ‚Äì Type Hints | peps.python.org</A>
									<DT><A HREF="https://www.youtube.com/watch?v=YaDYUQ5mD5Q&t=308s">NEW generic / alias syntax for python 3.12 (PEP 695) (intermediate) anthony explains #561 - YouTube</A>
									<DT><A HREF="https://mypy.readthedocs.io/en/stable/protocols.html">Protocols and structural subtyping - mypy 1.9.0 documentation</A>
									<DT><A HREF="https://vickiboykis.com/2023/12/11/why-if-type_checking/">Why if TYPE_CHECKING?</A>
									<DT><A HREF="https://docs.python.org/3/library/typing.html#constant">typing ‚Äî Support for type hints ‚Äî Python 3.12.3 documentation</A>
									<DT><A HREF="https://stackoverflow.com/questions/61545580/how-does-mypy-use-typing-type-checking-to-resolve-the-circular-import-annotation">python - How does mypy use typing.TYPE_CHECKING to resolve the circular import annotation problem? - Stack Overflow</A>
									<DT><A HREF="https://www.youtube.com/watch?v=tH3Nul6jDQM">typing the untype-able with mypy plugins (advanced) anthony explains #574 - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>py-packaging</H3>
								<DL><p>
									<DT><H3 FOLDED>pyproject.toml</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/pull/2187/files#diff-50c86b7ed8ac2cf95bd48334961bf0530cdc77b5a56f852c5c61b89d735fd711">Modernize setup.py by Eliulm ¬∑ Pull Request #2187 ¬∑ tinygrad/tinygrad</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/d8a50d96f943c656672dc0cdb3f5fd3ed53abe57/setup.py">tinygrad/setup.py: keep for backwards compability</A>
										<DT><A HREF="https://gregoryszorc.com/blog/2023/10/30/my-user-experience-porting-off-setup.py/">Porting Off setup.py</A>
										<DT><A HREF="https://github.com/pypa/setuptools/issues/2088">Please do not remove `setup.py install` as it is needed for distribution packagers ¬∑ Issue #2088 ¬∑ pypa/setuptools</A>
										<DT><A HREF="https://setuptools.pypa.io/en/latest/userguide/dependency_management.html">optional dependencies: setuptools extra_require</A>
										<DT><A HREF="https://packaging.python.org/en/latest/guides/writing-pyproject-toml/">Writing your pyproject.toml - Python Packaging User Guide</A>
										<DT><A HREF="https://packaging.python.org/en/latest/specifications/dependency-specifiers/#dependency-specifiers">Dependency specifiers - Python Packaging User Guide</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/pull/2277">George Hotz suggestion: modernize setup.py</A>
									</DL><p>
									<DT><H3 FOLDED>setup.py</H3>
									<DL><p>
										<DT><A HREF="https://github.com/python-poetry/poetry">python-poetry/poetry: Python packaging and dependency management made easy</A>
										<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/setup.py">DeepSpeed/setup.py at master ¬∑ microsoft/DeepSpeed</A>
										<DT><A HREF="https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#flat-layout">Package Discovery and Namespace Packages - setuptools 69.2.0.post20240313 documentation</A>
										<DT><A HREF="https://xebia.com/blog/a-practical-guide-to-using-setup-py/">A Practical Guide to Using Setup.py - Xebia</A>
										<DT><A HREF="https://github.com/beeware/briefcase/issues/1270">Install a python requirement with an --extra-index-url argument</A>
									</DL><p>
									<DT><H3 FOLDED>py-build-system</H3>
									<DL><p>
										<DT><A HREF="https://build.pypa.io/en/latest/index.html">build 1.2.1</A>
										<DT><A HREF="https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/">pyproject.toml - pip documentation v22.2</A>
										<DT><A HREF="https://stackoverflow.com/questions/62983756/what-is-pyproject-toml-file-for">What is pyproject.toml file for?</A>
										<DT><A HREF="https://docs.python.org/3/library/functions.html#compile">Built-in Functions ‚Äî Python 3.10.8 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>wheel</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pypa/wheel">pypa/wheel: The official binary distribution format for Python</A>
										<DT><A HREF="https://wheel.readthedocs.io/en/stable/user_guide.html">installing wheels ‚Äî wheel 0.43.0 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>requirements.txt</H3>
									<DL><p>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/integration-tests/requirements.txt">text-generation-inference/integration-tests/requirements.txt</A>
									</DL><p>
									<DT><A HREF="https://github.com/python-poetry/poetry">python-poetry/poetry: Python packaging and dependency management made easy</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/setup.py">DeepSpeed/setup.py at master ¬∑ microsoft/DeepSpeed</A>
									<DT><A HREF="https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#flat-layout">Package Discovery and Namespace Packages - setuptools 69.2.0.post20240313 documentation</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/setup.py">DeepSpeed/setup.py at master</A>
									<DT><A HREF="https://xebia.com/blog/a-practical-guide-to-setuptools-and-pyproject-toml/">A Practical Guide to Setuptools and Pyproject.toml - Xebia</A>
									<DT><A HREF="https://peps.python.org/pep-0517/">PEP 517 ‚Äì A build-system independent format for source trees | peps.python.org</A>
									<DT><A HREF="https://peps.python.org/pep-0518/">PEP 518 ‚Äì Specifying Minimum Build System Requirements for Python Projects | peps.python.org</A>
								</DL><p>
								<DT><H3 FOLDED>py-cli</H3>
								<DL><p>
									<DT><H3 FOLDED>py-cli-typer</H3>
									<DL><p>
										<DT><A HREF="https://typer.tiangolo.com/">Typer</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/pyproject.toml">text-generation-server = 'text_generation_server.cli:app'</A>
										<DT><A HREF="https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#a-full-example">Writing your pyproject.toml - Python Packaging User Guide</A>
										<DT><A HREF="https://typer.tiangolo.com/tutorial/typer-command/">typer command - Typer</A>
									</DL><p>
									<DT><H3 FOLDED>py-cli-click</H3>
									<DL><p>
										<DT><A HREF="https://click.palletsprojects.com/en/8.1.x/">Welcome to Click ‚Äî Click Documentation (8.1.x)</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>py-serialization-deserialization</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ijl/orjson">ijl/orjson: Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy</A>
								</DL><p>
								<DT><H3 FOLDED>py-internals</H3>
								<DL><p>
									<DT><H3 FOLDED>py-bytecode</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>py-logging</H3>
									<DL><p>
										<DT><H3 FOLDED>loguru</H3>
										<DL><p>
											<DT><A HREF="https://github.com/Delgan/loguru">Delgan/loguru: Python logging made (stupidly) simple</A>
											<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/cli.py">text-generation-inference/server/text_generation_server/cli.py</A>
										</DL><p>
										<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/9b6ef9e1f0d8acaefd989440b27da9069aa69207/deepspeed/utils/logging.py">DeepSpeed/deepspeed/utils/logging.py</A>
										<DT><A HREF="https://docs.python.org/3/howto/logging.html#">Logging HOWTO ‚Äî Python 3.12.3 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>py-imports</H3>
									<DL><p>
										<DT><H3 FOLDED>importlib</H3>
										<DL><p>
											<DT><A HREF="https://docs.python.org/3/library/importlib.html">importlib ‚Äî The implementation of import ‚Äî Python 3.12.3 documentation</A>
											<DT><A HREF="https://github.com/wangzyon/pyInfer/blob/bff1d9800ffd773ab6745f2ea98d4a83dfdb032a/pyinfer/utils/common/config.py#L40">import_modules_from_strings</A>
											<DT><A HREF="https://github.com/huggingface/diffusers/blob/3511a9623f5beabf360df44cc7cb78e33d13ff4e/src/diffusers/utils/import_utils.py#L764">diffusers/src/diffusers/utils/import_utils.py</A>
										</DL><p>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=e6zFlbEU76I">Python 3 Gets TONS of New Features | Prime News - YouTube</A>
									<DT><A HREF="https://www.bitecode.dev/">Bite code! | Substack</A>
									<DT><A HREF="https://www.python.org/">Welcome to Python.org</A>
									<DT><A HREF="https://www.bitecode.dev/p/whats-up-python-the-gil-removed-a">What's up, Python? The GIL removed, a new compiler, optparse deprecated...</A>
									<DT><A HREF="https://github.com/LaurentMazare/hojo">LaurentMazare/hojo: A small python library to run iterators in a separate process</A>
									<DT><A HREF="https://www.geeksforgeeks.org/how-to-get-list-of-parameters-name-from-a-function-in-python/">How to get list of parameters name from a function in Python? - GeeksforGeeks</A>
									<DT><A HREF="https://www.bitecode.dev/p/python-variables-references-and-mutability">Python variables, references and mutability - Bite code!</A>
									<DT><A HREF="https://www.bitecode.dev/p/whats-up-python-the-gil-removed-a">What's up, Python? The GIL removed, a new compiler</A>
									<DT><A HREF="https://www.geeksforgeeks.org/how-to-get-list-of-parameters-name-from-a-function-in-python/">How to get list of parameters name from a function</A>
									<DT><A HREF="https://github.com/twtrubiks/python-notes/blob/master/MappingProxyType_tutorial.py">python-notes/MappingProxyType_tutorial.py</A>
									<DT><A HREF="https://www.teach.cs.toronto.edu/~csc110y/fall/notes/06-memory-model/04-python-memory-model-1.html">6.4 The Python Memory Model: Introduction</A>
									<DT><A HREF="https://www.youtube.com/@MichaelFoord/videos">py core object model, closures, decorators, references, iterators</A>
								</DL><p>
								<DT><H3 FOLDED>py-context-managers</H3>
								<DL><p>
									<DT><A HREF="https://docs.python.org/3/library/contextlib.html">contextlib ‚Äî Utilities for with-statement contexts ‚Äî Python 3.10.6 documentation</A>
									<DT><A HREF="https://book.pythontips.com/en/latest/context_managers.html">27. Context Managers ‚Äî Python Tips 0.1 documentation</A>
									<DT><A HREF="https://lwn.net/Articles/706424/">Python context managers [LWN.net]</A>
								</DL><p>
								<DT><H3 FOLDED>py-testing</H3>
								<DL><p>
									<DT><H3 FOLDED>pytest</H3>
									<DL><p>
										<DT><H3 FOLDED>pytest-examples</H3>
										<DL><p>
											<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/pytest.ini">TensorRT-LLM/tests/pytest.ini at main ¬∑ NVIDIA/TensorRT-LLM</A>
											<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/tests/unit/ops/adam/test_adamw.py">DeepSpeed/tests/unit/ops/adam/test_adamw.py</A>
											<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/integration-tests/conftest.py#L284">text-generation-inference/integration-tests/conftest.py</A>
										</DL><p>
										<DT><H3 FOLDED>pytest-debug</H3>
										<DL><p>
											<DT><A HREF="https://chatgpt.com/c/07ff34de-b541-4a89-9242-bb2db5aff30f">Pytest Debugging Options</A>
										</DL><p>
										<DT><H3 FOLDED>syrupy</H3>
										<DL><p>
										</DL><p>
										<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-2-moving">Testing with Python (part 2): moving to pytest</A>
										<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-3-pytest">Testing with Python (part 3): pytest setup - Bite code!</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/dc514df2afad386739bf8471ab351a86d5c5ffc7/test/conftest.py#L4">pytorch/test/conftest.py</A>
										<DT><A HREF="https://docs.pytest.org/en/7.1.x/how-to/parametrize.html">How to parametrize fixtures and test functions ‚Äî pytest documentation</A>
										<DT><A HREF="https://github.com/tophat/syrupy">tophat/syrupy: :pancakes: The sweeter pytest snapshot plugin</A>
									</DL><p>
									<DT><H3 FOLDED>unittest</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/test/test_tensor.py">tinygrad/test/test_tensor.py at master ¬∑ tinygrad/tinygrad</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/test_layer.py">TensorRT-LLM/tests/test_layer.py</A>
										<DT><A HREF="https://docs.python.org/3/library/unittest.html#classes-and-functions">unittest ‚Äî Unit testing framework ‚Äî Python 3.12.3 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>property-based testing</H3>
									<DL><p>
										<DT><H3 FOLDED>hypothesis</H3>
										<DL><p>
											<DT><A HREF="https://www.cs.toronto.edu/~david/course-notes/csc110-111/04-function-specification-and-correctness/04-testing-functions-2.html">4.4 Testing Functions II: hypothesis</A>
											<DT><A HREF="https://www.inspiredpython.com/course/testing-with-hypothesis/testing-your-python-code-with-hypothesis">Testing your Python Code with Hypothesis ‚Ä¢ Inspired Python</A>
											<DT><A HREF="https://hypothesis.readthedocs.io/en/latest/quickstart.html">Quick start guide ‚Äî Hypothesis 6.100.2 documentation</A>
											<DT><A HREF="https://hypothesis.readthedocs.io/en/latest/data.html">What you can generate and how ‚Äî Hypothesis 6.100.2 documentation</A>
											<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/test/test_tensor.py">tinygrad/test/test_tensor.py at master</A>
										</DL><p>
									</DL><p>
									<DT><A HREF="https://abseil.io/docs/python/guides/testing">abseil / Testing</A>
									<DT><A HREF="https://github.com/ezyang/expecttest">ezyang/expecttest ("golden" tests)</A>
									<DT><A HREF="https://github.com/abseil/abseil-py/tree/main">abseil/abseil-py: Abseil Common Libraries (Python)</A>
									<DT><A HREF="https://www.bitecode.dev/p/xmas-decoration-part-1">Xmas decoration, part 1 - Bite code!</A>
									<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-1-the-basics">Testing with Python (part 1): the basics - Bite code!</A>
									<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-2-moving">Testing with Python (part 2): moving to pytest</A>
									<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-4-why-and?utm_source=post-email-title&publication_id=1516188&post_id=144370735&utm_campaign=email-post-title&isFreemail=true&r=1tutvb&triedRedirect=true&utm_medium=email">Testing with Python (part 4): why and what to test?</A>
									<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-5-the-different">Testing with Python (part 5): the different types of tests</A>
									<DT><A HREF="https://github.com/george0st/qgate-perf">george0st/qgate-perf: Performance tests under quality gate solution.</A>
								</DL><p>
								<DT><H3 FOLDED>py-refactor</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookincubator/Bowler">facebookincubator/Bowler: Safe code refactoring for modern Python.</A>
								</DL><p>
								<DT><H3 FOLDED>python-linter-code-formater</H3>
								<DL><p>
									<DT><H3 FOLDED>ruff</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/ruff.toml">tinygrad/ruff.toml at master ¬∑ tinygrad/tinygrad</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://towardsdatascience.com/if-you-can-write-functions-you-can-use-dask-bbb6d8b3a248">If You Can Write Functions, You Can Use Dask</A>
								<DT><A HREF="https://github.com/abseil/abseil-py/tree/main">abseil/abseil-py: Abseil Common Libraries (Python)</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/9b6ef9e1f0d8acaefd989440b27da9069aa69207/deepspeed/utils/logging.py">DeepSpeed/deepspeed/utils/logging.py</A>
								<DT><A HREF="https://www.youtube.com/watch?v=yadfyn6-TzE">20240104 Iterators, Generators - YouTube</A>
								<DT><A HREF="https://towardsdatascience.com/virtual-environments-104c62d48c54">A Guide to Python‚Äôs Virtual Environments</A>
								<DT><A HREF="https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/convert.py#L4">errors as values: safetensors/bindings/python/convert.py</A>
								<DT><A HREF="https://www.youtube.com/watch?v=SzL2Oo3RktU">Breaking up long lines of code in Python - YouTube</A>
								<DT><H3 FOLDED>py-imports</H3>
								<DL><p>
									<DT><H3 FOLDED>py-lazy-imports</H3>
									<DL><p>
										<DT><A HREF="https://github.com/chengzeyi/stable-fast/blob/fffe290680ec2ddc01f511e8e7fc62357ed901d8/src/sfast/dynamo/backends/registry.py#L4">stable-fast/src/sfast/dynamo/backends/registry.py</A>
										<DT><A HREF="https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py">optuna/optuna/integration/__init__.py at master</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/42cae93b942ec904ead46c26c42be24422adc92c/src/diffusers/utils/import_utils.py#L760">diffusers/src/diffusers/utils/import_utils.py</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/67bef2027cc461af5bbe73b3c0f35bb1350f5aa8/src/diffusers/pipelines/consistency_models/__init__.py">diffusers/src/diffusers/pipelines/consistency_models/__init__.py</A>
									</DL><p>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>Rust</H3>
							<DL><p>
								<DT><H3 FOLDED>rust-installation</H3>
								<DL><p>
									<DT><A HREF="https://www.rust-lang.org/tools/install">Install Rust - Rust Programming Language</A>
									<DT><A HREF="https://forge.rust-lang.org/infra/other-installation-methods.html">Other Installation Methods - Rust Forge</A>
									<DT><A HREF="https://sourabhbajaj.com/mac-setup/Rust/">Rust ¬∑ macOS Setup Guide</A>
								</DL><p>
								<DT><H3 FOLDED>rust-procedural-macros</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=geovSK3wMB8">Procedural Macros in Rust (part 1) - YouTube</A>
									<DT><A HREF="https://github.com/dtolnay/cargo-expand">dtolnay/cargo-expand: Subcommand to show result of macro expansion</A>
									<DT><A HREF="https://github.com/dtolnay/proc-macro-workshop">dtolnay/proc-macro-workshop: Learn to write Rust procedural macros‚ÄÉ‚ÄÉ[Rust Latam conference, Montevideo Uruguay, March 2019]</A>
								</DL><p>
								<DT><H3 FOLDED>rust-python-bindings</H3>
								<DL><p>
									<DT><A HREF="https://github.com/prql/prql/tree/main/prql-python">prql/prql-python at main ¬∑ prql/prql</A>
									<DT><A HREF="https://github.com/PyO3/pyo3">PyO3/pyo3: Rust bindings for the Python interpreter</A>
									<DT><A HREF="https://pyo3.rs/v0.14.5/index.html">Introduction - PyO3 user guide</A>
									<DT><A HREF="https://github.com/google/autocxx">google/autocxx: Tool for safe ergonomic Rust/C++ interop driven from existing C++ headers</A>
									<DT><A HREF="https://github.com/LaurentMazare/hojo">LaurentMazare/hojo: A small python library to run iterators in a separate process</A>
								</DL><p>
								<DT><H3 FOLDED>rust-logging</H3>
								<DL><p>
									<DT><A HREF="https://users.rust-lang.org/t/unable-to-enable-anything-above-info-logging/27470">Unable to enable anything above INFO logging - help - The Rust Programming Language Forum</A>
									<DT><A HREF="https://rust-lang-nursery.github.io/rust-cookbook/development_tools/debugging/log.html">Log Messages - Rust Cookbook</A>
								</DL><p>
								<DT><H3 FOLDED>rust-async</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=kSQ9-JSl0z4">Rust Live | Asynchronous Rust - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=gkU4NGSe21I">Zed Decoded: Async Rust</A>
								</DL><p>
								<DT><H3 FOLDED>rust-debug</H3>
								<DL><p>
									<DT><A HREF="https://lldb.llvm.org/">LLDB Homepage ‚Äî The LLDB Debugger</A>
									<DT><A HREF="https://www.youtube.com/watch?v=8D74GaBIYI4">Rust: GDB debugging - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>rust-people</H3>
								<DL><p>
									<DT><A HREF="https://github.com/oovm?tab=stars">oovm (SasakiSaki) / Starred</A>
									<DT><A HREF="https://github.com/kykosic">kykosic (Kyle Kosic) (xAI &amp; OpenAI)</A>
									<DT><A HREF="https://github.com/LaurentMazare">LaurentMazare (Laurent Mazare) (Huggingface)</A>
								</DL><p>
								<DT><H3 FOLDED>rust-crates</H3>
								<DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/66915951/rust-use-vs-mod">import - Rust use vs mod?</A>
									<DT><A HREF="https://github.com/rust-itertools/itertools">rust-itertools/itertools: Extra iterator adaptors, iterator methods, free functions, and macros.</A>
								</DL><p>
								<DT><H3 FOLDED>rust-parallel</H3>
								<DL><p>
									<DT><A HREF="https://docs.rs/rayon/1.8.0/rayon/index.html">rayon: Data-parallelism to convert sequential computations into parallel</A>
								</DL><p>
								<DT><H3 FOLDED>rust-internals</H3>
								<DL><p>
									<DT><H3 FOLDED>rustc</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=q2vJ8Faundw">Parallel</A>
									</DL><p>
									<DT><H3 FOLDED>Borrow Checker</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=KVwP6nY4xgA">Rust's borrow rules: but why, really?</A>
										<DT><A HREF="https://doc.rust-lang.org/1.8.0/book/references-and-borrowing.html">References and Borrowing</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>cgo</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=KYdlqhb267c&list=LL&index=4">RustConf 2023 - Integrating Rust and Go: Lessons from Github Code Search - YouTube</A>
									<DT><A HREF="https://pkg.go.dev/cmd/cgo">cgo command - cmd/cgo - Go Packages</A>
									<DT><A HREF="https://doc.rust-lang.org/nomicon/ffi.html">FFI - The Rustonomicon</A>
									<DT><A HREF="https://github.com/ollama/ollama/blob/main/llm/ext_server/ext_server.h">ollama/llm/ext_server/ext_server.h extern "C"</A>
								</DL><p>
								<DT><H3 FOLDED>cargo</H3>
								<DL><p>
									<DT><A HREF="https://crates.io/crates/cargo-update">cargo-update - crates.io: Rust Package Registry</A>
								</DL><p>
								<DT><A HREF="https://doc.rust-lang.org/stable/rust-by-example/mod/visibility.html">Visibility - Rust By Example</A>
								<DT><A HREF="https://replit.com/@antferdom/rustlings#.replit">rustlings - Replit</A>
								<DT><A HREF="https://doc.rust-lang.org/cargo/commands/cargo-tree.html">cargo-tree - Display a tree visualization of a dependency graph</A>
								<DT><A HREF="https://doc.rust-lang.org/book/ch02-00-guessing-game-tutorial.html">The Rust Programming Language (main)</A>
								<DT><A HREF="https://www.thecodeteacher.com/question/96947/How-do-I-%22use%22-or-import-a-local-Rust-file?">mod: Include internal code</A>
								<DT><A HREF="https://learning-rust.github.io/docs/a4.cargo,crates_and_basic_project_structure.html">Cargo, Crates and Basic Project Structure | Learning Rust</A>
								<DT><A HREF="https://github.com/clap-rs/clap/blob/master/examples/cargo-example.md">clap: command line argparser</A>
								<DT><A HREF="https://www.youtube.com/watch?v=D1NAREuicNs">Flame Graphs and ASM optimizations</A>
								<DT><A HREF="https://dev.to/tangram/writing-the-fastest-gbdt-libary-in-rust-197k">Writing the fastest GBDT libary in Rust - DEV Community</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-h1oa6GYvV8">Rust Memory Ordering (Atomics and Locks Chapter 3) - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=QHjSJC6yrs8&t=20s">Programming in Rust: Enums, Tagged Unions, Memory Layout and Pattern Matching - YouTube</A>
								<DT><A HREF="https://github.com/slawlor/ractor">slawlor/ractor: Rust actor framework</A>
								<DT><A HREF="https://github.com/LaurentMazare/tch-rs">LaurentMazare/tch-rs: Rust bindings for the C++ api of PyTorch.</A>
								<DT><A HREF="https://github.com/kykosic/actix-pytorch-example/tree/master">kykosic/actix-pytorch-example: An example of using Torch rust bindings to serve trained machine learning models via Actix Web</A>
								<DT><A HREF="https://github.com/sxyazi/yazi">sxyazi/yazi: üí• Blazing fast terminal file manager written in Rust, based on async I/O.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=VJsPd24gByY">Episode 006: Zig and Rust - YouTube</A>
								<DT><A HREF="https://github.com/matklad/xshell">matklad/xshell</A>
								<DT><A HREF="https://github.com/dsherret/dax">dsherret/dax: Cross platform shell tools for Deno inspired by zx.</A>
								<DT><A HREF="https://github.com/google/zx">google/zx: A tool for writing better scripts</A>
								<DT><A HREF="https://google.github.io/comprehensive-rust/">Welcome to Comprehensive Rust ü¶Ä - Comprehensive Rust ü¶Ä</A>
								<DT><A HREF="https://github.com/google/comprehensive-rust">google/comprehensive-rust: This is the Rust course used by the Android team at Google. It provides you the material to quickly teach Rust.</A>
								<DT><A HREF="https://github.com/google/zerocopy">google/zerocopy</A>
								<DT><A HREF="https://github.com/google/autocxx">google/autocxx: Tool for safe ergonomic Rust/C++ interop driven from existing C++ headers</A>
								<DT><A HREF="https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html">Recoverable Errors with Result - The Rust Programming Language</A>
								<DT><A HREF="https://dev.to/chaudharypraveen98/form-validation-in-rust-404l">Form Validation in Rust (Actix-Web) - DEV Community</A>
								<DT><A HREF="https://www.youtube.com/watch?v=KVwP6nY4xgA">Rust's borrow rules: but why, really?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=g6mUtBVESb0">Rust's trait system is a proof engine, let's make it prove us an ABI!</A>
								<DT><A HREF="https://github.com/LukeMathWalker/pavex/tree/main">LukeMathWalker/pavex: An easy-to-use Rust framework for building robust and performant APIs</A>
								<DT><A HREF="https://github.com/nadavrot/compressor">nadavrot/compressor: An educational implementation of a modern compressor in Rust</A>
								<DT><A HREF="https://www.youtube.com/watch?v=fpkjmE-56Gw">Rust Allocators and Memory Management - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Kdpfhj3VM04">Compiler-Driven Development in Rust - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>C++</H3>
							<DL><p>
								<DT><H3 FOLDED>cpp-installation</H3>
								<DL><p>
									<DT><A HREF="http://www-scf.usc.edu/~csci104/20142/installation/gccmac.html">CSCI 104 ‚Äì Installing G++ on a Mac</A>
									<DT><A HREF="https://www.moncefbelyamani.com/how-to-install-xcode-homebrew-git-rvm-ruby-on-mac/">including M1 Apple Silicon</A>
									<DT><A HREF="https://stackoverflow.com/questions/68880134/gdb-no-bottle-available-gdb-install">macos - gdb: no bottle available-gdb install (ARM not supported)</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-compilation</H3>
								<DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/3178342/compiling-a-c-program-with-gcc">Compiling a C++ program with gcc - Stack Overflow</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-comptime</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=5eneQ9mFbpA">The Superpower of C++ - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-memory-model</H3>
								<DL><p>
									<DT><H3 FOLDED>RAII</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://medium.com/swlh/writing-c-when-youre-a-java-developer-memory-management-7c42e222645e">Memory Management</A>
									<DT><A HREF="https://isocpp.org/wiki/faq/freestore-mgmt">Memory Management</A>
									<DT><A HREF="https://thenumb.at/rpp/">Oxidizing C++</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-libc</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>cpp-standard</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>stl-algorithms</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=zlJg9mCNfkQ">Thrust and the C++ Standard Algorithms - Conor Hoekstra - GTC 2021 - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=W2tWOdzgXHA">GoingNative 2013 C++ Seasoning</A>
									<DT><A HREF="https://www.youtube.com/watch?v=h4Jl1fk3MkQ">CppCon 2016: Marshall Clow ‚ÄúSTL Algorithms - why you should use them, and how to write your own" - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=2olsGf6JIkU">CppCon 2018: Jonathan Boccara ‚Äú105 STL Algorithms in Less Than an Hour‚Äù - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=48gV1SNm3WA">C++Now 2019: Conor Hoekstra ‚ÄúAlgorithm Intuition‚Äù</A>
									<DT><A HREF="https://github.com/codereport/Content/tree/main/Talks">Content/Talks at main ¬∑ codereport/Content</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-error-handling</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=uj9ozuzZy6g">C++23's std::expected - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>carbon</H3>
								<DL><p>
									<DT><A HREF="https://github.com/carbon-language/carbon-lang">carbon-language/carbon-lang: Carbon Language's main repository: documents, design, implementation, and related tools. (NOTE: Carbon Language is experimental; see README)</A>
								</DL><p>
								<DT><H3 FOLDED>cosmopolitan</H3>
								<DL><p>
									<DT><A HREF="https://github.com/jart/cosmopolitan">jart/cosmopolitan: build-once run-anywhere c library</A>
								</DL><p>
								<DT><A HREF="https://medium.com/swlh/writing-c-when-youre-a-java-developer-memory-management-7c42e222645e">Memory Management</A>
								<DT><A HREF="https://isocpp.org/wiki/faq/freestore-mgmt">Memory Management</A>
								<DT><A HREF="https://www.youtube.com/watch?v=BP6NxVxDQIs&t=37s">CppCon 2016: Timur Doumler ‚ÄúWant fast C++? Know your hardware!" - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Qv1Yn-3lvtU">Refactoring to C++23 with GCC 13 - YouTube</A>
								<DT><A HREF="https://github.com/carbon-language/carbon-lang">carbon-language/carbon-lang: Carbon Language's main repository: documents, design, implementation, and related tools. (NOTE: Carbon Language is experimental; see README)</A>
								<DT><A HREF="https://github.com/harrism/cpp11-range/tree/70f844968c5f669ce85f8ce4cbd24a3584c57f4b">harrism/cpp11-range</A>
								<DT><A HREF="https://omairmajid.com/posts/2020-07-08-what-is-glibcxx-error/">2020-07-08-what-is-glibcxx-error</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-erXR6k9TeE">C++Now 2018: Rong Lu ‚ÄúC++ Development with Visual Studio Code‚Äù - YouTube</A>
								<DT><A HREF="https://learn.microsoft.com/en-us/cpp/cpp/object-lifetime-and-resource-management-modern-cpp?view=msvc-170">Object lifetime and resource management (RAII) | Microsoft Learn</A>
								<DT><A HREF="https://learn.microsoft.com/en-us/cpp/cpp/templates-cpp?view=msvc-170&source=recommendations">Templates (C++) | Microsoft Learn</A>
								<DT><A HREF="https://github.com/google/autocxx">google/autocxx: Tool for safe ergonomic Rust/C++ interop driven from existing C++ headers</A>
								<DT><A HREF="https://github.com/google/mosaic">google/mosaic: A C++ bindings generator for Rust.</A>
								<DT><A HREF="https://thenumb.at/rpp/">Oxidizing C++</A>
								<DT><A HREF="https://github.com/federico-busato/Modern-CPP-Programming">federico-busato/Modern-CPP-Programming: Modern C++ Programming Course (C++03/11/14/17/20/23/26)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=FnMfhWiSweo">Low-Latency Trading Systems in C++: Templated Meta-State Machines in HFT - Jason McGuiness - ACCU 23 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=fsIfQqRjc1U">1 Problem, 4 C++'s - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=YBtnqaMTfHg">Glean: Code Indexing at Meta - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=50sQUgBZCIA">{fmt}: The Cool Parts - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MUISz2qA640&t=19s">Will Ada Replace C/C++? - YouTube</A>
								<DT><A HREF="https://gist.github.com/Chillee/fbd504a65312893df1b402624042a965">Educational implementations</A>
								<DT><A HREF="https://gist.github.com/Chillee/c729ac9d1995665ea9426226c4203ca5">elapsed_time</A>
								<DT><A HREF="https://github.com/mcinglis/c-style">mcinglis/c-style: My favorite C programming practices.</A>
								<DT><A HREF="https://www.geeksforgeeks.org/exit0-vs-exit1-in-c-c-with-examples/">exit(0) vs exit(1) in C/C++ with Examples - GeeksforGeeks</A>
								<DT><A HREF="https://www.youtube.com/watch?v=g7CCaRwRVBQ&list=PL71Y0EmrppR0KyZvQWj63040UEzKQU7n8">Advanced C #1: Function Pointers - YouTube</A>
								<DT><A HREF="https://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html">What Every C Programmer Should Know About Undefined Behavior #1/3 - The LLVM Project Blog</A>
								<DT><A HREF="https://www.youtube.com/watch?v=n7Tl1qJxTew">Uninitialized Uses in Systems C++ Programming: The Bytes Before the C++ Types - JF Bastien - YouTube</A>
								<DT><A HREF="https://github.com/CppOnlineConference/CppOnline2024">CppOnlineConference/CppOnline2024: Slide Repository For CppOnline 2024</A>
							</DL><p>
							<DT><H3 FOLDED>Zig</H3>
							<DL><p>
								<DT><H3 FOLDED>zig-build-system</H3>
								<DL><p>
									<DT><A HREF="https://ziglearn.org/chapter-3/">Chapter 3 - Build system | ziglearn.org</A>
									<DT><A HREF="https://www.youtube.com/watch?v=-XLSyaJ6m3o&t=902s">WTF is Build.Zig? by Ed Yu - YouTube</A>
									<DT><A HREF="https://github.com/kimmolinna/duckdb-zig-build/">kimmolinna/duckdb-zig-build: DuckDB is an in-process SQL OLAP Database Management System</A>
									<DT><A HREF="https://github.com/akhildevelops/cudaz">akhildevelops/cudaz: A Zig Cuda wrapper</A>
								</DL><p>
								<DT><H3 FOLDED>zig-people</H3>
								<DL><p>
									<DT><A HREF="https://github.com/fengb">fengb (Benjamin Feng)</A>
								</DL><p>
								<DT><H3 FOLDED>zig-http-web-server</H3>
								<DL><p>
									<DT><A HREF="https://github.com/zigzap/zap">zigzap/zap: blazingly fast backends in zig</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=_WccWcx0p4k">Zig overview</A>
								<DT><A HREF="https://www.youtube.com/watch?v=iZFXAN8kpPo">Advanced Hello World in Zig - Loris Cro - YouTube</A>
								<DT><A HREF="https://log2base2.com/c-with-dsa?utm_src=youtube&utm_target=ycwdeug1&gclid=CjwKCAjwt52mBhB5EiwA05YKo4S_xtW1JucfNad8OMbuQj0b_tg5eej5VfMOx5M8PdeFGvIrlEiHeRoCxHkQAvD_BwE">Learn C Programming | Pointers Visualization | Log2Base2</A>
								<DT><A HREF="https://www.youtube.com/watch?v=vHWiDx_l4V0">What's a Memory Allocator Anyway? - Benjamin Feng - YouTube</A>
								<DT><A HREF="https://github.com/fengb/zee_alloc">fengb/zee_alloc: tiny Zig allocator primarily targeting WebAssembly</A>
								<DT><A HREF="https://www.youtube.com/watch?v=CwXixVcliP0">How to Build Software From Source - Andrew Kelley - Software You Can Love Vancouver 2023 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=x1N9JPPPC18&t=4s">Proficient Parallel Programming - King Butcher - Software You Can Love VC 2023 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=8MbREuiLQrM">Zig Compiler Internals - Andrew Kelley - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=1N85yU6RMcY">Ziglibc: Sweeping out the rug from underneath C - Jonathan Marler - Software You Can Love 2022 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=BiYPrMjPU60">Zig Lexer : Finished it!!! - YouTube</A>
								<DT><A HREF="https://github.com/unum-cloud/ucall">unum-cloud/ucall: Remote Procedure Calls - 50x lower latency and 70x higher bandwidth than FastAPI, implementing REST &amp; JSON-RPC over io_uring and SIMDJSON ‚òéÔ∏è</A>
								<DT><A HREF="https://www.youtube.com/watch?v=VJsPd24gByY">Episode 006: Zig and Rust - YouTube</A>
								<DT><A HREF="https://github.com/allyourcodebase/zlib">allyourcodebase/zlib: https://www.zlib.net/</A>
								<DT><A HREF="https://www.youtube.com/watch?v=xIPrwrBAU2c">Zig Data Structure Katas - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=D5XTnYAgIp0">Episode 013: Prepare Repair - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=a--v9mt8ep0">Episode 21: Grid.Write - YouTube</A>
								<DT><A HREF="https://zig.news/edyu/zig-package-manager-wtf-is-zon-2-0110-update-1jo3">Zig Package Manager 2 - WTF is Build.Zig.Zon and Build.Zig (0.11.0 Update) - Zig NEWS</A>
								<DT><A HREF="https://www.youtube.com/watch?v=pnnx1bkFXng">i changed my mind about zig - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>Go</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ollama/ollama/blob/main/llm/ext_server/ext_server.h">ollama/llm/ext_server/ext_server.h extern "C"</A>
								<DT><A HREF="https://github.com/tliron/py4go">tliron/py4go: Tight bidirectional integration between Go and Python</A>
								<DT><A HREF="https://github.com/tliron/py4go/tree/main/examples/hello-world">py4go/examples/hello-world at main ¬∑ tliron/py4go</A>
								<DT><A HREF="https://github.com/bytedance/gopkg?tab=readme-ov-file">bytedance/gopkg: Universal Utilities for Go</A>
							</DL><p>
							<DT><H3 FOLDED>Java</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=BaUrpq_7KMk">How Netflix Really Uses Java - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>Gleam</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=D88S_RdagP8">Introduction to Gleam's Concurrency Model</A>
								<DT><A HREF="https://www.youtube.com/@lpil">Louis Pilfold - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>array programming</H3>
							<DL><p>
								<DT><A HREF="https://www.brainstobytes.com/hands-on-numpy-universal-functions-and-array-oriented-programming/">Hands-on NumPy(IV): Universal Functions and Array-oriented Programming</A>
								<DT><A HREF="https://link.springer.com/chapter/10.1007/978-3-030-74386-4_1">Tensor Computation | SpringerLink</A>
								<DT><A HREF="https://www.youtube.com/watch?v=aFal9-SJjGY&list=PL_lsbAsL_o2BivkGLiDfHY9VqWlaNoZ2O&index=40">Keynote: The Promise of PyTorch as a General-Purpose Array-Oriented Computational..- Travis Oliphant - YouTube</A>
							</DL><p>
						</DL><p>
						<DT><A HREF="https://research.google/blog/advancements-in-machine-learning-for-machine-learning/">Advancements in machine learning for machine learning (Google AI)</A>
						<DT><A HREF="https://carpedm30.notion.site/AI-Compiler-Study-aaf4cff2c8734e50ad95ac6230dbd80b">‚ôüÔ∏è AI Compiler Study</A>
						<DT><A HREF="https://gist.github.com/sophiawisdom/ccdff5b7ebcd782393dbc5be3f0866f9">shittytransformer.py</A>
						<DT><A HREF="https://www.thonking.ai/p/strangely-matrix-multiplications">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</A>
						<DT><A HREF="https://github.com/daadaada/turingas">daadaada/turingas: Assembler for NVIDIA Volta and Turing GPUs</A>
						<DT><A HREF="https://github.com/banach-space/llvm-tutor">banach-space/llvm-tutor: A collection of out-of-tree LLVM passes for teaching and learning</A>
						<DT><A HREF="https://github.com/yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs">yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs: Optimizing SGEMM kernel functions on NVIDIA GPUs to a close-to-cuBLAS performance.</A>
						<DT><A HREF="https://twitter.com/cis_female/status/1737448740620013751/photo/1">Sophia: Quantitative Analsysis</A>
						<DT><A HREF="https://startup.jobs/graph-compiler-engineer-openai-4658227">Graph Compiler Engineer at OpenAI</A>
						<DT><A HREF="https://github.com/IST-DASLab">IST Austria Distributed Algorithms and Systems Lab</A>
						<DT><A HREF="https://github.com/microsoft/Olive">microsoft/Olive: Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation.</A>
						<DT><A HREF="https://pytorchtoatoms.substack.com/p/nvidia-quantum-x800-next-generation">NVIDIA Quantum-X800: Next Generation Infiniband 800Gbit/s Network Topology</A>
						<DT><A HREF="https://x.com/PytorchToAtoms">(1) Pytorch To Atoms (@PytorchToAtoms) / X</A>
						<DT><A HREF="https://www.youtube.com/watch?v=XD9DqcZOB3A">Cornell ECE 5545: Guest Lecture: CentML Gennady Pekhimenko - YouTube</A>
						<DT><A HREF="https://github.com/dorjeduck/llm.mojo">dorjeduck/llm.mojo: port of Andrjey Karpathy's llm.c to Mojo</A>
						<DT><A HREF="https://research.colfax-intl.com/tutorial-python-binding-for-cuda-libraries-in-pytorch/">Tutorial: Python bindings for CUDA libraries in PyTorch ‚Äì Colfax Research</A>
						<DT><A HREF="https://twitter.com/cis_female/status/1782576009239560336/photo/1">(1) sophia (chrysanthemum princess) (@cis_female) / X</A>
						<DT><A HREF="https://twitter.com/cis_female/status/1660386176761724928">(1) sophia (chrysanthemum princess) en X: "I implemented the transformer in 30 minutes and 30 lines of python which compiles to 6670 instructions of gpu microcode (lower-level assembly), or ~100kb total (twitter js is ~1000kb). This shitty implementation achieves 40% of the A100's *THEORETICAL MAXIMUM* performance" / X</A>
						<DT><A HREF="https://gist.github.com/sophiawisdom/4b3886a251d0728625dd8d1f76e9eb60">cublas fp16 matmul</A>
						<DT><A HREF="https://gist.github.com/sophiawisdom/b8e63dc8ce6037b6032eeb010b93e446">layernorm</A>
						<DT><A HREF="https://github.com/facebookincubator/dynolog/tree/main?tab=readme-ov-file#gpu-monitoring">facebookincubator/dynolog: Dynolog is a telemetry daemon for performance monitoring and tracing. It exports metrics from different components in the system like the linux kernel, CPU, disks, Intel PT, GPUs etc. Dynolog also integrates with pytorch and can trigger traces for distributed training applications.</A>
						<DT><A HREF="https://www.youtube.com/watch?v=6BiNzPdy6YA">[REFAI Seminar 04/16/24] ML for ML Compilers at Google - YouTube</A>
						<DT><A HREF="https://www.modular.com/blog/how-to-be-confident-in-your-performance-benchmarking">Modular: How to Be Confident in Your Performance Benchmarking</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/IEEE_754">IEEE 754 - Wikipedia</A>
						<DT><A HREF="https://github.com/chengzeyi/yatc">chengzeyi/yatc: A Pure Python Deep Learning Compiler</A>
					</DL><p>
					<DT><H3 FOLDED>sw-large-transformer-model-inference-optimization</H3>
					<DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-quantization</H3>
						<DL><p>
							<DT><H3 FOLDED>Post-training quantization (PTQ)</H3>
							<DL><p>
								<DT><A HREF="https://www.philschmid.de/static-quantization-optimum">Static Quantization with HF Optimum</A>
								<DT><A HREF="https://www.philschmid.de/bert-deepspeed-inference">Accelerate BERT inference with DeepSpeed-Inference on GPUs</A>
								<DT><H3 FOLDED>LLM.int8()</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/joaoalvarenga/bloom-8bit">bloom-8bit</A>
									<DT><A HREF="https://huggingface.co/blog/hf-bitsandbytes-integration">A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes</A>
									<DT><A HREF="https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es">GPT-J 8-bit compression</A>
								</DL><p>
								<DT><H3 FOLDED>int4</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/NolanoOrg/status/1635409631530057728">LLaMa int4</A>
									<DT><A HREF="https://github.com/openai/triton/issues/675">int4 support ¬∑ Issue #675 ¬∑ openai/triton</A>
								</DL><p>
								<DT><H3 FOLDED>PyTorch</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization ‚Äî PyTorch 2.0 documentation</A>
									<DT><A HREF="https://github.com/Xilinx/brevitas">Xilinx/brevitas: Brevitas: neural network quantization in PyTorch</A>
								</DL><p>
								<DT><A HREF="https://blog.speechmatics.com/gpu-quantisation">Fast and Accurate GPU Quantization for Transformers</A>
							</DL><p>
							<DT><H3 FOLDED>Mixed Precision Training</H3>
							<DL><p>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">Train With Mixed Precision - NVIDIA Docs</A>
								<DT><A HREF="https://arxiv.org/pdf/1710.03740.pdf">MIXED PRECISION TRAINING</A>
								<DT><A HREF="https://www.youtube.com/watch?v=0y0X97TJPcY&list=PLUHjJ91-nf0T4vYN0Wqg_nCF-Vo9CkGdJ">PyTorch - Introduction and Tensors - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>Transformer Engine (FP8)</H3>
							<DL><p>
								<DT><A HREF="https://github.com/cchan/nanoGPT-fp8">cchan/nanoGPT-fp8</A>
								<DT><A HREF="https://twitter.com/itsclivetime/status/1655515089506820097">(1) Clive Chan en Twitter: "WIP FP8 training on consumer graphics cards - üßµ/4 I hacked nanoGPT to use TransformerEngine on RTX 4090 and ran a few iterations of GPT-2 training: - nanoGPT Block (+flashattn) =&amp;gt; TE TransformerLayer (both BF16): 15% faster - BF16 =&amp;gt; FP8: additional +18% https://t.co/cJNWehoGeu" / Twitter</A>
								<DT><A HREF="https://blogs.bing.com/Engineering-Blog/october-2021/Bing-delivers-more-contextualized-search-using-quantized-transformer-inference-on-NVIDIA-GPUs-in-Azu">Microsoft's Bing example</A>
							</DL><p>
							<DT><H3 FOLDED>Activation-aware Weight Quantization</H3>
							<DL><p>
								<DT><A HREF="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq: AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</A>
								<DT><A HREF="https://twitter.com/jilin_14/status/1683377972840124417">TinyChat (RTX4090)</A>
							</DL><p>
							<DT><A HREF="https://github.com/mit-han-lab/smoothquant">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</A>
							<DT><A HREF="https://arxiv.org/pdf/2004.09602.pdf">Integer Quantization For Deep Learing Inference: Principles and Evaluation</A>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html">Transformer Engine documentation ‚Äî Transformer Engine 0.7.0 documentation</A>
							<DT><A HREF="https://github.com/IST-DASLab/gptq">IST-DASLab/gptq: Code for the ICLR 2023 paper "GPTQ: Accurate Post-training Quantization of Generative Pretrained Transformers".</A>
							<DT><A HREF="https://huggingface.co/docs/transformers/main_classes/quantization#fp4-quantization">Quantize ü§ó Transformers models</A>
							<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
							<DT><A HREF="https://github.com/IST-DASLab/marlin">IST-DASLab/marlin: FP16xINT4 LLM inference kernel that can achieve near-ideal ~4x speedups up to medium batchsizes of 16-32 tokens.</A>
							<DT><A HREF="https://github.com/IST-DASLab/QUIK">IST-DASLab/QUIK: Repository for the QUIK project, enabling the use of 4bit kernels for generative inference</A>
							<DT><H3 FOLDED>sw-Post-training quantization (PTQ)</H3>
							<DL><p>
								<DT><A HREF="https://www.philschmid.de/static-quantization-optimum">Static Quantization with HF Optimum</A>
								<DT><A HREF="https://www.philschmid.de/bert-deepspeed-inference">Accelerate BERT inference with DeepSpeed-Inference on GPUs</A>
								<DT><H3 FOLDED>LLM.int8()</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/joaoalvarenga/bloom-8bit">bloom-8bit</A>
									<DT><A HREF="https://huggingface.co/blog/hf-bitsandbytes-integration">A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes</A>
									<DT><A HREF="https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es">GPT-J 8-bit compression</A>
								</DL><p>
								<DT><H3 FOLDED>int4</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/NolanoOrg/status/1635409631530057728">LLaMa int4</A>
									<DT><A HREF="https://github.com/openai/triton/issues/675">int4 support ¬∑ Issue #675 ¬∑ openai/triton</A>
								</DL><p>
								<DT><H3 FOLDED>PyTorch</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization ‚Äî PyTorch 2.0 documentation</A>
									<DT><A HREF="https://github.com/Xilinx/brevitas">Xilinx/brevitas: Brevitas: neural network quantization in PyTorch</A>
								</DL><p>
								<DT><A HREF="https://blog.speechmatics.com/gpu-quantisation">Fast and Accurate GPU Quantization for Transformers</A>
							</DL><p>
							<DT><H3 FOLDED>sw-Mixed Precision Training</H3>
							<DL><p>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">Train With Mixed Precision - NVIDIA Docs</A>
								<DT><A HREF="https://arxiv.org/pdf/1710.03740.pdf">MIXED PRECISION TRAINING</A>
								<DT><A HREF="https://www.youtube.com/watch?v=0y0X97TJPcY&list=PLUHjJ91-nf0T4vYN0Wqg_nCF-Vo9CkGdJ">PyTorch - Introduction and Tensors - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>sw-Transformer Engine (FP8)</H3>
							<DL><p>
								<DT><A HREF="https://github.com/cchan/nanoGPT-fp8">cchan/nanoGPT-fp8</A>
								<DT><A HREF="https://twitter.com/itsclivetime/status/1655515089506820097">(1) Clive Chan en Twitter: "WIP FP8 training on consumer graphics cards - üßµ/4 I hacked nanoGPT to use TransformerEngine on RTX 4090 and ran a few iterations of GPT-2 training: - nanoGPT Block (+flashattn) =&amp;gt; TE TransformerLayer (both BF16): 15% faster - BF16 =&amp;gt; FP8: additional +18% https://t.co/cJNWehoGeu" / Twitter</A>
								<DT><A HREF="https://blogs.bing.com/Engineering-Blog/october-2021/Bing-delivers-more-contextualized-search-using-quantized-transformer-inference-on-NVIDIA-GPUs-in-Azu">Microsoft's Bing example</A>
							</DL><p>
							<DT><H3 FOLDED>sw-Activation-aware Weight Quantization</H3>
							<DL><p>
								<DT><A HREF="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq: AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</A>
								<DT><A HREF="https://twitter.com/jilin_14/status/1683377972840124417">TinyChat (RTX4090)</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-sparsity</H3>
						<DL><p>
							<DT><H3 FOLDED>DeepSpeed</H3>
							<DL><p>
								<DT><A HREF="https://www.deepspeed.ai/tutorials/sparse-attention/#how-to-config-sparsity-structures">DeepSpeed Sparse Attention</A>
							</DL><p>
							<DT><H3 FOLDED>Pruning</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://github.com/EleutherAI/gpt-neox/blob/main/configs/sparse.yml">gpt-neox/sparse.yml at main ¬∑ EleutherAI/gpt-neox ¬∑ GitHub</A>
							<DT><A HREF="https://docs.cerebras.net/en/latest/wsc/how_to_guides/sparsity.html#id1">Train a model with weight sparsity (Beta) ‚Äî Cerebras Developer Documentation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=4gKYE9-YtP0">MICRO'23 TorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs - YouTube</A>
							<DT><A HREF="https://github.com/AlibabaResearch/flash-llm">AlibabaResearch/flash-llm: Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity</A>
							<DT><A HREF="https://github.com/ptillet/torch-blocksparse">ptillet/torch-blocksparse: Block-sparse primitives for PyTorch</A>
							<DT><A HREF="https://github.com/IST-DASLab/SparseFinetuning">IST-DASLab/SparseFinetuning: Repository for Sparse Finetuning of LLMs via modified version of the MosaicML llmfoundry</A>
							<DT><H3 FOLDED>sw-transformer-inference-sparsity-DeepSpeed</H3>
							<DL><p>
								<DT><A HREF="https://www.deepspeed.ai/tutorials/sparse-attention/#how-to-config-sparsity-structures">DeepSpeed Sparse Attention</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-pruning</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-architectural-optimization</H3>
						<DL><p>
							<DT><H3 FOLDED>Flash-Attention</H3>
							<DL><p>
								<DT><A HREF="https://github.com/HazyResearch/flash-attention">HazyResearch/flash-attention: Fast and memory-efficient exact attention</A>
								<DT><H3 FOLDED>issues</H3>
								<DL><p>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/253">ModuleNotFoundError: No module named 'torch' ¬∑ Issue #253 ¬∑ HazyResearch/flash-attention</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/246">No Module Named 'torch' ¬∑ Issue #246 ¬∑ HazyResearch/flash-attention</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/131">installing dropout_layer_norm ¬∑ Issue #131 ¬∑ HazyResearch/flash-attention</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/250">ModuleNotFoundError: No module named 'dropout_layer_norm' when trying to import flash_attn.ops.layer_norm ¬∑ Issue #250 ¬∑ HazyResearch/flash-attention</A>
								</DL><p>
								<DT><A HREF="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA) ‚Äî PyTorch Tutorials 2.1.0+cu121 documentation</A>
								<DT><H3 FOLDED>FlashFFTConv</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/realDanFu/status/1724127071902011611">FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores</A>
								</DL><p>
								<DT><A HREF="https://gist.github.com/Chillee/41baf11aac8036d25d637321c48dad20">You Could Have Invented Flash-Attention!</A>
							</DL><p>
							<DT><H3 FOLDED>Adaptive Computation</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2207.07061.pdf">Confident Adaptive Language Modeling (CALM)</A>
								<DT><A HREF="https://github.com/hao-ai-lab/LookaheadDecoding">hao-ai-lab/LookaheadDecoding</A>
								<DT><A HREF="https://arxiv.org/pdf/2305.10427.pdf">Accelerating Transformer Inference for Translation via Parallel Decoding</A>
							</DL><p>
							<DT><H3 FOLDED>FasterTransformer</H3>
							<DL><p>
								<DT><A HREF="https://fast-transformers.github.io/">Fast Transformers for PyTorch</A>
								<DT><A HREF="https://fast-transformers.github.io/#research">Fast Transformers for PyTorch</A>
								<DT><A HREF="https://github.com/NVIDIA/FasterTransformer">NVIDIA/FasterTransformer: Transformer related optimization, including BERT, GPT</A>
								<DT><A HREF="https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566">How to convert the SalesForce CodeGen models to GPT-J</A>
								<DT><A HREF="https://developer.nvidia.com/blog/increasing-inference-acceleration-of-kogpt-with-fastertransformer/">Increasing Inference Acceleration of KoGPT with NVIDIA FasterTransformer | NVIDIA Technical Blog</A>
								<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-CWES52119/?ncid=em-even-124008-vt33">Connect with the Experts: GPU Performance Analysis and Optimization | NVIDIA On-Demand</A>
								<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-S51196/?ncid=em-even-124008-vt33">FP8</A>
								<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/tree/6ea1c77c7fabf1a046463eceddce1839efc63e60">NVIDIA/FasterTransformer at 6ea1c77c7fabf1a046463eceddce1839efc63e60</A>
								<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/blob/6ea1c77c7fabf1a046463eceddce1839efc63e60/examples/pytorch/gpt/gpt_example.py#L237">FasterTransformer/examples/pytorch/gpt/gpt_example.py at 6ea1c77c7fabf1a046463eceddce1839efc63e60 ¬∑ NVIDIA/FasterTransformer</A>
							</DL><p>
							<DT><H3 FOLDED>Videos</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4577s">Let's build GPT: from scratch, in code, spelled out. - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>Model Format Conversion</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>Jax</H3>
							<DL><p>
								<DT><H3 FOLDED>Pallas</H3>
								<DL><p>
									<DT><A HREF="https://jax.readthedocs.io/en/latest/_images/pallas_flow.png">pallas_flow.png 908√ó832 pixels</A>
									<DT><A HREF="https://bnikolic.co.uk/blog/python/jax/2020/10/20/jax-outputgraph.html">Jax: Visualising the computational graph of a jax program | B. Nikolic Software and Computing Blog</A>
									<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/design.html">Pallas Design ‚Äî JAX documentation</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://github.com/google/automl/tree/master/lion#language-modeling">Google AutoML: Lion Optimizer over Adam</A>
							<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT/triton">NVIDIA/DeepLearningExamples ¬∑ GitHub</A>
							<DT><A HREF="https://gist.github.com/moyix/0f37da9c21c4ddfa0ab39ddad1639db4">Convert a SalesForce CodeGen model's weights to plain GPT-J</A>
							<DT><A HREF="https://huggingface.co/docs/optimum/bettertransformer/overview">Better Transformer</A>
							<DT><A HREF="https://github.com/triton-inference-server/model_analyzer">triton-inference-server/model_analyzer: Triton Model Analyzer is a CLI tool to help with better understanding of the compute and memory requirements of the Triton Inference Server models.</A>
							<DT><A HREF="https://developer.nvidia.com/blog/increasing-inference-acceleration-of-kogpt-with-fastertransformer/">General Optimizations List</A>
							<DT><A HREF="https://github.com/feifeibear/LLMSpeculativeSampling">Speculative Decoding</A>
							<DT><A HREF="https://github.com/tomaarsen/attention_sinks">tomaarsen/attention_sinks: Extend existing LLMs way beyond the original training length with constant memory usage, and without retraining</A>
							<DT><A HREF="https://github.com/stas00/tinypar">TP/PP/DP implementation of llama using apex blocks</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-distillation</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>runtime optimizations (cpp or rust)</H3>
						<DL><p>
							<DT><A HREF="https://github.com/ggerganov/llama.cpp">ggerganov/llama.cpp: LLM inference in C/C++</A>
							<DT><A HREF="https://github.com/huggingface/candle">huggingface/candle: Minimalist ML framework for Rust</A>
							<DT><A HREF="https://github.com/kykosic/actix-pytorch-example">kykosic/actix-pytorch-example (xAI prototype)</A>
							<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md">llama.cpp/examples/server/README.md at master ¬∑ ggerganov/llama.cpp</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-profiling</H3>
						<DL><p>
							<DT><H3 FOLDED>Transformers</H3>
							<DL><p>
								<DT><A HREF="https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb">Huggingface Transformers PyTorchBenchmarking</A>
								<DT><A HREF="https://huggingface.co/docs/transformers/benchmarks">Benchmarks</A>
								<DT><A HREF="https://github.com/huggingface/transformers/blob/2d506ea4c4980a4cab43c2940d9836ddfd629524/src/transformers/benchmark/benchmark_args_utils.py">benchmark_args_utils.py</A>
							</DL><p>
							<DT><H3 FOLDED>PyTorch</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/docs/stable/benchmark_utils.html">Benchmark Utils - torch.utils.benchmark</A>
								<DT><A HREF="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler ‚Äî PyTorch Tutorials 1.13.1+cu117 documentation</A>
								<DT><A HREF="https://github.com/octoml/octoml-profile">octoml/octoml-profile: Home for OctoML PyTorch Profiler</A>
								<DT><A HREF="https://medium.com/pytorch/profiling-pytorch-language-models-with-octoml-profile-eda7ece6b7bd">Profiling PyTorch language models with octoml-profile | by Octonauts | PyTorch | Apr, 2023 | Medium</A>
							</DL><p>
							<DT><A HREF="https://github.com/DataCrunch-io/large_transformer_training_playbook/blob/main/transformers.ipynb">Transformers.ipynb: Megatron-LM &amp; DeepSpeed &amp; HF Transformers</A>
							<DT><A HREF="https://github.com/pythonprofilers/memory_profiler">pythonprofilers/memory_profiler: Monitor Memory usage of Python code</A>
							<DT><A HREF="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">TensorFlow Profiler: Profile model performance ¬†|¬† TensorBoard</A>
							<DT><A HREF="https://unix.stackexchange.com/questions/125429/tracking-down-where-disk-space-has-gone-on-linux">du</A>
							<DT><A HREF="https://docs.contrastsecurity.com/en/python-middleware.html">Configure middleware</A>
							<DT><A HREF="https://www.cyberciti.biz/open-source/install-ncdu-on-linux-unix-ncurses-disk-usage/">ncdu</A>
							<DT><A HREF="https://github.com/Syllo/nvtop">Syllo/nvtop: GPUs process monitoring for AMD, Intel and NVIDIA</A>
							<DT><A HREF="https://github.com/gperftools/gperftools">gperftools/gperftools: Main gperftools repository</A>
						</DL><p>
						<DT><H3 FOLDED>serving</H3>
						<DL><p>
							<DT><H3 FOLDED>serving-research</H3>
							<DL><p>
								<DT><H3 FOLDED>serving-research-web-server</H3>
								<DL><p>
									<DT><H3 FOLDED>Robyn</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sparckles/Robyn">sparckles/Robyn: Robyn is a Super Fast Async Python Web Framework with a Rust runtime.</A>
									</DL><p>
									<DT><H3 FOLDED>uvicorn</H3>
									<DL><p>
										<DT><H3 FOLDED>uvicorn-logger</H3>
										<DL><p>
											<DT><A HREF="https://github.com/roy-pstr/fastapi-custom-exception-handlers-and-logs/blob/master/logger.py">logger.py</A>
											<DT><A HREF="https://github.com/encode/uvicorn/blob/0efd3835da6dcc713f74aadf7b52779d0d1fa17d/uvicorn/config.py#L357">uvicorn: config.py#L357 (log_config=None)</A>
											<DT><A HREF="https://docs.python.org/3/library/logging.config.html">logging.config ‚Äî Logging configuration ‚Äî Python 3.12.3 documentation</A>
											<DT><A HREF="https://github.com/encode/uvicorn/blob/0efd3835da6dcc713f74aadf7b52779d0d1fa17d/uvicorn/config.py#L32">uviconr: LOG_LEVELS config.py#L32</A>
											<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/9b6ef9e1f0d8acaefd989440b27da9069aa69207/deepspeed/utils/logging.py">DeepSpeed/deepspeed/utils/logging.py</A>
										</DL><p>
										<DT><A HREF="https://github.com/encode/uvicorn">encode/uvicorn: An ASGI web server, for Python. ü¶Ñ</A>
										<DT><A HREF="https://github.com/Lightning-AI/LitServe/blob/ee1a7b53772332a937bdb548277b28cc54ba16e0/src/litserve/server.py#L25">LitServe/src/litserve/server.py</A>
									</DL><p>
									<DT><H3 FOLDED>zap</H3>
									<DL><p>
										<DT><A HREF="https://github.com/zigzap/zap">zigzap/zap: blazingly fast backends in zig</A>
									</DL><p>
									<DT><A HREF="https://github.com/facebook/wangle">facebook/wangle: Wangle is a framework providing a set of common client/server abstractions for building services in a consistent, modular, and composable way.</A>
									<DT><A HREF="https://github.com/Lightning-AI/LitServe?tab=readme-ov-file#implement-a-server">Lightning-AI/LitServe: Deploy AI models at scale. High-throughput serving engine for AI/ML models that uses the latest state-of-the-art model deployment techniques.</A>
								</DL><p>
								<DT><H3 FOLDED>serving-research-rust</H3>
								<DL><p>
									<DT><H3 FOLDED>axum</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=Wnb_n5YktO8&t=30s">Decrusting the axum crate - YouTube</A>
										<DT><A HREF="https://crates.io/crates/axum">axum - crates.io: Rust Package Registry</A>
										<DT><A HREF="https://github.com/tokio-rs/axum">tokio-rs/axum: Ergonomic and modular web framework built with Tokio, Tower, and Hyper</A>
										<DT><A HREF="https://github.com/serde-rs/serde">serde-rs/serde: Serialization framework for Rust</A>
										<DT><A HREF="https://github.com/tokio-rs/tokio">tokio-rs/tokio: A runtime for writing reliable asynchronous applications with Rust. Provides I/O, networking, scheduling, timers, ...</A>
										<DT><A HREF="https://github.com/tokio-rs/tracing">tokio-rs/tracing: Application level tracing for Rust.</A>
										<DT><A HREF="https://crates.io/crates/tracing-subscriber">tracing-subscriber - crates.io: Rust Package Registry</A>
										<DT><A HREF="https://github.com/hyperium/hyper">hyperium/hyper: An HTTP library for Rust (HTTP parser)</A>
										<DT><A HREF="https://github.com/tower-rs/tower">tower-rs/tower: async fn(Request) (Middleware)</A>
										<DT><A HREF="https://docs.rs/matchit/latest/matchit/">matchit (route matching)</A>
										<DT><A HREF="https://docs.rs/hyper/1.2.0/hyper/">hyper - Rust</A>
										<DT><A HREF="https://docs.rs/hyper/0.14.20/hyper/server/struct.Builder.html">Builder in hyper::server - Rust</A>
										<DT><A HREF="https://tokio.rs/blog/2021-05-14-inventing-the-service-trait">Inventing the Service trait | Tokio - An asynchronous Rust runtime</A>
										<DT><A HREF="https://docs.rs/axum/0.6.20/axum/attr.debug_handler.html">debug_handler in axum (FromRequestParts) MAIN</A>
										<DT><A HREF="https://github.com/tokio-rs/axum/blob/2ec68d6c4dab10b83b9195c3acd4ccc7c26d0e8a/axum/src/handler/mod.rs#L206-248">FromRequest -&gt; Handler (macro)</A>
										<DT><A HREF="https://github.com/tokio-rs/axum/blob/2ec68d6c4dab10b83b9195c3acd4ccc7c26d0e8a/axum-core/src/response/into_response.rs#L395">IntoResponse (last arg special)</A>
										<DT><A HREF="https://docs.rs/axum/0.6.20/axum/extract/struct.State.html">extactors::State (impl FromRequestParts)</A>
										<DT><A HREF="https://docs.rs/axum/0.6.20/axum/handler/struct.HandlerService.html">HandlerService in axum::handler</A>
										<DT><A HREF="https://docs.rs/tower/0.4.13/tower/trait.Service.html">tower::Service (Future -&gt; self.state.clone())</A>
										<DT><A HREF="https://crates.io/crates/tokio-blocking">tokio-blocking (inference step)</A>
										<DT><A HREF="https://docs.rs/axum-extra/0.9.2/axum_extra/">axum_extra - (utilities)</A>
									</DL><p>
									<DT><A HREF="https://github.com/kykosic/actix-pytorch-example">kykosic/actix-pytorch-example (xAI prototype)</A>
									<DT><A HREF="https://github.com/kykosic/actix-tensorflow-example">kykosic/actix-tensorflow-example</A>
									<DT><A HREF="https://github.com/hyperium/hyper">hyperium/hyper: An HTTP library for Rust</A>
									<DT><A HREF="https://github.com/hyperium/h2">hyperium/h2: HTTP 2.0 client &amp; server implementation for Rust.</A>
									<DT><A HREF="https://www.youtube.com/watch?v=P-v8xRhpquM">Rust Programming Part 2 HTTP Server Using Result Type &amp; Responding To Client - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Wnb_n5YktO8&t=30s">Decrusting the axum crate - YouTube</A>
									<DT><A HREF="https://blog.hippoml.com/unified-datacenter-local-foundation-model-serving-beyond-docker-way-a929003fa07c">Unified DataCenter &amp; Local Foundation Model Serving: Beyond Docker Way | by HippoML Blog | Jan, 2024 | Medium</A>
								</DL><p>
								<DT><H3 FOLDED>network services</H3>
								<DL><p>
									<DT><A HREF="https://github.com/cloudflare/pingora">cloudflare/pingora: A library for building fast, reliable and evolvable network services.</A>
									<DT><A HREF="https://blog.cloudflare.com/pingora-open-source">Open sourcing Pingora: our Rust framework for building programmable network services</A>
									<DT><A HREF="https://github.com/cloudflare/pingora/blob/main/docs/quick_start.md">pingora/docs/quick_start.md at main ¬∑ cloudflare/pingora</A>
									<DT><A HREF="https://github.com/cloudflare/pingora/blob/main/pingora-proxy/examples/load_balancer.rs">pingora/pingora-proxy/examples/load_balancer.rs at main ¬∑ cloudflare/pingora</A>
									<DT><A HREF="https://github.com/bytedance/g3">bytedance/g3: Enterprise-oriented Generic Proxy Solutions</A>
								</DL><p>
								<DT><H3 FOLDED>Network Messaging Protocol</H3>
								<DL><p>
									<DT><H3 FOLDED>NATS</H3>
									<DL><p>
										<DT><H3 FOLDED>nats-protobuf</H3>
										<DL><p>
											<DT><A HREF="https://natsbyexample.com/examples/messaging/protobuf/go">NATS by Example - Protobuf for Message Payloads (Go)</A>
											<DT><A HREF="https://github.com/savaki/nats-protobuf">savaki/nats-protobuf: write protobuf services with NATS as the transport; service discovery simplified</A>
										</DL><p>
										<DT><H3 FOLDED>nats-client</H3>
										<DL><p>
											<DT><H3 FOLDED>Client Protocol</H3>
											<DL><p>
												<DT><A HREF="https://docs.nats.io/reference/reference-protocols/nats-protocol">Client Protocol | NATS Docs</A>
											</DL><p>
											<DT><A HREF="https://github.com/3kwa/goingnats">Python minimal NATS client (no asyncio)</A>
											<DT><A HREF="https://github.com/nats-io/nats.py">nats-io/nats.py: Python3 client for NATS (asycio)</A>
											<DT><A HREF="https://github.com/nats-io/nats.net.v2">nats-io/nats.net.v2: Full Async C# / .NET client for NATS</A>
											<DT><A HREF="https://github.com/nats-io/nats.zig">nats-io/nats.zig: Zig Client for NATS</A>
											<DT><A HREF="https://github.com/rutgerbrf/zig-nats/blob/master/example/main.zig">zig-nats/example/main.zig at master ¬∑ rutgerbrf/zig-nats</A>
										</DL><p>
										<DT><A HREF="https://github.com/nats-io/nats-server">nats-io/nats-server: High-Performance server for NATS.io, the cloud and edge native messaging system.</A>
										<DT><A HREF="https://nats.io/download/">NATS.io ‚Äì Cloud Native, Open Source, High-performance Messaging</A>
										<DT><A HREF="https://gcoolinfo.medium.com/comparing-nats-nats-streaming-and-nats-jetstream-ec2d9f426dc8">Comparing NATS, NATS Streaming and NATS JetStream | by George Koulouris | Medium</A>
										<DT><A HREF="https://github.com/nats-io/nats.c">nats-io/nats.c: A C client for NATS</A>
										<DT><A HREF="https://github.com/ConnectEverything/nats-by-example/#getting-started">ConnectEverything/nats-by-example: Collection of runnable, reference examples using NATS (https://nats.io)</A>
										<DT><A HREF="https://docs.nats.io/nats-concepts/what-is-nats">What is NATS | NATS Docs</A>
										<DT><A HREF="https://docs.nats.io/nats-concepts/jetstream">JetStream | NATS Docs</A>
										<DT><A HREF="https://www.youtube.com/watch?v=JNQM_aq9pd4">aggregator (multiple backends e.g. /rest/* &amp; /nats/*)</A>
										<DT><A HREF="https://github.com/gcool-info/nats-playground">gcool-info/nats-playground: Playground for a secure, Highly Available NATS Cluster with message persistence (using JetStream)</A>
										<DT><A HREF="https://www.youtube.com/watch?v=SLb4rdI5lIM">NATS for Modern Messaging and Microservices - YouTube</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=kH7P1ZX44DQ&list=LL&index=29">How to Design a Network Messaging Protocol!</A>
								</DL><p>
								<DT><H3 FOLDED>Jetstream</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/JetStream/tree/main">google/JetStream: A throughput and memory optimized engine for LLM inference on TPUs!</A>
									<DT><A HREF="https://www.pjm.com/-/media/etools/jetstream/introduction-to-jetstream.ashx">ntroduction-to-jetstream</A>
									<DT><A HREF="https://nats-io.github.io/nats.net.v2/documentation/jetstream/intro.html#:~:text=JetStream%20is%20the%20built%2Din,functionalities%20and%20qualities%20of%20service.">JetStream</A>
									<DT><A HREF="https://nats-io.github.io/nats.net.v2/documentation/serialization.html">Serialization</A>
									<DT><A HREF="https://gcoolinfo.medium.com/comparing-nats-nats-streaming-and-nats-jetstream-ec2d9f426dc8">Comparing NATS, NATS Streaming and NATS JetStream | by George Koulouris | Medium</A>
								</DL><p>
								<DT><H3 FOLDED>websockets</H3>
								<DL><p>
									<DT><A HREF="https://github.com/kykosic/python-websocket-exmple/blob/master/server.py">python-websocket-exmple/server.py at master ¬∑ kykosic/python-websocket-exmple</A>
								</DL><p>
								<DT><H3 FOLDED>gRPC</H3>
								<DL><p>
									<DT><H3 FOLDED>Protocol Buffers</H3>
									<DL><p>
										<DT><A HREF="https://github.com/protocolbuffers">Protocol Buffers</A>
										<DT><A HREF="https://github.com/protocolbuffers/protobuf">protocolbuffers/protobuf: Protocol Buffers - Google's data interchange format</A>
										<DT><A HREF="https://flatbuffers.dev/">FlatBuffers: FlatBuffers</A>
										<DT><A HREF="https://github.com/google/flatbuffers">google/flatbuffers: FlatBuffers: Memory Efficient Serialization Library</A>
										<DT><A HREF="https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/">LinkedIn Adopts Protocol Buffers for Microservices Integration and Reduces Latency by up to 60%</A>
										<DT><A HREF="https://www.youtube.com/watch?v=9IxE2UQqJCw&t=7s">Reduce Latency By 60% With ProtoBufs!!! | Prime Reacts - YouTube</A>
										<DT><A HREF="https://protobuf.dev/overview/">Overview | Protocol Buffers Documentation</A>
										<DT><A HREF="https://gist.github.com/jambonn/1f5fffc23f97f8413372a438739c1bff">How to Install Protobuf on Ubuntu 20.04</A>
										<DT><A HREF="https://protobuf.dev/overview/#updating-defs">Overview | Protocol Buffers Documentation</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/proto/generate.proto">text-generation-inference/proto/generate.proto at main ¬∑ huggingface/text-generation-inference</A>
									</DL><p>
									<DT><H3 FOLDED>Rust</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=kerKXChDmsE">Tonic makes gRPC in Rust stupidly simple</A>
										<DT><A HREF="https://github.com/hyperium/tonic">hyperium/tonic: A native gRPC client &amp; server implementation with async/await support.</A>
										<DT><A HREF="https://github.com/tokio-rs/prost">tokio-rs/prost: PROST! a Protocol Buffers implementation for the Rust Language</A>
										<DT><A HREF="https://github.com/hyperium/tonic/blob/master/tonic-build/README.md">tonic/tonic-build/README.md at master ¬∑ hyperium/tonic</A>
									</DL><p>
									<DT><H3 FOLDED>client</H3>
									<DL><p>
										<DT><A HREF="https://github.com/fullstorydev/grpcurl">fullstorydev/grpcurl: Like cURL, but for gRPC: Command-line tool for interacting with gRPC servers</A>
									</DL><p>
									<DT><H3 FOLDED>uds: Unix Domain Socket</H3>
									<DL><p>
										<DT><A HREF="https://github.com/grpc/grpc/tree/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/python/uds">uds: Unix Domain Socket Example in gRPC Python</A>
										<DT><A HREF="https://grpc.io/docs/languages/python/quickstart/">Quick start | Python | gRPC</A>
									</DL><p>
									<DT><A HREF="https://github.com/grpc/grpc/blob/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/python/uds/async_greeter_server.py">(EXAMPLE) grpc/examples/python/uds/async_greeter_server.py at d68161a64f191b8d8d5afe0507e7a2291f91ff1a ¬∑ grpc/grpc</A>
									<DT><A HREF="https://www.youtube.com/watch?v=SDnPul2-N9w">Big picture</A>
									<DT><A HREF="https://www.youtube.com/watch?v=uGYZn6xk-hA">Serialization formats: JSON and Protobuf</A>
									<DT><A HREF="https://janhendrikewers.uk/pydantic_vs_protobuf_vs_namedtuple_vs_dataclasses.html">Pydantic vs Protobuf vs Namedtuples vs Dataclasses</A>
									<DT><A HREF="https://github.com/bazelbuild/starlark">bazelbuild/starlark: Starlark Language</A>
									<DT><A HREF="https://github.com/stripe/skycfg">stripe/skycfg: Skycfg is an extension library for the Starlark language that adds support for constructing Protocol Buffer messages.</A>
									<DT><A HREF="https://www.youtube.com/watch?v=kerKXChDmsE">Tonic makes gRPC in Rust stupidly simple</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/7dbaf9e9013060af52024ea1a8b361b107b50a69/router/src/server.rs#L115">text-generation-inference/router/src/server.rs at 7dbaf9e9013060af52024ea1a8b361b107b50a69 ¬∑ huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/fullstorydev/grpcurl">fullstorydev/grpcurl: Like cURL, but for gRPC: Command-line tool for interacting with gRPC servers</A>
									<DT><A HREF="https://github.com/grpc/grpc/tree/master/src/python/grpcio">grpc/src/python/grpcio at master</A>
									<DT><A HREF="https://grpc.github.io/grpc/python/grpc.html">gRPC ‚Äî gRPC Python 1.62.0 documentation</A>
									<DT><A HREF="https://github.com/grpc/grpc/blob/master/doc/python/server_reflection.md">grpc/doc/python/server_reflection.md (reflection)</A>
									<DT><A HREF="https://github.com/d5h-foss/grpc-interceptor">d5h-foss/grpc-interceptor: Simplified Python gRPC interceptors</A>
									<DT><A HREF="https://www.youtube.com/watch?v=M1qt83N3JWg">¬øQu√© es gRPC? - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>WSGI</H3>
								<DL><p>
									<DT><A HREF="https://www.fullstackpython.com/wsgi-servers.html">WSGI Servers - Full Stack Python</A>
								</DL><p>
								<DT><H3 FOLDED>environ</H3>
								<DL><p>
									<DT><A HREF="https://github.com/triton-inference-server/server/blob/ddd6c4b4a286970e0b6c18dcd5c90c7a121d3e48/qa/L0_backend_fastertransformer/test.sh">Triton Inference Server: $TRITON_DIR (fastertransformer backend)</A>
									<DT><A HREF="https://twitter.com/wangzhr4/status/1783772055294329159/photo/1">sensitive information as local env vars</A>
									<DT><A HREF="https://chat.openai.com/c/607f5eb0-ab64-494d-acdb-b1a3f7319695">Exposure of Sensitive Information &amp; Security practices</A>
									<DT><A HREF="https://github.com/vllm-project/vllm/blob/8674f9880e2d8574c2adc759027e0f27dc9b95de/setup.py#L31">setup.py#L31 # cannot import envs directly because it depends on vllm which is not installed yet</A>
									<DT><A HREF="https://github.com/vllm-project/vllm/blob/8674f9880e2d8574c2adc759027e0f27dc9b95de/setup.py#L18">setup.py#L18</A>
								</DL><p>
								<DT><H3 FOLDED>asycio</H3>
								<DL><p>
									<DT><H3 FOLDED>pyInfer</H3>
									<DL><p>
										<DT><A HREF="https://github.com/wangzyon/pyInfer">wangzyon/pyInfer: async inference for machine learning model</A>
										<DT><A HREF="https://github.com/wangzyon/pyInfer/blob/master/pyinfer/utils/common/registry.py">pyInfer/pyinfer/utils/common/registry.py REGISTER ENGINE</A>
									</DL><p>
									<DT><A HREF="https://github.com/facebookincubator/later">facebookincubator/later: A framework for python asyncio with batteries included for people writing services in python asyncio</A>
									<DT><A HREF="https://github.com/wangzyon/pyInfer/blob/master/pyinfer/core/engine/engine.py">pyInfer/pyinfer/core/engine/engine.py at master ¬∑ wangzyon/pyInfer</A>
								</DL><p>
								<DT><H3 FOLDED>serving-research-web-server-max_tokens</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-best-practices.md">max_batch_size * max_input_len * alpha + max_batch_size * max_beam_width * (1 - alpha)</A>
								</DL><p>
								<DT><H3 FOLDED>serving-research-torch.compile</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/commit/78f87d5a0c7d82911a639c397577284868a53c42">Temporary implem of torch.compile on our stuff. ¬∑ huggingface/text-generation-inference@78f87d5</A>
								</DL><p>
								<DT><A HREF="https://github.com/geohot/minikeyvalue/blob/master/src/server.go">minikeyvalue/src/server.go at master ¬∑ geohot/minikeyvalue</A>
								<DT><A HREF="https://github.com/theroyallab/tabbyAPI/blob/main/endpoints/OAI/router.py">tabbyAPI/endpoints/OAI/router.py (good &amp; minimal FastAPI ref impl)</A>
								<DT><A HREF="https://github.com/triton-inference-server/server/blob/ddd6c4b4a286970e0b6c18dcd5c90c7a121d3e48/qa/L0_http/http_test.py">server/qa/L0_http/http_test.py</A>
								<DT><A HREF="https://github.com/triton-inference-server/server/blob/ddd6c4b4a286970e0b6c18dcd5c90c7a121d3e48/qa/L0_http/python_http_aio_test.py">server/qa/L0_http/python_http_aio_test.py (readiness)</A>
								<DT><A HREF="https://github.com/triton-inference-server/client">triton-inference-server/client: Triton Python, C++ and Java client libraries</A>
								<DT><A HREF="https://github.com/triton-inference-server/client/blob/a00b97131ffd46c814ae3c2d4eca98266d19f804/src/python/library/tritonclient/http/_client.py#L434">triton client: model_version API structure _client.py#L434</A>
								<DT><A HREF="https://www.youtube.com/watch?v=BhtxEDwgylU&list=LL&index=10">Use AppMap with VS Code Dev Containers - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=6BiNzPdy6YA">[REFAI Seminar 04/16/24] ML for ML Compilers at Google (AutoFDO)</A>
								<DT><A HREF="https://github.com/facebook/proxygen">facebook/proxygen: A collection of C++ HTTP libraries including an easy to use HTTP server.</A>
								<DT><A HREF="https://engineering.fb.com/2014/11/05/production-engineering/introducing-proxygen-facebook-s-c-http-framework/">Introducing Proxygen, Facebook's C++ HTTP framework - Engineering at Meta</A>
								<DT><A HREF="https://www.youtube.com/watch?v=t08OA4CfRFs">Using Rust to write scalable Python APIs - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>serving-inference-engine</H3>
							<DL><p>
								<DT><A HREF="https://github.com/wangzyon/pyInfer/blob/master/pyinfer/core/engine/engine.py">pyInfer/pyinfer/core/engine/engine.py</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/legacy/models/providers/diffusers.py">DeepSpeed-MII/mii/legacy/models/providers/diffusers.py</A>
							</DL><p>
							<DT><H3 FOLDED>serving-FastAPI-(Starlette)</H3>
							<DL><p>
								<DT><H3 FOLDED>Starlette</H3>
								<DL><p>
									<DT><A HREF="https://github.com/encode/starlette/blob/9f16bf5c25e126200701f6e04330864f4a91a898/docs/server-push.md?plain=1#L30">routes without decorators, but constructors and funcs</A>
									<DT><A HREF="https://github.com/tiangolo/fastapi/blob/38929aae1b6d42848652705e5ca618a675dba0e1/fastapi/routing.py#L651">fastapi/fastapi/routing.py</A>
								</DL><p>
								<DT><H3 FOLDED>FastAPI-lifespan</H3>
								<DL><p>
									<DT><A HREF="https://medium.com/@life-is-short-so-enjoy-it/fastapi-experiment-lifespan-feature-7f87de5601db">FastAPI: experiment lifespan feature | by Life-is-short--so--enjoy-it | Medium</A>
								</DL><p>
								<DT><A HREF="https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&hw=ph&test=db">HTTP Servers Benchmarks</A>
								<DT><A HREF="https://github.com/roy-pstr/fastapi-custom-exception-handlers-and-logs/blob/master/main.py">fastapi-custom-exception-handlers-and-logs/main.py at master ¬∑ roy-pstr/fastapi-custom-exception-handlers-and-logs</A>
								<DT><A HREF="https://github.com/tiangolo/fastapi/blob/38929aae1b6d42848652705e5ca618a675dba0e1/fastapi/routing.py#L655C13-L655C23">FastAPI: routes as list is deprecated routing.py#L655C13-L655C23</A>
								<DT><A HREF="https://github.com/tiangolo/fastapi/blob/38929aae1b6d42848652705e5ca618a675dba0e1/tests/test_extra_routes.py#L22">FastAPI: router no decorated test_extra_routes.py#L22</A>
								<DT><A HREF="https://github.com/tiangolo/fastapi/blob/38929aae1b6d42848652705e5ca618a675dba0e1/tests/test_include_route.py">fastapi/tests/test_include_route.py</A>
								<DT><A HREF="https://www.youtube.com/watch?v=t08OA4CfRFs">Using Rust to write scalable Python APIs - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=row-SdNdHFE">Best Practice to Make HTTP Request in FastAPI Application - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=tGD3653BrZ8">How FastAPI Handles Requests Behind the Scenes - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=vkQZe8Idbtg">Fastapi endpoints testing with pytest | Tutorial 1 - YouTube</A>
								<DT><A HREF="https://viktorsapozhok.github.io/fastapi-oauth2-postgres/">Structuring FastAPI application with multiple services using 3-tier design pattern. | Vanilla Ninja</A>
								<DT><A HREF="https://github.com/Lightning-AI/LitServe?tab=readme-ov-file#implement-a-server">Lightning-AI/LitServe: Deploy AI models at scale. High-throughput serving engine for AI/ML models that uses the latest state-of-the-art model deployment techniques.</A>
								<DT><A HREF="https://github.com/Lightning-AI/LitServe/blob/ee1a7b53772332a937bdb548277b28cc54ba16e0/src/litserve/server.py#L25">LitServe/src/litserve/server.py</A>
							</DL><p>
							<DT><H3 FOLDED>serving-test</H3>
							<DL><p>
								<DT><H3 FOLDED>serving-test-device</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/dc514df2afad386739bf8471ab351a86d5c5ffc7/torch/testing/_internal/common_cuda.py">pytorch/torch/testing/_internal/common_cuda.py</A>
								</DL><p>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_throughput.py#L126">vllm/benchmarks/benchmark_throughput.py</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_serving.py">vllm/benchmarks/benchmark_serving.py</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/integration-tests/conftest.py#L284">text-generation-inference/integration-tests/conftest.py</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/discussions?discussions_q=is%3Aopen+test">tinygrad/tinygrad ¬∑ Discussions ¬∑ GitHub</A>
							</DL><p>
							<DT><H3 FOLDED>serving-benchmark</H3>
							<DL><p>
								<DT><H3 FOLDED>serving-test-regression</H3>
								<DL><p>
								</DL><p>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_throughput.py#L126">vllm/benchmarks/benchmark_throughput.py</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/launch_tgi_server.sh">vllm/benchmarks/launch_tgi_server.sh</A>
								<DT><A HREF="https://www.youtube.com/watch?v=7njmta3SlxE">Hao Zhang - Chatbot Arena (UCSD / LMSys) - YouTube</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_serving.py">vllm/benchmarks/benchmark_serving.py</A>
								<DT><A HREF="https://twitter.com/dzhulgakov/status/1737917306565697990?s=46&t=yqOem5ktaowo8FyJ-ilbzQ">(1) Dmytro Dzhulgakov en X: "I‚Äôm all pro open benchmarks, but comparing **public** LLM endpoints just doesn‚Äôt work unless there‚Äôs a confirmed huge user base like OpenAI. In fact, performance may be even anti-correlated with popularityüòâ Here‚Äôs why üßµ" / X</A>
								<DT><A HREF="https://twitter.com/anyscalecompute/status/1737883193720922413">(1) Anyscale en X: "üìàWe‚Äôre excited to introduce the LLMPerf leaderboard: the first public and open source leaderboard for benchmarking performance of various LLM inference providers in the market. Our goal with this leaderboard is to equip users and developers with a clear understanding of the... https://t.co/XGF4fhkaWG" / X</A>
								<DT><A HREF="https://github.com/fw-ai/benchmark">fw-ai/benchmark: Benchmark suite for LLMs from Fireworks.ai</A>
								<DT><A HREF="https://github.com/ray-project/llmperf">ray-project/llmperf: LLMPerf is a library for validating and benchmarking LLMs</A>
								<DT><A HREF="https://httpd.apache.org/docs/current/programs/ab.html">ab - Apache HTTP server benchmarking tool - Apache HTTP Server Version 2.4</A>
								<DT><A HREF="https://github.com/grafana/k6">grafana/k6: A modern load testing tool, using Go and JavaScript - https://k6.io</A>
								<DT><A HREF="https://gist.github.com/aliesbelik/840eff7c5bc78a141eab8e36f2f4edf2">Benchmarking &amp; load testing tools</A>
								<DT><A HREF="https://github.com/hatoo/oha">hatoo/oha: Ohayou(„Åä„ÅØ„Çà„ÅÜ), HTTP load generator, inspired by rakyll/hey with tui animation.</A>
								<DT><A HREF="https://github.com/sharkdp/hyperfine">sharkdp/hyperfine: A command-line benchmarking tool</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/tree/main/benchmarks">vllm/benchmarks at main</A>
							</DL><p>
							<DT><H3 FOLDED>serving-types</H3>
							<DL><p>
								<DT><H3 FOLDED>InferenceRequest</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/Megatron-LM/blob/ccfeda47cb5ca10ee3c4efd9b78c6bb15c2cd3d2/megatron/core/inference_params.py#L1">Megatron-LM: inference_params.py#L1</A>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/74a1be88f589bdd53e6b110528ae65dba5a1e9af/examples/mamba.py#L297">Tinygrad: inference_params (mamba.py)#L297</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/inference-request.md">TensorRT-LLM/docs/source/advanced/inference-request.md</A>
								</DL><p>
								<DT><H3 FOLDED>pydantic</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/types.py">text-generation-inference/clients/python/text_generation/types.py (pydantic v2)</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>serving-diffusion</H3>
							<DL><p>
								<DT><H3 FOLDED>DeepCache</H3>
								<DL><p>
									<DT><A HREF="https://github.com/horseee/DeepCache">horseee/DeepCache: [CVPR 2024] DeepCache: Accelerating Diffusion Models for Free</A>
									<DT><A HREF="https://huggingface.co/docs/diffusers/main/en/optimization/deepcache">DeepCache</A>
									<DT><H3 FOLDED>tmp</H3>
									<DL><p>
										<DT><A HREF="https://medium.com/swlh/writing-c-when-youre-a-java-developer-memory-management-7c42e222645e">Writing C++ When You‚Äôre a Java Developer: Memory Management | by Alexandre Lombard | The Startup | Medium</A>
										<DT><A HREF="https://mypy.readthedocs.io/en/stable/protocols.html">Protocols and structural subtyping - mypy 1.9.0 documentation</A>
										<DT><A HREF="https://colab.research.google.com/github/google/seqio/blob/main/seqio/notebooks/Basics_Task_and_Mixtures.ipynb#scrollTo=QTyxusscJgwO">[seqio basics] Task and Mixtures.ipynb - Colab</A>
										<DT><A HREF="https://github.com/google-research-datasets/natural-questions">google-research-datasets/natural-questions: Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems.</A>
										<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1778234586599727285">https://twitter.com/i/bookmarks?post_id=1778234586599727285</A>
										<DT><A HREF="https://arxiv.org/abs/2404.07143">[2404.07143] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</A>
										<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8">LLM Inference Series: 4. KV caching, a deeper look | by Pierre Lienhart | Medium</A>
										<DT><A HREF="https://github.com/openai/triton/blob/main/python/triton/testing.py">triton/python/triton/testing.py at main ¬∑ openai/triton</A>
										<DT><A HREF="https://github.com/horseee/DeepCache/blob/master/stable_diffusion.py#L29">DeepCache/stable_diffusion.py at master ¬∑ horseee/DeepCache</A>
										<DT><A HREF="https://huggingface.co/docs/diffusers/main/en/optimization/deepcache">DeepCache</A>
										<DT><A HREF="https://arxiv.org/pdf/2312.00858.pdf">https://arxiv.org/pdf/2312.00858.pdf</A>
									</DL><p>
									<DT><A HREF="https://github.com/dbolya/tomesd">dbolya/tomesd: Speed up Stable Diffusion with this one simple trick!</A>
									<DT><A HREF="https://github.com/chengzeyi/stable-fast/issues/110">Support for DeepCache ¬∑ Issue #110 ¬∑ chengzeyi/stable-fast</A>
									<DT><A HREF="https://developer.nvidia.com/blog/nvidia-h200-tensor-core-gpus-and-nvidia-tensorrt-llm-set-mlperf-llm-inference-records/">NVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM Inference Records | NVIDIA Technical Blog</A>
									<DT><A HREF="https://blogs.nvidia.com/blog/tensorrt-llm-inference-mlperf/">NVIDIA Hopper Leaps Ahead in Generative AI at MLPerf | NVIDIA Blog</A>
									<DT><A HREF="https://arxiv.org/abs/2312.00858">[2312.00858] DeepCache: Accelerating Diffusion Models for Free</A>
									<DT><A HREF="https://developer.nvidia.com/blog/nvidia-h200-tensor-core-gpus-and-nvidia-tensorrt-llm-set-mlperf-llm-inference-records/">NVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM</A>
									<DT><A HREF="https://blogs.nvidia.com/blog/tensorrt-llm-inference-mlperf/">NVIDIA Hopper Leaps Ahead in Generative AI at MLPerf</A>
								</DL><p>
								<DT><H3 FOLDED>diffusers</H3>
								<DL><p>
									<DT><H3 FOLDED>diffusers-imports</H3>
									<DL><p>
										<DT><A HREF="https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py">optuna/optuna/integration/__init__.py at master</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/42cae93b942ec904ead46c26c42be24422adc92c/src/diffusers/utils/import_utils.py#L760">diffusers/src/diffusers/utils/import_utils.py</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/67bef2027cc461af5bbe73b3c0f35bb1350f5aa8/src/diffusers/pipelines/consistency_models/__init__.py">diffusers/src/diffusers/pipelines/consistency_models/__init__.py</A>
									</DL><p>
									<DT><A HREF="https://github.com/huggingface/diffusers/blob/fe5f035f797a5fa663a98030c9d0ec2f982cd09d/docs/source/en/using-diffusers/custom_pipeline_overview.md">diffusers/docs/source/en/using-diffusers/custom_pipeline_overview.md</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=JgP2WgNIq_w">How to Deploy HuggingFace‚Äôs Stable Diffusion Pipeline with Triton Inference Server</A>
								<DT><A HREF="https://developer.nvidia.com/blog/nvidia-h200-tensor-core-gpus-and-nvidia-tensorrt-llm-set-mlperf-llm-inference-records/">NVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM</A>
								<DT><A HREF="https://mlcommons.org/benchmarks/inference-datacenter/">Benchmark MLPerf Inference: Datacenter | MLCommons V3.1</A>
								<DT><A HREF="https://huggingface.co/ByteDance/SDXL-Lightning">ByteDance/SDXL-Lightning ¬∑ Hugging Face</A>
								<DT><A HREF="https://github.com/huggingface/diffusers/blob/fe5f035f797a5fa663a98030c9d0ec2f982cd09d/docs/source/en/using-diffusers/custom_pipeline_overview.md">diffusers/docs/source/en/using-diffusers/custom_pipeline_overview.md</A>
							</DL><p>
							<DT><H3 FOLDED>Triton Inference Server</H3>
							<DL><p>
								<DT><H3 FOLDED>FasterTransformer (TensorRT-LLM)</H3>
								<DL><p>
									<DT><A HREF="https://nvidia.github.io/TensorRT-LLM/">Welcome to TensorRT-LLM‚Äôs Documentation! ‚Äî tensorrt_llm documentation</A>
									<DT><A HREF="https://github.com/triton-inference-server/fastertransformer_backend#run-inter-node-t-x-p--gpus-per-node-models">triton-inference-server/fastertransformer_backend</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0">TensorRT-LLM</A>
									<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend">triton-inference-server/tensorrtllm_backend: The Triton TensorRT-LLM Backend</A>
									<DT><A HREF="https://carper.ai/diff-models-a-new-way-to-edit-code/">Diff Models ‚Äì A New Way to Edit Code | CarperAI</A>
								</DL><p>
								<DT><H3 FOLDED>client</H3>
								<DL><p>
									<DT><A HREF="https://github.com/triton-inference-server/client">triton-inference-server/client</A>
								</DL><p>
								<DT><H3 FOLDED>server</H3>
								<DL><p>
									<DT><A HREF="https://github.com/triton-inference-server/server">server</A>
									<DT><H3 FOLDED>dynamic batching</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>concurrent model execution</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=JgP2WgNIq_w">How to Deploy HuggingFace‚Äôs Stable Diffusion Pipeline with Triton Inference Server</A>
								</DL><p>
								<DT><H3 FOLDED>backends</H3>
								<DL><p>
									<DT><H3 FOLDED>TensorRT-LLM</H3>
									<DL><p>
										<DT><H3 FOLDED>TensorRT</H3>
										<DL><p>
											<DT><A HREF="https://segmentfault.com/a/1190000039977778">Introduction</A>
											<DT><A HREF="https://www.youtube.com/watch?v=eGDMJ3MY4zk&t=450s">Accelerated Inference in PyTorch 2.X with Torch-TensorRT</A>
											<DT><A HREF="https://github.com/pytorch/TensorRT">pytorch/TensorRT: PyTorch/TorchScript/FX compiler for NVIDIA GPUs using TensorRT</A>
											<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51714/">Exploring TensorRT</A>
											<DT><A HREF="https://pytorch.org/TensorRT/ts/getting_started_with_python_api.html#getting-started-with-python-api">Using Torch-TensorRT in Python ‚Äî Torch-TensorRT v2.2.0.dev0+4da330d documentation</A>
											<DT><A HREF="https://pytorch.org/TensorRT/py_api/torch_tensorrt.html">torch_tensorrt ‚Äî Torch-TensorRT</A>
											<DT><A HREF="https://pypi.org/project/torch-tensorrt/">torch-tensorrt ¬∑ PyPI</A>
											<DT><A HREF="https://pytorch.org/TensorRT/">Torch-TensorRT ‚Äî Torch-TensorRT v2.2.0.dev</A>
											<DT><A HREF="https://developer.nvidia.com/tensorrt">TensorRT SDK | NVIDIA Developer</A>
											<DT><A HREF="https://els-rd.github.io/transformer-deploy/python/">Direct use TensorRT in Python script (no server)</A>
											<DT><A HREF="https://www.photoroom.com/inside-photoroom/stable-diffusion-25-percent-faster-and-save-seconds">Stable Diffusion</A>
											<DT><A HREF="https://github.com/wangzyon/trt_learn">wangzyon/trt_learn: TensorRT encapsulation, learn, rewrite, practice.</A>
											<DT><A HREF="https://leimao.github.io/blog/Docker-TensorRT/">TensorRT In Docker - Lei Mao's Log Book</A>
											<DT><A HREF="https://github.com/stars/pommedeterresautee/lists/quantization">pommedeterresautee's list / quantization</A>
										</DL><p>
										<DT><H3 FOLDED>tensorrt-llm-build</H3>
										<DL><p>
											<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5d8ca2faf74c494f220c8f71130340b513eea9a9/docs/source/installation/build-from-source-linux.md">TensorRT-LLM/docs/source/installation/build-from-source-linux.md at 5d8ca2faf74c494f220c8f71130340b513eea9a9 ¬∑ NVIDIA/TensorRT-LLM</A>
										</DL><p>
										<DT><A HREF="https://nvidia.github.io/TensorRT-LLM/">Welcome to TensorRT-LLM‚Äôs Documentation! ‚Äî tensorrt_llm documentation</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/main/docs/source">TensorRT-LLM/docs/source at main ¬∑ NVIDIA/TensorRT-LLM</A>
										<DT><A HREF="https://github.com/triton-inference-server/fastertransformer_backend#run-inter-node-t-x-p--gpus-per-node-models">triton-inference-server/fastertransformer_backend</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0">TensorRT-LLM</A>
										<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend">triton-inference-server/tensorrtllm_backend: The Triton TensorRT-LLM Backend</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/README.md">TensorRT-LLM/examples/openai_triton/README.md</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/openai_triton/manual_plugin">TensorRT-LLM/examples/openai_triton/manual_plugin</A>
									</DL><p>
									<DT><H3 FOLDED>FasterTransformer</H3>
									<DL><p>
										<DT><A HREF="https://github.com/triton-inference-server/fastertransformer_backend">triton-inference-server/fastertransformer_backend</A>
										<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/blob/main/docs/t5_guide.md">FasterTransformer/docs/t5_guide.md at main ¬∑ NVIDIA/FasterTransformer</A>
										<DT><A HREF="https://github.com/triton-inference-server/fastertransformer_backend#run-inter-node-t-x-p--gpus-per-node-models">triton-inference-server/fastertransformer_backend</A>
										<DT><A HREF="https://www.youtube.com/watch?v=MDqNwSTLimU">[NVIDIA] Faster Transformer</A>
										<DT><A HREF="https://github.com/OpenNMT/OpenNMT-tf">DecoderTransformer: OpenNMT/OpenNMT-tf</A>
										<DT><A HREF="https://github.com/NVIDIA/FasterTransformer">NVIDIA/FasterTransformer: Transformer related optimization, including BERT, GPT</A>
										<DT><A HREF="https://carper.ai/diff-models-a-new-way-to-edit-code/">Diff Models ‚Äì A New Way to Edit Code | CarperAI</A>
									</DL><p>
									<DT><A HREF="https://github.com/triton-inference-server/fastertransformer_backend">triton-inference-server/fastertransformer_backend</A>
									<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend">triton-inference-server/tensorrtllm_backend: The Triton TensorRT-LLM Backend</A>
									<DT><A HREF="https://github.com/triton-inference-server/backend/blob/main/docs/backend_platform_support_matrix.md">backend/backend_platform_support_matrix.md</A>
								</DL><p>
								<DT><A HREF="https://github.com/triton-inference-server/server">triton-inference-server/server: The Triton Inference Server provides an optimized cloud and edge inferencing solution.</A>
								<DT><A HREF="https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer">Triton Inference Server - FasterTransformer GPT-J and GPT-NeoX 20B - CoreWeave</A>
								<DT><A HREF="https://docs.coreweave.com/machine-learning-and-ai/inference/examples/triton-inference/triton-inference-server-fastertransformer#gpt-neox-20b">FasterTransformer GPT-J and GPT: NeoX 20B - CoreWeave</A>
								<DT><A HREF="https://github.com/triton-inference-server/server/blob/main/CONTRIBUTING.md">Server</A>
								<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT/triton">Deploying the BERT model on Triton Inference Server</A>
								<DT><A HREF="https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html">GPipe</A>
								<DT><A HREF="https://developer.nvidia.com/blog/accelerated-inference-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inference-server/?nvid=nv-int-txtad-664399-vt27#cid=an01_nv-int-txtad_en-us">Accelerated Inference for Large Transformer Models Using NVIDIA Triton Inference Server | NVIDIA Technical Blog</A>
								<DT><A HREF="https://developer.nvidia.com/blog/deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server/">Deploying GPT-J and T5 with NVIDIA Triton Inference Server | NVIDIA Technical Blog</A>
								<DT><A HREF="https://developer.nvidia.com/blog/power-your-ai-inference-with-new-nvidia-triton-and-nvidia-tensorrt-features/?ncid=so-link-726143&=&linkId=100000196119468#cid=dl05_so-link_en-us">PyTriton</A>
								<DT><A HREF="https://www.reddit.com/r/MachineLearning/comments/qn8com/p_optimization_of_hugging_face_transformer_models/">Nvidia TensorRT + Nvidia Triton inference server</A>
								<DT><A HREF="https://medium.com/nvidia-ai/how-to-deploy-almost-any-hugging-face-model-on-nvidia-triton-inference-server-with-an-8ee7ec0e6fc4">Big Picture: Example showcase demo</A>
								<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-S52370/?ncid=em-even-124008-vt33">Inference of Large Language Models with NVIDIA Triton Inference Server (Presented by CoreWeave) | NVIDIA On-Demand</A>
								<DT><A HREF="https://github.com/ELS-RD/transformer-deploy">Transformer Deployment</A>
								<DT><A HREF="https://carper.ai/diff-models-a-new-way-to-edit-code/">Diff Models ‚Äì A New Way to Edit Code | CarperAI</A>
							</DL><p>
							<DT><H3 FOLDED>Torch Serve</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch/serve">pytorch/serve: Serve, optimize and scale PyTorch models in production</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/the-future-of-c-model-deployment/1282">The future of C++ model deployment</A>
							</DL><p>
							<DT><H3 FOLDED>Tensorflow Serving</H3>
							<DL><p>
								<DT><H3 FOLDED>Saxml</H3>
								<DL><p>
									<DT><H3 FOLDED>serving-alpa</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/zhuohan123/status/1629007867834695681">AlpaServe: model parallelism</A>
										<DT><A HREF="https://www.youtube.com/watch?v=qzYoMldlyoA&t=13s">Trends Driving Big Models</A>
										<DT><A HREF="https://www.youtube.com/watch?v=LGYYRRKxCjE">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning - YouTube</A>
									</DL><p>
									<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
									<DT><A HREF="https://cloud.google.com/kubernetes-engine/docs/tutorials/tpu-multihost-saxml#load_the_model">Serve an LLM using multi-host TPUs on GKE with Saxml ¬†|¬† Kubernetes Engine ¬†|¬† Google Cloud</A>
									<DT><A HREF="https://github.com/google/saxml/blob/e060b11a7c2de1cca75370a8cddd8f89d6f4664c/saxml/server/pax/lm/servable_lm_model_test.py#L279">saxml/saxml/server/pax/lm/servable_lm_model_test.py</A>
									<DT><A HREF="https://github.com/google/JetStream/tree/main">google/JetStream: A throughput and memory optimized engine for LLM inference on TPUs!</A>
								</DL><p>
								<DT><H3 FOLDED>jax-serving</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/serving/README.md">jax2tf: serving examples</A>
								</DL><p>
								<DT><A HREF="https://alpa.ai/tutorials/opt_serving.html#launch-a-web-server-to-serve-the-opt-models">Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa ‚Äî Alpa 0.2.3.dev17 documentation</A>
								<DT><A HREF="https://github.com/alpa-projects/alpa">alpa-projects/alpa: Training and serving large-scale neural networks with auto parallelization.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=e85Ceq2g5z0&t=1s">(Day 1 - Breakout Session) StableHLO &amp; PJRT - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-XyysbKrShk">PyTorch üíô XLA - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>Deepspeed-MII</H3>
							<DL><p>
								<DT><H3 FOLDED>Deepspeed-MII-engine</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md">DeepSpeed/inference-tutorial.md at master ¬∑ microsoft/DeepSpeed ¬∑ GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeedExamples">microsoft/DeepSpeedExamples: Example models using DeepSpeed</A>
									<DT><A HREF="https://github.com/huggingface/accelerate/blob/55691b14c21748319bc7004f09d2ea6019e71a25/benchmarks/big_model_inference.py#L100">accelerate/big_model_inference.py at 55691b14c21748319bc7004f09d2ea6019e71a25 ¬∑ huggingface/accelerate</A>
									<DT><A HREF="https://lightning.ai/pages/community/serve-stable-diffusion-three-times-faster/">Stable Diffusion kernel injection</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/pull/4604">DeepSpeed-FastGen by cmikeh2 ¬∑ Pull Request #4604 ¬∑ microsoft/DeepSpeed</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/4828d71d076b9a5cbe7ef48007cc5b51907c3319/blogs/deepspeed-fastgen/README.md">DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</A>
								</DL><p>
								<DT><H3 FOLDED>Deepspeed-MII-architecture</H3>
								<DL><p>
									<DT><A HREF="https://github.com/antferdom/infrastructure/blob/master/rest_server/server.py">infrastructure/server.py at master ¬∑ antferdom/infrastructure ¬∑ GitHub</A>
									<DT><A HREF="https://github.com/pythonprofilers/memory_profiler">pythonprofilers/memory_profiler: Monitor Memory usage of Python code</A>
									<DT><A HREF="https://github.com/huggingface/accelerate/blob/3cb9d5fd9c78c1da9fbc3127d6e63679a2475c6a/src/accelerate/utils/modeling.py#L443">accelerate/modeling.py at 3cb9d5fd9c78c1da9fbc3127d6e63679a2475c6a ¬∑ huggingface/accelerate</A>
									<DT><A HREF="https://github.com/huggingface/accelerate/blob/5e6351502aa2117d9f73da4c001e9baa87c65b67/tests/test_big_modeling.py#L600">accelerate/test_big_modeling.py at 5e6351502aa2117d9f73da4c001e9baa87c65b67 ¬∑ huggingface/accelerate</A>
									<DT><A HREF="https://github.com/huggingface/accelerate/blob/55691b14c21748319bc7004f09d2ea6019e71a25/benchmarks/big_model_inference.py#L100">accelerate/big_model_inference.py at 55691b14c21748319bc7004f09d2ea6019e71a25 ¬∑ huggingface/accelerate</A>
									<DT><A HREF="https://github.com/huggingface/transformers/blob/370f0ca18c8e4577357df59936e790acdecef4ac/tests/models/t5/test_modeling_tf_t5.py#L551">transformers/test_modeling_tf_t5.py at 370f0ca18c8e4577357df59936e790acdecef4ac ¬∑ huggingface/transformers</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TableQuestionAnsweringPipeline">Pipelines</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline">Pipelines</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeedExamples">microsoft/DeepSpeedExamples: Example models using DeepSpeed</A>
									<DT><A HREF="https://arxiv.org/pdf/2207.00032.pdf">https://arxiv.org/pdf/2207.00032.pdf</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md">DeepSpeed/inference-tutorial.md at master ¬∑ microsoft/DeepSpeed ¬∑ GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/server.py">DeepSpeed-MII/server.py at main ¬∑ microsoft/DeepSpeed-MII ¬∑ GitHub</A>
									<DT><A HREF="https://github.com/mallorbc/Finetune_LLMs/blob/main/inference/query.py">Finetune_LLMs/query.py at main ¬∑ mallorbc/Finetune_LLMs ¬∑ GitHub</A>
								</DL><p>
								<DT><H3 FOLDED>Deepspeed-MII-deployment-stack-trace</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/deployment.py#LL128C1-L129C1">DeepSpeed-MII/deployment.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 ¬∑ microsoft/DeepSpeed-MII ¬∑ GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/models/score/generate.py">DeepSpeed-MII/generate.py at main ¬∑ microsoft/DeepSpeed-MII ¬∑ GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/models/score/generate.py#L12">DeepSpeed-MII/generate.py at main ¬∑ microsoft/DeepSpeed-MII</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/models/score/score_template.py">DeepSpeed-MII/score_template.py at main ¬∑ microsoft/DeepSpeed-MII ¬∑ GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/deployment.py#L147">DeepSpeed-MII/deployment.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 ¬∑ microsoft/DeepSpeed-MII</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/utils.py#L155">DeepSpeed-MII/utils.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 ¬∑ microsoft/DeepSpeed-MII</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/server.py">DeepSpeed-MII/server.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 ¬∑ microsoft/DeepSpeed-MII ¬∑ GitHub</A>
								</DL><p>
								<DT><H3 FOLDED>Deepspeed-MII-benchmarking</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/issues/204">Issue #204: Benchmarking MII performance</A>
								</DL><p>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII#getting-started-with-mii">microsoft/DeepSpeed-MII: MII makes low-latency and high-throughput inference possible, powered by DeepSpeed.</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/examples/local/text-generation-zero-example.py#L24">DeepSpeed-MII/text-generation-zero-example.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 ¬∑ microsoft/DeepSpeed-MII</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/models/load_models.py">DeepSpeed-MII/load_models.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 ¬∑ microsoft/DeepSpeed-MII ¬∑ GitHub</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/examples/benchmark/txt2img/baseline-sd.py">DeepSpeed-MII/baseline-sd.py at main ¬∑ microsoft/DeepSpeed-MII ¬∑ GitHub</A>
								<DT><A HREF="https://github.com/huggingface/diffusers/blob/v0.7.1/examples/text_to_image/train_text_to_image.py#L603">diffusers/train_text_to_image.py at v0.7.1 ¬∑ huggingface/diffusers ¬∑ GitHub</A>
								<DT><A HREF="https://github.com/huggingface/diffusers/blob/d2285f51589bbee18673272611b709d306e7f911/examples/research_projects/intel_opts/textual_inversion_dfq/text2images.py#L68">diffusers/text2images.py at d2285f51589bbee18673272611b709d306e7f911 ¬∑ huggingface/diffusers</A>
								<DT><A HREF="https://github.com/pytorch/serve">pytorch/serve: Serve, optimize and scale PyTorch models in production</A>
								<DT><A HREF="https://docs.python.org/3/library/inspect.html">inspect ‚Äî Inspect live objects ‚Äî Python 3.11.3 documentation</A>
								<DT><A HREF="https://www.deepspeed.ai/2022/09/09/zero-inference.html">ZeRO-Inference: Democratizing massive model inference - DeepSpeed</A>
								<DT><A HREF="https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/welcoming-mistral-phi-jais-code-llama-nvidia-nemotron-and-more/ba-p/3982699#:~:text=Today,%20we%20are%20also%20pleased,API%20endpoint%20to%20their%20applications.">Azure AI: Model as a Service</A>
								<DT><A HREF="https://github.com/Azure/azure-sdk-for-python">Azure/azure-sdk-for-python: This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://docs.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python.</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen">FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</A>
								<DT><A HREF="https://www.youtube.com/watch?v=fkHYRWMmTmM">[short] DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference - YouTube</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen">DeepSpeed/blogs/deepspeed-fastgen at master ¬∑ microsoft/DeepSpeed</A>
							</DL><p>
							<DT><H3 FOLDED>Text Generation Inference (TGI)</H3>
							<DL><p>
								<DT><H3 FOLDED>tgi-client</H3>
								<DL><p>
									<DT><A HREF="https://superfastpython.com/asyncio-gather/">How to Use asyncio.gather() in Python</A>
									<DT><A HREF="https://stackoverflow.com/questions/2785954/creating-a-list-in-python-with-multiple-copies-of-a-given-object-in-a-single-lin">Creating a list in Python with multiple copies of a given object in a single line - Stack Overflow</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-launcher</H3>
								<DL><p>
									<DT><H3 FOLDED>PyTorch checkpointing</H3>
									<DL><p>
										<DT><A HREF="https://mail.google.com/mail/u/0/#inbox?compose=CllgCKCFShQFztGvfWXCdvvFKVPMZgRSNlMlmGFlhSNBPphbqHsjpFVWTxZZBRhfsPrlVtnVXWL">Inbox - antonio.jfdominguez@gmail.com - Gmail</A>
										<DT><A HREF="https://github.com/huggingface/safetensors/blob/96061e97bb7fc4ea6cdd1f79f58701efc4710d22/docs/source/index.mdx#L41">safetensors/docs/source/index.mdx at 96061e97bb7fc4ea6cdd1f79f58701efc4710d22 ¬∑ huggingface/safetensors</A>
										<DT><A HREF="https://github.com/huggingface/safetensors/blob/96061e97bb7fc4ea6cdd1f79f58701efc4710d22/bindings/python/tests/test_pt_comparison.py#L194">safetensors/bindings/python/tests/test_pt_comparison.py at 96061e97bb7fc4ea6cdd1f79f58701efc4710d22 ¬∑ huggingface/safetensors</A>
										<DT><A HREF="https://chat.openai.com/c/ac65640c-ae35-4308-bc9e-208baf30a139">Optimizing PyTorch Distributed Launch</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/228b31047858eda15d58a8bc03831f197e4c2120/extra/utils.py#L37">tinygrad/extra/utils.py at 228b31047858eda15d58a8bc03831f197e4c2120 ¬∑ tinygrad/tinygrad</A>
										<DT><A HREF="https://github.com/facebookresearch/llama-recipes/blob/373000b2ac13f3e52b5df11cec79ed5f2e5b9cbe/src/llama_recipes/inference/model_utils.py#L8">llama-recipes/src/llama_recipes/inference/model_utils.py at 373000b2ac13f3e52b5df11cec79ed5f2e5b9cbe ¬∑ facebookresearch/llama-recipes</A>
										<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/c8d6a1f34ab6f1b6bd468d256e535a61f98f114c/convert.py#L1016">llama.cpp/convert.py at c8d6a1f34ab6f1b6bd468d256e535a61f98f114c ¬∑ ggerganov/llama.cpp</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/serialization.py#L866">pytorch/torch/serialization.py at main ¬∑ pytorch/pytorch</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/96a982ad8fc232479384476b1596a880697cc1d0/Makefile">text-generation-inference/Makefile at 96a982ad8fc232479384476b1596a880697cc1d0 ¬∑ huggingface/text-generation-inference</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/96a982ad8fc232479384476b1596a880697cc1d0/server/text_generation_server/utils/weights.py">text-generation-inference/server/text_generation_server/utils/weights.py at 96a982ad8fc232479384476b1596a880697cc1d0 ¬∑ huggingface/text-generation-inference</A>
										<DT><A HREF="https://pytorch.org/docs/stable/elastic/run.html">torchrun (Elastic Launch) ‚Äî PyTorch 2.1 documentation</A>
										<DT><A HREF="https://huggingface.co/bigscience/bloom-560m/blob/main/config.json">config.json ¬∑ bigscience/bloom-560m at main</A>
										<DT><A HREF="https://pytorch.org/torchx/latest/cli.html">CLI ‚Äî PyTorch/TorchX main documentation</A>
										<DT><A HREF="https://github.githistory.xyz/huggingface/text-generation-inference/blob/main/Makefile">Git History - Makefile</A>
										<DT><A HREF="https://github.com/grpc/grpc/tree/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/python/uds">grpc/examples/python/uds at d68161a64f191b8d8d5afe0507e7a2291f91ff1a ¬∑ grpc/grpc</A>
										<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
										<DT><A HREF="https://github.com/datacrunch-research/text-generation-inference/commit/fbf28546706038db51870bd53684ceb528affe6b">(COMPLETE): minimal working PyTorch checkpointing in launcher ¬∑ datacrunch-research/text-generation-inference@fbf2854</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>tgi-shardmanager</H3>
								<DL><p>
									<DT><A HREF="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/">Scaling services with Shard Manager - Engineering at Meta</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-router (serving system)</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/tree/main/router">README</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/469">FastTokenizer: heuristics for the scheduler</A>
									<DT><A HREF="https://www.usenix.org/conference/osdi22/presentation/yu">Orca: A Distributed Serving System for Transformer-Based Generative Models | USENIX</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/pull/210">feat(router): Dynamic batch sizing</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/pull/210">Dynamic batch sizing</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/batch-manager.md">TensorRT-LLM/docs/source/advanced/batch-manager.md</A>
									<DT><A HREF="https://github.com/IBM/text-generation-router">IBM/text-generation-router: Routing proxy for TGIS</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-inference-engine</H3>
								<DL><p>
									<DT><H3 FOLDED>tgi-quantization</H3>
									<DL><p>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/pull/438">Inference support for GPTQ (llama + falcon tested) + Quantization script by Narsil ¬∑ Pull Request #438 ¬∑ huggingface/text-generation-inference</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/455">SPQR discussion (Meta AI)</A>
									</DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/376">Improve inference speed of Santacoder and Starcoder (and others) ¬∑ Issue #376 ¬∑ huggingface/text-generation-inference</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-inference-container</H3>
								<DL><p>
									<DT><H3 FOLDED>container</H3>
									<DL><p>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/Dockerfile">text-generation-inference/Dockerfile at main ¬∑ huggingface/text-generation-inference ¬∑ GitHub</A>
									</DL><p>
									<DT><A HREF="https://github.com/awslabs/llm-hosting-container">awslabs/llm-hosting-container: Large Language Model Hosting Container</A>
									<DT><A HREF="https://github.com/aws/sagemaker-python-sdk">aws/sagemaker-python-sdk: A library for training and deploying machine learning models on Amazon SageMaker</A>
									<DT><A HREF="https://github.com/aws/deep-learning-containers/blob/master/pytorch/inference/buildspec.yml">deep-learning-containers/buildspec.yml at master ¬∑ aws/deep-learning-containers ¬∑ GitHub</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-benchmark</H3>
								<DL><p>
									<DT><A HREF="https://github.com/bigcode-project/bigcode-inference-benchmark/blob/main/scripts/run_grid.sh">bigcode-inference-benchmark: run_grid.sh</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-profiling</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/1014">Profile TGI with nsys launch/start/stop lead to Error: ShardCannotStart ¬∑ Issue #1014 ¬∑ huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/1129">multi-gpu</A>
									<DT><A HREF="https://developer.nvidia.com/nsight-systems">Nsight Systems | NVIDIA Developer | NVIDIA Developer</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/863">NCCL nsight-systems</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-fork</H3>
								<DL><p>
									<DT><H3 FOLDED>IBM</H3>
									<DL><p>
										<DT><A HREF="https://github.com/IBM/text-generation-inference/blob/main/integration_tests/text_generation_tests/test_server.py">integration_tests: test_server.py</A>
										<DT><A HREF="https://twitter.com/YiTayML/status/1714315484357857766">The Effiency Misnomer</A>
										<DT><A HREF="https://github.com/foundation-model-stack/foundation-model-stack">foundation-model-stack/foundation-model-stack</A>
										<DT><A HREF="https://github.com/IBM/text-generation-inference">IBM/text-generation-inference: IBM development fork</A>
										<DT><A HREF="https://github.com/caikit/caikit-nlp">caikit/caikit-nlp</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>tgi-videos</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=6OozhhI6U4g&t=3483s">Open Assistant Inference Backend Development (Hands-On Coding) - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=jlMAX2Oaht0">Text-generation-inference (TGI) deployment optimization and benchmarking - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=jKbbvy-xB4w&t=1894s">HuggingFace: Text Generation Inference: Part 1 - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-embeddings</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/199">[Feature] Return embeddings ¬∑ Issue #199 ¬∑ huggingface/text-generation-inference</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-billing</H3>
								<DL><p>
									<DT><A HREF="https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#example-response-2">OpenAI GPT Response: usage</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/pull/578">enh: Adding additional response header X-Total-Tokens by brightsparc ¬∑ Pull Request #578 ¬∑ huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/637">Return total tokens generated as an http response header ¬∑ Issue #637 ¬∑ huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/435">Request a new api endpoint to check and retrieve token length for given text/prompt ¬∑ Issue #435 ¬∑ huggingface/text-generation-inference</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-discussion</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/1819">Planned/Potential of significant work #1819</A>
								</DL><p>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference">huggingface/text-generation-inference</A>
								<DT><A HREF="https://huggingface.co/inference-endpoints">Inference Endpoints - Hugging Face</A>
								<DT><A HREF="https://huggingface.co/docs/transformers/v4.28.1/en/pipeline_webserver">Using pipelines for a webserver</A>
								<DT><A HREF="https://huggingface.co/bigscience/mt0-xxl-mt">bigscience/mt0-xxl-mt ¬∑ Hugging Face</A>
								<DT><A HREF="https://github.com/google-research/multilingual-t5">google-research/multilingual-t5</A>
								<DT><A HREF="https://huggingface.github.io/text-generation-inference/">Text Generation Inference API</A>
								<DT><A HREF="https://twitter.com/yacineMTB/status/1691208981698498560">HF Transformers Software Complexity</A>
								<DT><A HREF="https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c">Hugging Face Transformer Inference Under 1 Millisecond Latency</A>
							</DL><p>
							<DT><H3 FOLDED>vLLM</H3>
							<DL><p>
								<DT><H3 FOLDED>nm-vllm</H3>
								<DL><p>
									<DT><H3 FOLDED>vllm-fp8</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/collections/neuralmagic/fp8-llms-for-vllm-666742ed2b78b7ac8df13127">FP8 LLMs for vLLM - a neuralmagic Collection</A>
									</DL><p>
									<DT><A HREF="https://github.com/neuralmagic/nm-vllm">neuralmagic/nm-vllm (fork)</A>
									<DT><A HREF="https://github.com/neuralmagic/nm-vllm/blob/788b4e526d379aa6b910cb1932a756d17cbcc997/vllm/model_executor/weight_utils.py#L81">weight_utils.py#L81 convert_bin_to_safetensor_file (checkpoint format conversion procedure)</A>
									<DT><A HREF="https://github.com/datacrunch-research/transmogrifier/blob/main/transmogrifier/convert.py#L156">transmogrifier/transmogrifier/convert.py at main ¬∑ datacrunch-research/transmogrifier</A>
									<DT><A HREF="https://github.com/neuralmagic/AutoFP8">neuralmagic/AutoFP8</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024">DeepSpeed/blogs/deepspeed-fp6/03-05-2024 at master ¬∑ microsoft/DeepSpeed</A>
									<DT><A HREF="https://github.com/neuralmagic/compressed-tensors">neuralmagic/compressed-tensors: A safetensors extension to efficiently store sparse quantized tensors on disk</A>
								</DL><p>
								<DT><H3 FOLDED>vllm-splitwise</H3>
								<DL><p>
									<DT><A HREF="https://github.com/vllm-project/vllm/pull/2809">Add Splitwise implementation to vLLM</A>
									<DT><A HREF="https://www.microsoft.com/en-us/research/blog/splitwise-improves-gpu-usage-by-splitting-llm-inference-phases/">Splitwise improves GPU usage by splitting LLM inference phases - Microsoft Research</A>
									<DT><A HREF="https://arxiv.org/abs/2311.18677">[2311.18677] Splitwise: Efficient generative LLM inference using phase splitting</A>
									<DT><A HREF="https://github.com/Mutinifni/splitwise-sim">Mutinifni/splitwise-sim: LLM serving cluster simulator</A>
								</DL><p>
								<DT><H3 FOLDED>vllm-benchmark</H3>
								<DL><p>
									<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/launch_tgi_server.sh#L4">vllm/benchmarks/launch_tgi_server.sh</A>
									<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_throughput.py#L200">vllm/benchmarks/benchmark_throughput.py</A>
								</DL><p>
								<DT><H3 FOLDED>vllm-meetings</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit#slide=id.g2650ce3df47_0_0">vLLM @ Fourth Meetup (Public) - Google Slides</A>
									<DT><A HREF="https://docs.google.com/presentation/d/1OF6GBbxDNwlgwmyYiCyN98W3t2HNfFlhpFJlqJgp_aI/mobilepresent?slide=id.g273594aa4df_1_6">vLLM @ Fourth Meetup (Public) - Google Slides (Neural Magic)</A>
								</DL><p>
								<DT><A HREF="https://github.com/vllm-project/vllm">vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs</A>
								<DT><A HREF="https://vllm.ai/">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</A>
								<DT><A HREF="https://twitter.com/edacih/status/1671885000030191616">Continuous batching &amp; PagedAttention (TW thread)</A>
								<DT><A HREF="https://www.anyscale.com/blog/continuous-batching-llm-inference">Continuous batching &amp; PagedAttention</A>
								<DT><A HREF="https://github.com/EmbeddedLLM/vllm-rocm">EmbeddedLLM/vllm-rocm: ROCm-enabled vLLM: A high-throughput and memory-efficient inference and serving engine for LLMs</A>
								<DT><A HREF="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda/src/llm/paged/cache_engine.rs">aici/rllm/rllm-cuda/src/llm/paged/cache_engine.rs</A>
								<DT><A HREF="https://github.com/triton-inference-server/tutorials/blob/main/Quick_Deploy/vLLM/README.md#deploying-a-vllm-model-in-triton">tutorials/Quick_Deploy/vLLM/README.md</A>
								<DT><A HREF="https://www.youtube.com/watch?v=xPvjwqX1m_I">vLLM and Neural Magic Office Hours - June 5, 2024 - YouTube</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/4a6769053ab2616f7f490e6ec5b8241e76ef0c2a/vllm/envs.py#L4">vllm/vllm/envs.py</A>
							</DL><p>
							<DT><H3 FOLDED>serving-coreweave</H3>
							<DL><p>
								<DT><A HREF="https://docs.coreweave.com/coreweave-machine-learning-and-ai/inference">Inference | CoreWeave</A>
								<DT><A HREF="https://docs.coreweave.com/">CoreWeave Cloud - CoreWeave</A>
								<DT><A HREF="https://github.com/kserve/kserve">kserve/kserve: Standardized Serverless ML Inference Platform on Kubernetes</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference">huggingface/text-generation-inference: Large Language Model Text Generation Inference</A>
								<DT><A HREF="https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer">https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer</A>
							</DL><p>
							<DT><H3 FOLDED>serving-modal</H3>
							<DL><p>
								<DT><A HREF="https://modal.com/docs/examples/trtllm_llama">Serverless TensorRT-LLM (LLaMA 3 8B) | Modal Docs</A>
								<DT><A HREF="https://github.com/modal-labs">Modal Labs</A>
								<DT><A HREF="https://github.com/modal-labs/serverless-queuing-theory-model">modal-labs/serverless-queuing-theory-model: Implementation of a serverless queuing theory model to explore optimal policies and tradeoffs between latency &amp; utilization</A>
								<DT><A HREF="https://github.com/modal-labs/mountpoint-s3">modal-labs/mountpoint-s3: A simple, high-throughput file client for mounting an Amazon S3 bucket as a local file system.</A>
								<DT><A HREF="https://github.com/modal-labs/modal-examples/blob/main/07_web_endpoints/basic_web.py">modal-examples/07_web_endpoints/basic_web.py at main ¬∑ modal-labs/modal-examples</A>
							</DL><p>
							<DT><H3 FOLDED>AWS DJL</H3>
							<DL><p>
								<DT><A HREF="https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-fastertransformer.html">Large model inference with FasterTransformer and DJL Serving - Amazon SageMaker</A>
								<DT><A HREF="https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/llm-workshop/lab5-flan-t5-xxl/flan-xxl-sagemaker-fastertransformer-s5cmd.ipynb">amazon-sagemaker-examples/inference/generativeai/llm-workshop/lab5-flan-t5-xxl/flan-xxl-sagemaker-fastertransformer-s5cmd.ipynb at main ¬∑ aws/amazon-sagemaker-examples</A>
								<DT><A HREF="https://github.com/deepjavalibrary/djl-serving">deepjavalibrary/djl-serving: A universal scalable machine learning model deployment solution</A>
								<DT><A HREF="https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/optimizations/aitemplate/lmi-aitemplate-stablediff.ipynb">amazon-sagemaker-examples/inference/generativeai/optimizations/aitemplate/lmi-aitemplate-stablediff.ipynb at main ¬∑ aws/amazon-sagemaker-examples</A>
							</DL><p>
							<DT><H3 FOLDED>serving-checkpointing</H3>
							<DL><p>
								<DT><H3 FOLDED>nm-compressed-tensors</H3>
								<DL><p>
									<DT><A HREF="https://github.com/neuralmagic/compressed-tensors">neuralmagic/compressed-tensors: A safetensors extension to efficiently store sparse quantized tensors on disk</A>
								</DL><p>
								<DT><H3 FOLDED>serving-checkpointing-safetensors</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/200">equivalent of torch.load from io stream to gpu</A>
									<DT><A HREF="https://github.com/vladmandic/sd-loader/blob/main/bench.py">sd-loader/bench.py at main ¬∑ vladmandic/sd-loader</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/200">using fp16 or fp32 (load times)</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/18">Some clarification about PyTorch format</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/flash_llama.py#L216-L311">(TGI): Safetensors lazy_loading (flash_llama.py)</A>
									<DT><A HREF="https://huggingface.co/docs/safetensors/speed#gpu-benchmark">Speed Comparison</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/67">Support for model streaming (disk -&gt; VRAM)? ¬∑ Issue #67</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/pull/189/files">Adding memmap example to read from file</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/242">tensorstore</A>
								</DL><p>
								<DT><H3 FOLDED>serving-checkpointing-torch</H3>
								<DL><p>
									<DT><H3 FOLDED>distributed</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/blog/performant-distributed-checkpointing/?utm_content=258825622&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">PyTorch Distributed checkpointing in Production</A>
										<DT><A HREF="https://www.youtube.com/watch?v=ldBmHNva_Fw&t=31s">Distributed Checkpoint - Iris Zhang &amp; Chien-Chin Huang, Meta - YouTube</A>
										<DT><A HREF="https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html">Getting Started with Distributed Checkpoint (DCP)</A>
									</DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/state-of-model-creation-initialization-seralization-in-pytorch-core/1240">State of model creation/initialization/seralization in PyTorch Core</A>
									<DT><H3 FOLDED>skipping initialization of module parameters</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/29523">Option to skip random weight initialization at module instance creation</A>
										<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to_empty">meta device + added a to_empty function 2 from the meta device</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/nn/utils/init.py#L5">nn/utils/init.py at main Added device/dtype kwargs to all nn.Modules</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/96161">[torchdistx] Future of the large model initialization META device</A>
										<DT><A HREF="https://pytorch.org/torchdistx/latest/fake_tensor.html">Fake Tensor ‚Äî torchdistX 0.2.0 documentation</A>
										<DT><H3 FOLDED>problems: not solve 2x model parameters being in memory</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/90465">[FSDP] Revisit meta device initialization ¬∑ Issue #90465 ¬∑ pytorch/pytorch</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>loading parameters</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/module.py#L1941">pytorch/torch/nn/modules/module.py at main ¬∑ pytorch/pytorch</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/103405">(PROFILING) load model on infer so much memory on GPU and CPU ¬∑ Issue #103405</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/64601">Reuse tensors from state_dict in load_state_dict ¬∑ Issue #64601</A>
										<DT><A HREF="https://github.com/stas00/transformers/blob/main/src/transformers/modeling_utils.py#L2968">transformers/src/transformers/modeling_utils.py (reuse_tensor)</A>
										<DT><H3 FOLDED>state_dict</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/97196">Sequential/Partial unpickling and loading of models ¬∑ Issue #97196</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/75242">(stass) torch.load: that loads one submodule at a time</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/64327">RFC: multi-part `torch.load`/`torch.save` to support huge models and/or low CPU memory</A>
										</DL><p>
										<DT><H3 FOLDED>spliting</H3>
										<DL><p>
											<DT><A HREF="https://github.com/huggingface/transformers/issues/13548">RFC: split checkpoint load/save for huge models ¬∑ Issue #13548 ¬∑ huggingface/transformers</A>
											<DT><A HREF="https://github.com/finetunej/transformers/blob/ca5d90ac1965982db122a649c2c9c902bde74a03/src/transformers/modeling_utils.py#L417-L443">transformers/src/transformers/modeling_utils.py</A>
											<DT><A HREF="https://github.com/huggingface/transformers/blob/c030fc891395d11249046e36b9e0219685b33399/src/transformers/modeling_utils.py#L1045">floating_point_ops</A>
										</DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/87033">Saving and loading from physical storage ¬∑ Issue #87033</A>
										<DT><H3 FOLDED>mmap</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/619ae87a1d1ae086f59a64d3b71dbfe4af8b804a/torch/serialization.py#L1010">serialization.py: mmap</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/619ae87a1d1ae086f59a64d3b71dbfe4af8b804a/test/test_serialization.py">test_serialization_mmap_loading</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/pull/101446">[WIP] Let torch.load load memory-mapped tensors</A>
											<DT><A HREF="https://github.com/nadavrot/memset_benchmark?tab=readme-ov-file">nadavrot/memset_benchmark: This repository contains high-performance implementations of memset and memcpy in assembly.</A>
										</DL><p>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md">TorchScript serialization</A>
									<DT><A HREF="https://pytorch-dev-podcast.simplecast.com/episodes/serialization">Serialization | PyTorch Developer Podcast</A>
									<DT><A HREF="https://github.com/msaroufim/cpuoffload/tree/main">msaroufim/cpuoffload</A>
								</DL><p>
								<DT><H3 FOLDED>serving-checkpointing-DeepSpeed</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/issues/2379">DeepSpeed: Pre-sharding</A>
								</DL><p>
								<DT><H3 FOLDED>transmogrifier</H3>
								<DL><p>
									<DT><A HREF="https://mail.google.com/mail/u/0/#inbox?compose=CllgCKCFShQFztGvfWXCdvvFKVPMZgRSNlMlmGFlhSNBPphbqHsjpFVWTxZZBRhfsPrlVtnVXWL">Inbox - antonio.jfdominguez@gmail.com - Gmail</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py#L158">safetensors/bindings/python/convert.py at main ¬∑ huggingface/safetensors</A>
									<DT><A HREF="https://github.com/huggingface/huggingface_hub/blob/ef48c7f7311c0717db98f1b555c8e61d76fc1870/src/huggingface_hub/hf_api.py#L4225">https://github.com/huggingface/huggingface_hub/blob/ef48c7f7311c0717db98f1b555c8e61d76fc1870/src/huggingface_hub/hf_api.py#L4225</A>
									<DT><A HREF="https://chat.openai.com/c/3e8059da-4190-4d44-996e-38c5f08b0fb6">Argparse Args Programmatically</A>
									<DT><A HREF="https://github.com/datacrunch-research/transmogrifier/blob/main/transmogrifier/convert.py">transmogrifier/transmogrifier/convert.py at main ¬∑ datacrunch-research/transmogrifier</A>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/228b31047858eda15d58a8bc03831f197e4c2120/extra/utils.py#L37">https://github.com/tinygrad/tinygrad/blob/228b31047858eda15d58a8bc03831f197e4c2120/extra/utils.py#L37</A>
									<DT><A HREF="https://github.com/openai/triton/blob/96b04493f11a72bf4f2fcc746259ce84b10cc730/python/triton/testing.py#L160">https://github.com/openai/triton/blob/96b04493f11a72bf4f2fcc746259ce84b10cc730/python/triton/testing.py#L160</A>
									<DT><A HREF="https://github.com/facebookresearch/llama-recipes/blob/373000b2ac13f3e52b5df11cec79ed5f2e5b9cbe/src/llama_recipes/inference/model_utils.py#L8">llama-recipes/src/llama_recipes/inference/model_utils.py at 373000b2ac13f3e52b5df11cec79ed5f2e5b9cbe ¬∑ facebookresearch/llama-recipes</A>
									<DT><A HREF="https://www.notion.so/datacrunchio/model-transmogrifier-27445d5b11134f7d9745039fd66dbda8">https://www.notion.so/datacrunchio/model-transmogrifier-27445d5b11134f7d9745039fd66dbda8</A>
									<DT><A HREF="https://github.com/huggingface/hf_transfer">huggingface/hf_transfer</A>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/c8d6a1f34ab6f1b6bd468d256e535a61f98f114c/convert.py#L1016">llama.cpp/convert.py at c8d6a1f34ab6f1b6bd468d256e535a61f98f114c ¬∑ ggerganov/llama.cpp</A>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/tree/34b2a5e1ee4fe6295fb4420eb91131d743694c65">ggerganov/llama.cpp at 34b2a5e1ee4fe6295fb4420eb91131d743694c65</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/utils/gptq/exllama.py#L40">text-generation-inference/server/text_generation_server/utils/gptq/exllama.py at main ¬∑ huggingface/text-generation-inference</A>
									<DT><A HREF="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/tree/main">stabilityai/stable-diffusion-xl-base-1.0 at main</A>
									<DT><A HREF="https://huggingface.co/meta-llama/Llama-2-13b-hf/tree/main">meta-llama/Llama-2-13b-hf at main</A>
									<DT><A HREF="https://github.com/huggingface/huggingface_hub/blob/ef48c7f7311c0717db98f1b555c8e61d76fc1870/src/huggingface_hub/hf_api.py#L2956">huggingface_hub/src/huggingface_hub/hf_api.py at ef48c7f7311c0717db98f1b555c8e61d76fc1870 ¬∑ huggingface/huggingface_hub</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/serialization.py#L866">pytorch/torch/serialization.py at main ¬∑ pytorch/pytorch</A>
									<DT><A HREF="https://github.com/allenai/cached_path">allenai/cached_path: A file utility for accessing both local and remote files through a unified interface.</A>
								</DL><p>
								<DT><H3 FOLDED>buffers</H3>
								<DL><p>
									<DT><A HREF="https://www.tutorialspoint.com/How-an-entire-file-is-read-into-buffer-and-returned-as-a-string-in-Python">How an entire file is read into buffer and returned as a string in Python?</A>
									<DT><A HREF="https://stackoverflow.com/questions/59026110/python-read-data-from-buffer">Python read data from Buffer - Stack Overflow</A>
									<DT><A HREF="https://docs.python.org/3/library/os.html">os.read</A>
									<DT><A HREF="https://docs.python.org/3/library/io.html">io: readinto</A>
									<DT><A HREF="https://www.youtube.com/watch?v=THWDx_RyZ6A">Episode 011: Let's Go to the Disk! - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>NVIDIA GPUDirect Storage (GDS)</H3>
								<DL><p>
									<DT><H3 FOLDED>cuFile</H3>
									<DL><p>
										<DT><H3 FOLDED>kvikio</H3>
										<DL><p>
											<DT><A HREF="https://github.com/rapidsai/kvikio/blob/branch-24.04/python/tests/test_defaults.py">kvikio/python/tests/test_defaults.py (compat_mode)</A>
											<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/topics/cufile-compatibility.html">cuFile Compatibility Mode - NVIDIA Docs</A>
											<DT><A HREF="https://docs.rapids.ai/api/libkvikio/nightly/">Compatibility Mode (KVIKIO_COMPAT_MODE)</A>
										</DL><p>
										<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html">cuFile API Reference Guide - NVIDIA Docs</A>
										<DT><A HREF="https://github.com/rapidsai/kvikio">rapidsai/kvikio</A>
										<DT><A HREF="https://github.com/rapidsai/kvikio/pull/135">Overload `numpy.fromfile()` and `cupy.fromfile()` by madsbk ¬∑ Pull Request #135 ¬∑ rapidsai/kvikio</A>
										<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py at main ¬∑ alpa-projects/alpa</A>
										<DT><A HREF="https://github.com/pfnet/pytorch-pfn-extras/blob/f6b127063ec910b71788db2ae6ef96a3d89832b1/tests/pytorch_pfn_extras_tests/cuda_tests/test_allocator.py">pytorch-pfn-extras/tests/pytorch_pfn_extras_tests/cuda_tests/test_allocator.py at f6b127063ec910b71788db2ae6ef96a3d89832b1 ¬∑ pfnet/pytorch-pfn-extras</A>
										<DT><A HREF="https://pytorch-pfn-extras.readthedocs.io/en/latest/user_guide/cuda.html">CUDA (CuPy Interoperability) ‚Äî pytorch-pfn-extras documentation</A>
										<DT><A HREF="https://github.com/NVIDIA/apex/blob/810ffae374a2b9cb4b5c5e28eaeca7d7998fca0c/apex/contrib/csrc/gpu_direct_storage/gds.cpp">apex/apex/contrib/csrc/gpu_direct_storage/gds.cpp</A>
										<DT><A HREF="https://github.com/NVIDIA/MagnumIO/blob/main/gds/samples/cufile_sample_022.cc">cuFile Batch APIs</A>
										<DT><A HREF="https://chat.openai.com/c/61bb588b-35a7-42a6-90e9-c2355e5646a9">Identify Disk Storage Type</A>
									</DL><p>
									<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">NVIDIA GPUDirect Storage Overview Guide - NVIDIA Docs</A>
									<DT><A HREF="https://ieeexplore.ieee.org/document/7973709">Offloading Communication Control Logic in GPU</A>
									<DT><A HREF="https://github.com/Mellanox/gpu_direct_rdma_access">Mellanox/gpu_direct_rdma_access: example code for using DC QP for providing RDMA READ and WRITE operations to remote GPU memory</A>
									<DT><A HREF="https://github.com/NVIDIA/gdrcopy">NVIDIA/gdrcopy: A fast GPU memory copy library based on NVIDIA GPUDirect RDMA technology</A>
									<DT><A HREF="https://github.com/NVIDIA/gds-nvidia-fs">NVIDIA/gds-nvidia-fs: NVIDIA GPUDirect Storage Driver</A>
									<DT><A HREF="https://github.com/lw?tab=stars">lw (Luca Wehrstedt)</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py</A>
								</DL><p>
								<DT><H3 FOLDED>serving-checkpointing-serialization</H3>
								<DL><p>
									<DT><H3 FOLDED>json</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ijl/orjson">ijl/orjson: Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy</A>
										<DT><A HREF="https://github.com/bytedance/sonic">bytedance/sonic: A blazingly fast JSON serializing &amp; deserializing library</A>
									</DL><p>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/358">checkpoint quantized weights (bitsandbyres.nf4, SpQR etc)</A>
									<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/checkpoints">ml-engineering/checkpoint</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/824f2ffd5124d24935811bc738ed903796ab13ac/alpa/serialization.py#L54">(JAX) Alpa: serialization.py</A>
								</DL><p>
								<DT><H3 FOLDED>checkpointing-tinygrad</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=2QO3vzwHXhg&t=3551s">converting to float16 slowing down</A>
								</DL><p>
								<DT><A HREF="https://github.com/xai-org/grok-1/blob/main/checkpoint.py">grok-1/checkpoint.py at main</A>
								<DT><A HREF="https://scale.com/blog/reduce-cold-start-time-llm-inference">How To Reduce Cold Start Times For LLM Inference (Scale AI)</A>
								<DT><A HREF="https://github.com/facebookresearch/HolisticTraceAnalysis">facebookresearch/HolisticTraceAnalysis</A>
								<DT><A HREF="https://github.com/huggingface/hf_transfer">huggingface/hf_transfer</A>
								<DT><A HREF="https://docs.ffcv.io/bottleneck_doctor.html">The Bottleneck Doctor ‚Äî FFCV documentation</A>
								<DT><A HREF="https://github.com/coreweave/tensorizer">coreweave/tensorizer: Module, Model, and Tensor Serialization/Deserialization</A>
								<DT><A HREF="https://github.com/allenai/cached_path">allenai/cached_path: A file utility for accessing both local and remote files through a unified interface.</A>
								<DT><A HREF="https://openai.com/index/scaling-kubernetes-to-7500-nodes/">Scaling Kubernetes to 7,500 nodes | OpenAI -&gt; CPU &amp; GPU balloons</A>
							</DL><p>
							<DT><H3 FOLDED>Chat</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/chat/">HuggingChat</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/1082">Support for HF Chat templates? ¬∑ Issue #1082 ¬∑ huggingface/text-generation-inference</A>
								<DT><A HREF="https://huggingface.co/docs/transformers/main/chat_templating">Templates for Chat Models</A>
							</DL><p>
							<DT><H3 FOLDED>benchmark</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/dzhulgakov/status/1737917306565697990?s=46&t=yqOem5ktaowo8FyJ-ilbzQ">(1) Dmytro Dzhulgakov en X: "I‚Äôm all pro open benchmarks, but comparing **public** LLM endpoints just doesn‚Äôt work unless there‚Äôs a confirmed huge user base like OpenAI. In fact, performance may be even anti-correlated with popularityüòâ Here‚Äôs why üßµ" / X</A>
								<DT><A HREF="https://twitter.com/anyscalecompute/status/1737883193720922413">(1) Anyscale en X: "üìàWe‚Äôre excited to introduce the LLMPerf leaderboard: the first public and open source leaderboard for benchmarking performance of various LLM inference providers in the market. Our goal with this leaderboard is to equip users and developers with a clear understanding of the... https://t.co/XGF4fhkaWG" / X</A>
								<DT><A HREF="https://github.com/fw-ai/benchmark">fw-ai/benchmark: Benchmark suite for LLMs from Fireworks.ai</A>
								<DT><A HREF="https://github.com/ray-project/llmperf">ray-project/llmperf: LLMPerf is a library for validating and benchmarking LLMs</A>
								<DT><A HREF="https://httpd.apache.org/docs/current/programs/ab.html">ab - Apache HTTP server benchmarking tool - Apache HTTP Server Version 2.4</A>
								<DT><A HREF="https://github.com/grafana/k6">grafana/k6: A modern load testing tool, using Go and JavaScript - https://k6.io</A>
								<DT><A HREF="https://gist.github.com/aliesbelik/840eff7c5bc78a141eab8e36f2f4edf2">Benchmarking &amp; load testing tools</A>
								<DT><A HREF="https://github.com/hatoo/oha">hatoo/oha: Ohayou(„Åä„ÅØ„Çà„ÅÜ), HTTP load generator, inspired by rakyll/hey with tui animation.</A>
								<DT><A HREF="https://github.com/sharkdp/hyperfine">sharkdp/hyperfine: A command-line benchmarking tool</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/tree/main/benchmarks">vllm/benchmarks at main</A>
							</DL><p>
							<DT><H3 FOLDED>miscellaneous</H3>
							<DL><p>
								<DT><A HREF="https://github.com/cloudflare/quiche">cloudflare/quiche: ü•ß Savoury implementation of the QUIC transport protocol and HTTP/3</A>
								<DT><A HREF="https://cloudflare-quic.com/">QUIC | Cloudflare</A>
								<DT><A HREF="https://www.youtube.com/watch?v=FqljO9B5grM">Impeccable API Design: What you MUST CONSIDER before deploying APIs to production</A>
								<DT><A HREF="https://github.com/w3c/webtransport/blob/main/explainer.md">webtransport/explainer.md at main ¬∑ w3c/webtransport</A>
								<DT><A HREF="https://github.com/Motsepe-Jr/AI-research-papers-pseudo-code/blob/main/Distributed%20Inference%20Papers/Fast_Distributed_Inference_Serving_for_Large_Language_Models.ipynb">AI-research-papers-pseudo-code/Distributed Inference Papers/Fast_Distributed_Inference_Serving_for_Large_Language_Models.ipynb at main ¬∑ Motsepe-Jr/AI-research-papers-pseudo-code ¬∑ GitHub</A>
								<DT><A HREF="https://www.flickr.com/photos/neurollero/sets/366106/with/51970885/">neuro | Flickr</A>
								<DT><A HREF="https://jina.ai/news/inference-how-can-jina-ai-offer-the-best-in-class-model-as-a-service-so-affordably/">Jina: The Science of Model Deployment</A>
								<DT><A HREF="https://docs.cerebras.net/en/latest/wsc/getting-started/csctl.html#csctl">csctl: CLI tool for job monitoring ‚Äî Cerebras Developer Documentation</A>
								<DT><A HREF="https://github.com/alibaba/hiactor">alibaba/hiactor: Hiactor is a distributed C++ actor framework.</A>
								<DT><A HREF="https://github.com/hpcaitech/EnergonAI">hpcaitech/EnergonAI: Large-scale model inference.</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2404.16283">transmogrifier</A>
							<DT><A HREF="https://www.usenix.org/conference/osdi22/presentation/yu">Orca: A Distributed Serving System for Transformer-Based Generative Models | USENIX</A>
							<DT><A HREF="https://www.run.ai/">Run:ai - AI Optimization and Orchestration</A>
							<DT><A HREF="https://x.com/haoailab/status/1805307696297689119">(1) Hao AI Lab en X: "Multiple LLM serving has emerged as a crucial and costly demand. Want to co-serve multiple LLMs with better utilization? Introducing MuxServe - flexible spatial-temporal multiplexing - up to 1.8x higher throughput Blog: https://t.co/Pep94vUFTw Paper: https://t.co/X1Jhov3QOY https://t.co/mXrHMSLPS1" / X</A>
							<DT><A HREF="https://github.com/Lightning-AI/LitServe">Lightning-AI/LitServe: Deploy AI models at scale. High-throughput serving engine for AI/ML models that uses the latest state-of-the-art model deployment techniques.</A>
							<DT><A HREF="https://github.com/microsoft/ParrotServe">microsoft/ParrotServe: [OSDI'24] Serving LLM-based Applications Efficiently with Semantic Variable</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-scheduler</H3>
						<DL><p>
							<DT><H3 FOLDED>TorchX</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/torchx/main/basics.html">Basic Concepts ‚Äî PyTorch/TorchX main documentation</A>
								<DT><A HREF="https://airflow.apache.org/">Apache Airflow</A>
								<DT><A HREF="https://pytorch.org/torchx/main/quickstart.html">Quickstart ‚Äî PyTorch/TorchX main documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Y7T01L0a4G4">Automating PyTorch Using TorchX to Make Data Centric ML Workflows</A>
							</DL><p>
							<DT><A HREF="https://docs.cerebras.net/en/latest/wsc/getting-started/csctl.html#csctl">csctl: CLI tool for job monitoring ‚Äî Cerebras Developer Documentation</A>
							<DT><A HREF="https://slurm.schedmd.com/SC22/Slurm-and-or-vs-Kubernetes.pdf">Slurm and kubernetes</A>
							<DT><A HREF="https://slurm.schedmd.com/">slurm.schedmd.com</A>
							<DT><A HREF="https://github.com/GoogleCloudPlatform/hpc-toolkit">GoogleCloudPlatform/hpc-toolkit: Cloud HPC Toolkit is an open-source software offered by Google Cloud which makes it easy for customers to deploy HPC environments on Google Cloud.</A>
							<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-model-parallelism</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-notebooks</H3>
						<DL><p>
							<DT><A HREF="https://github.com/bigcode-project/Megatron-LM/blob/raymond-notebooks/notebooks/transformer_parameter_count.ipynb">Megatron-LM/transformer_parameter_count.ipynb</A>
							<DT><A HREF="https://www.philschmid.de/gptj-deepspeed-inference#3-optimize-gpt-j-for-gpu-using-deepspeeds-inferenceengine">Accelerate GPT-J inference with DeepSpeed-Inference on GPUs</A>
							<DT><A HREF="https://www.philschmid.de/static-quantization-optimum">Static Quantization with Hugging Face `optimum` for ~3x latency improvements</A>
							<DT><A HREF="https://www.philschmid.de/fine-tune-flan-t5-deepspeed">Fine-tune FLAN-T5 XL/XXL using DeepSpeed &amp; Hugging Face Transformers</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-benchmark</H3>
						<DL><p>
							<DT><A HREF="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</A>
							<DT><A HREF="https://huggingface.co/docs/transformers/benchmarks">https://huggingface.co/docs/transformers/benchmarks</A>
							<DT><A HREF="https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb">notebooks/benchmark.ipynb at main ¬∑ huggingface/notebooks ¬∑ GitHub</A>
							<DT><A HREF="https://github.com/huggingface/transformers/blob/main/tests/benchmark/test_benchmark.py">transformers/test_benchmark.py at main ¬∑ huggingface/transformers ¬∑ GitHub</A>
							<DT><A HREF="https://github.com/huggingface/transformers/blob/c256bc6d104b1e85a138f3cf0e5f9f85d1197a25/src/transformers/benchmark/benchmark_utils.py#L633">transformers/benchmark_utils.py at c256bc6d104b1e85a138f3cf0e5f9f85d1197a25 ¬∑ huggingface/transformers</A>
							<DT><A HREF="https://colab.research.google.com/drive/17tla0i10y2fsQF0FNy_yuV6hxqPjqS6h#scrollTo=19ec9a99">transformers.ipynb - Colaboratory</A>
							<DT><A HREF="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py">https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py</A>
							<DT><A HREF="https://pytorch.org/docs/stable/benchmark_utils.html">https://pytorch.org/docs/stable/benchmark_utils.html</A>
							<DT><A HREF="https://github.com/pytorch/pytorch/blob/master/torch/utils/benchmark/examples/compare.py">https://github.com/pytorch/pytorch/blob/master/torch/utils/benchmark/examples/compare.py</A>
							<DT><A HREF="https://github.com/mli/transformers-benchmarks">https://github.com/mli/transformers-benchmarks</A>
							<DT><A HREF="https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling">https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling</A>
							<DT><A HREF="https://github.com/huggingface/transformers-bloom-inference">transformers-bloom-inference</A>
							<DT><A HREF="https://github.com/bigcode-project/Megatron-LM/issues/20">Benchmarking Memory Consumption of Optimizers Adam v.s. Adan</A>
							<DT><A HREF="https://github.com/huggingface/blog/blob/main/bloom-inference-pytorch-scripts.md">Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate</A>
							<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/examples">DeepSpeed/examples</A>
							<DT><A HREF="https://github.com/microsoft/DeepSpeedExamples/blob/master/inference/huggingface/text-generation/inference-test.py">DeepSpeedExamples/inference-test.py</A>
							<DT><A HREF="https://huggingface.co/blog/bloom-inference-pytorch-scripts">Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate</A>
							<DT><A HREF="https://github.com/huggingface/transformers-bloom-inference/blob/main/bloom-inference-scripts/bloom-accelerate-inference.py#L46">transformers-bloom-inference/bloom-accelerate-inference.py</A>
							<DT><A HREF="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference/tree/main/benchmark</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-lectures</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=IGu7ivuy1Ag&t=590s">How a Transformer works at inference vs training time - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=7P6wllBoHwU">SparseGPT : Get Rid of 100 Billion Parameters - YouTube</A>
							<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance</A>
						</DL><p>
						<DT><A HREF="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#distillation">Large Transformer Model Inference Optimization | Lil'Log</A>
						<DT><A HREF="https://www.zhihu.com/people/liang-de-peng">GiantPandaCV (chinese high-proffesional discussions)</A>
						<DT><A HREF="https://websites.umich.edu/~amberljc/file/LLM-Systems-Basics.pdf">LLM-Systems-Basics (Jiachen Liu)</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance</A>
						<DT><A HREF="https://github.com/antferdom/tuning_playbook/blob/main/language_models/Resources.md">tuning_playbook</A>
						<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples">NVIDIA/DeepLearningExamples</A>
						<DT><A HREF="https://arxiv.org/pdf/2302.14017.pdf">Full Stack Optimization of Transformer Inference: a Survey</A>
						<DT><A HREF="https://huggingface.co/docs/transformers/perf_infer_gpu_one">Efficient Inference on a Single GPU</A>
						<DT><A HREF="https://nat.dev/compare">OpenPlayground</A>
						<DT><A HREF="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/Dev_Guide.md">Transformer Model Optimization Tool Dev Guide</A>
						<DT><A HREF="https://excalidraw.com/">Excalidraw | Hand-drawn look &amp; feel ‚Ä¢ Collaborative ‚Ä¢ Secure</A>
						<DT><A HREF="https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices">LLM Inference Performance Engineering: Best Practices | Databricks Blog</A>
						<DT><A HREF="https://rahulschand.github.io/gpu_poor/">Tokens/s simulation</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance (2024 main)</A>
						<DT><A HREF="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/">Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</A>
						<DT><A HREF="https://www.modular.com/blog/how-to-be-confident-in-your-performance-benchmarking">Modular: How to Be Confident in Your Performance Benchmarking</A>
						<DT><A HREF="https://github.com/xjdr-alt/mla_blog_translation">DeepSeek-V2 High-performance Inference Optimization Notes: MLA Optimization</A>
						<DT><A HREF="https://github.com/DefTruth/Awesome-LLM-Inference">DefTruth/Awesome-LLM-Inference: üìñA curated list of Awesome LLM Inference Paper with codes, TensorRT-LLM, vLLM, streaming-llm, AWQ, SmoothQuant, WINT8/4, Continuous Batching, FlashAttention, PagedAttention etc.</A>
					</DL><p>
					<DT><H3 FOLDED>ML Sys</H3>
					<DL><p>
						<DT><H3 FOLDED>Operating Systems</H3>
						<DL><p>
							<DT><H3 FOLDED>os-configuration</H3>
							<DL><p>
								<DT><H3 FOLDED>bootstrap</H3>
								<DL><p>
									<DT><A HREF="https://www.ventoy.net/en/index.html">Ventoy</A>
								</DL><p>
								<DT><H3 FOLDED>.dotfiles</H3>
								<DL><p>
									<DT><H3 FOLDED>repos</H3>
									<DL><p>
										<DT><A HREF="https://github.com/eieioxyz/Beyond-Dotfiles-in-100-Seconds">eieioxyz/Beyond-Dotfiles-in-100-Seconds</A>
									</DL><p>
									<DT><H3 FOLDED>docs</H3>
									<DL><p>
										<DT><A HREF="https://wiki.archlinux.org/title/Dotfiles">Dotfiles - ArchWiki</A>
										<DT><A HREF="https://dotfiles.github.io/">GitHub does dotfiles - dotfiles.github.io</A>
										<DT><A HREF="https://www.anishathalye.com/2014/08/03/managing-your-dotfiles/">Managing Your Dotfiles</A>
									</DL><p>
									<DT><A HREF="https://superuser.com/questions/886132/where-is-the-zshrc-file-on-mac">Where is the .zshrc file</A>
									<DT><A HREF="https://tldp.org/LDP/abs/html/sample-bashrc.html">Sample .bashrc and .bash_profile Files</A>
									<DT><A HREF="https://www.youtube.com/watch?v=r_MpUP6aKiQ&t=540s">~/.dotfiles in 100 Seconds</A>
									<DT><A HREF="https://www.atlassian.com/git/tutorials/dotfiles">How to store dotfiles</A>
									<DT><A HREF="https://news.ycombinator.com/item?id=11070797">Ask HN: What do you use to manage dotfiles?</A>
									<DT><A HREF="https://superuser.com/questions/39751/add-directory-to-path-if-its-not-already-there/753948#753948">bash - Add directory to $PATH if it's not already there</A>
								</DL><p>
								<DT><A HREF="https://github.com/fastai/fastsetup">fastai/fastsetup: Setup all the things</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5d8ca2faf74c494f220c8f71130340b513eea9a9/docker/common/install_pytorch.sh">TensorRT-LLM/docker/common/install_pytorch.sh</A>
								<DT><H3 FOLDED>os-setup</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tinygrad/tinyos">tinygrad/tinyos</A>
								</DL><p>
								<DT><A HREF="https://github.com/tinygrad/tinyos">tinygrad/tinyos</A>
							</DL><p>
							<DT><H3 FOLDED>version control</H3>
							<DL><p>
								<DT><H3 FOLDED>git</H3>
								<DL><p>
									<DT><H3 FOLDED>pre-commit</H3>
									<DL><p>
										<DT><A HREF="https://pre-commit.ci/">pre-commit.ci</A>
										<DT><A HREF="https://pre-commit.com/">pre-commit</A>
										<DT><A HREF="https://github.com/pre-commit/pre-commit">pre-commit/pre-commit: A framework for managing and maintaining multi-language pre-commit hooks.</A>
									</DL><p>
									<DT><H3 FOLDED>monorepo</H3>
									<DL><p>
										<DT><A HREF="https://github.com/web3infra-foundation/mega?tab=readme-ov-file">mega: unofficial open source implementation of Google Piper</A>
										<DT><A HREF="https://cacm.acm.org/research/why-google-stores-billions-of-lines-of-code-in-a-single-repository/">Why Google Stores Billions of Lines of Code in a Single Repository</A>
									</DL><p>
									<DT><H3 FOLDED>gitignore</H3>
									<DL><p>
										<DT><A HREF="https://stackoverflow.com/questions/2545602/how-can-i-git-ignore-subfolders-subdirectories#:~:text=*%2F*%20ignores%20all%20subdirectories%20but,%2F*%20all%20work%20for%20me.">git rm -r --cached .</A>
									</DL><p>
									<DT><A HREF="https://github.com/web3infra-foundation/mega">web3infra-foundation/mega: Mega is an unofficial open source implementation of Google Piper.</A>
									<DT><A HREF="https://git-scm.com/docs/git-diff">git-diff</A>
									<DT><A HREF="https://git-scm.com/docs/git-commit">commit</A>
									<DT><A HREF="https://chris.beams.io/posts/git-commit/">How to Write a Git Commit Message</A>
									<DT><A HREF="https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh">Connecting to GitHub with SSH - GitHub Docs</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Basics-Viewing-the-Commit-History">Viewing the Commit History</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Branching-Rebasing">Rebasing</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Tools-Reset-Demystified">Reset Demystified</A>
									<DT><A HREF="https://stackoverflow.com/questions/448919/how-can-i-remove-a-commit-on-github">How can I remove a commit on GitHub?</A>
									<DT><A HREF="https://stackoverflow.com/questions/3701404/how-can-i-list-all-commits-that-changed-a-specific-file">follow file</A>
									<DT><A HREF="https://stackoverflow.com/questions/2505096/cloning-a-private-github-repo">Cloning a private Github repo</A>
									<DT><A HREF="https://stackoverflow.com/questions/68775869/support-for-password-authentication-was-removed-please-use-a-personal-access-to">Support for password authentication was removed</A>
									<DT><A HREF="https://stackoverflow.com/questions/11188801/connect-local-repo-with-remote-repo">connect local repo with remote repo</A>
									<DT><A HREF="https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-user-account/managing-email-preferences/setting-your-commit-email-address">Setting your commit email address</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Tools-Submodules">Git - Submodules</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Basics-Working-with-Remotes">Git - Working with Remotes</A>
									<DT><A HREF="https://github.blog/2016-02-01-working-with-submodules/">Working with submodules - The GitHub Blog</A>
									<DT><A HREF="https://gist.github.com/myusuf3/7f645819ded92bda6677">How effectively delete a git submodule.</A>
									<DT><A HREF="https://twitter.com/JI/status/1546948817462800384">No more "--set-upstream origin"</A>
									<DT><A HREF="https://www.leshenko.net/p/ugit/#read-tree-to-index">Git Internals - Learn by Building Your Own Git</A>
									<DT><A HREF="https://www.conventionalcommits.org/en/v1.0.0/">Conventional Commits</A>
								</DL><p>
								<DT><H3 FOLDED>Github</H3>
								<DL><p>
									<DT><A HREF="https://docs.github.com/en/rest/reference/users#get-a-user">Users - GitHub Docs</A>
									<DT><A HREF="https://cli.github.com/">GitHub CLI | Take GitHub to the command line</A>
									<DT><A HREF="https://education.github.com/discount_requests/teacher_application">Request a discount - GitHub Education</A>
								</DL><p>
								<DT><H3 FOLDED>Trunk-based development</H3>
								<DL><p>
									<DT><A HREF="https://www.atlassian.com/continuous-delivery/continuous-integration/trunk-based-development">Trunk-based Development | Atlassian</A>
									<DT><A HREF="https://trunkbaseddevelopment.com/">Introduction</A>
									<DT><A HREF="https://github.com/ezyang/ghstack">ezyang/ghstack: Submit stacked diffs to GitHub on the command line</A>
								</DL><p>
								<DT><H3 FOLDED>Stacking change</H3>
								<DL><p>
									<DT><A HREF="https://graphite.dev/blog/post/DThX8ffP1gmxWJChEv0y">Graphite - Stacking changes</A>
									<DT><A HREF="https://jg.gg/2018/09/29/stacked-diffs-versus-pull-requests/">Stacked Diffs Versus Pull Requests | Jackson Gabbard's Blog</A>
									<DT><A HREF="https://kurtisnusbaum.medium.com/stacked-diffs-keeping-phabricator-diffs-small-d9964f4dcfa6">Stacked Diffs: Keeping Phabricator Diffs Small | by Kurtis Nusbaum | Medium</A>
									<DT><A HREF="https://news.ycombinator.com/item?id=26922633">Stacked Diffs versus Pull Requests (2018) | Hacker News</A>
									<DT><A HREF="https://newsletter.pragmaticengineer.com/p/stacked-diffs">Stacked Diffs (and why you should know about them)</A>
									<DT><A HREF="https://github.com/ezyang/ghstack">ezyang/ghstack: Submit stacked diffs to GitHub on the command line</A>
								</DL><p>
								<DT><A HREF="https://pre-commit.ci/">pre-commit.ci</A>
							</DL><p>
							<DT><H3 FOLDED>package manager</H3>
							<DL><p>
								<DT><H3 FOLDED>Brew</H3>
								<DL><p>
									<DT><H3 FOLDED>tap</H3>
									<DL><p>
										<DT><A HREF="https://docs.brew.sh/Taps">Taps (Third-Party Repositories)</A>
									</DL><p>
									<DT><H3 FOLDED>leaves</H3>
									<DL><p>
										<DT><A HREF="https://thoughtbot.com/blog/brew-leaves">brew leaves</A>
									</DL><p>
									<DT><H3 FOLDED>casks</H3>
									<DL><p>
										<DT><A HREF="https://formulae.brew.sh/cask/">homebrew-cask</A>
										<DT><A HREF="https://github.com/Homebrew/homebrew-cask">CLI workflow for the administration of applications as bin</A>
									</DL><p>
									<DT><A HREF="https://brew.sh/">The Missing Package Manager for macOS (or Linux)</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Symbolic_link">Symbolic link</A>
									<DT><A HREF="https://clubmate.fi/make-a-symlink-in-linux-or-mac-os-x/">Make a symlink</A>
									<DT><A HREF="https://formulae.brew.sh/">Formulae</A>
									<DT><A HREF="https://zanshin.net/2014/02/03/how-to-list-brew-dependencies/">List Brew Dependencies</A>
									<DT><A HREF="https://rick.cogley.info/post/use-homebrew-zsh-instead-of-the-osx-default/">Use Homebrew zsh Instead of the OS X Default</A>
									<DT><A HREF="https://stackoverflow.com/questions/55732972/curl-56-libressl-ssl-read-ssl-error-syscall-errno-54/55735219">curl: (56) LibreSSL SSL_read: SSL_ERROR_SYSCALL, errno 54</A>
									<DT><A HREF="https://www.unix.com/os-x-apple-/161123-what-directory-brew-prefix-homebrew.html">directory "$(brew --prefix)"?</A>
									<DT><A HREF="https://docs.brew.sh/Formula-Cookbook">Formula Cookbook ‚Äî Homebrew Documentation</A>
									<DT><A HREF="https://docs.brew.sh/FAQ">FAQ ‚Äî Homebrew Documentation</A>
									<DT><A HREF="https://apple.stackexchange.com/questions/373411/how-to-install-a-specific-version-of-ocaml-on-macos">install a specific version</A>
									<DT><A HREF="https://stackoverflow.com/questions/13477363/how-can-i-brew-link-a-specific-version">How can I brew link a specific version?</A>
									<DT><A HREF="https://docs.brew.sh/Tips-N%27-Tricks">Tips and Tricks ‚Äî Homebrew Documentation</A>
									<DT><A HREF="https://remarkablemark.org/blog/2017/02/03/install-brew-package-version/">How to install an older homebrew package</A>
									<DT><A HREF="https://devhints.io/homebrew">Homebrew cheatsheet</A>
									<DT><A HREF="https://docs.w3cub.com/homebrew/manpage">Brew - Homebrew - W3cubDocs</A>
									<DT><A HREF="https://stackoverflow.com/questions/16246352/how-do-i-specify-ldflags-and-cppflags-for-configure">LDFLAGS and CPPFLAGS</A>
									<DT><A HREF="https://stackoverflow.com/questions/65502748/why-does-brew-cleanup-or-brew-cleanup-n-dont-show-any-output">cleanup</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>windows manager</H3>
							<DL><p>
								<DT><A HREF="https://en.wikipedia.org/wiki/X_Window_System">X Window System</A>
								<DT><A HREF="https://wiki.haskell.org/Xmonad/General_xmonad.hs_config_tips">Xmonad/General xmonad.hs config tips</A>
							</DL><p>
							<DT><H3 FOLDED>Docker</H3>
							<DL><p>
								<DT><H3 FOLDED>docker-examples</H3>
								<DL><p>
									<DT><A HREF="https://github.com/docker/docker-rust-hello">docker/docker-rust-hello: A simple Rust application</A>
								</DL><p>
								<DT><H3 FOLDED>docker-image</H3>
								<DL><p>
									<DT><H3 FOLDED>Dockerfile</H3>
									<DL><p>
										<DT><H3 FOLDED>dockerfile-python</H3>
										<DL><p>
											<DT><A HREF="https://github.com/docker-library/python/blob/bf5951cfa2b2f6c3dabf428549c9dca658ecee81/3.12/bullseye/Dockerfile">python/3.12/bullseye/Dockerfile</A>
										</DL><p>
										<DT><A HREF="http://dockerfile.github.io/">Dockerfile Project - Trusted Automated Docker Builds</A>
										<DT><A HREF="http://dockerfile.github.io/#/python">Python Dockerfile</A>
										<DT><A HREF="https://docs.docker.com/build/dockerfile/frontend/#stable-channel">Custom Dockerfile syntax | Docker Docs</A>
										<DT><A HREF="https://docs.docker.com/reference/dockerfile/">Dockerfile reference | Docker Docs</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/d49d4324a3340609126e64b10f979507636fe5ed/test/Dockerfile#L2">tinygrad/test/Dockerfile (minimal python example)</A>
										<DT><A HREF="https://github.com/docker-library/python/blob/875ce40a1ae98c7c37b1652e65f76376ac93a911/apply-templates.sh#L22">python/apply-templates.sh</A>
									</DL><p>
									<DT><H3 FOLDED>docker-image-optimization</H3>
									<DL><p>
										<DT><A HREF="https://rodneyosodo.medium.com/minimizing-python-docker-images-cf99f4468d39">Minimizing python docker images. During the transition to a micro... | by Rodney Osodo | Medium</A>
									</DL><p>
									<DT><A HREF="https://github.com/docker-library">Docker Official Images</A>
									<DT><A HREF="https://registry.hub.docker.com/">Docker Hub Container Image Library | App Containerization</A>
								</DL><p>
								<DT><H3 FOLDED>docker-container</H3>
								<DL><p>
									<DT><H3 FOLDED>NGC</H3>
									<DL><p>
										<DT><A HREF="https://www.nvidia.com/pt-br/gpu-cloud1/containers/">NGC Containers | NVIDIA</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/Dockerfile">pytorch/Dockerfile</A>
										<DT><A HREF="https://catalog.ngc.nvidia.com/containers?filters=platform%7CPyTorch%7Cpltfm_pytorch&orderBy=weightPopularDESC&query=&page=&pageSize=">Pytorch container</A>
										<DT><A HREF="https://github.com/psaboia/devcontainer-nvidia-base">psaboia/devcontainer-nvidia-base: Example of how to setup a NVIDIA DevContainer with GPU Support for Tensorflow/Keras, that follows the page https://alankrantas.medium.com/setup-a-nvidia-devcontainer-with-gpu-support-for-tensorflow-keras-on-windows-d00e6e204630</A>
										<DT><A HREF="https://hub.docker.com/layers/nvidia/cuda/11.8.0-devel-ubuntu22.04/images/sha256-60eda04ab6790aa76d73bf0df245b361eabc6d8f7b6f6cf9846c70f399b9a1eb">Image Layer Details - nvidia/cuda:11.8.0-devel-ubuntu22.04 | Docker Hub</A>
										<DT><A HREF="https://hub.docker.com/r/nvidia/cuda/tags">nvidia/cuda Tags | Docker Hub</A>
										<DT><A HREF="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver">Triton Inference Server | NVIDIA NGC</A>
									</DL><p>
									<DT><H3 FOLDED>docker-container-checkpointing</H3>
									<DL><p>
										<DT><A HREF="https://github.com/checkpoint-restore/checkpointctl">checkpoint-restore/checkpointctl: A tool for in-depth analysis of container checkpoints</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>docker-debug</H3>
								<DL><p>
									<DT><H3 FOLDED>interactive</H3>
									<DL><p>
										<DT><A HREF="https://chat.openai.com/c/05357f44-c5ed-4a6d-bce7-5545860507cd">docker run -ti -p 3000:3000 -v $inferno_dir:/source/inferno &lt;image&gt;</A>
										<DT><A HREF="https://stackoverflow.com/questions/47831774/docker-run-with-volume">docker run with --volume</A>
										<DT><A HREF="https://docs.docker.com/storage/volumes/">Volumes | Docker Docs</A>
									</DL><p>
									<DT><H3 FOLDED>dev-container</H3>
									<DL><p>
										<DT><H3 FOLDED>dev-container-examples</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/acc466751b2723eb913fd3148b4f054189bbf1ab/.devcontainer/README.md">pytorch/.devcontainer/README.md</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/.devcontainer/Dockerfile">pytorch/.devcontainer/Dockerfile</A>
											<DT><A HREF="https://github.com/psaboia/devcontainer-nvidia-base">psaboia/devcontainer-nvidia-base</A>
											<DT><A HREF="https://github.com/devcontainers/images/tree/main/src/base-ubuntu">images/src/base-ubuntu at main ¬∑ devcontainers/images</A>
										</DL><p>
										<DT><H3 FOLDED>dev-container-template</H3>
										<DL><p>
											<DT><A HREF="https://github.com/devcontainers/template-starter">devcontainers/template-starter: A template explaining how to author custom dev container Templates</A>
											<DT><A HREF="https://containers.dev/templates">Available Dev Container Templates</A>
										</DL><p>
										<DT><H3 FOLDED>dev-container-devcontainer.json</H3>
										<DL><p>
											<DT><A HREF="https://github.com/devcontainers/images">devcontainers/images: Repository for pre-built dev container images published under mcr.microsoft.com/devcontainers</A>
											<DT><A HREF="https://containers.dev/implementors/json_reference/#image-specific">Dev Container metadata reference</A>
										</DL><p>
										<DT><H3 FOLDED>dev-container-nvidia</H3>
										<DL><p>
											<DT><A HREF="https://github.com/psaboia/devcontainer-nvidia-base">psaboia/devcontainer-nvidia-base: Example of how to setup a NVIDIA DevContainer with GPU Support for Tensorflow/Keras, that follows the page https://alankrantas.medium.com/setup-a-nvidia-devcontainer-with-gpu-support-for-tensorflow-keras-on-windows-d00e6e204630</A>
										</DL><p>
										<DT><A HREF="https://containers.dev/implementors/json_reference/">Dev Container metadata reference</A>
										<DT><A HREF="https://github.com/devcontainers">devcontainers</A>
										<DT><A HREF="https://code.visualstudio.com/docs/devcontainers/containers#_create-a-devcontainerjson-file">dev containers &amp; VS code</A>
										<DT><A HREF="https://code.visualstudio.com/docs/devcontainers/containers">Developing inside a Container using Visual Studio Code Remote Development</A>
										<DT><A HREF="https://code.visualstudio.com/docs/devcontainers/create-dev-container">Rebuild: postCreateCommand</A>
										<DT><A HREF="https://code.visualstudio.com/docs/devcontainers/containers#_getting-started">Developing inside a Container using Visual Studio Code Remote Development</A>
										<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#docker">Installing the NVIDIA Container Toolkit</A>
										<DT><A HREF="https://www.youtube.com/watch?v=p9L7YFqHGk4">Customize Dev Containers in VS Code with Dockerfiles and Docker Compose</A>
										<DT><A HREF="https://www.youtube.com/watch?v=BhtxEDwgylU">Use AppMap with VS Code Dev Containers - YouTube</A>
										<DT><A HREF="https://github.com/devcontainers/images/tree/main/src/base-ubuntu">images/src/base-ubuntu at main ¬∑ devcontainers/images</A>
										<DT><A HREF="https://docs.github.com/en/codespaces/setting-up-your-project-for-codespaces/adding-a-dev-container-configuration/introduction-to-dev-containers">Introduction to dev containers - GitHub Docs</A>
										<DT><A HREF="https://containers.dev/guide/dockerfile">Using Images, Dockerfiles, and Docker Compose</A>
									</DL><p>
									<DT><H3 FOLDED>docker-logs</H3>
									<DL><p>
										<DT><A HREF="https://forums.docker.com/t/capture-ouput-of-docker-build-into-a-log-file/123178">Capture ouput of docker build into a log file?</A>
									</DL><p>
									<DT><A HREF="https://github.com/wagoodman/dive">wagoodman/dive: A tool for exploring each layer in a docker image</A>
								</DL><p>
								<DT><H3 FOLDED>docker-nvidia</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide">Installation Guide ‚Äî NVIDIA drivers</A>
									<DT><A HREF="https://medium.datadriveninvestor.com/setting-up-carla-simulator-for-the-self-driving-cars-specialization-d38d4f6a0486">Setting up CARLA Simulator for the Self-Driving Cars Specialization | by Viridiana Romero Martinez | DataDrivenInvestor</A>
									<DT><A HREF="https://awesomeopensource.com/project/Amin-Tgz/awesome-CARLA">Awesome Carla</A>
									<DT><A HREF="https://hub.docker.com/layers/nvidia/cuda/11.8.0-devel-ubuntu22.04/images/sha256-60eda04ab6790aa76d73bf0df245b361eabc6d8f7b6f6cf9846c70f399b9a1eb">Image Layer Details - nvidia/cuda:11.8.0-devel-ubuntu22.04 | Docker Hub</A>
									<DT><A HREF="https://hub.docker.com/r/nvidia/cuda/tags">nvidia/cuda Tags | Docker Hub</A>
								</DL><p>
								<DT><H3 FOLDED>docker-build</H3>
								<DL><p>
									<DT><A HREF="https://github.com/docker/buildx">docker/buildx: Docker CLI plugin for extended build capabilities with BuildKit</A>
								</DL><p>
								<DT><H3 FOLDED>docker-internals</H3>
								<DL><p>
									<DT><A HREF="https://blog.lizzie.io/linux-containers-in-500-loc.html">Linux containers in 500 lines of code</A>
								</DL><p>
								<DT><H3 FOLDED>docker-footprint</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookincubator/senpai">facebookincubator/senpai: Senpai is an automated memory sizing tool for container applications.</A>
								</DL><p>
								<DT><H3 FOLDED>docker-install</H3>
								<DL><p>
									<DT><A HREF="https://github.com/docker/docker-install">docker/docker-install: Docker installation script</A>
								</DL><p>
								<DT><H3 FOLDED>testcontainers</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=sNg0bnMF_qY">Testcontainers have forever changed the way I write tests - YouTube</A>
									<DT><A HREF="https://testcontainers.com/">Testcontainers</A>
									<DT><A HREF="https://www.youtube.com/watch?v=sNg0bnMF_qY">Testcontainers have forever changed the way I write tests</A>
								</DL><p>
								<DT><A HREF="https://blog.lizzie.io/linux-containers-in-500-loc.html">Linux containers in 500 lines of code</A>
								<DT><A HREF="https://github.com/wagoodman/dive">wagoodman/dive: A tool for exploring each layer in a docker image</A>
								<DT><A HREF="https://www.youtube.com/watch?v=rIrNIzy6U_g&t=247s">100+ Docker Concepts you Need to Know</A>
								<DT><A HREF="https://gist.github.com/anupambhatnagar/07ebff374bc45e4b63eb42893cca7e87">Commonly used Docker commands</A>
								<DT><A HREF="https://stackoverflow.com/questions/48957195/how-to-fix-docker-got-permission-denied-issue">How to fix docker: Got permission denied issue</A>
								<DT><A HREF="https://stackoverflow.com/questions/51188657/image-is-being-used-by-stopped-container/51189547">image is being used by stopped container</A>
								<DT><A HREF="https://gist.github.com/biera/fa4fcca8a3150dfa2438">remove all docker containers</A>
								<DT><A HREF="https://docs.docker.com/engine/install/linux-postinstall/">Post-installation steps for Linux | NON-ROOT USER</A>
								<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide">Installation Guide ‚Äî NVIDIA drivers</A>
								<DT><A HREF="https://www.youtube.com/watch?v=J0NuOlA2xDc&t=5s">Never install locally</A>
								<DT><A HREF="https://github.com/docker/buildx">docker/buildx: Docker CLI plugin for extended build capabilities with BuildKit</A>
								<DT><A HREF="https://github.com/facebookincubator/senpai">facebookincubator/senpai: Senpai is an automated memory sizing tool for container applications.</A>
								<DT><A HREF="https://av.tib.eu/media/46137">Senpai - Automatic memory sizing for containers - TIB AV-Portal</A>
							</DL><p>
							<DT><H3 FOLDED>Linux</H3>
							<DL><p>
								<DT><H3 FOLDED>linux-distros</H3>
								<DL><p>
									<DT><H3 FOLDED>ubuntu</H3>
									<DL><p>
										<DT><A HREF="https://askubuntu.com/questions/1162491/how-can-you-tell-the-version-of-ubuntu-on-a-system-in-a-sh-bash-script">print system version</A>
										<DT><A HREF="https://help.ubuntu.com/community/EnvironmentVariables#File-location_related_variables">EnvironmentVariables - Community Help Wiki</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>users</H3>
								<DL><p>
									<DT><A HREF="https://manpages.ubuntu.com/manpages/bionic/man8/useradd.8.html">Ubuntu Manpage: useradd - create a new user or update default new user information</A>
									<DT><A HREF="https://learnubuntu.com/list-users/">How to List Users in Ubuntu Command Line</A>
									<DT><A HREF="https://www.cyberciti.biz/faq/create-a-user-account-on-ubuntu-linux/">How to create a user account on Ubuntu Linux - nixCraft</A>
									<DT><A HREF="https://www.geeksforgeeks.org/id-command-in-linux-with-examples/">id command in Linux with examples - GeeksforGeeks</A>
									<DT><A HREF="https://askubuntu.com/questions/2214/how-do-i-add-a-user-to-the-sudo-group">How do I add a user to the "sudo" group? - Ask Ubuntu</A>
									<DT><A HREF="https://unix.stackexchange.com/questions/3568/how-to-switch-between-users-on-one-terminal">How to switch between users on one terminal?</A>
									<DT><A HREF="https://www.cyberciti.biz/faq/linux-list-users-command/">Linux List All Users In The System Command - nixCraft</A>
									<DT><A HREF="https://chatgpt.com/c/54a41702-74ee-4dc6-a049-4637eb63c617">Create and Switch Users</A>
								</DL><p>
								<DT><H3 FOLDED>linux-package manager</H3>
								<DL><p>
									<DT><H3 FOLDED>apt-get</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>apt</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>update &amp; upgrade</H3>
									<DL><p>
										<DT><H3 FOLDED>debfoster</H3>
										<DL><p>
											<DT><A HREF="https://manpages.ubuntu.com/manpages/trusty/man8/debfoster.8.html#name">Ubuntu Manpage: debfoster ‚Äî weed unnecessary Debian packages</A>
											<DT><A HREF="https://ubunlog.com/en/debfoster-clean-maintenance-ubuntu/">Debfoster, clean your system and keep only the important packages | Ubunlog</A>
											<DT><A HREF="https://ubuntuforums.org/showthread.php?t=24403">HOWTO: using debfoster in practice</A>
										</DL><p>
										<DT><A HREF="https://chat.openai.com/c/51c6dd70-1f39-4bf7-843e-837d48b6f0e4">apt-rdepends</A>
										<DT><A HREF="https://askubuntu.com/questions/44122/how-to-upgrade-a-single-package-using-apt-get">apt-get install --only-upgrade &lt;packagename&gt; (unitary upgrade)</A>
									</DL><p>
									<DT><H3 FOLDED>/var/lib/apt/lists</H3>
									<DL><p>
										<DT><A HREF="https://askubuntu.com/questions/179955/var-lib-apt-lists-is-huge">package management - /var/lib/apt/lists is huge - Ask Ubuntu</A>
										<DT><A HREF="https://github.com/devcontainers/images/blob/main/docs/TIPS.md/#why-do-dockerfiles-in-this-repository-use-run-statements-with-commands-separated-by">images/docs/TIPS.md at main ¬∑ devcontainers/images</A>
									</DL><p>
									<DT><A HREF="https://unix.stackexchange.com/questions/20979/how-do-i-list-all-installed-programs">application - How do I list all installed programs?</A>
									<DT><A HREF="https://unix.stackexchange.com/questions/561263/how-to-get-a-list-of-which-packages-were-installed-with-apt-get-by-a-user-and-no">How to get a list of which packages were installed with apt-get by a user and not by dependencies?</A>
									<DT><A HREF="https://chat.openai.com/c/51c6dd70-1f39-4bf7-843e-837d48b6f0e4">apt &amp; apt-get: /etc/apt/sources.list and other .list files in /etc/apt/sources.list.d/</A>
								</DL><p>
								<DT><H3 FOLDED>linux-configuration</H3>
								<DL><p>
									<DT><A HREF="https://www.omgubuntu.co.uk/2010/05/transfer-your-packages-to-a-clean-install">Transfer your packages to a clean install</A>
									<DT><A HREF="https://launchpad.net/oneconf">OneConf in Launchpad</A>
									<DT><A HREF="https://wiki.ubuntu.com/OneConf">OneConf - Ubuntu Wiki</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5d8ca2faf74c494f220c8f71130340b513eea9a9/docker/common/install_pytorch.sh">TensorRT-LLM/docker/common/install_pytorch.sh</A>
								</DL><p>
								<DT><H3 FOLDED>editors</H3>
								<DL><p>
									<DT><H3 FOLDED>VS Code</H3>
									<DL><p>
										<DT><A HREF="https://vscode-docs.readthedocs.io/en/latest/customization/themes/">Themes - vscode-docs</A>
										<DT><A HREF="https://code.visualstudio.com/docs/getstarted/settings">Workspace Settings</A>
									</DL><p>
									<DT><H3 FOLDED>vim</H3>
									<DL><p>
										<DT><H3 FOLDED>configuration</H3>
										<DL><p>
											<DT><A HREF="http://cream.sourceforge.net/home.html">Cream :: a modern configuration</A>
											<DT><A HREF="https://stackoverflow.com/questions/40576522/enable-vi-mouse-wheel-scrolling-using-bash-on-ubuntu-on-windows-10/40715383">Set mouse</A>
										</DL><p>
										<DT><H3 FOLDED>syntax highlighting</H3>
										<DL><p>
											<DT><A HREF="https://vim.fandom.com/wiki/Forcing_Syntax_Coloring_for_files_with_odd_extensions">Forcing Syntax Coloring</A>
											<DT><A HREF="https://www.cyberciti.biz/faq/turn-on-or-off-color-syntax-highlighting-in-vi-or-vim/">Turn On or Off Color Syntax Highlighting</A>
											<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/syntax.html">Huge Vim documentation: syntax</A>
											<DT><A HREF="https://vi.stackexchange.com/questions/5780/list-known-filetypes">List known filetypes</A>
										</DL><p>
										<DT><H3 FOLDED>shortcuts</H3>
										<DL><p>
											<DT><A HREF="http://www.keyxl.com/aaa8263/290/VIM-keyboard-shortcuts.htm">78 Keyboard Shortcuts for VIM</A>
										</DL><p>
										<DT><H3 FOLDED>cheatsheet</H3>
										<DL><p>
											<DT><A HREF="https://www.worldtimzone.com/res/vi.html">Short cheatsheet</A>
											<DT><A HREF="https://vim.rtorr.com/">Medium Vim Cheat Sheet</A>
										</DL><p>
										<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/help.html">Vim documentation: help</A>
										<DT><A HREF="https://unix.stackexchange.com/questions/161821/how-can-i-delete-all-lines-in-a-file-using-vi">Delete all lines frong a given file</A>
										<DT><H3 FOLDED>vim-configuration</H3>
										<DL><p>
											<DT><A HREF="http://cream.sourceforge.net/home.html">Cream :: a modern configuration</A>
											<DT><A HREF="https://stackoverflow.com/questions/40576522/enable-vi-mouse-wheel-scrolling-using-bash-on-ubuntu-on-windows-10/40715383">Set mouse</A>
										</DL><p>
										<DT><H3 FOLDED>vim-syntax-highlighting</H3>
										<DL><p>
											<DT><A HREF="https://vim.fandom.com/wiki/Forcing_Syntax_Coloring_for_files_with_odd_extensions">Forcing Syntax Coloring</A>
											<DT><A HREF="https://www.cyberciti.biz/faq/turn-on-or-off-color-syntax-highlighting-in-vi-or-vim/">Turn On or Off Color Syntax Highlighting</A>
											<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/syntax.html">Huge Vim documentation: syntax</A>
											<DT><A HREF="https://vi.stackexchange.com/questions/5780/list-known-filetypes">List known filetypes</A>
										</DL><p>
										<DT><H3 FOLDED>vim-shortcuts</H3>
										<DL><p>
											<DT><A HREF="http://www.keyxl.com/aaa8263/290/VIM-keyboard-shortcuts.htm">78 Keyboard Shortcuts for VIM</A>
										</DL><p>
										<DT><H3 FOLDED>vim-cheatsheet</H3>
										<DL><p>
											<DT><A HREF="https://www.worldtimzone.com/res/vi.html">Short cheatsheet</A>
											<DT><A HREF="https://vim.rtorr.com/">Medium Vim Cheat Sheet</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>neovim</H3>
									<DL><p>
										<DT><H3 FOLDED>config</H3>
										<DL><p>
											<DT><A HREF="https://medium.com/life-at-moka/step-up-your-game-with-neovim-62ba814166d7">Step Up Your Game with Neovim</A>
											<DT><A HREF="https://github.com/tpope/vim-fugitive">A Git wrapper</A>
											<DT><A HREF="https://github.com/preservim/nerdtree">A tree explorer plugin</A>
											<DT><A HREF="https://vi.stackexchange.com/questions/514/how-do-i-change-the-current-splits-width-and-height">change the current split's width and height</A>
											<DT><A HREF="https://www.youtube.com/watch?v=ZEFXeRIFvN0&t=404s">Command Line: Neovim Installation and Configuration</A>
											<DT><A HREF="https://github.com/junegunn/vim-plug">junegunn/vim-plug: Minimalist Vim Plugin Manager</A>
											<DT><A HREF="https://github.com/morhetz/gruvbox">morhetz/gruvbox: Retro groove color scheme for Vim</A>
											<DT><A HREF="https://www.youtube.com/watch?v=iagjeLuxnMs">My Entire Neovim + Tmux Workflow As A DevOps Engineer On MacOS - YouTube</A>
										</DL><p>
										<DT><H3 FOLDED>installation</H3>
										<DL><p>
											<DT><A HREF="https://github.com/neovim/neovim">neovim/neovim: Vim-fork focused on extensibility and usability</A>
											<DT><A HREF="https://formulae.brew.sh/formula/neovim#default">neovim ‚Äî Homebrew Formulae</A>
										</DL><p>
										<DT><H3 FOLDED>neovim-config</H3>
										<DL><p>
											<DT><A HREF="https://medium.com/life-at-moka/step-up-your-game-with-neovim-62ba814166d7">Step Up Your Game with Neovim</A>
											<DT><A HREF="https://github.com/tpope/vim-fugitive">A Git wrapper</A>
											<DT><A HREF="https://github.com/preservim/nerdtree">A tree explorer plugin</A>
											<DT><A HREF="https://vi.stackexchange.com/questions/514/how-do-i-change-the-current-splits-width-and-height">change the current split's width and height</A>
											<DT><A HREF="https://www.youtube.com/watch?v=ZEFXeRIFvN0&t=404s">Command Line: Neovim Installation and Configuration</A>
											<DT><A HREF="https://github.com/junegunn/vim-plug">junegunn/vim-plug: Minimalist Vim Plugin Manager</A>
											<DT><A HREF="https://github.com/morhetz/gruvbox">morhetz/gruvbox: Retro groove color scheme for Vim</A>
											<DT><A HREF="https://www.youtube.com/watch?v=iagjeLuxnMs">My Entire Neovim + Tmux Workflow As A DevOps Engineer On MacOS - YouTube</A>
										</DL><p>
										<DT><H3 FOLDED>neovim-installation</H3>
										<DL><p>
											<DT><A HREF="https://github.com/neovim/neovim">neovim/neovim: Vim-fork focused on extensibility and usability</A>
											<DT><A HREF="https://formulae.brew.sh/formula/neovim#default">neovim ‚Äî Homebrew Formulae</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>sublime text</H3>
									<DL><p>
										<DT><A HREF="https://packagecontrol.io/installation">Installation - Package Control</A>
										<DT><A HREF="https://packagecontrol.io/docs/usage">Usage - Package Control</A>
									</DL><p>
									<DT><A HREF="https://support.typora.io/Spellcheck/">Spellcheck - Typora Support</A>
									<DT><A HREF="https://support.typora.io/Line-Break/">Whitespace and Line Breaks - Typora Support</A>
								</DL><p>
								<DT><H3 FOLDED>linux-disk</H3>
								<DL><p>
									<DT><A HREF="https://man7.org/linux/man-pages/man8/lsblk.8.html">lsblk(8) - Linux manual page</A>
								</DL><p>
								<DT><H3 FOLDED>linux-filesystem</H3>
								<DL><p>
									<DT><A HREF="https://www.lustre.org/">Lustre: distributed filesystem</A>
								</DL><p>
								<DT><H3 FOLDED>linux-memory</H3>
								<DL><p>
									<DT><H3 FOLDED>LWN.net: What every programmer should know about memory</H3>
									<DL><p>
										<DT><A HREF="https://lwn.net/Articles/250967/">1. Introduction</A>
										<DT><A HREF="https://lwn.net/Articles/252125/">2. CPU caches</A>
										<DT><A HREF="https://lwn.net/Articles/253361/">3. Virtual Memory</A>
										<DT><A HREF="https://lwn.net/Articles/254445/">4. NUMA support</A>
										<DT><A HREF="https://lwn.net/Articles/255364/">5. What programmers can do - cache optimization</A>
										<DT><A HREF="https://lwn.net/Articles/256433/">6. What programmers can do - multi-threaded optimizations</A>
										<DT><A HREF="https://lwn.net/Articles/257209/">7. Memory performance tools</A>
										<DT><A HREF="https://lwn.net/Articles/258154/">8. Future technologies</A>
										<DT><A HREF="https://lwn.net/Articles/258188/">9. Examples and Benchmark Programs: matmul</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=L79vSP8yV2g">Base [4]: Memory Management - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=vHWiDx_l4V0&t=2436s">What's a Memory Allocator Anyway? - Benjamin Feng - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Sxx0TDPT0t8">üåø Week 17 Hobby Kernel Dev in C, x86: Physical memory allocator pt2 üåø - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>linux-networking</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=ck4WvYM9V4c">Linux Networking: How The Kernel Handles A TCP Connection - YouTube</A>
									<DT><A HREF="https://chatgpt.com/c/e57a5ce7-00de-486a-a8c2-c95db90cf417">non-blocking flat tree</A>
								</DL><p>
								<DT><H3 FOLDED>dkms</H3>
								<DL><p>
									<DT><A HREF="https://askubuntu.com/questions/408605/what-does-dkms-do-how-do-i-use-it">What does DKMS do? How do I use it? - Ask Ubuntu</A>
									<DT><A HREF="https://github.com/dell/dkms">dell/dkms: Dynamic Kernel Module Support</A>
								</DL><p>
								<DT><H3 FOLDED>shell</H3>
								<DL><p>
									<DT><H3 FOLDED>shell-languages</H3>
									<DL><p>
										<DT><H3 FOLDED>Bash</H3>
										<DL><p>
											<DT><A HREF="https://itnext.io/programmable-completion-for-bash-on-macos-f81a0103080b">Programmable Completion for Bash</A>
											<DT><A HREF="https://itnext.io/upgrading-bash-on-macos-7138bd1066ba">Upgrading</A>
											<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/Programmable-Completion.html">Programmable Completion (Bash Manual)</A>
											<DT><A HREF="https://www.cyberciti.biz/faq/how-to-turn-on-or-off-colors-in-bash/">How To Turn On/Off Colors For ls Command</A>
											<DT><A HREF="https://linoxide.com/how-tos/change-linux-shell-prompt-with-different-colors/">How to Change Bash Shell Prompt Colorful</A>
											<DT><A HREF="https://tldp.org/LDP/abs/html/">Advanced Bash-Scripting Guide</A>
											<DT><A HREF="https://stackoverflow.com/questions/589149/bash-script-to-cd-to-directory-with-spaces-in-pathname">cd to directory with WHITESPACES in pathname</A>
											<DT><A HREF="https://stackoverflow.com/questions/5130968/how-can-i-copy-the-output-of-a-command-directly-into-my-clipboard">copy the output of a command into clipboard</A>
											<DT><A HREF="https://www.cyberciti.biz/faq/apple-mac-osx-terminal-color-ls-output-option/">How to enable colorized output for ls command</A>
											<DT><A HREF="https://linuxize.com/post/bash-comments/">Writing Comments</A>
											<DT><A HREF="https://stackoverflow.com/questions/17066250/create-timestamp-variable-in-bash-script">Create timestamp variable in bash script - Stack Overflow</A>
											<DT><A HREF="https://github.com/dylanaraps/pure-bash-bible">dylanaraps/pure-bash-bible: üìñ A collection of pure bash alternatives to external processes.</A>
											<DT><H3 FOLDED>bash-error-handling</H3>
											<DL><p>
												<DT><A HREF="https://chatgpt.com/c/73e6a395-5da9-448f-b956-56c47bc98735">set -Eeuo</A>
											</DL><p>
											<DT><H3 FOLDED>bash-set</H3>
											<DL><p>
												<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html">The Set Builtin (Bash Reference Manual)</A>
											</DL><p>
											<DT><H3 FOLDED>bash-completions</H3>
											<DL><p>
												<DT><A HREF="https://itnext.io/programmable-completion-for-bash-on-macos-f81a0103080b">Programmable Completion for Bash</A>
												<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/Programmable-Completion.html">Programmable Completion (Bash Manual)</A>
											</DL><p>
											<DT><H3 FOLDED>bash-colors</H3>
											<DL><p>
												<DT><A HREF="https://www.cyberciti.biz/faq/how-to-turn-on-or-off-colors-in-bash/">How To Turn On/Off Colors For ls Command</A>
												<DT><A HREF="https://linoxide.com/how-tos/change-linux-shell-prompt-with-different-colors/">How to Change Bash Shell Prompt Colorful</A>
												<DT><A HREF="https://www.cyberciti.biz/faq/apple-mac-osx-terminal-color-ls-output-option/">How to enable colorized output for ls command</A>
											</DL><p>
											<DT><H3 FOLDED>bash-script</H3>
											<DL><p>
												<DT><A HREF="https://linuxize.com/post/bash-comments/">Writing Comments</A>
												<DT><A HREF="https://stackoverflow.com/questions/17066250/create-timestamp-variable-in-bash-script">Create timestamp variable in bash script - Stack Overflow</A>
											</DL><p>
											<DT><A HREF="https://github.com/dylanaraps/pure-bash-bible">dylanaraps/pure-bash-bible</A>
											<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/">Top (Bash Reference Manual)</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>shell-editors</H3>
									<DL><p>
										<DT><H3 FOLDED>VS Code</H3>
										<DL><p>
											<DT><A HREF="https://vscode-docs.readthedocs.io/en/latest/customization/themes/">Themes - vscode-docs</A>
										</DL><p>
										<DT><H3 FOLDED>vim</H3>
										<DL><p>
											<DT><H3 FOLDED>configuration</H3>
											<DL><p>
												<DT><A HREF="http://cream.sourceforge.net/home.html">Cream :: a modern configuration</A>
												<DT><A HREF="https://stackoverflow.com/questions/40576522/enable-vi-mouse-wheel-scrolling-using-bash-on-ubuntu-on-windows-10/40715383">Set mouse</A>
											</DL><p>
											<DT><H3 FOLDED>syntax highlighting</H3>
											<DL><p>
												<DT><A HREF="https://vim.fandom.com/wiki/Forcing_Syntax_Coloring_for_files_with_odd_extensions">Forcing Syntax Coloring</A>
												<DT><A HREF="https://www.cyberciti.biz/faq/turn-on-or-off-color-syntax-highlighting-in-vi-or-vim/">Turn On or Off Color Syntax Highlighting</A>
												<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/syntax.html">Huge Vim documentation: syntax</A>
												<DT><A HREF="https://vi.stackexchange.com/questions/5780/list-known-filetypes">List known filetypes</A>
											</DL><p>
											<DT><H3 FOLDED>shortcuts</H3>
											<DL><p>
												<DT><A HREF="http://www.keyxl.com/aaa8263/290/VIM-keyboard-shortcuts.htm">78 Keyboard Shortcuts for VIM</A>
											</DL><p>
											<DT><H3 FOLDED>cheatsheet</H3>
											<DL><p>
												<DT><A HREF="https://www.worldtimzone.com/res/vi.html">Short cheatsheet</A>
												<DT><A HREF="https://vim.rtorr.com/">Medium Vim Cheat Sheet</A>
											</DL><p>
											<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/help.html">Vim documentation: help</A>
											<DT><A HREF="https://unix.stackexchange.com/questions/161821/how-can-i-delete-all-lines-in-a-file-using-vi">Delete all lines frong a given file</A>
										</DL><p>
										<DT><H3 FOLDED>neovim</H3>
										<DL><p>
											<DT><H3 FOLDED>config</H3>
											<DL><p>
												<DT><A HREF="https://medium.com/life-at-moka/step-up-your-game-with-neovim-62ba814166d7">Step Up Your Game with Neovim</A>
												<DT><A HREF="https://github.com/tpope/vim-fugitive">A Git wrapper</A>
												<DT><A HREF="https://github.com/preservim/nerdtree">A tree explorer plugin</A>
												<DT><A HREF="https://vi.stackexchange.com/questions/514/how-do-i-change-the-current-splits-width-and-height">change the current split's width and height</A>
												<DT><A HREF="https://www.youtube.com/watch?v=ZEFXeRIFvN0&t=404s">Command Line: Neovim Installation and Configuration</A>
												<DT><A HREF="https://github.com/junegunn/vim-plug">junegunn/vim-plug: Minimalist Vim Plugin Manager</A>
												<DT><A HREF="https://github.com/morhetz/gruvbox">morhetz/gruvbox: Retro groove color scheme for Vim</A>
												<DT><A HREF="https://www.youtube.com/watch?v=iagjeLuxnMs">My Entire Neovim + Tmux Workflow As A DevOps Engineer On MacOS - YouTube</A>
											</DL><p>
											<DT><H3 FOLDED>installation</H3>
											<DL><p>
												<DT><A HREF="https://github.com/neovim/neovim">neovim/neovim: Vim-fork focused on extensibility and usability</A>
												<DT><A HREF="https://formulae.brew.sh/formula/neovim#default">neovim ‚Äî Homebrew Formulae</A>
											</DL><p>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>tmux</H3>
									<DL><p>
										<DT><A HREF="https://wiki.archlinux.org/index.php/Tmux#256_colors">tmux - ArchWiki</A>
										<DT><A HREF="https://stackoverflow.com/questions/18760281/how-to-increase-scrollback-buffer-size-in-tmux">scroll - How to increase scrollback buffer size?</A>
										<DT><A HREF="https://mutelight.org/practical-tmux">Practical Tmux</A>
										<DT><A HREF="https://www.runrails.com/tmux/scrolling-in-tmux/#:~:text=2%20%2D%20With%20keyboard%20shortcuts,around%20with%20the%20arrow%20keys.&text=Just%20as%20with%20the%20mouse,to%20add%20them%20to%20your%20.&text=Note%20that%20you%20have%20to,bound%20as%20the%20command%20key.">How to scroll back in Tmux</A>
									</DL><p>
									<DT><H3 FOLDED>fuzzy finder</H3>
									<DL><p>
										<DT><A HREF="https://github.com/junegunn/fzf#using-homebrew">junegunn/fzf: A command-line fuzzy finder</A>
										<DT><A HREF="https://www.youtube.com/watch?v=qgG5Jhi_Els">Vim universe. fzf - command line fuzzy finder - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=hJzqEAf2U4I">I made the greatest tool ever! | tmux &amp; cht.sh &amp; fzf - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>shell-profiles</H3>
									<DL><p>
										<DT><A HREF="https://unix.stackexchange.com/questions/476593/when-should-i-use-bashrc-and-when-profile">When should I use .bashrc and when .profile?</A>
									</DL><p>
									<DT><H3 FOLDED>grep</H3>
									<DL><p>
										<DT><H3 FOLDED>ripgrep</H3>
										<DL><p>
											<DT><A HREF="https://github.com/BurntSushi/ripgrep">ripgrep: ripgrep recursively searches directories for a regex pattern</A>
											<DT><A HREF="https://github.com/BurntSushi/ripgrep/blob/master/GUIDE.md">ripgrep/GUIDE.md</A>
											<DT><A HREF="https://github.com/BurntSushi/ripgrep/issues/623">hidden files to be searched by default ¬∑ Issue #623 ¬∑ BurntSushi/ripgrep</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>man &amp; info &amp; help</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=vnBCnd2L0dY">Linux Experts Read 'info' Pages (NOT 'man' pages) - YouTube</A>
										<DT><A HREF="https://man7.org/linux/man-pages/man1/ldd.1.html">ldd - print shared object dependencies</A>
									</DL><p>
									<DT><H3 FOLDED>file manager</H3>
									<DL><p>
										<DT><H3 FOLDED>yazi</H3>
										<DL><p>
											<DT><A HREF="https://github.com/sxyazi/yazi">sxyazi/yazi: üí• Blazing fast terminal file manager written in Rust, based on async I/O.</A>
											<DT><A HREF="https://yazi-rs.github.io/docs/configuration/overview">Configuration | Yazi</A>
											<DT><A HREF="https://yazi-rs.github.io/docs/quick-start/">shell wrapper: change dir</A>
											<DT><A HREF="https://github.com/sxyazi/yazi/issues/801">yazi-adaptor not compiling ¬∑ Issue #801 ¬∑ sxyazi/yazi</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>shell-pipes</H3>
									<DL><p>
										<DT><A HREF="https://github.com/akavel/up">akavel/up: Ultimate Plumber is a tool for writing Linux pipes with instant live preview</A>
									</DL><p>
									<DT><H3 FOLDED>shell-trash</H3>
									<DL><p>
										<DT><A HREF="https://github.com/andreafrancia/trash-cli">andreafrancia/trash-cli: Command line interface to the freedesktop.org trashcan.</A>
									</DL><p>
									<DT><H3 FOLDED>std-out-err</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tiangolo/typer/blob/04eba6b70203287176d2823753513226bf778872/docs/tutorial/printing.md#standard-output-and-standard-error">Standard Output and Standard Error</A>
									</DL><p>
									<DT><H3 FOLDED>shell-automation</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=9KAp_zWeI34">Automating Everything in Linux with ENTR! - YouTube</A>
										<DT><A HREF="https://eradman.com/entrproject/">entr(1)</A>
										<DT><A HREF="https://github.com/tinygrad/tinyos">tinygrad/tinyos</A>
									</DL><p>
									<DT><A HREF="https://ss64.com/osx/">An A-Z Index of the Apple macOS command line</A>
									<DT><A HREF="https://jqlang.github.io/jq/">jq</A>
									<DT><A HREF="https://gist.github.com/sts10/daadbc2f403bdffad1b6d33aff016c0a">A curated list of command-line utilities written in Rust</A>
									<DT><A HREF="https://github.com/sharkdp/bat">bat: A cat(1) clone with wings.</A>
									<DT><A HREF="https://github.com/sxyazi/yazi">sxyazi/yazi: üí• Blazing fast terminal file manager written in Rust, based on async I/O.</A>
									<DT><A HREF="https://github.com/Byron/dua-cli">Byron/dua-cli: View disk space usage and delete unwanted data, fast.</A>
									<DT><A HREF="https://github.com/sharkdp/fd">sharkdp/fd: A simple, fast and user-friendly alternative to 'find'</A>
									<DT><A HREF="https://github.com/Canop/dysk">Canop/dysk: A linux utility to get information on filesystems, like df but better</A>
									<DT><A HREF="https://github.com/Syllo/nvtop">Syllo/nvtop: GPU &amp; Accelerator process monitoring for AMD, Apple, Huawei, Intel, NVIDIA and Qualcomm</A>
									<DT><A HREF="https://github.com/Xfennec/progress">Xfennec/progress: Linux tool to show progress for cp, mv, dd, ... (formerly known as cv)</A>
									<DT><A HREF="https://github.com/atuinsh/atuin">atuinsh/atuin: ‚ú® Magical shell history</A>
									<DT><A HREF="https://github.com/facebookarchive/pcicrawler">facebookarchive/pcicrawler: pcicrawler is a Python based command line interface tool which can be used to display, filter and export information about PCI (Peripheral Component Interconnect) or PCIe buses and devices, as well as PCI topology.</A>
									<DT><A HREF="http://www.faqs.org/faqs/unix-faq/shell/shell-differences/">UNIX shell differences and how to change your shell</A>
									<DT><A HREF="https://johndjameson.com/blog/updating-your-shell-with-homebrew/">Updating Your Shell with Homebrew</A>
									<DT><A HREF="https://unix.stackexchange.com/questions/167631/finding-the-original-file-of-a-symbolic-link/167632">readlink - Finding the original file of a symbolic link</A>
									<DT><A HREF="https://ss64.com/osx/ln.html">ln Man Page - Symbolic links</A>
									<DT><A HREF="https://www.baeldung.com/linux/head-tail-commands">Head &amp; Tail</A>
									<DT><A HREF="https://explainshell.com/explain?cmd=tr+%22%3A%22+%22%5Cn%22+%3C%3C%3C%22%24PATH%22">explainshell.com - tr ":" "\n" &lt;&lt;&lt;"$PATH"</A>
									<DT><A HREF="https://explainshell.com/">explainshell.com - match command-line arguments to their help text</A>
									<DT><A HREF="https://github.com/BurntSushi/ripgrep">BurntSushi/ripgrep: ripgrep recursively searches directories for a regex pattern while respecting your gitignore</A>
									<DT><A HREF="https://github.com/chubin/cheat.sh">chubin/cheat.sh: the only cheat sheet you need</A>
									<DT><A HREF="https://man7.org/linux/man-pages/man1/dmesg.1.html">dmesg: print or control the kernel ring buffer</A>
									<DT><A HREF="https://explainshell.com/">explainshell.com</A>
									<DT><A HREF="https://github.com/Xfennec/progress">Xfennec/progress: Linux tool to show progress for cp, mv, dd</A>
									<DT><A HREF="https://jqlang.github.io/jq/">jq: JSON processor</A>
									<DT><A HREF="https://man7.org/linux/man-pages/man1/du.1.html">du(1) - Linux manual page (du -sh &lt;dir&gt;)</A>
									<DT><A HREF="https://man7.org/linux/man-pages/man1/ldd.1.html">ldd - print shared object dependencies</A>
									<DT><A HREF="https://github.com/facebookincubator/below">facebookincubator/below: A time traveling resource monitor for modern Linux systems</A>
									<DT><A HREF="https://github.com/aristocratos/btop">aristocratos/btop: A monitor of resources</A>
								</DL><p>
								<DT><H3 FOLDED>std-out-err</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tiangolo/typer/blob/04eba6b70203287176d2823753513226bf778872/docs/tutorial/printing.md#standard-output-and-standard-error">Standard Output and Standard Error</A>
								</DL><p>
								<DT><H3 FOLDED>UNIX</H3>
								<DL><p>
									<DT><H3 FOLDED>learning</H3>
									<DL><p>
										<DT><H3 FOLDED>$PATH</H3>
										<DL><p>
											<DT><A HREF="https://askubuntu.com/questions/600018/how-to-display-path-as-one-directory-per-line">How to display $PATH as one directory per line?</A>
											<DT><A HREF="https://kb.iu.edu/d/acar">Set or modify a path in Unix</A>
										</DL><p>
										<DT><A HREF="https://kb.iu.edu/d/acmq">current values of all environment variables and functions</A>
										<DT><A HREF="https://tldp.org/LDP/Linux-Filesystem-Hierarchy/html/usr.html">/usr</A>
										<DT><A HREF="https://refspecs.linuxfoundation.org/FHS_3.0/fhs/ch04s09.html">4.9.¬†/usr/local : Local hierarchy</A>
										<DT><A HREF="https://blog.balthazar-rouberol.com/text-processing-in-the-shell">Text processing in the shell</A>
									</DL><p>
									<DT><H3 FOLDED>building</H3>
									<DL><p>
										<DT><A HREF="http://www.lemis.com/grog/Documentation/Lions/book.pdf">Book: Commentary on the sixth edition</A>
									</DL><p>
									<DT><A HREF="https://kb.iu.edu/d/affo">What do some common Unix file extensions mean?</A>
								</DL><p>
								<DT><H3 FOLDED>GNU</H3>
								<DL><p>
									<DT><A HREF="https://melpa.org/#/">MELPA</A>
									<DT><A HREF="https://www.gnu.org/software/emacs/download.html#nonfree">Emacs download</A>
									<DT><A HREF="https://www.gnu.org/software/emacs/refcards/pdf/refcard.pdf">GNU Emacs Reference Card</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/GNU_Autotools">GNU Autotools - Wikipedia</A>
									<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/">Top (Bash Reference Manual)</A>
								</DL><p>
								<DT><A HREF="https://ss64.com/osx/">An A-Z Index of the Apple macOS command line</A>
								<DT><A HREF="https://learnubuntu.com/list-users/">How to List Users in Ubuntu Command Line</A>
								<DT><A HREF="https://unix.stackexchange.com/questions/20979/how-do-i-list-all-installed-programs">application - How do I list all installed programs?</A>
								<DT><A HREF="https://unix.stackexchange.com/questions/561263/how-to-get-a-list-of-which-packages-were-installed-with-apt-get-by-a-user-and-no">How to get a list of which packages were installed with apt-get by a user and not by dependencies?</A>
								<DT><A HREF="https://www.cyberciti.biz/faq/create-a-user-account-on-ubuntu-linux/">How to create a user account on Ubuntu Linux - nixCraft</A>
								<DT><A HREF="https://www.youtube.com/watch?v=vnBCnd2L0dY">Linux Experts Read 'info' Pages (NOT 'man' pages) - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=d0gS5TXarXc&t=80s">Signals. I spent 2 years to understand this part</A>
								<DT><A HREF="https://github.com/facebookincubator/below?tab=readme-ov-file">facebookincubator/below: A time traveling resource monitor for modern Linux systems</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Ia5jyz8sOCM">No really, how does Linux run executables?</A>
								<DT><H3 FOLDED>ssh</H3>
								<DL><p>
									<DT><A HREF="https://docs.digitalocean.com/products/droplets/how-to/add-ssh-keys/create-with-openssh/">How to Create SSH Keys with OpenSSH on MacOS or Linux :: DigitalOcean Documentation</A>
									<DT><A HREF="https://docs.digitalocean.com/products/droplets/how-to/connect-with-ssh/">How to Connect to Droplets with SSH :: DigitalOcean Documentation</A>
									<DT><A HREF="https://linuxize.com/post/using-the-ssh-config-file/">Using the SSH Config File | Linuxize</A>
									<DT><A HREF="https://www.openssh.com/">OpenSSH</A>
								</DL><p>
								<DT><H3 FOLDED>linux-checkpoint</H3>
								<DL><p>
									<DT><A HREF="https://github.com/checkpoint-restore/criu">checkpoint-restore/criu: Checkpoint/Restore tool</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>macOS</H3>
							<DL><p>
								<DT><H3 FOLDED>M1</H3>
								<DL><p>
									<DT><A HREF="https://support.apple.com/en-us/HT211861">Rosetta 2 overview</A>
									<DT><A HREF="https://medium.com/mkdir-awesome/how-to-install-x86-64-homebrew-packages-on-apple-m1-macbook-54ba295230f">How to Install x86_64 Homebrew Packages on Apple M1 MacBook</A>
									<DT><A HREF="https://news.ycombinator.com/item?id=25132217">Run x86 Apps (including homebrew) in the Terminal on Apple Silicon | Hacker News</A>
									<DT><A HREF="https://wickedchicken.github.io/post/macos-nix-setup/">MacOS Nix Setup (an alternative to Homebrew)</A>
									<DT><H3 FOLDED>monitoring</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tlkh/asitop">tlkh/asitop: Perf monitoring CLI tool for Apple Silicon (nvtop)</A>
									</DL><p>
									<DT><H3 FOLDED>m1-monitoring</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tlkh/asitop">tlkh/asitop: Perf monitoring CLI tool for Apple Silicon (nvtop)</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>finder</H3>
								<DL><p>
									<DT><A HREF="https://support.apple.com/guide/mac-help/see-the-devices-connected-to-your-mac-mchlp1039/mac">Finder options: Connected devices</A>
									<DT><A HREF="https://mac4u.tech/how-to-see-hidden-files-and-folders-in-macos-big-sur/">How to see hidden files and folders in Apple macOS Big Sur</A>
								</DL><p>
								<DT><H3 FOLDED>xterm</H3>
								<DL><p>
									<DT><A HREF="https://zsh.sourceforge.io/Doc/Release/Prompt-Expansion.html#Prompt-Expansion">zsh: 13 Prompt Expansion</A>
									<DT><A HREF="https://www.makeuseof.com/customize-zsh-prompt-macos-terminal/">PS1</A>
									<DT><A HREF="https://www.cyberciti.biz/faq/change-default-shell-to-bash-on-macos-catalina/">chsh (Catalina/BigSur)</A>
								</DL><p>
								<DT><A HREF="https://support.apple.com/en-hk/guide/terminal/apdc6c1077b-5d5d-4d35-9c19-60f2397b2369/2.10/mac/10.15">launchd - manage daemons and agents</A>
								<DT><A HREF="https://discussions.apple.com/thread/250756110">show hidden files and folders</A>
								<DT><A HREF="https://www.cyberciti.biz/faq/apple-mac-osx-terminal-color-ls-output-option/">How to enable colorized output for ls command</A>
								<DT><A HREF="https://embeddedartistry.com/blog/2017/02/24/installing-llvm-clang-on-osx/">Installing LLVM/Clang on OS X</A>
								<DT><A HREF="https://ss64.com/osx/">An A-Z Index of the Apple macOS command line</A>
							</DL><p>
							<DT><H3 FOLDED>build from source</H3>
							<DL><p>
								<DT><A HREF="https://serverfault.com/questions/46381/learning-to-compile-things-from-source-on-unix-linux-osx">compile things from source</A>
								<DT><A HREF="https://zig.news/kristoff/compile-a-c-c-project-with-zig-368j">Compile a C/C++ Project with Zig - Zig NEWS</A>
								<DT><A HREF="https://www.youtube.com/watch?v=wFlyUzUVFhw">Zig Build System &amp; How to Build Software From Source ‚Ä¢ Andrew Kelley ‚Ä¢ GOTO 2023 - YouTube</A>
								<DT><H3 FOLDED>make</H3>
								<DL><p>
									<DT><H3 FOLDED>debug</H3>
									<DL><p>
										<DT><A HREF="https://chat.openai.com/c/926b55b6-8995-4593-aff8-3150238c1398">.PHONY: debug \n debug: @echo</A>
									</DL><p>
									<DT><H3 FOLDED>makefile</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/docker.Makefile">pytorch/docker.Makefile at main ¬∑ pytorch/pytorch</A>
									</DL><p>
									<DT><A HREF="https://chat.openai.com/c/926b55b6-8995-4593-aff8-3150238c1398">.PHONY: debug \n debug: @echo</A>
									<DT><H3 FOLDED>untitled folder</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>make-debug</H3>
									<DL><p>
										<DT><A HREF="https://chat.openai.com/c/926b55b6-8995-4593-aff8-3150238c1398">.PHONY: debug \n debug: @echo</A>
									</DL><p>
									<DT><H3 FOLDED>cmake</H3>
									<DL><p>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>from-source-linker-(ld)</H3>
								<DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/9688200/difference-between-shared-objects-so-static-libraries-a-and-dlls-so">Difference shared objects (.so), static libraries (.a), and DLL's (.so)</A>
								</DL><p>
								<DT><H3 FOLDED>shared-libraries</H3>
								<DL><p>
									<DT><A HREF="https://www.hpc.dtu.dk/?page_id=1180">LD_LIBRARY_PATH ‚Äì or: How to get yourself into trouble!</A>
									<DT><A HREF="https://askubuntu.com/questions/1461829/set-ld-library-path-for-with-symlinks">environment variables - Set LD_LIBRARY_PATH for with symlinks - Ask Ubuntu</A>
									<DT><A HREF="https://help.ubuntu.com/community/EnvironmentVariables#File-location_related_variables">EnvironmentVariables - Community Help Wiki</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=wFlyUzUVFhw">Zig Build System &amp; How to Build Software From Source ‚Ä¢ Andrew Kelley</A>
							</DL><p>
							<DT><H3 FOLDED>build-system</H3>
							<DL><p>
								<DT><H3 FOLDED>bazel</H3>
								<DL><p>
									<DT><H3 FOLDED>installation</H3>
									<DL><p>
										<DT><A HREF="https://blog.bazel.build/2018/08/22/bazel-homebrew.html">Bazel in Homebrew - Bazel</A>
										<DT><A HREF="https://docs.bazel.build/versions/main/install-os-x.html#install-with-installer-mac-os-x">Installing Bazel on macOS - Bazel main</A>
										<DT><A HREF="https://github.com/bazelbuild/bazelisk">bazelbuild/bazelisk: A user-friendly launcher for Bazel.</A>
										<DT><A HREF="https://formulae.brew.sh/formula/bazelisk">bazelisk ‚Äî Homebrew Formulae</A>
									</DL><p>
									<DT><H3 FOLDED>examples</H3>
									<DL><p>
										<DT><A HREF="https://github.com/bazelbuild/examples/tree/main/bzlmod/01-depend_on_bazel_module">examples/bzlmod/01-depend_on_bazel_module</A>
										<DT><A HREF="https://github.com/grpc/grpc/blob/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/protos/BUILD">grpc/examples/protos/BUILD "helloworld"</A>
									</DL><p>
									<DT><H3 FOLDED>v7</H3>
									<DL><p>
										<DT><A HREF="https://github.com/bazelbuild/bazel/issues/18958">enable_bzlmod: Flip the default value of `--enable_bzlmod` to true ¬∑ Issue #18958 ¬∑ bazelbuild/bazel</A>
									</DL><p>
									<DT><H3 FOLDED>modules</H3>
									<DL><p>
										<DT><H3 FOLDED>dependency graph</H3>
										<DL><p>
											<DT><A HREF="https://bazel.build/tutorials/cpp-dependency">Review the dependency graph ¬†|¬† Bazel</A>
										</DL><p>
										<DT><A HREF="https://bazel.build/external/module">Bazel modules</A>
										<DT><A HREF="https://bazel.build/rules/lib/globals/module">MODULE.bazel files ¬†|¬† Bazel</A>
										<DT><A HREF="https://docs.google.com/document/d/1moQfNcEIttsk6vYanNKIy3ZuK53hQUFq1b1r0rmsYVg/edit">Bazel External Dependencies Overhaul</A>
										<DT><A HREF="https://github.com/bazelbuild/bazel/issues/18958">enable_bzlmod: Flip the default value of `--enable_bzlmod` to true ¬∑ Issue #18958 ¬∑ bazelbuild/bazel</A>
									</DL><p>
									<DT><H3 FOLDED>starlark</H3>
									<DL><p>
										<DT><A HREF="https://github.com/google/starlark-go">google/starlark-go: Starlark in Go: the Starlark configuration language, implemented in Go</A>
									</DL><p>
									<DT><H3 FOLDED>rules</H3>
									<DL><p>
										<DT><H3 FOLDED>cpp</H3>
										<DL><p>
											<DT><A HREF="https://github.com/bazelbuild/examples/blob/main/cpp-tutorial/stage1/main/hello-world.cc">examples/cpp-tutorial/stage1/main/hello-world.cc at main ¬∑ bazelbuild/examples</A>
											<DT><A HREF="https://docs.bazel.build/versions/1.0.0/tutorial/cpp.html#understand-the-build-file">Build Tutorial - C++ - Bazel 1.0.0</A>
											<DT><A HREF="https://bazel.build/tutorials/cpp-dependency">Review the dependency graph ¬†|¬† Bazel</A>
										</DL><p>
										<DT><H3 FOLDED>rust</H3>
										<DL><p>
											<DT><A HREF="https://news.ycombinator.com/item?id=24847702">Not exactly true. Google uses Bazel for Rust code</A>
											<DT><A HREF="https://github.com/bazelbuild/rules_rust/tree/main/examples/ffi">rules_rust</A>
										</DL><p>
									</DL><p>
									<DT><A HREF="https://github.com/OasisDigital/bazelcon-2019">OasisDigital/bazelcon-2019: Bazel examples for Bazel Boot Camp</A>
									<DT><A HREF="https://www.google.com/search?q=papers+on+Googles+build+infrastructure+bazel&client=safari&rls=en&sxsrf=AOaemvLW8YwdatuWueomwqIEZZgYu-7-Cw%3A1642121078072&ei=dsfgYYvjA9DSa727vMgG&ved=0ahUKEwiL9qTcgbD1AhVQ6RoKHb0dD2kQ4dUDCA0&uact=5&oq=papers+on+Googles+build+infrastructure+bazel&gs_lcp=Cgdnd3Mtd2l6EAMyBwghEAoQoAE6BwgAEEcQsAM6BQghEKsCSgQIQRgASgQIRhgAUJsDWOEJYLELaAFwAHgAgAGVAYgB5gWSAQMwLjaYAQCgAQHIAQjAAQE&sclient=gws-wiz">papers on Googles build infrastructure bazel - Google Search</A>
									<DT><A HREF="https://oasisdigital.com/class/bazel">Building with Bazel | Oasis Digital</A>
									<DT><A HREF="https://www.buildbuddy.io/">BuildBuddy</A>
									<DT><A HREF="https://bazel.build/reference/glossary#dependency">Bazel Glossary</A>
									<DT><A HREF="https://docs.google.com/document/d/1moQfNcEIttsk6vYanNKIy3ZuK53hQUFq1b1r0rmsYVg/edit">Bazel External Dependencies Overhaul - Documentos de Google</A>
									<DT><A HREF="https://www.visualcapitalist.com/wp-content/uploads/2017/02/1276_lines_of_code_sep2015_fb.png">1276_lines_of_code_sep2015_fb.png 1.276√ó4.670 pixels</A>
									<DT><A HREF="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext#:~:text=Google%2DScale&text=The%20Google%20codebase%20includes%20approximately,Google's%20entire%2018%2Dyear%20existence">Why Google Stores Billions of Lines of Code in a Single Repository (monorepo)</A>
									<DT><A HREF="https://github.com/jetstack/cert-manager/pull/4184/files/611bac67cf6dc8b58130b9cb43486d4ddda1b387#diff-23618edb36c476683f5a0c453d4d8fdb296b4c9fc3b21163dbd05b0c7d038708">code example in Go</A>
									<DT><A HREF="https://docs.bazel.build/versions/1.0.0/tutorial/cpp.html#understand-the-build-file">Build Tutorial - C++ - Bazel 1.0.0</A>
									<DT><A HREF="https://www.reddit.com/r/bazel/">bazel subreddit</A>
								</DL><p>
								<DT><H3 FOLDED>facebook-getdeps</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebook/proxygen/blob/8e338869dc41dee6d197d5cf627482f0a9159bb8/build/fbcode_builder/README.md">proxygen/build/fbcode_builder/README.md at 8e338869dc41dee6d197d5cf627482f0a9159bb8 ¬∑ facebook/proxygen</A>
								</DL><p>
								<DT><H3 FOLDED>interoperability</H3>
								<DL><p>
									<DT><A HREF="https://github.com/kriasoft/relay-starter-kit/blob/c985206/src/utils/password_hash.cc">NAPI C++ example</A>
									<DT><A HREF="https://koistya.medium.com/how-to-call-c-c-code-from-node-js-86a773033892">How to call C/C++ code from Node.js</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Interface_description_language">Interface description language (IDL) - Wikipedia</A>
									<DT><A HREF="https://cxx.rs/">Rust ‚ù§Ô∏è C++</A>
								</DL><p>
								<DT><H3 FOLDED>papers</H3>
								<DL><p>
									<DT><A HREF="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45880.pdf">google CI</A>
									<DT><A HREF="https://www.microsoft.com/en-us/research/uploads/prod/2018/03/build-systems-final.pdf">Build Systems aÃÄ la Carte</A>
								</DL><p>
								<DT><A HREF="https://scons.org/">SCons: A software construction tool - SCons</A>
								<DT><A HREF="https://docs.engflow.com/docs/re.html">Remote Execution Service | Documentation</A>
								<DT><A HREF="https://github.com/web3infra-foundation/mega">web3infra-foundation/mega: Mega is an unofficial open source implementation of Google Piper.</A>
								<DT><A HREF="https://github.com/facebook/buck2">facebook/buck2: Build system, successor to Buck</A>
								<DT><A HREF="https://buck2.build/">Buck2 build system website | Buck2</A>
								<DT><A HREF="https://github.com/facebook/buck2/tree/main/examples/hello_world">buck2/examples/hello_world at main ¬∑ facebook/buck2</A>
								<DT><A HREF="https://github.com/facebook/buck2/blob/main/examples/hello_world/BUCK">buck2/examples/hello_world/BUCK at main ¬∑ facebook/buck2</A>
							</DL><p>
							<DT><H3 FOLDED>os-file-formats</H3>
							<DL><p>
								<DT><H3 FOLDED>parquet</H3>
								<DL><p>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=bISBNVtXZ6M">Nimble, A New Columnar File Format - Yoav Helfman, Meta - YouTube</A>
								<DT><A HREF="https://github.com/facebookexternal/nimble">facebookexternal/nimble: New file format for storage of large columnar datasets.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-ZtyRgD1c40">Velox and Composable Data Management - Pedro Pedreira, Meta - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=bISBNVtXZ6M&t=1169s">Nimble, A New Columnar File Format - Yoav Helfman, Meta - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>os-emulation</H3>
							<DL><p>
								<DT><A HREF="https://fms.komkon.org/EMUL8/HOWTO.html">HOWTO: Writing a Computer Emulator</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Kq849CpGd88">QEMU Performance</A>
							</DL><p>
							<DT><A HREF="https://en.wikipedia.org/wiki/POSIX">POSIX</A>
							<DT><A HREF="https://www.youtube.com/watch?v=d0gS5TXarXc&t=80s">Signals. I spent 2 years to understand this part</A>
							<DT><A HREF="https://github.com/sdmg15/Best-websites-a-programmer-should-visit?tab=readme-ov-file">sdmg15/Best-websites-a-programmer-should-visit: :link: Some useful websites for programmers.</A>
						</DL><p>
						<DT><H3 FOLDED>ml-sys-networking</H3>
						<DL><p>
							<DT><H3 FOLDED>IP</H3>
							<DL><p>
								<DT><A HREF="https://lwn.net/Articles/960913/">So you think you understand IP fragmentation? [LWN.net]</A>
							</DL><p>
							<DT><A HREF="https://www.linkedin.com/pulse/network-acceleration-genai-workloads-sharada-yeluri-g8xbc/?trackingId=4J97SZQpTwenJBkn4Hr6nA%3D%3D">(1) In Network Acceleration for AI/ML Workloads | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/pulse/gpu-fabrics-genai-workloads-sharada-yeluri-j8ghc/?trackingId=5mCxOsDBSoG07wlKbx6o2g%3D%3D">(1) GPU Fabrics for GenAI Workloads | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/pulse/network-acceleration-genai-workloads-sharada-yeluri-g8xbc/?trackingId=JYRFyD33QkmHY5eoE24rCg%3D%3D">(1) In Network Acceleration for AI/ML Workloads | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/pulse/llm-inference-hwsw-optimizations-sharada-yeluri-wfdyc/?trackingId=IAo88qPrQLeVto9JLwix6w%3D%3D">(1) LLM Inference - HW/SW Optimizations | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/pulse/tearing-down-memory-wall-sharada-yeluri/?trackingId=dn9NrSPARD2R%2BUQ9KN6CXQ%3D%3D">(1) Tearing Down the Memory Wall | LinkedIn</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ck4WvYM9V4c">Linux Networking: How The Kernel Handles A TCP Connection - YouTube</A>
							<DT><H3 FOLDED>RDMA</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=kDJHA7TNtDk">NSDI '23 - Empowering Azure Storage with RDMA - YouTube</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>Systems Performance: Enterprise and the Cloud</H3>
						<DL><p>
							<DT><H3 FOLDED>bpf</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/bpftoolkit">Netflix-Skunkworks/bpftoolkit</A>
								<DT><A HREF="https://www.youtube.com/watch?v=7pmXdG8-7WU&list=PLZT-fvrVTVfPVLkSYPlGo6Vwp9uze9H2q&index=5">Netflix talks about Extended BPF - A new software type</A>
								<DT><A HREF="https://manpages.ubuntu.com/manpages/focal/en/man8/biosnoop.bt.8.html">biosnoop.bt - Block I/O tracing tool, showing per I/O latency</A>
								<DT><A HREF="https://github.com/aquasecurity/tracee">aquasecurity/tracee: Linux Runtime Security and Forensics using eBPF</A>
								<DT><A HREF="https://github.com/Netflix/bpftop">Netflix/bpftop: bpftop provides a dynamic real-time view of running eBPF programs. It displays the average runtime, events per second, and estimated total CPU % for each program.</A>
							</DL><p>
							<DT><H3 FOLDED>WSPerfLab</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/WSPerfLab">Netflix-Skunkworks/WSPerfLab: Project for testing web-service implementations.</A>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/WSPerfLab/blob/master/test-results/RxNetty_vs_Tomcat_April2015.pdf">RxNetty vs Tomcat Performance Results</A>
							</DL><p>
							<DT><H3 FOLDED>Service Capacity Modeling</H3>
							<DL><p>
								<DT><H3 FOLDED>AWS</H3>
								<DL><p>
									<DT><A HREF="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html">Amazon EBS volume types - Amazon EBS</A>
								</DL><p>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/service-capacity-modeling">Netflix-Skunkworks/service-capacity-modeling</A>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/service-capacity-modeling/blob/main/notebooks/demo.ipynb">service-capacity-modeling/notebooks/demo.ipynb at main ¬∑ Netflix-Skunkworks/service-capacity-modeling</A>
							</DL><p>
							<DT><H3 FOLDED>sys-perftools</H3>
							<DL><p>
								<DT><H3 FOLDED>Performance Monitoring Counters (PMC)</H3>
								<DL><p>
									<DT><A HREF="https://github.com/brendangregg/pmc-cloud-tools">brendangregg/pmc-cloud-tools: PMC (Performance Monitoring Counter) tools for the cloud</A>
									<DT><A HREF="https://www.brendangregg.com/blog/2017-05-04/the-pmcs-of-ec2.html">The PMCs of EC2: Measuring IPC</A>
									<DT><A HREF="https://twitter.com/__tinygrad__/status/1768408123826721045">tinybox shows:</A>
								</DL><p>
								<DT><A HREF="https://manpages.ubuntu.com/manpages/focal/en/man8/biosnoop.bt.8.html">biosnoop.bt - Block I/O tracing tool, showing per I/O latency</A>
								<DT><A HREF="https://github.com/hoytech/vmtouch">hoytech/vmtouch: Portable file system cache diagnostics and control</A>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/corepipe">Netflix-Skunkworks/corepipe: perform a coredump of a running process</A>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/diffy">netflix/diffy: digital forensics and incident response (DFIR)</A>
								<DT><A HREF="https://www.redhat.com/sysadmin/stop-using-telnet-test-port">Stop using Telnet to test ports | Red Hat</A>
								<DT><A HREF="https://manpages.ubuntu.com/manpages/focal/en/man8/iotop.8.html">Ubuntu Manpage: iotop - simple top-like I/O monitor</A>
								<DT><A HREF="https://github.com/wagoodman/dive">wagoodman/dive: A tool for exploring each layer in a docker image</A>
								<DT><A HREF="https://justine.lol/rusage/">Portable rusage command: Report resource usage statistics when launching cmd programms</A>
								<DT><A HREF="https://chat.openai.com/c/6f34e9bd-60ae-4c54-848e-479b93d3e219">df -h: list disk usage of volumes or mount points</A>
								<DT><A HREF="https://mariusschulz.com/blog/fast-searching-with-ripgrep">Fast Searching with ripgrep</A>
								<DT><A HREF="https://github.com/facebookincubator/below">facebookincubator/below: A time traveling resource monitor for modern Linux systems</A>
							</DL><p>
							<DT><H3 FOLDED>observability</H3>
							<DL><p>
								<DT><A HREF="https://netflixtechblog.com/tagged/observability">Observability ‚Äì Netflix TechBlog</A>
								<DT><A HREF="https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55">Linux Performance Analysis in 60,000 Milliseconds | by Netflix Technology Blog | Netflix TechBlog</A>
							</DL><p>
							<DT><A HREF="https://www.brendangregg.com/">Performance Monitoring Counters (PMC)</A>
							<DT><A HREF="https://www.brendangregg.com/linuxperf.html">Linux Performance</A>
							<DT><A HREF="https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55">Linux Performance Analysis in 60,000 Milliseconds | by Netflix Technology Blog | Netflix TechBlog</A>
							<DT><A HREF="https://github.com/quickwit-oss/quickwit">quickwit-oss/quickwit: Cloud-native search engine for observability. An open-source alternative to Datadog, Elasticsearch, Loki, and Tempo.</A>
							<DT><A HREF="https://www.brendangregg.com/Slides/SBSRE_perf_meetup_aug2017.pdf">Netflix Performance Meetup (2017)</A>
							<DT><A HREF="https://github.com/facebook/watchman">facebook/watchman: Watches files and records, or triggers actions, when they change.</A>
							<DT><A HREF="https://www.linkedin.com/feed/update/urn:li:activity:7189636365620240384/">Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services</A>
						</DL><p>
						<DT><H3 FOLDED>Distributed Systems</H3>
						<DL><p>
							<DT><H3 FOLDED>Large-Scale Information Retrieval Systems</H3>
							<DL><p>
								<DT><A HREF="https://videolectures.net/wsdm09_dean_cblirs/">Videolectures</A>
								<DT><A HREF="https://x.com/arvidkahl/status/1803441726767366420">(1) Arvid Kahl en X: "Full-text search on 500GB+ of data is keeping me awake at night. MySQL's full-text index just can't handle this. Often takes minutes. And Meilisearch, as fast as it is, is hard to wrangle to get it to get only precise results. Anyone here experienced with search at this size?" / X</A>
								<DT><A HREF="https://www.postgresql.org/docs/current/textsearch.html">PostgreSQL: Documentation: 16: Chapter¬†12.¬†Full Text Search</A>
								<DT><A HREF="https://hive.apache.org/">Apache Hive</A>
								<DT><A HREF="https://prestodb.io/">Presto: Free, Open-Source SQL Query Engine for any Data</A>
								<DT><A HREF="https://github.com/quickwit-oss/tantivy">quickwit-oss/tantivy: Tantivy is a full-text search engine library inspired by Apache Lucene and written in Rust</A>
							</DL><p>
							<DT><H3 FOLDED>distributed-sys-lectures</H3>
							<DL><p>
								<DT><H3 FOLDED>6.824</H3>
								<DL><p>
									<DT><A HREF="https://pdos.csail.mit.edu/6.824/index.html">6.824 Home Page: Spring 2021</A>
									<DT><A HREF="https://mit-6824-notes.book.triplez.cn/lectures/1-introduction/">Lecture 1: Introduction | MIT 6.824 Notebook</A>
									<DT><A HREF="https://wizardforcel.gitbooks.io/distributed-systems-engineering-lecture-notes/content/l01-intro.html">Introduction ¬∑ Distributed Systems Engineering Lecture notes (MIT 6.824)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=3HunZHHrk1Q">Remote Procedure Call (RPC)</A>
									<DT><A HREF="https://learncs.me/mit/6.824">NOTES</A>
									<DT><H3 FOLDED>source code</H3>
									<DL><p>
										<DT><A HREF="https://github.com/glodknife/MIT-6.824-2016/blob/46100bcf7b276b8824a8aac979e7c4e12b217038/src/kvpaxos/server.go#L58">MIT-6.824-2016/server.go</A>
										<DT><A HREF="https://github.com/glodknife/MIT-6.824-2016/tree/master/src">MIT-6.824-2016/src at master ¬∑ glodknife/MIT-6.824-2016</A>
									</DL><p>
									<DT><H3 FOLDED>labs</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>questions</H3>
									<DL><p>
									</DL><p>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>distributed-sys-key-value store</H3>
							<DL><p>
								<DT><H3 FOLDED>REST</H3>
								<DL><p>
									<DT><A HREF="https://medium.com/extend/what-is-rest-a-simple-explanation-for-beginners-part-1-introduction-b4a072f8740f">What is REST?</A>
									<DT><A HREF="https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm">Representational State Transfer (REST)</A>
									<DT><A HREF="https://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm">Architectural Styles and the Design of Network-based Software Architectures</A>
								</DL><p>
								<DT><H3 FOLDED>Seaweedfs</H3>
								<DL><p>
									<DT><A HREF="https://github.com/chrislusf/seaweedfs">chrislusf/seaweedfs</A>
								</DL><p>
								<DT><H3 FOLDED>Bigtable</H3>
								<DL><p>
									<DT><A HREF="https://cloud.google.com/bigtable">Bigtable |¬† Google Cloud</A>
								</DL><p>
								<DT><H3 FOLDED>python</H3>
								<DL><p>
									<DT><H3 FOLDED>uWSGI</H3>
									<DL><p>
										<DT><A HREF="https://flask.palletsprojects.com/en/1.1.x/deploying/uwsgi/">uWSGI ‚Äî Flask Documentation (1.1.x)</A>
										<DT><A HREF="https://uwsgi-docs.readthedocs.io/en/latest/">uWSGI ‚Äî uWSGI 2.0 documentation</A>
										<DT><A HREF="https://uwsgi-docs.readthedocs.io/en/latest/Download.html">Getting uWSGI ‚Äî uWSGI 2.0 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>Flask</H3>
									<DL><p>
										<DT><A HREF="https://flask.palletsprojects.com/en/1.1.x/">Welcome to Flask ‚Äî Flask Documentation (1.1.x)</A>
									</DL><p>
									<DT><H3 FOLDED>Plyvel</H3>
									<DL><p>
										<DT><A HREF="https://plyvel.readthedocs.io/en/latest/">Plyvel ‚Äî Plyvel 1.3.0 documentation</A>
										<DT><A HREF="https://readthedocs.org/projects/plyvel/downloads/pdf/latest/">plyvel docs</A>
										<DT><A HREF="https://githubmemory.com/repo/wbolster/plyvel/issues/114?page=2">installation failed on mac osx 10.15 - plyvel</A>
										<DT><A HREF="https://github.com/wbolster/plyvel/issues/13">IOError: IO error: lock ¬∑ Issue #13 ¬∑ wbolster/plyvel</A>
									</DL><p>
									<DT><H3 FOLDED>Gunicorn</H3>
									<DL><p>
										<DT><A HREF="https://gunicorn.org/">Gunicorn - Python WSGI HTTP Server for UNIX</A>
									</DL><p>
									<DT><A HREF="https://docs.python.org/3/library/json.html">json ‚Äî JSON encoder and decoder</A>
									<DT><A HREF="https://stackoverflow.com/questions/6269765/what-does-the-b-character-do-in-front-of-a-string-literal">'b' character in front of a string literal? -- Python</A>
									<DT><A HREF="https://ipython.org/install.html">Installing IPython ‚Äî IPython</A>
									<DT><A HREF="https://stackoverflow.com/questions/56658553/module-not-found-error-in-vs-code-despite-the-fact-that-i-installed-it">Module not found error in VS code despite the fact that I installed it - Stack Overflow</A>
									<DT><H3 FOLDED>tempfile</H3>
									<DL><p>
										<DT><A HREF="https://docs.python.org/3/library/tempfile.html">tempfile ‚Äî Generate temporary files and directories</A>
										<DT><A HREF="http://www.chiark.greenend.org.uk/doc/python-pyxattr/html/module.html">Interface to extended filesystem attributes</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="http://localhost:3000/">‚Äélocalhost:3000</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cAFjZ1gXBxc&t=6222s">Geohot : A distributed key value store in under 1000 lines</A>
								<DT><H3 FOLDED>leveldb</H3>
								<DL><p>
									<DT><A HREF="https://github.com/bagonyi/homebrew-formulae">bagonyi/homebrew-formulae -- 1.22 Stable</A>
									<DT><A HREF="https://github.com/google/leveldb/blob/master/doc/index.md#concurrency">google/leveldb</A>
									<DT><A HREF="https://github.com/google/leveldb/blob/master/doc/index.md#concurrency">leveldb/index.md at master ¬∑ google/leveldb</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/LevelDB">LevelDB - Wikipedia</A>
								</DL><p>
								<DT><A HREF="https://httpstatuses.com/">HTTP Status Codes ‚Äî httpstatuses.com</A>
								<DT><A HREF="https://stackoverflow.com/questions/11583562/how-to-kill-a-process-running-on-particular-port-in-linux">lsof - How to kill a process running on particular port in Linux?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=v44bAtgEEUw">Building Redis From Scratch In Golang</A>
							</DL><p>
							<DT><H3 FOLDED>distributed-sys-queue-systems</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=yKPVZgA6Oe0">What makes Kafka special? | System Design - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>distributed-sys-actor-model</H3>
							<DL><p>
								<DT><A HREF="http://dist-prog-book.com/chapter/3/message-passing.html#akka">Message Passing and the Actor Model</A>
							</DL><p>
							<DT><H3 FOLDED>blockchain</H3>
							<DL><p>
								<DT><A HREF="https://bitcoin.org/bitcoin.pdf">Bitcoin whitepaper</A>
							</DL><p>
							<DT><H3 FOLDED>databases</H3>
							<DL><p>
								<DT><H3 FOLDED>db-relational</H3>
								<DL><p>
									<DT><H3 FOLDED>postgreSQL</H3>
									<DL><p>
										<DT><A HREF="https://stackoverflow.com/questions/17633422/psql-fatal-database-user-does-not-exist">psql: FATAL: database "&lt;user&gt;" does not exist</A>
										<DT><A HREF="https://www.postgresql.org/docs/9.2/app-psql.html">PostgreSQL: Documentation: 9.2: psql</A>
										<DT><A HREF="https://alvinalexander.com/blog/post/postgresql/log-in-postgresql-database/">How to log into a Postgresql database</A>
										<DT><A HREF="https://stackoverflow.com/questions/26040493/how-to-show-data-in-a-table-by-using-psql-command-line-interface">How to show data in a table by using psql</A>
										<DT><A HREF="https://stackoverflow.com/questions/48180282/how-to-populate-a-heroku-postgresql-database-with-a-sql-file">How to populate a heroku postgresql database with a sql file</A>
										<DT><A HREF="https://stackoverflow.com/questions/21307786/load-sql-file-data-in-to-single-table-in-postgres">Load sql file data in to single table in postgres</A>
										<DT><A HREF="https://dba.stackexchange.com/questions/46125/why-does-postgres-generate-an-already-used-pk-value">already used PK value?</A>
										<DT><A HREF="https://stackoverflow.com/questions/64354458/named-parameter-not-bound-date-format-native-query-in-spring-boot">Named parameter not bound :</A>
										<DT><A HREF="https://devcenter.heroku.com/articles/heroku-cli-commands">Heroku CLI Commands | Heroku Dev Center</A>
										<DT><A HREF="https://www.postgresql.org/docs/9.5/sql-insert.html">PostgreSQL: Documentation: 9.5: INSERT</A>
										<DT><A HREF="https://ketansingh.me/posts/how-postgres-stores-rows/">How Postgres Stores Rows</A>
									</DL><p>
									<DT><A HREF="https://www.w3schools.com/sql/sql_groupby.asp">w3schools sql clauses</A>
									<DT><A HREF="https://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Querying/JPQL">JPA</A>
									<DT><A HREF="https://en.wikibooks.org/wiki/Structured_Query_Language/Create_Table">Structured Query Language/Create Table</A>
									<DT><A HREF="https://www.w3schools.com/sql/">SQL Tutorial</A>
									<DT><A HREF="https://www.pearson.com/us/higher-education/program/Garcia-Molina-Database-Systems-The-Complete-Book-2nd-Edition/PGM2429.html">Database Systems: The Complete Book, 2nd Edition | Pearson</A>
								</DL><p>
								<DT><A HREF="https://dbdb.io/db/leveldb">Database of Databases - LevelDB</A>
								<DT><A HREF="https://web.stanford.edu/class/cs245/">CS 245: Principles of Data-Intensive Systems (Winter 2021)</A>
								<DT><A HREF="https://towardsdatascience.com/10-quick-sql-tips-after-writing-daily-in-sql-for-3-years-37bdba0637d0">10 Quick SQL Tips After Writing Daily in SQL for 3 Years | Mar, 2022</A>
								<DT><A HREF="https://engineering.fb.com/2022/04/26/developer-tools/sql-notebooks/">SQL Notebooks: Combining the power of Jupyter and SQL editors for data analytics - Engineering at Meta</A>
								<DT><A HREF="http://airbnb.io/airpal/">A web-based query execution tool built on top of Facebook's PrestoDB.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-1VGwmFKKf8">The Billion Rows Challenge in Rust - an intro to Rust for data engineering - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=zSn8il5Mo5s">The Rise of Oracle, SQL and the Relational Database - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>canary vs shadow traffic</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/shorts/ckYBLT1VG-M">Shadow Trafficking Explained - YouTube</A>
							</DL><p>
							<DT><A HREF="https://jsonformatter.curiousconcept.com/#">JSON Formatter &amp; Validator</A>
							<DT><A HREF="https://www.youtube.com/watch?v=VWrpnT8rwVY">"Functional distributed systems beyond request/response" by Melinda Lu - YouTube</A>
							<DT><A HREF="https://canvas.mit.edu/courses/11164">6.829 Computer Networks</A>
							<DT><A HREF="https://raft.github.io/">Raft Consensus Algorithm</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/%CE%A0-calculus">œÄ-calculus - Wikipedia</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Petri_net">Petri net - Wikipedia</A>
							<DT><A HREF="http://dist-prog-book.com/chapter/3/message-passing.html#akka">Message Passing and the Actor Model</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Vch4BWUVzMM">SimTigerBeetle (Director's Cut!): Distributed systems failure simulator</A>
							<DT><A HREF="https://www.youtube.com/watch?v=DfLKd3WlTuw">Programming Distributed Systems - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>web</H3>
						<DL><p>
							<DT><H3 FOLDED>web scrapping</H3>
							<DL><p>
								<DT><A HREF="https://www.selenium.dev/documentation/webdriver/locating_elements/">Locating elements | Selenium</A>
								<DT><A HREF="https://stackoverflow.com/questions/61308799/unable-to-locate-elements-in-selenium-python">Unable to locate elements in Selenium (Python)</A>
								<DT><A HREF="https://www.guru99.com/accessing-forms-in-webdriver.html#2">Selenium Form WebElement: TextBox, Button, sendkeys(), click()</A>
								<DT><A HREF="https://www.selenium.dev/documentation/webdriver/keyboard/">Keyboard | Selenium</A>
								<DT><A HREF="https://www.w3.org/TR/webdriver/#keyboard-actions">WebDriver</A>
								<DT><A HREF="https://stackoverflow.com/questions/2632677/python-integer-incrementing-with">i++</A>
								<DT><A HREF="https://towardsdatascience.com/web-automation-nightmares-6-tricks-to-overcome-them-4241089953e3">Web Automation Nightmares: 6 Tricks to Overcome Them | by Aw Khai Sheng | Towards Data Science</A>
								<DT><A HREF="https://stackoverflow.com/questions/33225947/can-a-website-detect-when-you-are-using-selenium-with-chromedriver/33403473#33403473">selenium fingerprint</A>
							</DL><p>
							<DT><H3 FOLDED>architecture</H3>
							<DL><p>
								<DT><A HREF="https://orkhanscience.medium.com/software-architecture-patterns-5-mins-read-e9e3c8eb47d2">Software Architecture Patterns: 4 minute read</A>
								<DT><A HREF="https://www.infoq.com/">InfoQ: Software Development News, Trends &amp; Best Practices</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Pattern-Oriented_Software_Architecture">Pattern-Oriented Software Architecture - Wikipedia</A>
								<DT><A HREF="https://www.amazon.com/Pattern-Oriented-Software-Architecture-System-Patterns/dp/0471958697">Pattern-Oriented Software Architecture Volume 1: A System of Patterns: Buschmann, Frank, Meunier, Regine, Rohnert, Hans, Sommerlad, Peter, Stal, Michael, Michael Stal</A>
								<DT><A HREF="https://www.amazon.com/Principles-Computer-System-Design-Introduction/dp/0123749573">Principles of Computer System Design: An Introduction: Saltzer, Jerome, Kaashoek, M. Frans: 9780123749574: Amazon.com: Books</A>
								<DT><A HREF="https://medium.com/@olgamitroshyna/software-architecture-i-wish-i-had-known-about-this-earlier-4df43eae57db">Software Architecture: I wish I had known about this earlier...</A>
							</DL><p>
							<DT><H3 FOLDED>dynamic-web-page</H3>
							<DL><p>
								<DT><H3 FOLDED>JavaScript</H3>
								<DL><p>
									<DT><A HREF="https://www.awwwards.com/websites/">Website Design Inspiration</A>
									<DT><A HREF="https://github.com/nhn/tui.image-editor#powerful-filter-function">Full-featured photo image editor using canvas</A>
									<DT><A HREF="http://vanilla-js.com/">Vanilla JS</A>
									<DT><A HREF="https://create.editorx.com/html/editor/web/renderer/new?metaSiteId=0c39fce3-65b4-4514-a1f0-81618914e347&siteId=2dd00095-350d-41e3-8997-5af977b6a6b2">Editor X</A>
									<DT><A HREF="https://getbootstrap.com/">Bootstrap</A>
									<DT><A HREF="https://css-tricks.com/seamless-responsive-photo-grid/">https://css-tricks.com/seamless-responsive-photo-grid/</A>
									<DT><A HREF="https://codepen.io/ibrahima92/full/zYYqqbZ">Image gallery</A>
									<DT><A HREF="https://codepen.io/marcobiedermann/full/vYYyVzK">CSS Grid Gallery</A>
									<DT><A HREF="https://www.freecodecamp.org/news/how-to-deploy-a-static-website-for-free-in-only-3-minutes-with-google-drive/">How to deploy a static website</A>
									<DT><A HREF="https://codepen.io/topics/gsap">CodePen Topics</A>
								</DL><p>
								<DT><H3 FOLDED>Maven</H3>
								<DL><p>
									<DT><A HREF="https://maven.apache.org/run.html">Maven ‚Äì Running Apache Maven</A>
									<DT><A HREF="https://maven.apache.org/pom.html#Dependency_Management">POM Reference</A>
									<DT><A HREF="http://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html">Guide to installing 3rd party JARs</A>
									<DT><A HREF="https://www.pegaxchange.com/2017/10/19/setup-maven-java-project-macos/">Setting up Maven on Mac OS and Creating Java Project</A>
								</DL><p>
								<DT><H3 FOLDED>Spring</H3>
								<DL><p>
									<DT><H3 FOLDED>jsp</H3>
									<DL><p>
										<DT><A HREF="https://stackoverflow.com/questions/2148658/iterate-over-elements-of-list-and-map-using-jstl-cforeach-tag">Iterate over elements of List and Map</A>
										<DT><A HREF="https://docs.oracle.com/cd/E17802_01/products/products/jsp/jstl/1.1/docs/tlddocs/index.html">All Tags / Functions</A>
										<DT><A HREF="http://www.w3big.com/jsp/jstl-core-foreach-tag.html">C: forEach</A>
									</DL><p>
									<DT><A HREF="https://start.spring.io/">Spring Initializr</A>
									<DT><A HREF="https://docs.spring.io/spring-boot/docs/current/maven-plugin/reference/html/#repackage">Spring Boot Maven Plugin Documentation</A>
									<DT><A HREF="https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#boot-features-spring-mvc-template-engines">Spring Boot Reference Documentation</A>
									<DT><A HREF="https://spring.io/guides/gs/spring-boot/">Building an Application with Spring Boot</A>
									<DT><A HREF="https://spring.io/guides/gs/serving-web-content/">Serving Web Content with Spring MVC</A>
									<DT><A HREF="https://stackoverflow.com/questions/36697663/circular-view-path-error-spring-boot/41918545">Circular View path error Spring boot</A>
									<DT><A HREF="https://spring.io/guides/tutorials/rest/">Building REST services with Spring</A>
									<DT><A HREF="https://docs.spring.io/spring-data/jpa/docs/current/reference/html/#repositories">Spring Data JPA - Reference Documentation</A>
									<DT><A HREF="https://spring.io/projects/spring-hateoas">Spring HATEOAS</A>
									<DT><A HREF="http://keenformatics.blogspot.com/2013/08/how-to-solve-json-infinite-recursion.html">How To Solve JSON infinite recursion Stackoverflow</A>
									<DT><A HREF="https://docs.spring.io/spring-boot/docs/2.1.5.RELEASE/reference/html/boot-features-testing.html">46.¬†Testing</A>
									<DT><A HREF="https://www.javadoc.io/doc/org.mockito/mockito-core/2.23.4/org/mockito/Mockito.html">Mockito (Mockito 2.23.4 API)</A>
									<DT><A HREF="https://www.callicoder.com/hibernate-spring-boot-jpa-one-to-many-mapping-example/">JPA / Hibernate One to Many Mapping Example</A>
									<DT><A HREF="https://openclassrooms.com/en/courses/5684146-create-web-applications-efficiently-with-the-spring-boot-mvc-framework/6157116-make-services-unit-testable-using-dependency-injection">services unit testable using dependency injection</A>
									<DT><A HREF="https://www.baeldung.com/get-user-in-spring-security">Retrieve User Information in Spring Security</A>
									<DT><A HREF="https://petstore.swagger.io/#/">Swagger UI</A>
									<DT><A HREF="https://octoperf.com/blog/2018/03/08/securing-rest-api-spring-security/#testing-the-application-1">Securing a Rest API with Spring Security</A>
									<DT><A HREF="https://docs.spring.io/spring-hateoas/docs/current/api/org/springframework/hateoas/EntityModel.html">EntityModel (Spring HATEOAS 1.3.0 API)</A>
								</DL><p>
								<DT><A HREF="https://tools.ietf.org/html/rfc8288#section-1">RFC 8288 - Web Linking</A>
								<DT><A HREF="https://tools.ietf.org/html/rfc3986">RFC 3986 - Uniform Resource Identifier (URI)</A>
								<DT><A HREF="https://tools.ietf.org/html/rfc7807">RFC 7807 - Problem Details for HTTP APIs</A>
								<DT><A HREF="http://stateless.co/hal_specification.html">The Hypertext Application Language</A>
								<DT><A HREF="https://fetch.spec.whatwg.org/#origin-header">Fetch Standard</A>
								<DT><A HREF="https://www.objectdb.com/java/jpa/query/jpql/expression">JPA Query Language (JPQL / Criteria) Expression Syntax</A>
								<DT><A HREF="https://stackoverflow.com/questions/6842289/jpa-multiple-select-query">JPA multiple select query</A>
								<DT><A HREF="https://en.wikibooks.org/wiki/Java_Persistence/Querying#Query_Results">Java Persistence/Querying</A>
							</DL><p>
							<DT><H3 FOLDED>static-web-page</H3>
							<DL><p>
								<DT><A HREF="https://pages.github.com/">GitHub Pages | Websites for you and your projects, hosted directly from your GitHub repository. Just edit, push, and your changes are live.</A>
							</DL><p>
							<DT><A HREF="https://cloud.google.com/blog/products/application-development/rest-vs-rpc-what-problems-are-you-trying-to-solve-with-your-apis">REST vs. RPC: what problems are you trying to solve with your APIs? | Google Cloud Blog</A>
							<DT><A HREF="https://twitter.com/bibryam/status/1531316906581495811">2021 State of the API design - a visual report</A>
						</DL><p>
						<DT><H3 FOLDED>High Performance Computing</H3>
						<DL><p>
							<DT><H3 FOLDED>hpc-simulation</H3>
							<DL><p>
								<DT><A HREF="https://www.gameenginebook.com/">Game Engine Architecture</A>
							</DL><p>
							<DT><H3 FOLDED>hpc-observability</H3>
							<DL><p>
								<DT><A HREF="https://www.smartmontools.org/">smartmontools</A>
								<DT><A HREF="https://twitter.com/marcan42/status/1361151198921826308?lang=en">M1 SSD degradation</A>
								<DT><A HREF="https://serverfault.com/questions/480726/how-to-measure-total-writes-performed-to-ssd-in-linux">ubuntu - How to measure total writes performed to SSD in Linux? - Server Fault</A>
							</DL><p>
							<DT><A HREF="https://web.stanford.edu/class/cs245/">CS 245: Principles of Data-Intensive Systems (Winter 2021)</A>
							<DT><A HREF="https://biojulia.net/post/hardware/">What scientists must know about hardware to write fast code</A>
							<DT><A HREF="https://unias.github.io/docklet/book/en/notebook/gallery.html#linguistics-and-text-mining">Scientific Computation ¬∑ GitBook</A>
							<DT><A HREF="https://willcrichton.net/notes/k-corrset/">Analyzing Data 180,000x Faster with Rust</A>
						</DL><p>
						<DT><H3 FOLDED>ml-sys-storage</H3>
						<DL><p>
							<DT><A HREF="https://github.com/louwrentius/fio-plot">louwrentius/fio-plot: Create charts from FIO storage benchmark tool output</A>
							<DT><A HREF="https://github.com/datenlord/datenlord?tab=readme-ov-file">datenlord/datenlord: DatenLord, Computing Defined Storage, an application-orientated, cloud-native distributed storage system</A>
							<DT><A HREF="https://github.com/axboe/fio">axboe/fio: Flexible I/O Tester</A>
							<DT><A HREF="https://github.com/louwrentius/showtools">louwrentius/showtools: Shows detailed disk or network device information</A>
							<DT><A HREF="https://github.com/datenlord/s3-server">datenlord/s3-server: Generic S3 server implementation</A>
						</DL><p>
						<DT><H3 FOLDED>Cloud</H3>
						<DL><p>
							<DT><H3 FOLDED>GenAI Infrastructure</H3>
							<DL><p>
								<DT><H3 FOLDED>Open Compute Project (OCP)</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=2l6gI-ksdKs">What's inside a Facebook Datacenter Open Compute Rack? - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=PrOEznqzq40">Intro to the Open Compute Project OCP - YouTube</A>
									<DT><A HREF="https://www.cio.com/article/250874/how-and-why-facebook-excels-at-data-center-efficiency.html#:~:text=Facebook's%20data%20center%20design%2C%20which,in%20hours%20rather%20than%20months.">How (and Why) Facebook Excels at Data Center Efficiency</A>
									<DT><A HREF="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/">OCP Summit 2022: Open hardware for AI infrastructure</A>
									<DT><A HREF="https://www.opencompute.org/projects/cooling-environments">Cooling Environments ¬ª Open Compute Project</A>
									<DT><A HREF="https://www.youtube.com/watch?v=2udD56VHEK0">Facebook's Open Networking Hardware - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>GPU Platform</H3>
								<DL><p>
									<DT><A HREF="https://cos.googlesource.com/cos/tools">cos/tools - Git at Google</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/cos-gpu-installer">GoogleCloudPlatform/cos-gpu-installer: Scripts to build and use a container to install GPU drivers on Container-Optimized OS images</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/hpc-toolkit">GoogleCloudPlatform/hpc-toolkit: Cloud HPC Toolkit is an open-source software offered by Google Cloud which makes it easy for customers to deploy HPC environments on Google Cloud.</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/slurm-gcp#hybrid">GoogleCloudPlatform/slurm-gcp</A>
									<DT><A HREF="https://docs.coreweave.com/coreweave-machine-learning-and-ai/get-started-with-ml-and-ai#sunk-slurm-on-kubernetes">Sunk: SLURM on Kubernetes (CoreWeave)</A>
									<DT><A HREF="https://github.com/coreweave/kubernetes-cloud">coreweave/kubernetes-cloud: Getting Started with the CoreWeave Kubernetes GPU Cloud</A>
									<DT><A HREF="https://googlecloudplatform.github.io/magic-modules/">Overview | Magic Modules</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/ml-testing-accelerators/tree/52b290a149b760b270085d4f8191188f986047ed">GoogleCloudPlatform/ml-testing-accelerators</A>
								</DL><p>
								<DT><A HREF="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">Building Meta‚Äôs GenAI Infrastructure - Engineering at Meta</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ptGDaGUXInw">Mark Russinovich | Generative AI in the Cloud: Inside Microsoft AI Innovation - YouTube</A>
								<DT><A HREF="https://arxiv.org/pdf/2311.18677.pdf">Splitwise: Efficient Generative LLM Inference Using Phase Splitting (MS)</A>
								<DT><A HREF="https://github.com/S-LoRA/S-LoRA">S-LoRA/S-LoRA: S-LoRA: Serving Thousands of Concurrent LoRA Adapters</A>
								<DT><A HREF="https://github.com/predibase/lorax">predibase/lorax: Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs</A>
								<DT><A HREF="https://github.com/sabetAI/BLoRA">sabetAI/BLoRA: batched loras</A>
								<DT><A HREF="https://arxiv.org/abs/2305.07759">[2305.07759] TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</A>
								<DT><A HREF="https://arxiv.org/abs/2306.11644">[2306.11644] Textbooks Are All You Need</A>
								<DT><A HREF="https://arxiv.org/pdf/2310.02238.pdf">Who‚Äôs Harry Potter? Approximate Unlearning in LLMs</A>
								<DT><A HREF="https://news.microsoft.com/source/features/innovation/azure-quantum-majorana-topological-qubit/">In a historic milestone, Azure Quantum demonstrates formerly elusive physics needed to build scalable topological qubits - Source</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Rk3nTUfRZmo&t=330s">What runs ChatGPT? Inside Microsoft's AI supercomputer</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ptGDaGUXInw">Mark Russinovich | Generative AI in the Cloud: Inside Microsoft AI Innovation</A>
								<DT><A HREF="https://news.microsoft.com/source/features/innovation/azure-quantum-majorana-topological-qubit/">elusive physics needed to build scalable topological qubits</A>
							</DL><p>
							<DT><H3 FOLDED>cloud-system</H3>
							<DL><p>
								<DT><A HREF="https://github.com/google/paxml">google/paxml: Pax is a Jax-based machine learning framework for training large scale models. Pax allows for advanced and fully configurable experimentation and parallelization, and has demonstrated industry leading model flop utilization rates.</A>
								<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/pax">JAX-Toolbox/rosetta/rosetta/projects/pax at main ¬∑ NVIDIA/JAX-Toolbox</A>
								<DT><A HREF="https://nats.io/">NATS.io ‚Äì Cloud Native, Open Source, High-performance Messaging</A>
								<DT><A HREF="https://websites.umich.edu/~amberljc/file/LLM-Systems-Basics.pdf">LLM-Systems-Basics (Jiachen Liu)</A>
							</DL><p>
							<DT><H3 FOLDED>Kubernetes</H3>
							<DL><p>
								<DT><H3 FOLDED>GKE</H3>
								<DL><p>
									<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/whats-new-with-gke-at-google-cloud-next">What‚Äôs new with GKE at Google Cloud Next</A>
								</DL><p>
								<DT><H3 FOLDED>Dynamic Resource Allocation</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna">Dynamic Resource Allocation for GPUs in Kubernetes - Google Docs</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-local</H3>
								<DL><p>
									<DT><A HREF="https://kind.sigs.k8s.io/">kind: local run</A>
								</DL><p>
								<DT><H3 FOLDED>kserve</H3>
								<DL><p>
									<DT><A HREF="https://github.com/kserve/modelmesh-serving">kserve/modelmesh-serving: Controller for ModelMesh</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-basics</H3>
								<DL><p>
									<DT><A HREF="https://hamel.dev/notes/k8s/02-Basics.html">k8s</A>
									<DT><A HREF="https://www.youtube.com/watch?v=90kZRyPcRZw">Kubernetes Deconstructed: Understanding Kubernetes by Breaking It</A>
								</DL><p>
								<DT><H3 FOLDED>Device sharig strategies</H3>
								<DL><p>
									<DT><H3 FOLDED>Time-Slicing</H3>
									<DL><p>
										<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html">Time-Slicing GPUs in Kubernetes ‚Äî NVIDIA GPU Operator 23.9.2 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>NVIDIA Multi-Instance GPU</H3>
									<DL><p>
										<DT><A HREF="https://www.nvidia.com/en-us/technologies/multi-instance-gpu/">Multi-Instance GPU (MIG) | NVIDIA</A>
									</DL><p>
									<DT><H3 FOLDED>CUDA Multi-Process Service (MPS)</H3>
									<DL><p>
										<DT><A HREF="https://docs.nvidia.com/deploy/mps/index.html">Multi-Process Service :: GPU Deployment and Management Documentation</A>
										<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2021/06/MPS_ORNL_20210817.pdf">Introduction (slides)</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=Q2GuTUO170w">Sharing Is Caring: GPU Sharing and CDI in Device Plugins - Evan Lezar, NVIDIA &amp; David Porter, Google - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=jbpIFCkEEng">Mastering GPU Management in Kubernetes Using the Operator Pattern (2024)</A>
								</DL><p>
								<DT><A HREF="https://cloud.google.com/anthos-config-management/docs/concepts/kustomize">Configure Kubernetes with Kustomize ¬†|¬† Anthos Config Management ¬†|¬† Google Cloud</A>
								<DT><A HREF="https://www.youtube.com/watch?v=06bKlSmVwIg">CNCF Live Webinar: Overcoming the GPU shortage with virtual Kubelets &amp; distributed cloud - YouTube</A>
								<DT><A HREF="https://virtual-kubelet.io/">Virtual Kubelet | Home</A>
								<DT><A HREF="https://github.com/virtual-kubelet/virtual-kubelet">virtual-kubelet/virtual-kubelet: Virtual Kubelet is an open source Kubernetes kubelet implementation.</A>
								<DT><A HREF="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet | Kubernetes</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cwiAW5TZsfo">On-Demand Systems and Scaled Training Using the JobSet API - Abdullah Gharaibeh &amp; Vanessa Sochat - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Q2GuTUO170w">Sharing Is Caring: GPU Sharing and CDI in Device Plugins - Evan Lezar, NVIDIA &amp; David Porter, Google - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=jbpIFCkEEng">Mastering GPU Management in Kubernetes Using the Operator Pattern (2024)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cwiAW5TZsfo">On-Demand Systems and Scaled Training Using the JobSet API</A>
							</DL><p>
							<DT><H3 FOLDED>job scheduler</H3>
							<DL><p>
								<DT><H3 FOLDED>SLURM</H3>
								<DL><p>
									<DT><A HREF="https://slurm.schedmd.com/">slurm.schedmd.com</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/slurm-gcp">GoogleCloudPlatform/slurm-gcp</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/hpc-toolkit">GoogleCloudPlatform/hpc-toolkit: Cloud HPC Toolkit is an open-source software offered by Google Cloud which makes it easy for customers to deploy HPC environments on Google Cloud.</A>
									<DT><A HREF="https://github.com/facebookincubator/submitit">facebookincubator/submitit: Python 3.8+ toolbox for submitting jobs to Slurm</A>
								</DL><p>
								<DT><H3 FOLDED>job-manager-kubernetes</H3>
								<DL><p>
									<DT><H3 FOLDED>GKE</H3>
									<DL><p>
										<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/whats-new-with-gke-at-google-cloud-next">What‚Äôs new with GKE at Google Cloud Next</A>
									</DL><p>
									<DT><H3 FOLDED>Dynamic Resource Allocation</H3>
									<DL><p>
										<DT><A HREF="https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna">Dynamic Resource Allocation for GPUs in Kubernetes - Google Docs</A>
									</DL><p>
									<DT><H3 FOLDED>local</H3>
									<DL><p>
										<DT><A HREF="https://kind.sigs.k8s.io/">kind: local run</A>
									</DL><p>
									<DT><H3 FOLDED>kserve</H3>
									<DL><p>
										<DT><A HREF="https://github.com/kserve/modelmesh-serving">kserve/modelmesh-serving: Controller for ModelMesh</A>
									</DL><p>
									<DT><H3 FOLDED>basics</H3>
									<DL><p>
										<DT><A HREF="https://hamel.dev/notes/k8s/02-Basics.html">k8s</A>
										<DT><A HREF="https://www.youtube.com/watch?v=90kZRyPcRZw">Kubernetes Deconstructed: Understanding Kubernetes by Breaking It Down - Carson Anderson, DOMO - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>utils</H3>
									<DL><p>
										<DT><A HREF="https://github.com/alibaba/kt-connect">alibaba/kt-connect: A toolkit for Integrating with your kubernetes dev environment more efficiently</A>
										<DT><A HREF="https://github.com/alibaba/kubeskoop">alibaba/kubeskoop</A>
									</DL><p>
									<DT><A HREF="https://cloud.google.com/anthos-config-management/docs/concepts/kustomize">Configure Kubernetes with Kustomize ¬†|¬† Anthos Config Management ¬†|¬† Google Cloud</A>
									<DT><A HREF="https://www.youtube.com/watch?v=06bKlSmVwIg">CNCF Live Webinar: Overcoming the GPU shortage with virtual Kubelets &amp; distributed cloud - YouTube</A>
									<DT><A HREF="https://virtual-kubelet.io/">Virtual Kubelet | Home</A>
									<DT><A HREF="https://github.com/virtual-kubelet/virtual-kubelet">virtual-kubelet/virtual-kubelet: Virtual Kubelet is an open source Kubernetes kubelet implementation.</A>
									<DT><A HREF="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet | Kubernetes</A>
									<DT><A HREF="https://www.youtube.com/watch?v=cwiAW5TZsfo">On-Demand Systems and Scaled Training Using the JobSet API - Abdullah Gharaibeh &amp; Vanessa Sochat - YouTube</A>
									<DT><A HREF="https://github.com/kubernetes/kubernetes/issues/95492">Kubernetes won't run 50,000 Jobs ¬∑ Issue #95492 ¬∑ kubernetes/kubernetes</A>
									<DT><A HREF="https://www.cncf.io/blog/2020/08/10/why-the-kubernetes-scheduler-is-not-enough-for-your-ai-workloads/">Scheduler (Level 2)</A>
									<DT><A HREF="https://www.run.ai/">Run:ai - AI Optimization and Orchestration</A>
									<DT><A HREF="https://www.cncf.io/wp-content/uploads/2020/10/Kube-two-level-RM.pdf">Kubernetes native two-level resource managment for AI workloads</A>
									<DT><A HREF="https://github.com/project-codeflare/multi-cluster-app-dispatcher">project-codeflare/multi-cluster-app-dispatcher: Holistic job manager on Kubernetes</A>
									<DT><A HREF="https://github.com/bentoml/Yatai">bentoml/Yatai: Model Deployment at Scale on Kubernetes ü¶ÑÔ∏è</A>
									<DT><A HREF="https://docs.aws.amazon.com/parallelcluster/latest/ug/build-image.html">buildImage - AWS ParallelCluster</A>
									<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/high-performance-aiml-storage-through-local-ssd-support-on-gke">High performance AI/ML storage through Local SSD support on GKE | Google Cloud Blog</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/core/exp_manager.html">Experiment Manager ‚Äî NVIDIA NeMo</A>
								<DT><A HREF="https://github.com/google-deepmind/xmanager">google-deepmind/xmanager: A platform for managing machine learning experiments</A>
								<DT><A HREF="https://storage.googleapis.com/gresearch/xmanager/deepmind_xmanager_slides.pdf">XManager (Slides)</A>
								<DT><A HREF="https://github.com/google/xpk">google/xpk: xpk (Accelerated Processing Kit) is a software tool to help Cloud developers to orchestrate training jobs on accelerators such as TPUs and GPUs on GKE.</A>
								<DT><A HREF="https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/gke-a100-jax/train.py">ai-on-gke/gke-a100-jax/train.py at main ¬∑ GoogleCloudPlatform/ai-on-gke</A>
							</DL><p>
							<DT><H3 FOLDED>Infrasctructure as Code</H3>
							<DL><p>
								<DT><H3 FOLDED>Ansible</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=2c5b8Olk6Fk">Ansible and Automation before Mexico Vacation - Skills to get engineering roles. - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>Terraform</H3>
								<DL><p>
									<DT><A HREF="https://googlecloudplatform.github.io/magic-modules/">Magic Modules: Extending Terraform</A>
								</DL><p>
								<DT><H3 FOLDED>Pulumi</H3>
								<DL><p>
									<DT><A HREF="https://www.pulumi.com/solutions/ai/">AI Infrastructure at Any Scale | Pulumi</A>
								</DL><p>
								<DT><H3 FOLDED>xAI</H3>
								<DL><p>
									<DT><A HREF="https://x.ai/career/">Join xAI</A>
									<DT><A HREF="https://boards.greenhouse.io/xai/jobs/4099452007">Job Application for Infrastructure Engineer at xAI</A>
									<DT><A HREF="https://boards.greenhouse.io/xai/jobs/4099456007">Job Application for AI Engineer &amp; Researcher at xAI</A>
									<DT><A HREF="https://www.pulumi.com/solutions/ai/">AI Infrastructure at Any Scale | Pulumi</A>
									<DT><A HREF="https://github.com/pulumi/pulumi">pulumi/pulumi: Pulumi - Infrastructure as Code in any programming language. Build infrastructure intuitively on any cloud using familiar languages üöÄ</A>
								</DL><p>
								<DT><A HREF="https://github.com/kykosic/remote">kykosic/remote: Simple CLI tool for managing remote development instances</A>
								<DT><A HREF="https://github.com/pulumi/pulumi">pulumi/pulumi: Pulumi - Infrastructure as Code in any programming language. Build infrastructure intuitively on any cloud using familiar languages üöÄ</A>
								<DT><A HREF="https://github.com/matklad/xshell">matklad/xshell: Making Rust a Better Bash</A>
							</DL><p>
							<DT><H3 FOLDED>ml-sys-cloud-benchmarking</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/ArtificialAnlys">(1) Artificial Analysis (@ArtificialAnlys) / X</A>
								<DT><A HREF="https://twitter.com/lqiao/status/1767406833692737971">Fireworks beats Groq (Lin Qiao)</A>
								<DT><A HREF="https://www.linkedin.com/feed/update/urn:li:activity:7189636365620240384/">Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services</A>
								<DT><A HREF="https://llm-qoe.github.io/">Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services ¬∑ Andes Blog</A>
								<DT><A HREF="https://artificialanalysis.ai/models/deepseek-v2">DeepSeek-V2 - Quality, Performance &amp; Price Analysis | Artificial Analysis</A>
							</DL><p>
							<DT><H3 FOLDED>ml-sys-cloud-providers</H3>
							<DL><p>
								<DT><H3 FOLDED>Google Cloud Platform</H3>
								<DL><p>
									<DT><H3 FOLDED>Google Collab</H3>
									<DL><p>
										<DT><H3 FOLDED>third-party apps integration</H3>
										<DL><p>
											<DT><H3 FOLDED>Github</H3>
											<DL><p>
												<DT><A HREF="https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d">How to use Google Colab with GitHub via Google Drive - medium</A>
												<DT><A HREF="https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228">Google Drive + Google Colab + GitHub; Don‚Äôt Just Read, Do It!</A>
												<DT><A HREF="https://towardsdatascience.com/colaboratory-drive-github-the-workflow-made-simpler-bde89fba8a39">Colaboratory + Drive + Github</A>
												<DT><A HREF="https://medium.com/@ashwindesilva/how-to-use-google-colaboratory-to-clone-a-github-repository-e07cf8d3d22b">git clone</A>
											</DL><p>
											<DT><H3 FOLDED>Google Drive</H3>
											<DL><p>
												<DT><A HREF="https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d">How to use Google Colab with GitHub via Google Drive - medium</A>
												<DT><A HREF="https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228">Google Drive + Google Colab + GitHub; Don‚Äôt Just Read, Do It!</A>
												<DT><A HREF="https://towardsdatascience.com/colaboratory-drive-github-the-workflow-made-simpler-bde89fba8a39">Colaboratory + Drive + Github</A>
											</DL><p>
											<DT><A HREF="https://medium.com/@ashwindesilva/how-to-use-google-colaboratory-to-clone-a-github-repository-e07cf8d3d22b">Basic interoperability usage: Google Drive-Github</A>
										</DL><p>
										<DT><H3 FOLDED>remote rendering</H3>
										<DL><p>
											<DT><A HREF="https://davidrpugh.github.io/stochastic-expatriate-descent/openai/binder/google-colab/2020/04/16/remote-rendering-gym-envs.html">Rendering OpenAI Gym Envs on Binder and Google Colab</A>
											<DT><A HREF="https://stackoverflow.com/questions/53472940/nameerror-name-base-is-not-defined-openai-gym">Cannot render display - NameError: name 'base' is not defined OpenAI Gym - Stack Overflow</A>
											<DT><A HREF="https://colab.research.google.com/github/davidrpugh/stochastic-expatriate-descent/blob/2020-04-16-remote-rendering-gym-envs/_notebooks/2020-04-16-remote-rendering-gym-envs.ipynb#scrollTo=iS3_S__W01cs">remote-rendering-gym-envs.ipynb - Colaboratory</A>
										</DL><p>
										<DT><A HREF="https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403">memory mapped file formats like HDF5 (aka H5) or LMDB</A>
									</DL><p>
									<DT><A HREF="https://cloud.google.com/products/calculator/#id=4b1ece6b-db16-4e2b-bdf2-0d7370d75fb7">Google Cloud Pricing Calculator</A>
									<DT><A HREF="https://cloud.google.com/compute/gpus-pricing#tpus">GPU pricing ¬†|¬† Compute Engine: Virtual Machines (VMs) ¬†|¬† Google Cloud</A>
									<DT><A HREF="https://cloud.google.com/compute/docs/gpus">GPU platforms ¬†|¬† Compute Engine Documentation ¬†|¬† Google Cloud</A>
								</DL><p>
								<DT><H3 FOLDED>CoreWeave</H3>
								<DL><p>
									<DT><A HREF="https://docs.coreweave.com/">CoreWeave Cloud - CoreWeave</A>
									<DT><A HREF="https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer">https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer</A>
								</DL><p>
								<DT><H3 FOLDED>AWS DJL</H3>
								<DL><p>
									<DT><A HREF="https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-fastertransformer.html">Large model inference with FasterTransformer and DJL Serving - Amazon SageMaker</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://twitter.com/TigerBeetleDB/status/1729054660223553933">(1) TigerBeetle en X: "How do you catch up (and overtake) 30 years of test time? - We've ramped to 100 CPU cores. - 100 simultaneous TigerBeetle simulations, 24x7. - Each simulation (in 3.3 seconds) tests 39 minutes on avg. of real world runtime. 10 cores = 20 years 100 cores = 200 years (every day) https://t.co/mDDCBE74nY" / X</A>
							<DT><A HREF="https://www.tritondatacenter.com/">Take Control of Your Cloud Data | Triton DataCenter</A>
							<DT><A HREF="https://github.com/GoogleCloudPlatform/slurm-gcp">GoogleCloudPlatform/slurm-gcp</A>
							<DT><A HREF="https://github.com/quickwit-oss/quickwit">quickwit-oss/quickwit: Cloud-native search engine for observability. An open-source alternative to Datadog, Elasticsearch, Loki, and Tempo.</A>
							<DT><A HREF="https://gpulist.ai/">gpulist</A>
							<DT><A HREF="https://twitter.com/TigerBeetleDB/status/1729054660223553933">(1) TigerBeetle en X: "How do you catch up (and overtake) 30 years of test time?</A>
							<DT><A HREF="https://github.com/quickwit-oss/quickwit">quickwit-oss/quickwit: Cloud-native search engine for observability. An open-source alternative to Datadog, Elasticsearch</A>
						</DL><p>
						<DT><H3 FOLDED>sw-testing</H3>
						<DL><p>
							<DT><H3 FOLDED>sw-testing-errors</H3>
							<DL><p>
								<DT><A HREF="https://www.inngest.com/blog/python-errors-as-values">Python errors as values: Comparing useful patterns from Go and Rust - Inngest Blog</A>
								<DT><A HREF="https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/convert.py#L4">errors as values: safetensors/bindings/python/convert.py</A>
							</DL><p>
							<DT><H3 FOLDED>sw-testing-exception-handling</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>sw-testing-bugs</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ProjectPhysX/FluidX3D/releases/tag/v2.16">Release FluidX3D v2.16 (bug fixes) ¬∑ ProjectPhysX/FluidX3D</A>
								<DT><A HREF="https://private-user-images.githubusercontent.com/90851087/327656146-d013fc95-d977-460f-9de8-2a6fc7ddaec5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ3MzM2NDUsIm5iZiI6MTcxNDczMzM0NSwicGF0aCI6Ii85MDg1MTA4Ny8zMjc2NTYxNDYtZDAxM2ZjOTUtZDk3Ny00NjBmLTlkZTgtMmE2ZmM3ZGRhZWM1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDMlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAzVDEwNDkwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQ4ZTJiY2I4NTU2MzBmZmFkNWJiMzI1MzdkYjc4NTg1N2Q5ZjU0ZmZlYmQ3MTNlODFkNDE1NjYyMjlkNmZmY2EmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QXKhYOoKDu0s5FNsEZsht_Psxxfb1exaRzg8-OrI45Q">327656146-d013fc95-d977-460f-9de8-2a6fc7ddaec5.png 1.886√ó960 pixels</A>
								<DT><A HREF="https://chat.openai.com/">github-bug-issues</A>
							</DL><p>
							<DT><H3 FOLDED>Deterministic Simulation Testing</H3>
							<DL><p>
								<DT><A HREF="https://www.polarsignals.com/blog/posts/2024/05/28/mostly-dst-in-go">(Mostly) Deterministic Simulation Testing in Go</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/karpathy/status/1786085254006202541">(1) Andrej Karpathy en X: "Clearly LLMs must one day run in Space Step 1 we harden llm.c to pass the NASA code standards and style guides, certifying that the code is super safe, safe enough to run in Space. https://t.co/tYGrfdka4X (see the linked PDF) LLM training/inference in principle should be super..." / X</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/The_Power_of_10:_Rules_for_Developing_Safety-Critical_Code">The Power of 10: Rules for Developing Safety-Critical Code</A>
						</DL><p>
						<DT><A HREF="https://imbue.com/research/70b-infrastructure/">From bare metal to a 70B model: infrastructure set-up and scripts - imbue</A>
						<DT><A HREF="https://websites.umich.edu/~amberljc/file/LLM-Systems-Basics.pdf">LLM-Systems-Basics (Jiachen Liu)</A>
						<DT><A HREF="https://web.stanford.edu/class/cs245/">CS 245: Principles of Data-Intensive Systems (Winter 2021)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=TOyD-5QgpuE">Design a Code Execution System | System Design - YouTube</A>
						<DT><A HREF="https://www.youtube.com/watch?v=jFrGhodqC08">The cloud is over-engineered and overpriced (no music) - YouTube</A>
						<DT><A HREF="https://github.com/cuda-mode/awesomeMLSys">cuda-mode/awesomeMLSys: An ML Systems Onboarding list</A>
						<DT><A HREF="https://www.youtube.com/watch?v=GA4ONupSl8Y">Two Decades of Hardware Optimizations Down The Drain - YouTube</A>
						<DT><A HREF="https://12factor.net/">The Twelve-Factor App</A>
						<DT><A HREF="https://discord.com/blog/how-discord-supercharges-network-disks-for-extreme-low-latency">How Discord Supercharges Network Disks for Extreme Low Latency</A>
						<DT><A HREF="https://x.com/mvpatel2000/status/1803941734629388590">(1) Mihir Patel en X: "@StasBekman See blog https://t.co/A0JWbNWHVl in particular the HSDP section. When you shard across many GPUs with HSDP, you have multiple replicas. When a node goes down (take top left red square), we can bring up a new node and restore it's copy by pulling from top left green square (1/n) https://t.co/5jsGSr3ca6" / X</A>
						<DT><A HREF="https://github.com/hao-ai-lab">Hao AI Lab</A>
						<DT><A HREF="https://github.com/imbue-ai/cluster-health">imbue-ai/cluster-health</A>
						<DT><A HREF="https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55">Linux Performance Analysis in 60,000 Milliseconds | by Netflix Technology Blog | Netflix TechBlog</A>
					</DL><p>
					<DT><H3 FOLDED>sw-training-infrastructure</H3>
					<DL><p>
						<DT><H3 FOLDED>llm.c</H3>
						<DL><p>
							<DT><H3 FOLDED>llm.c-videos</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=l8pRSuU81PU">Let's reproduce GPT-2 (124M) - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4577s">Let's build GPT: from scratch, in code, spelled out. - YouTube</A>
							</DL><p>
							<DT><A HREF="https://github.com/karpathy/llm.c">karpathy/llm.c: LLM training in simple, raw C/CUDA</A>
							<DT><A HREF="https://github.com/Chillee/llm.c/commit/c39de5916835b5ade292bc96a8b81de4a517972e">attach a simple tutorial (layernorm)</A>
							<DT><A HREF="https://gist.github.com/geohot/7c9f10f5770f058a1de6ef0598e4c9d8">Outputted llm.c from tinygrad</A>
							<DT><A HREF="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/core/providers/cuda/math/softmax_warpwise_impl.cuh">onnxruntime/onnxruntime/core/providers/cuda/math/softmax_warpwise_impl.cuh</A>
							<DT><A HREF="https://x.com/karpathy/status/1799949853289804266">(1) Andrej Karpathy en X: "üìΩÔ∏è New 4 hour (lol) video lecture on YouTube: "Let‚Äôs reproduce GPT-2 (124M)" https://t.co/NMIVD1V6zr The video ended up so long because it is... comprehensive: we start with empty file and end up with a GPT-2 (124M) model: - first we build the GPT-2 network - then we optimize https://t.co/NDqTrLbrO4" / X</A>
							<DT><A HREF="https://github.com/karpathy/llm.c/discussions/481">Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 ¬∑ karpathy/llm.c ¬∑ Discussion #481</A>
							<DT><A HREF="https://arxiv.org/abs/2212.14034">[2212.14034] Cramming: Training a Language Model on a Single GPU in One Day</A>
						</DL><p>
						<DT><H3 FOLDED>torchtitan</H3>
						<DL><p>
							<DT><A HREF="https://github.com/pytorch/torchtitan">pytorch/torchtitan: A native PyTorch Library for large model training</A>
							<DT><A HREF="https://github.com/pytorch/torchtune">pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning</A>
						</DL><p>
						<DT><H3 FOLDED>sw-model-parallelism</H3>
						<DL><p>
							<DT><H3 FOLDED>sw-Megatron-LM</H3>
							<DL><p>
								<DT><H3 FOLDED>examples</H3>
								<DL><p>
									<DT><A HREF="https://github.com/bigcode-project/Megatron-LM/blob/c4468364e4225b32adb648d7918a3e645ff9e409/examples/pretrain_gpt_1B_santacoder.sh">BigCode: pretrain_gpt_1_santacoder.sh</A>
									<DT><A HREF="https://github.com/THUDM/CodeGeeX/blob/f1afae7b26ceaa040adcd4b4812b64f259b05647/scripts/pretrain_codegeex.sh#L47">CodeGeeX/pretrain_codegeex.sh</A>
								</DL><p>
								<DT><A HREF="https://huggingface.co/blog/bloom-megatron-deepspeed">The Technology Behind BLOOM Training</A>
								<DT><A HREF="https://github.com/microsoft/Megatron-DeepSpeed/blob/main/megatron/training.py#L137">training.py</A>
								<DT><A HREF="https://nvidia.github.io/apex/">Apex (A PyTorch Extension) ‚Äî Apex 0.1.0 documentation</A>
								<DT><A HREF="https://huggingface.co/blog/megatron-training">How to train a Language Model with Megatron-LM</A>
								<DT><A HREF="https://github.com/microsoft/Megatron-DeepSpeed/blob/7bbd7f0e390b120898235c446f076d8de219d474/megatron/arguments.py#L24">Megatron-DeepSpeed/arguments.py</A>
								<DT><A HREF="https://pytorch.org/docs/stable/distributed.html">Distributed communication package - torch.distributed ‚Äî PyTorch 2.0 documentation</A>
								<DT><A HREF="https://github.com/mli/transformers-benchmarks/blob/main/transformers.ipynb">transformers-benchmarks/transformers.ipynb at main ¬∑ mli/transformers-benchmarks</A>
							</DL><p>
							<DT><H3 FOLDED>sw-FSDP</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/docs/stable/distributed.html#profiling-collective-communication">Distributed communication package - torch.distributed ‚Äî PyTorch 2.0</A>
								<DT><A HREF="https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md">torchtitan/docs/fsdp.md at main ¬∑ pytorch/torchtitan</A>
							</DL><p>
							<DT><H3 FOLDED>jax-training</H3>
							<DL><p>
								<DT><H3 FOLDED>FLAX</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=__eG63ZP_5g">Day 2 Talks: JAX, Flax &amp; Transformers ü§ó - YouTube</A>
									<DT><A HREF="https://github.com/google/maxtext">google/maxtext: A simple, performant and scalable Jax LLM!</A>
									<DT><H3 FOLDED>flaxformer</H3>
									<DL><p>
										<DT><A HREF="https://github.com/google/flaxformer">google/flaxformer</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>Haliax</H3>
								<DL><p>
									<DT><A HREF="https://github.com/stanford-crfm/haliax">stanford-crfm/haliax: Named Tensors for Legible Deep Learning in JAX</A>
									<DT><A HREF="https://levanter.readthedocs.io/en/latest/Fine-Tuning/">Custom Fine-Tuning: Alpaca Tutorial - Levanter</A>
									<DT><A HREF="https://twitter.com/dlwh/status/1716901560532521100">(David Hall): Haliax release</A>
									<DT><A HREF="https://twitter.com/dlwh/status/1716900734120464834/photo/1">Named tensor library</A>
								</DL><p>
								<DT><H3 FOLDED>t5x + seqio</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google-research/text-to-text-transfer-transformer">google-research/text-to-text-transfer-transformer: Code for the paper "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"</A>
									<DT><A HREF="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md">Checkpoints</A>
									<DT><A HREF="https://github.com/google/flaxformer/blob/main/flaxformer/architectures/t5/t5_1_1.py#L70">flaxformer/flaxformer/architectures/t5/t5_1_1.py at main ¬∑ google/flaxformer</A>
									<DT><A HREF="https://github.com/google/maxtext">google/maxtext: A simple, performant and scalable Jax LLM!</A>
									<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x</A>
								</DL><p>
								<DT><H3 FOLDED>Paxml</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/pax">JAX-Toolbox/rosetta/rosetta/projects/pax at main ¬∑ NVIDIA/JAX-Toolbox</A>
									<DT><A HREF="https://github.com/google/paxml">google/paxml: Pax is a Jax-based machine learning framework for training large scale models. Pax allows for advanced and fully configurable experimentation and parallelization, and has demonstrated industry leading model flop utilization rates.</A>
									<DT><A HREF="https://github.com/google/praxis/tree/main">google/praxis</A>
								</DL><p>
								<DT><H3 FOLDED>Maxtext</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/maxtext">google/maxtext: A simple, performant and scalable Jax LLM!</A>
									<DT><A HREF="https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e">the world‚Äôs largest distributed LLM training job on TPU v5e | Google Cloud Blog</A>
								</DL><p>
								<DT><H3 FOLDED>Levanter</H3>
								<DL><p>
									<DT><A HREF="https://github.com/stanford-crfm/levanter">stanford-crfm/levanter: Legibile, Scalable, Reproducible Foundation Models with Named Tensors and Jax</A>
									<DT><A HREF="https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html">Levanter ‚Äî Legible, Scalable, Reproducible Foundation Models with JAX</A>
									<DT><H3 FOLDED>configs</H3>
									<DL><p>
										<DT><A HREF="https://github.com/NbAiLab/nb-levanter/blob/main/configs/mimir-mistral-7b-extended.yaml">nb-levanter/configs/mimir-mistral-7b-extended.yaml</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2311.08105">[2311.08105] DiLoCo: Distributed Low-Communication Training of Language Models</A>
							</DL><p>
							<DT><H3 FOLDED>hf-accelerate</H3>
							<DL><p>
								<DT><H3 FOLDED>hf-accelerate-inference</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/docs/transformers/perf_infer_gpu_one">Efficient Inference on a Single GPU</A>
								</DL><p>
								<DT><A HREF="https://github.com/huggingface/accelerate">huggingface/accelerate: Train and use PyTorch models with multi-GPU, TPU, mixed-precision</A>
								<DT><A HREF="https://huggingface.co/blog/bloom-inference-pytorch-scripts">Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate</A>
								<DT><A HREF="https://huggingface.co/docs/accelerate/usage_guides/big_modeling">Handling big models for inference (Sharding &amp; device map)</A>
								<DT><A HREF="https://huggingface.co/docs/transformers/perf_train_gpu_many">Efficient Training on Multiple GPUs</A>
								<DT><A HREF="https://github.com/huggingface/accelerate/blob/d4f5fd694e3acf9178b8075ea2379fd1839954f2/docs/source/usage_guides/big_modeling.mdx">Handling big models for inference</A>
								<DT><A HREF="https://github.com/huggingface/transformers/issues/16884">Transformers model shards</A>
								<DT><A HREF="https://towardsdatascience.com/sharded-a-new-technique-to-double-the-size-of-pytorch-models-3af057466dba">Sharded</A>
								<DT><A HREF="https://github.com/kingoflolz/mesh-transformer-jax">Mesh-Transformer-jax: Model parallel transformers in JAX and Haiku</A>
							</DL><p>
							<DT><H3 FOLDED>sw-mistral</H3>
							<DL><p>
								<DT><A HREF="https://github.com/stanford-crfm/mistral">Mistral: Framework for transparent and accessible LLMs training, built with Hugging Face ü§ó Transformers</A>
							</DL><p>
							<DT><H3 FOLDED>deepspeed-training</H3>
							<DL><p>
								<DT><A HREF="https://www.deepspeed.ai/tutorials/inference-tutorial/">Getting Started with DeepSpeed for Inferencing Transformer based Models - DeepSpeed</A>
								<DT><A HREF="https://www.deepspeed.ai/tutorials/inference-tutorial/#end-to-end-gpt-neo-27b-inference">GPT-NEOX-20B</A>
								<DT><A HREF="https://www.philschmid.de/bert-deepspeed-inference">Accelerate BERT inference with DeepSpeed-Inference on GPUs</A>
								<DT><A HREF="https://www.deepspeed.ai/tutorials/megatron/">Megatron-LM GPT2 - DeepSpeed</A>
								<DT><A HREF="https://www.deepspeed.ai/tutorials/zero/#zero-overview">Zero Redundancy Optimizer</A>
								<DT><A HREF="https://github.com/huggingface/transformers/blob/main/tests/deepspeed/ds_config_zero2.json">transformers/ds_config_zero2.json</A>
								<DT><A HREF="https://github.com/microsoft/Megatron-DeepSpeed">microsoft/Megatron-DeepSpeed: Ongoing research training transformer language models at scale, including: BERT &amp; GPT-2</A>
								<DT><A HREF="https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed">Huggingface Transformers DeepSpeed Integration</A>
							</DL><p>
							<DT><H3 FOLDED>mosaicml-training</H3>
							<DL><p>
								<DT><H3 FOLDED>composer</H3>
								<DL><p>
									<DT><A HREF="https://github.com/mosaicml/composer">mosaicml/composer: Train neural networks up to 7x faster</A>
								</DL><p>
								<DT><A HREF="https://www.mosaicml.com/">MosaicML | Home</A>
								<DT><A HREF="https://www.mosaicml.com/blog/coreweave-nvidia-h100-part-1?utm_source=twitter&utm_medium=social&utm_campaign=h100-blog-post">Benchmarking Large Language Models on NVIDIA H100 GPUs with CoreWeave (Part 1)</A>
							</DL><p>
							<DT><H3 FOLDED>training-infra-eth-zurich</H3>
							<DL><p>
								<DT><A HREF="https://github.com/eth-easl/fmengine">eth-easl/fmengine: Utilities for Training Very Large Models</A>
							</DL><p>
							<DT><H3 FOLDED>tensorflow-training</H3>
							<DL><p>
								<DT><H3 FOLDED>tensorflow-installation</H3>
								<DL><p>
									<DT><A HREF="https://velog.io/@qone/Apple-silicon-m1-%EB%A7%A5%EC%97%90-tensorflow%EC%84%A4%EC%B9%98%ED%95%98%EA%B3%A0-GPU%EA%B0%80%EC%86%8D-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0">Enable tensorflow metal (ARM GPU)</A>
									<DT><A HREF="https://gist.github.com/juliasilge/035d54c59436604d6142ebebf29aa224?permalink_comment_id=3981784">Installing R + Tensorflow on M1</A>
									<DT><A HREF="https://developer.apple.com/metal/tensorflow-plugin/">Tensorflow Plugin - Metal - Apple Developer</A>
									<DT><A HREF="https://github.com/robotology/robotology-superbuild/blob/master/doc/install-mambaforge.md#macos">Install mambaforge</A>
								</DL><p>
								<DT><H3 FOLDED>tensorflow-errors</H3>
								<DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/58012741/error-importing-tensorflow-alreadyexistserror-another-metric-with-the-same-nam">two Keras packages installed</A>
									<DT><A HREF="https://stackoverflow.com/questions/57082918/tensorflow-attributeerror-module-tensorflow-python-ops-nn-has-no-attribute">calling v1 API when using v2</A>
								</DL><p>
								<DT><H3 FOLDED>tensorflow-data</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google-research/google-research/blob/master/protein_lm/data.py">TFRecords $ data.py</A>
									<DT><A HREF="https://github.com/google/gin-config">Gin provides a lightweight configuration framework for Python</A>
								</DL><p>
								<DT><H3 FOLDED>tensorflow-docs</H3>
								<DL><p>
									<DT><A HREF="https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.10/tensorflow/g3doc/tutorials/mnist/tf/index.md">TensorFlow - TensorFlow Mechanics 101</A>
									<DT><A HREF="https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.10/tensorflow/g3doc/how_tos/documentation/index.md">TensorFlow - Writing TensorFlow Documentation</A>
								</DL><p>
								<DT><H3 FOLDED>tensorflow-visualization</H3>
								<DL><p>
									<DT><A HREF="https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin">Visualizing Data using the Embedding Projector in TensorBoard ¬†|¬† TensorFlow</A>
									<DT><A HREF="http://projector.tensorflow.org/">Embedding projector - visualization of high-dimensional data</A>
								</DL><p>
								<DT><H3 FOLDED>tensorflow-configuration</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/gin-config">Gin provides a lightweight configuration framework for Python</A>
								</DL><p>
								<DT><A HREF="https://www.tensorflow.org/api_docs/python/tf/nn">Module: tf.nn ¬†|¬† TensorFlow Core v2.8.0 (API)</A>
							</DL><p>
							<DT><H3 FOLDED>model-parallelism-mpi</H3>
							<DL><p>
								<DT><A HREF="https://www.open-mpi.org/">Open MPI: Open Source High Performance Computing</A>
							</DL><p>
							<DT><A HREF="https://github.com/kingoflolz/mesh-transformer-jax">Mesh-Transformer-jax: Model parallel transformers in JAX and Haiku</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-configuration</H3>
						<DL><p>
							<DT><H3 FOLDED>Hydra</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/hydra/blob/main/examples/experimental/rerun/my_app.py">hydra/examples/experimental/rerun/my_app.py at main ¬∑ facebookresearch/hydra</A>
								<DT><A HREF="https://github.com/facebookresearch/hydra">facebookresearch/hydra: Hydra is a framework for elegantly configuring complex applications</A>
								<DT><A HREF="https://blog.helsing.ai/strongly-typed-structured-configuration-in-hydra-8fb43522d224">Strongly-typed structured configuration in Hydra | by Robert Fink | Helsing Blog</A>
							</DL><p>
							<DT><H3 FOLDED>Gin</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://twitter.com/karpathy/status/1529159374672830464">how to flexibly configure and instantiate neural net architectures and trainers</A>
							<DT><A HREF="https://twitter.com/karpathy/status/1528808361558306817">How they store, catalogue, override, manage and plumb hyperparamaters configs</A>
							<DT><A HREF="https://github.com/google/gin-config">Gin provides a lightweight configuration framework for Python</A>
						</DL><p>
						<DT><H3 FOLDED>full-model-gradient-updates</H3>
						<DL><p>
							<DT><H3 FOLDED>full-model-gradient-updates-notebooks</H3>
							<DL><p>
								<DT><A HREF="https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es">finetune-gpt-j-6B-8bit.ipynb</A>
							</DL><p>
							<DT><A HREF="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/deepseed-flan-t5-summarization.ipynb">Fine-tune FLAN-T5 XL/XXL using DeepSpeed &amp; Hugging Face Transformers</A>
							<DT><A HREF="https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/">How To Finetune GPT Like Large Language Models on a Custom Dataset - Lightning AI</A>
							<DT><A HREF="https://arxiv.org/abs/2405.05904#:~:text=We%20demonstrate%20that%20large%20language,consistent%20with%20the%20model's%20knowledge.">[2405.05904] Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-peft</H3>
						<DL><p>
							<DT><H3 FOLDED>training-infra-LoRA</H3>
							<DL><p>
								<DT><A HREF="https://github.com/tloen/alpaca-lora">Code for reproducing the Stanford Alpaca InstructLLaMA</A>
								<DT><A HREF="https://www.youtube.com/watch?v=4LiKEghyoBo">LoRA - Low Rank Adaptation of Large Language Model: Source Code - YouTube</A>
								<DT><A HREF="https://github.com/punica-ai/punica">punica-ai/punica: Serving multiple LoRA finetuned LLM as one</A>
								<DT><A HREF="https://github.com/predibase/lorax">predibase/lorax: Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs</A>
								<DT><A HREF="https://github.com/S-LoRA/S-LoRA">S-LoRA/S-LoRA: S-LoRA: Serving Thousands of Concurrent LoRA Adapters</A>
							</DL><p>
							<DT><H3 FOLDED>training-infra-prompt-tuning</H3>
							<DL><p>
								<DT><A HREF="https://colab.research.google.com/github/corolla-johnson/mkultra/blob/master/tuning_finetune.ipynb">tuning_finetune_alice.ipynb - Colaboratory</A>
								<DT><A HREF="https://colab.research.google.com/github/corolla-johnson/mkultra/blob/master/tuning_world_info.ipynb#scrollTo=qyfpqA5po5FX">tuning_finetune_lite_3.ipynb - Colaboratory</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2403.14608">[2403.14608] Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</A>
							<DT><A HREF="https://github.com/huggingface/peft">huggingface/peft: ü§ó PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=KoOlcX3XLd4">EMNLP 2022 Tutorial - "Modular and Parameter-Efficient Fine-Tuning for NLP Models" - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=gCcgM7sl4Q4">LaMA-Adapter Finetuning: Source Code: Paper: PseudoCode</A>
							<DT><A HREF="https://twitter.com/giffmana/status/1776156462252761222">(1) Lucas Beyer (bl16) en X: "A bit late, but I just read ReFT, here's a quick thread. - A PEFT method - acts on activs `h` -&amp;gt; small inference overhead 1/5 https://t.co/iN1emxyXQ6" / X</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-huggingface</H3>
						<DL><p>
							<DT><H3 FOLDED>hf-datasets</H3>
							<DL><p>
								<DT><H3 FOLDED>hf-datasets-preprocessing</H3>
								<DL><p>
									<DT><A HREF="https://github.com/leogao2/lm_dataformat">LM_Dataformat: Utilities for storing data for LM training</A>
									<DT><A HREF="https://github.com/GEM-benchmark/NL-Augmenter">GEM-benchmark/NL-Augmenter: NL-Augmenter ü¶é ‚Üí üêç A Collaborative Repository of Natural Language Transformations</A>
								</DL><p>
								<DT><A HREF="https://huggingface.co/datasets/the_pile">the_pile ¬∑ Datasets at Hugging Face</A>
								<DT><A HREF="https://huggingface.co/docs/datasets/v1.15.1/installation.html">Installation ‚Äî datasets 1.15.1 documentation</A>
								<DT><A HREF="https://huggingface.co/course/chapter5/5">Creating your own dataset - Hugging Face Course</A>
								<DT><A HREF="https://huggingface.co/docs/datasets/loading">In-Memory data</A>
								<DT><A HREF="https://huggingface.co/docs/datasets/process">Datasets: Process</A>
							</DL><p>
							<DT><H3 FOLDED>hf-hub</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/docs/huggingface_hub/v0.10.1/en/how-to-upstream">Hub Client Library</A>
							</DL><p>
							<DT><H3 FOLDED>hf-eval</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/spaces/evaluate-metric/code_eval">Code Eval - a Hugging Face Space by evaluate-metric</A>
							</DL><p>
							<DT><H3 FOLDED>hf-transformers</H3>
							<DL><p>
								<DT><H3 FOLDED>hf-transformers-releases</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/transformers/releases/tag/v4.36.0">Release v4.36: Mixtral, Llava/BakLlava, SeamlessM4T v2, AMD ROCm, F.sdpa wide-spread support ¬∑ huggingface/transformers</A>
								</DL><p>
								<DT><H3 FOLDED>hf-transformers-t5</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/LiamFedus/status/1536791574612303872">Switch Transformer models in T5X/JAX (1.6T param)</A>
									<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x: Text-To-Text Transfer Transformer</A>
									<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x</A>
									<DT><A HREF="https://colab.research.google.com/drive/1xx7SgdLaAu23YFBirXmaQViDr8caowX_">T0 by mishin_learning.ipynb - Colaboratory</A>
									<DT><A HREF="https://github.com/bigscience-workshop/t-zero/blob/master/inference/README.md">Running inference with T0</A>
								</DL><p>
								<DT><H3 FOLDED>hf-transformers-generation</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/transformers/releases/tag/v4.26.0">Release v4.26.0: Generation configs, image processors, backbones and plenty of new models! ¬∑ huggingface/transformers</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model">Generation Config: Text generation strategies</A>
									<DT><A HREF="https://huggingface.co/gpt2/blob/main/generation_config.json">generation_config.json ¬∑ gpt2 at main</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationConfig">Generation</A>
									<DT><A HREF="https://github.com/huggingface/transformers/blob/e3d832ff87c6ec997125deaa4f1b239db8f9e613/src/transformers/generation/configuration_utils.py#L663">transformers/configuration_utils.py at e3d832ff87c6ec997125deaa4f1b239db8f9e613 ¬∑ huggingface/transformers ¬∑ GitHub</A>
									<DT><H3 FOLDED>Pipelines</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/pipelines#transformers.TextGenerationPipeline">Pipelines</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>hf-transformers-models</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel">OpenAI GPT2</A>
								</DL><p>
								<DT><H3 FOLDED>hf-transformers-training</H3>
								<DL><p>
								</DL><p>
								<DT><A HREF="https://twitter.com/yacineMTB/status/1691208981698498560">HF Transformers Software Complexity</A>
								<DT><A HREF="https://github.com/THUDM/SwissArmyTransformer">Flexible and powerful library to develop your own Transformer variants</A>
								<DT><A HREF="https://huggingface.co/spaces/nielsr/LayoutLMv2-FUNSD">LayoutLMv 2 FUNSD - a Hugging Face Space by nielsr</A>
								<DT><A HREF="https://huggingface.co/transformers/v4.8.0/glossary.html">Glossary ‚Äî transformers 4.7.0 documentation</A>
								<DT><A HREF="https://pypi.org/project/cascades/">cascades ¬∑ PyPI</A>
								<DT><A HREF="https://colab.research.google.com/drive/14wnxMvD9zsiBQo2FtTpxn6w2cpXCcb-7">OPT (30B) - Colaboratory</A>
								<DT><A HREF="https://huggingface.co/docs/transformers/model_doc/auto">Auto Classes</A>
								<DT><A HREF="https://twitter.com/NeelNanda5/status/1582876613397467137">Interpretability</A>
								<DT><A HREF="https://github.com/NielsRogge/Transformers-Tutorials?search=1">Transformers-Tutorials/ at master: NielsRogge</A>
								<DT><A HREF="https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L223">transformers/trainer.py at main ¬∑ huggingface/transformers ¬∑ GitHub</A>
							</DL><p>
							<DT><A HREF="https://huggingface.co/">The AI community building the future.</A>
							<DT><A HREF="https://www.philschmid.de/bert-deepspeed-inference">Accelerate BERT inference with DeepSpeed-Inference on GPUs</A>
							<DT><A HREF="https://huggingface.co/search/full-text">Full Text Search - Hugging Face</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-job-scheduler</H3>
						<DL><p>
							<DT><H3 FOLDED>SLURM</H3>
							<DL><p>
								<DT><A HREF="https://oatml.cs.ox.ac.uk/code.html#JvASlurm">Reproducibility and Code - OATML</A>
								<DT><A HREF="https://github.com/y0ast/slurm-for-ml">y0ast/slurm-for-ml: A Machine Learning workflow for Slurm.</A>
							</DL><p>
							<DT><A HREF="https://github.com/it4innovations/hyperqueue">It4innovations/hyperqueue: Scheduler for sub-node tasks for HPC systems with batch scheduling</A>
							<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-model-registry</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/_ScottCondron/status/1529420711034556416/photo/1">Centralized place for ML teams</A>
							<DT><A HREF="https://wandb.ai/home">Home ‚Äì Weights &amp; Biases</A>
						</DL><p>
						<DT><H3 FOLDED>training-job-scheduler</H3>
						<DL><p>
							<DT><H3 FOLDED>SLURM</H3>
							<DL><p>
								<DT><H3 FOLDED>slurm-submitit</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookincubator/submitit">facebookincubator/submitit: Python 3.8+ toolbox for submitting jobs to Slurm</A>
								</DL><p>
								<DT><A HREF="https://slurm.schedmd.com/">slurm.schedmd.com</A>
								<DT><A HREF="https://github.com/GoogleCloudPlatform/slurm-gcp">GoogleCloudPlatform/slurm-gcp</A>
								<DT><A HREF="https://github.com/GoogleCloudPlatform/hpc-toolkit">GoogleCloudPlatform/hpc-toolkit: Cloud HPC Toolkit is an open-source software offered by Google Cloud which makes it easy for customers to deploy HPC environments on Google Cloud.</A>
								<DT><A HREF="https://oatml.cs.ox.ac.uk/code.html#JvASlurm">Reproducibility and Code - OATML</A>
								<DT><A HREF="https://github.com/y0ast/slurm-for-ml">y0ast/slurm-for-ml: A Machine Learning workflow for Slurm.</A>
							</DL><p>
							<DT><H3 FOLDED>job-manager-kubernetes</H3>
							<DL><p>
								<DT><H3 FOLDED>GKE</H3>
								<DL><p>
									<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/whats-new-with-gke-at-google-cloud-next">What‚Äôs new with GKE at Google Cloud Next</A>
								</DL><p>
								<DT><H3 FOLDED>Dynamic Resource Allocation</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna">Dynamic Resource Allocation for GPUs in Kubernetes - Google Docs</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-local</H3>
								<DL><p>
									<DT><A HREF="https://kind.sigs.k8s.io/">kind: local run</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-kserve</H3>
								<DL><p>
									<DT><A HREF="https://github.com/kserve/modelmesh-serving">kserve/modelmesh-serving: Controller for ModelMesh</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-basics</H3>
								<DL><p>
									<DT><A HREF="https://hamel.dev/notes/k8s/02-Basics.html">k8s</A>
									<DT><A HREF="https://www.youtube.com/watch?v=90kZRyPcRZw">Kubernetes Deconstructed: Understanding Kubernetes by Breaking It Down - Carson Anderson, DOMO - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-utils</H3>
								<DL><p>
									<DT><A HREF="https://github.com/alibaba/kt-connect">alibaba/kt-connect: A toolkit for Integrating with your kubernetes dev environment more efficiently</A>
									<DT><A HREF="https://github.com/alibaba/kubeskoop">alibaba/kubeskoop</A>
								</DL><p>
								<DT><A HREF="https://cloud.google.com/anthos-config-management/docs/concepts/kustomize">Configure Kubernetes with Kustomize ¬†|¬† Anthos Config Management ¬†|¬† Google Cloud</A>
								<DT><A HREF="https://www.youtube.com/watch?v=06bKlSmVwIg">CNCF Live Webinar: Overcoming the GPU shortage with virtual Kubelets &amp; distributed cloud - YouTube</A>
								<DT><A HREF="https://virtual-kubelet.io/">Virtual Kubelet | Home</A>
								<DT><A HREF="https://github.com/virtual-kubelet/virtual-kubelet">virtual-kubelet/virtual-kubelet: Virtual Kubelet is an open source Kubernetes kubelet implementation.</A>
								<DT><A HREF="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet | Kubernetes</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cwiAW5TZsfo">On-Demand Systems and Scaled Training Using the JobSet API - Abdullah Gharaibeh &amp; Vanessa Sochat - YouTube</A>
								<DT><A HREF="https://github.com/kubernetes/kubernetes/issues/95492">Kubernetes won't run 50,000 Jobs ¬∑ Issue #95492 ¬∑ kubernetes/kubernetes</A>
								<DT><A HREF="https://www.cncf.io/blog/2020/08/10/why-the-kubernetes-scheduler-is-not-enough-for-your-ai-workloads/">Scheduler (Level 2)</A>
								<DT><A HREF="https://www.run.ai/">Run:ai - AI Optimization and Orchestration</A>
								<DT><A HREF="https://www.cncf.io/wp-content/uploads/2020/10/Kube-two-level-RM.pdf">Kubernetes native two-level resource managment for AI workloads</A>
								<DT><A HREF="https://github.com/project-codeflare/multi-cluster-app-dispatcher">project-codeflare/multi-cluster-app-dispatcher: Holistic job manager on Kubernetes</A>
								<DT><A HREF="https://github.com/bentoml/Yatai">bentoml/Yatai: Model Deployment at Scale on Kubernetes ü¶ÑÔ∏è</A>
								<DT><A HREF="https://docs.aws.amazon.com/parallelcluster/latest/ug/build-image.html">buildImage - AWS ParallelCluster</A>
								<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/high-performance-aiml-storage-through-local-ssd-support-on-gke">High performance AI/ML storage through Local SSD support on GKE | Google Cloud Blog</A>
							</DL><p>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/core/exp_manager.html">Experiment Manager ‚Äî NVIDIA NeMo</A>
							<DT><A HREF="https://github.com/google-deepmind/xmanager">google-deepmind/xmanager: A platform for managing machine learning experiments</A>
							<DT><A HREF="https://storage.googleapis.com/gresearch/xmanager/deepmind_xmanager_slides.pdf">XManager (Slides)</A>
							<DT><A HREF="https://github.com/google/xpk">google/xpk: xpk (Accelerated Processing Kit) is a software tool to help Cloud developers to orchestrate training jobs on accelerators such as TPUs and GPUs on GKE.</A>
							<DT><A HREF="https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/gke-a100-jax/train.py">ai-on-gke/gke-a100-jax/train.py at main ¬∑ GoogleCloudPlatform/ai-on-gke</A>
							<DT><A HREF="https://github.com/it4innovations/hyperqueue">It4innovations/hyperqueue: Scheduler for sub-node tasks for HPC systems with batch scheduling</A>
							<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
						</DL><p>
						<DT><A HREF="https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness">Training great LLMs entirely from ground zero in the wilderness as a startup ‚Äî Yi Tay</A>
						<DT><A HREF="https://x.com/leilavclark/status/1805700631199642094">From bare metal to a 70B model: infrastructure set-up and scripts</A>
						<DT><A HREF="https://github.com/imbue-ai/cluster-health">imbue-ai/cluster-health</A>
						<DT><A HREF="https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py#L148">transformers/training_args.py</A>
						<DT><A HREF="https://github.com/apple/corenet">apple/corenet: CoreNet: A library for training deep neural networks</A>
						<DT><A HREF="https://github.com/pytorch/torchtitan">pytorch/torchtitan: A native PyTorch Library for large model training</A>
						<DT><A HREF="https://developer.nvidia.com/blog/tips-on-scaling-storage-for-ai-training-and-inferencing/">Tips on Scaling Storage for AI Training and Inferencing | NVIDIA Technical Blog</A>
						<DT><A HREF="https://developer.nvidia.com/blog/storage-performance-basics-for-deep-learning/">Storage Performance Basics for Deep Learning | NVIDIA Technical Blog</A>
						<DT><A HREF="https://github.com/alibaba/Megatron-LLaMA">alibaba/Megatron-LLaMA: Best practice for training LLaMA models in Megatron-LM</A>
						<DT><A HREF="https://www.youtube.com/watch?v=HYvhj9W2qHQ">Cloud Native Data Loaders for Machine Learning Using Zarr and Xarray - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>sw-physics-simulation</H3>
					<DL><p>
						<DT><H3 FOLDED>MuJoCo</H3>
						<DL><p>
							<DT><H3 FOLDED>mujoco-installation</H3>
							<DL><p>
								<DT><A HREF="https://formulae.brew.sh/formula/glfw#default">glfw ‚Äî Homebrew Formulae</A>
								<DT><A HREF="https://blog.birost.com/a?ID=b71caef0-c790-4bde-933c-f3b3a0b3312b">Some tiny steps</A>
								<DT><A HREF="https://www.fullstaq.com/knowledge-hub/blogs/an-alternative-to-macos-dyld-library-path">DYLD_LIBRARY_PATH</A>
								<DT><A HREF="https://medium.com/macos-is-not-linux-and-other-nix-reflections/d-o-y-ou-ld-library-path-you-6ab0a6135a33">D(o) Y(ou) LD_LIBRARY_PATH</A>
							</DL><p>
							<DT><A HREF="https://mujoco.org/">MuJoCo ‚Äî Advanced Physics Simulation</A>
							<DT><A HREF="https://mujoco.readthedocs.io/en/latest/programming.html#initialization">Programming ‚Äî MuJoCo documentation</A>
							<DT><A HREF="https://twitter.com/GoogleDeepMind/status/1714627619742683245">GPU &amp; TPU acceleration through JAX</A>
						</DL><p>
					</DL><p>
					<DT><H3 FOLDED>sw-graphs</H3>
					<DL><p>
						<DT><H3 FOLDED>graphs-jax</H3>
						<DL><p>
							<DT><A HREF="https://github.com/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb">educational/intro_to_graph_nets_tutorial_with_jraph.ipynb at master ¬∑ deepmind/educational</A>
							<DT><A HREF="https://github.com/deepmind/jraph">deepmind/jraph: A Graph Neural Network Library in Jax</A>
						</DL><p>
						<DT><A HREF="https://www.dgl.ai/">Deep Graph Library</A>
					</DL><p>
					<DT><H3 FOLDED>sw-graphs-neural-networks</H3>
					<DL><p>
						<DT><A HREF="https://github.com/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb">educational/intro_to_graph_nets_tutorial_with_jraph.ipynb at master ¬∑ deepmind/educational</A>
						<DT><A HREF="https://github.com/deepmind/jraph">deepmind/jraph: A Graph Neural Network Library in Jax</A>
						<DT><A HREF="https://github.com/a-r-j/graphtype">a-r-j/graphtype: Type hinting for networkx Graphs</A>
					</DL><p>
					<DT><H3 FOLDED>sw-reinforcement-learning</H3>
					<DL><p>
						<DT><H3 FOLDED>rl-openai</H3>
						<DL><p>
							<DT><H3 FOLDED>gym</H3>
							<DL><p>
								<DT><A HREF="https://stackoverflow.com/questions/44150310/openai-gym-nameerror">python - openAi-gym NameError</A>
								<DT><A HREF="https://openai.github.io/mujoco-py/build/html/reference.html#mjviewer-3d-rendering">API reference ‚Äî mujoco-py 1.50.1.0 documentation</A>
							</DL><p>
							<DT><A HREF="https://beta.openai.com/docs/guides/embeddings/what-are-embeddings">Embeddings - OpenAI API</A>
							<DT><A HREF="http://gptprompts.wikidot.com/intro:logprobs">Logprobs</A>
						</DL><p>
						<DT><A HREF="https://github.com/kaesve/muzero">A clean implementation of MuZero and AlphaZero following the AlphaZero General framework.</A>
						<DT><A HREF="https://github.com/chiamp/muzero-cartpole">chiamp/muzero-cartpole: Applying DeepMind's MuZero algorithm to the cart pole environment in gym</A>
						<DT><A HREF="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">Introduction to Reinforcement Learning with David Silver (Deepmind)</A>
						<DT><A HREF="https://github.com/openai/baselines">openai/baselines: OpenAI Baselines: high-quality implementations of reinforcement learning algorithms</A>
					</DL><p>
					<DT><H3 FOLDED>sw-notebooks</H3>
					<DL><p>
						<DT><H3 FOLDED>jupyter</H3>
						<DL><p>
							<DT><A HREF="https://jupyter.org/">Project Jupyter | Home</A>
							<DT><A HREF="https://stackoverflow.com/questions/50982686/what-is-the-difference-between-jupyter-notebook-and-jupyterlab">python - What is the difference between Jupyter Notebook and JupyterLab? - Stack Overflow</A>
							<DT><A HREF="https://towardsdatascience.com/jypyter-notebook-shortcuts-bf0101a98330">Jupyter Notebook Shortcuts</A>
						</DL><p>
						<DT><A HREF="https://nn.labml.ai/normalization/deep_norm/index.html">Labml.ai</A>
						<DT><A HREF="https://notebooks.quantumstat.com/">NLP</A>
						<DT><A HREF="https://github.com/amrzv/awesome-colab-notebooks">awesome-colab-notebooks: Collection of google colaboratory notebooks for fast and easy experiments</A>
					</DL><p>
					<DT><H3 FOLDED>sw-interoperability</H3>
					<DL><p>
						<DT><H3 FOLDED>webassembly</H3>
						<DL><p>
							<DT><H3 FOLDED>wasm-internals</H3>
							<DL><p>
								<DT><A HREF="https://developer.mozilla.org/en-US/docs/WebAssembly/Understanding_the_text_format">Understanding WebAssembly text format - WebAssembly | MDN</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ojYEfRye6aE&t=13s">HELLO WEBASSEMBLY - wat</A>
								<DT><A HREF="https://developer.mozilla.org/en-US/docs/WebAssembly/Text_format_to_wasm">Converting WebAssembly text format to wasm - WebAssembly | MDN</A>
								<DT><A HREF="https://rustwasm.github.io/docs.html">Rust and WebAssembly Documentation | Rust and WebAssembly</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-cpp</H3>
							<DL><p>
								<DT><H3 FOLDED>Emscripten</H3>
								<DL><p>
									<DT><A HREF="https://emscripten.org/">Main ‚Äî Emscripten 3.0.1-git (dev) documentation</A>
									<DT><H3 FOLDED>installation</H3>
									<DL><p>
										<DT><A HREF="https://emscripten.org/docs/building_from_source/toolchain_what_is_needed.html#toolchain-what-you-need">Emscripten Toolchain Requirements ‚Äî Emscripten 3.1.9</A>
										<DT><A HREF="https://formulae.brew.sh/formula/emscripten">emscripten ‚Äî Homebrew Formulae</A>
										<DT><A HREF="https://formulae.brew.sh/formula/llvm#default">llvm ‚Äî Homebrew Formulae</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://medium.com/@tdeniffel/pragmatic-compiling-from-c-to-webassembly-a-guide-a496cc5954b8">Pragmatic compiling of C++ to WebAssembly. A Guide. | by Thomas Deniffel | Medium</A>
								<DT><A HREF="https://web.dev/loading-wasm/">Loading WebAssembly modules efficiently</A>
								<DT><A HREF="https://nodejs.dev/learn/nodejs-with-webassembly">Node.js with WebAssembly</A>
								<DT><A HREF="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/Interacting-with-code.html">Interacting with code ‚Äî Emscripten 3.1.9-git (dev) documentation</A>
								<DT><A HREF="https://web.dev/emscripten-npm/">Emscripten and npm</A>
								<DT><A HREF="https://log2base2.com/c-with-dsa?utm_src=youtube&utm_target=ycwdeug1&gclid=CjwKCAjwt52mBhB5EiwA05YKo4S_xtW1JucfNad8OMbuQj0b_tg5eej5VfMOx5M8PdeFGvIrlEiHeRoCxHkQAvD_BwE">Learn C Programming | Pointers Visualization | Log2Base2</A>
							</DL><p>
							<DT><A HREF="https://wasmcloud.dev/">wasmCloud Documentation</A>
							<DT><A HREF="https://pragprog.com/titles/khrust/programming-webassembly-with-rust/">Programming WebAssembly with Rust: Unified Development for Web, Mobile, and Embedded Applications by Kevin Hoffman</A>
							<DT><A HREF="https://pspdfkit.com/blog/2017/webassembly-a-new-hope/">WebAssembly: A New Hope | PSPDFKit</A>
							<DT><A HREF="https://github.com/WebAssembly/wabt">WebAssembly/wabt: The WebAssembly Binary Toolkit</A>
						</DL><p>
						<DT><H3 FOLDED>Rust bindings for Python</H3>
						<DL><p>
							<DT><A HREF="https://github.com/prql/prql/tree/main/prql-python">prql/prql-python at main ¬∑ prql/prql</A>
							<DT><A HREF="https://github.com/PyO3/pyo3">PyO3/pyo3: Rust bindings for the Python interpreter</A>
							<DT><A HREF="https://pyo3.rs/v0.14.5/index.html">Introduction - PyO3 user guide</A>
							<DT><A HREF="https://medium.com/@MatthieuL49/a-mixed-rust-python-project-24491e2af424">How to Mix Rust and Python in Your Project | by Matt | Medium</A>
							<DT><A HREF="https://github.com/PyO3/pyo3/issues/941">Running rust unit tests ¬∑ Issue #941 ¬∑ PyO3/pyo3</A>
							<DT><A HREF="http://saidvandeklundert.net/learn/2021-11-18-calling-rust-from-python-using-pyo3/">Calling Rust from Python using PyO3</A>
						</DL><p>
					</DL><p>
					<DT><H3 FOLDED>sw-profiling</H3>
					<DL><p>
						<DT><H3 FOLDED>sw-profiling-micro-benchmarking</H3>
						<DL><p>
							<DT><H3 FOLDED>MUST TRACK AND DO: cupy (RDMA), NCCL perf, fast memcpy using GPUs</H3>
							<DL><p>
								<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/playground/other/test_cupy_partial_transfer.py">alpa/playground/other/test_cupy_partial_transfer.py at main ¬∑ alpa-projects/alpa</A>
								<DT><A HREF="https://github.com/cupy/cupy/blob/cf9b4a517725b803fba4828ed0b1c93a231cc5da/cupyx/distributed/_nccl_comm.py#L56">cupy/cupyx/distributed/_nccl_comm.py at cf9b4a517725b803fba4828ed0b1c93a231cc5da ¬∑ cupy/cupy</A>
								<DT><A HREF="https://github.com/microsoft/debugpy/issues/783">use vscode to remote debug python program with tmux session ¬∑ Issue #783 ¬∑ microsoft/debugpy</A>
								<DT><A HREF="https://gist.github.com/geohot/a7b10a1fb8da8621c65412c028c73512">wtf_cuda.py</A>
								<DT><A HREF="https://gist.github.com/geohot/e11dc1b1058ed9e0bc6610249263b024">Test Bandwidth of all reduce</A>
								<DT><A HREF="https://gist.github.com/geohot/3b23bb6be846d3097834d07806ca6563">Fast memcpy using GPUs</A>
								<DT><A HREF="https://colab.research.google.com/drive/1Zk_jYW-Ptf-5svBDFSeBp8HSTnlZnNzY#scrollTo=jICeB9AhtHLz">wtf_cuda.ipynb - Colaboratory</A>
								<DT><A HREF="https://github.com/datacrunch-research/micro">datacrunch-research/micro: Miscellaneous¬†microbenchmarking playground</A>
							</DL><p>
						</DL><p>
						<DT><A HREF="https://www.techrepublic.com/article/how-to-check-drive-space-on-linux-from-the-command-line/">How to check drive space on Linux from the command line</A>
						<DT><A HREF="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-profiler-to-analyze-long-running-jobs%23using-profiler-to-analyze-long-running-jobs">PyTorch Profiler ‚Äî PyTorch Tutorials 1.12.1+cu102 documentation</A>
						<DT><A HREF="https://odsc.medium.com/optimizing-pytorch-performance-batch-size-with-pytorch-profiler-80e0bf39e80e">Optimizing PyTorch Performance: Batch Size with PyTorch Profiler | by ODSC - Open Data Science | Medium</A>
						<DT><A HREF="https://github.com/Syllo/nvtop">Syllo/nvtop: GPUs process monitoring for AMD, Intel and NVIDIA</A>
						<DT><A HREF="https://docs.contrastsecurity.com/en/python-middleware.html">Configure middleware</A>
						<DT><A HREF="https://www.cyberciti.biz/open-source/install-ncdu-on-linux-unix-ncurses-disk-usage/">ncdu</A>
						<DT><A HREF="https://unix.stackexchange.com/questions/125429/tracking-down-where-disk-space-has-gone-on-linux">du</A>
						<DT><A HREF="https://serverfault.com/questions/517483/how-to-read-memory-usage-in-htop">How to read memory usage in htop?</A>
					</DL><p>
					<DT><H3 FOLDED>concurrency</H3>
					<DL><p>
						<DT><A HREF="https://go.dev/blog/codelab-share">Share Memory By Communicating - The Go Programming Language</A>
						<DT><A HREF="https://go.dev/ref/mem">The Go Memory Model - The Go Programming Language</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Concurrency_(computer_science)">Concurrency (computer science) - Wikipedia</A>
						<DT><A HREF="https://stackoverflow.com/questions/36391421/explain-dont-communicate-by-sharing-memory-share-memory-by-communicating">go - Explain: Don't communicate by sharing memory; share memory by communicating - Stack Overflow</A>
						<DT><A HREF="https://man7.org/linux/man-pages/man7/shm_overview.7.html">shm_overview(7) - Linux manual page</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Communicating_sequential_processes">Communicating sequential processes - Wikipedia</A>
						<DT><A HREF="https://github.com/LaurentMazare/hojo">LaurentMazare/hojo: A small python library to run iterators in a separate process</A>
						<DT><A HREF="https://www.youtube.com/watch?v=Iqrd9vsLrak">Coroutine Patterns: Problems and Solutions Using Coroutines in a Modern Codebase</A>
					</DL><p>
					<DT><H3 FOLDED>multi-threading</H3>
					<DL><p>
						<DT><H3 FOLDED>OpenMP</H3>
						<DL><p>
							<DT><A HREF="https://github.com/jakaspeh/concurrency/blob/master/ompForSchedule.cpp">ompForSchedule.cpp examples</A>
							<DT><A HREF="http://jakascorner.com/blog/2016/06/omp-for-scheduling.html">OpenMP: For &amp; Scheduling</A>
							<DT><A HREF="https://smileipic.github.io/tutorials/perfs_parallel_computing.html">Parallel computing ‚Äî Smilei tutorials X.Y documentation</A>
							<DT><A HREF="https://github.com/facebookincubator/dispenso">facebookincubator/dispenso: The project provides high-performance concurrency, enabling highly parallel computation.</A>
						</DL><p>
						<DT><H3 FOLDED>MPI</H3>
						<DL><p>
							<DT><A HREF="https://mpi4py.readthedocs.io/en/stable/">MPI for Python ‚Äî MPI for Python 3.1.6 documentation</A>
						</DL><p>
						<DT><A HREF="https://www.pythonpool.com/cant-pickle-local-object/">Pickling local variables context</A>
						<DT><A HREF="http://jakascorner.com/blog/2016/06/omp-for-scheduling.html">OpenMP: For &amp; Scheduling</A>
						<DT><A HREF="https://www.youtube.com/watch?v=Iqrd9vsLrak">Coroutine Patterns: Problems and Solutions Using Coroutines in a Modern Codebase</A>
					</DL><p>
					<DT><H3 FOLDED>sw-diffusion-optimization</H3>
					<DL><p>
						<DT><H3 FOLDED>Pseudo-Linear Multistep Sampling (PLMS)</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2312.00858">[2312.00858] DeepCache: Accelerating Diffusion Models for Free</A>
							<DT><A HREF="https://github.com/horseee/DeepCache">horseee/DeepCache: [CVPR 2024] DeepCache: Accelerating Diffusion Models for Free</A>
						</DL><p>
						<DT><H3 FOLDED>minSDXL</H3>
						<DL><p>
							<DT><A HREF="https://github.com/cloneofsimo/minSDXL">cloneofsimo/minSDXL: Huggingface-compatible SDXL Unet implementation that is readily hackable</A>
						</DL><p>
						<DT><H3 FOLDED>stable-diffusion.cpp</H3>
						<DL><p>
							<DT><A HREF="https://github.com/leejet/stable-diffusion.cpp">leejet/stable-diffusion.cpp: Stable Diffusion in pure C/C++</A>
						</DL><p>
						<DT><A HREF="https://www.vrushankdes.ai/diffusion-inference-optimization">Diffusion Inference Optimization</A>
						<DT><A HREF="https://huggingface.co/blog/annotated-diffusion">The Annotated Diffusion Model</A>
						<DT><A HREF="https://huggingface.co/blog/deploy-deepfloydif-using-bentoml#building-and-serving-a-bento">Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action</A>
						<DT><A HREF="https://huggingface.co/docs/diffusers/main/en/stable_diffusion#memory">Effective and efficient diffusion</A>
						<DT><A HREF="https://huggingface.co/docs/diffusers/main/en/optimization/fp16">Memory and speed</A>
						<DT><A HREF="https://github.com/tmabraham/diffusion_reading_group">Diffusion Reading Group at EleutherAI</A>
						<DT><A HREF="https://huggingface.co/blog/sdxl_ort_inference">Accelerating SD Turbo and SDXL Turbo Inference with ONNX Runtime and Olive</A>
						<DT><A HREF="https://github.com/siliconflow/onediff/tree/main">siliconflow/onediff: OneDiff: An out-of-the-box acceleration library for diffusion models.</A>
					</DL><p>
					<DT><H3 FOLDED>sw-embeddings</H3>
					<DL><p>
						<DT><H3 FOLDED>embedding-database</H3>
						<DL><p>
							<DT><A HREF="https://github.com/chroma-core/chroma">chroma-core/chroma: the open source embedding database</A>
							<DT><A HREF="https://www.trychroma.com/">Chroma: Open-source embedding database</A>
							<DT><A HREF="https://www.pinecone.io/">Vector Database for Vector Search | Pinecone</A>
							<DT><A HREF="https://github.com/milvus-io/milvus">milvus-io/milvus: A cloud-native vector database, storage for next generation AI applications</A>
							<DT><A HREF="https://milvus.io/">Vector database - Milvus</A>
						</DL><p>
						<DT><A HREF="https://carbon.now.sh/?bg=rgba%28171%2C+184%2C+195%2C+1%29&t=seti&wt=none&l=auto&width=680&ds=true&dsyoff=20px&dsblur=68px&wc=true&wa=true&pv=56px&ph=56px&ln=false&fl=1&fm=Hack&fs=14px&lh=133%25&si=false&es=2x&wm=false">Carbon | Create and share beautiful images of your source code</A>
						<DT><A HREF="https://twitter.com/hxiao/status/1683369244937838597">JinaAI 35 M to 6 B</A>
						<DT><A HREF="https://platform.openai.com/docs/api-reference/embeddings">API Reference - OpenAI API</A>
						<DT><A HREF="https://github.com/Gage-Technologies/embedding-server/commit/d77b9c0dd0070a3ce643f0c7fe5579524951e2ba#diff-1164ff7fc1a41ffa864162f3fca2f0407c544ece208ef32c7853b6b34c6eb445">TGI</A>
						<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/199">[Feature] Return embeddings ¬∑ Issue #199 ¬∑ huggingface/text-generation-inference</A>
						<DT><A HREF="https://www.youtube.com/watch?v=93yueQQnqpM">RetrievalQA with LLaMA 2 70b &amp; Chroma DB - YouTube</A>
						<DT><A HREF="https://github.com/HKUNLP/instructor-embedding">HKUNLP/instructor-embedding: [ACL 2023] One Embedder, Any Task: Instruction-Finetuned Text Embeddings</A>
					</DL><p>
					<DT><H3 FOLDED>sw-audio</H3>
					<DL><p>
						<DT><A HREF="https://github.com/samim23/polymath">samim23/polymath: Convert any music library into a music production sample-library with ML</A>
						<DT><A HREF="https://github.com/spotify/basic-pitch">spotify/basic-pitch: A lightweight yet powerful audio-to-MIDI converter with pitch bend detection</A>
					</DL><p>
					<DT><H3 FOLDED>sw-docs</H3>
					<DL><p>
						<DT><H3 FOLDED>markup languages</H3>
						<DL><p>
							<DT><H3 FOLDED>Markdown</H3>
							<DL><p>
								<DT><A HREF="https://support.typora.io/Markdown-Reference/">Reference</A>
								<DT><A HREF="http://theme.typora.io/">Themes Gallery ‚Äî Typora</A>
								<DT><A HREF="https://pandoc.org/#">Pandoc - About</A>
								<DT><A HREF="https://sindresorhus.com/github-markdown-css/">Manual</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>LaTeX</H3>
						<DL><p>
							<DT><A HREF="https://github.com/Armael/coq-procrastination/blob/master/manual/manual.tex">{coql}</A>
							<DT><A HREF="https://www.overleaf.com/learn/latex/Page_size_and_margins">Page size and margins</A>
							<DT><A HREF="https://www.overleaf.com/learn/latex/Paragraph_formatting">Paragraph formatting</A>
							<DT><A HREF="https://www.overleaf.com/learn/latex/Sections_and_chapters">Sections and chapters</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/36880/insert-a-blank-page-after-current-page">Insert a blank page after current page</A>
							<DT><A HREF="https://www.overleaf.com/learn/latex/Bold,_italics_and_underlining">Bold, italics and underlining - Overleaf, Online LaTeX Editor</A>
							<DT><A HREF="https://dle.rae.es/eximir?m=form">eximir | Definici√≥n | Diccionario de la lengua espa√±ola | RAE - ASALE</A>
							<DT><A HREF="http://www.personal.ceu.hu/tex/breaking.htm">Line and Page Breaking</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/325297/how-to-scale-a-tikzcd-diagram">How to scale a tikzcd diagram-adjust size</A>
							<DT><A HREF="https://tools.ietf.org/doc/texlive-doc/latex/tikz-cd/tikz-cd-doc.pdf">diagrams</A>
							<DT><A HREF="http://www.ccs.neu.edu/home/wand/csg264/latex/mathpartir/mathpartir.pdf">inference rules</A>
							<DT><A HREF="https://www.overleaf.com/project/60c202634ef716ece06a7e7b">Inference Rules ith the semantic Package</A>
							<DT><A HREF="https://www.geeksforgeeks.org/inequalities-in-latex/">Inequalities in LaTeX</A>
							<DT><A HREF="https://almdemo.polarion.com/polarion/help/index.jsp?topic=%2Fcom.polarion.xray.doc.user%2Fguide%2Fxid1661363.html">Help - Polarion ALM Platform</A>
							<DT><A HREF="https://truben.no/latex/table/">Untitled - Table Editor</A>
							<DT><A HREF="https://nasa.github.io/nasa-latex-docs/html/">NASA-LaTeX-Docs ‚Äî NASA-LaTeX-Docs documentation</A>
							<DT><A HREF="https://nasa.github.io/nasa-latex-docs/html/examples/table.html">LaTeX Tables ‚Äî NASA-LaTeX-Docs documentation</A>
							<DT><A HREF="https://people.engr.tamu.edu/hlee42/csce222/truth-table.pdf">disjunction truth table</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/61033/setting-toc-depth-not-working">table of contents - Setting TOC depth</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/422197/latex-environment-to-write-in-plain-text-mode">write in plain text mode</A>
							<DT><A HREF="https://liliaquituisacasamaniego.com/2013/03/25/latex-file-ended-while-scanning-use-of-writefile/">LaTeX ‚Äì file ended while scanning use of @writefile</A>
							<DT><A HREF="https://llevatilde.es/palabra/extr%C3%A1e">¬øLleva tilde extrae? | LlevaTilde.es</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/30757/change-the-word-chapter-to-something-else">naming - Change the word "Chapter" to something else - TeX - LaTeX Stack Exchange</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/12597/renaming-the-bibliography-page-using-bibtex">Renaming the bibliography page using BibTeX</A>
							<DT><A HREF="http://metodos.fam.cie.uva.es/~latex/apuntes/apuntes19.pdf">Bibtex</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/329707/how-to-increase-textheight-without-changing-the-topmargin">How to increase \textheight, without changing the topmargin - TeX - LaTeX Stack Exchange</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/List_of_mathematical_symbols_by_subject#Group_theory">List of mathematical symbols by subject - Wikipedia</A>
							<DT><A HREF="https://twitter.com/BorisAKnyazev/status/1531275326482956291">"Dear paper writers, consider using \ùò∂ùò¥ùò¶ùò±ùò¢ùò§ùò¨ùò¢ùò®ùò¶[ùóØùóÆùó∞ùó∏ùóøùó≤ùó≥=ùóΩùóÆùó¥ùó≤]{ùò©ùò∫ùò±ùò¶ùò≥ùò≥ùò¶ùòß} instead of simply \ùò∂ùò¥ùò¶ùò±ùò¢ùò§ùò¨ùò¢ùò®ùò¶{ùò©ùò∫ùò±ùò¶ùò≥ùò≥ùò¶ùòß}</A>
						</DL><p>
						<DT><H3 FOLDED>diverso</H3>
						<DL><p>
							<DT><A HREF="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html">Directives ‚Äî Sphinx documentation</A>
							<DT><A HREF="https://mkdocs.readthedocs.io/en/0.9/user-guide/writing-your-docs/">MkDocs</A>
							<DT><A HREF="https://docs.readthedocs.io/en/stable/tutorial/index.html#getting-started">Read the Docs tutorial ‚Äî Tutorial</A>
							<DT><A HREF="https://carla.readthedocs.io/en/latest/">CARLA Simulator (Example)</A>
							<DT><H3 FOLDED>Read the docs template</H3>
							<DL><p>
								<DT><A HREF="https://github.com/readthedocs/tutorial-template/">readthedocs/tutorial-template: Template for the Read the Docs tutorial</A>
								<DT><A HREF="https://docs.readthedocs.io/en/stable/tutorial/index.html">Read the Docs tutorial ‚Äî Read the Docs user documentation 7.6.0 documentation</A>
								<DT><A HREF="https://docs.readthedocs.io/en/stable/tutorial/index.html#getting-started">Read the Docs tutorial ‚Äî Read the Docs user documentation 7.6.0 documentation</A>
								<DT><A HREF="https://www.ericholscher.com/blog/2016/jul/1/sphinx-and-rtd-for-writers/">An introduction to Sphinx and Read the Docs for Technical Writers ‚Äî Eric Holscher</A>
								<DT><A HREF="https://github.com/diverso-lab/core/wiki/1.-Home">1. Home ¬∑ diverso-lab/core Wiki</A>
								<DT><A HREF="https://www.wordreference.com/es/translation.asp?tranword=languagje">languagje - English-Spanish Dictionary - WordReference.com</A>
							</DL><p>
							<DT><A HREF="https://www.ericholscher.com/blog/2016/jul/1/sphinx-and-rtd-for-writers/">An introduction to Sphinx and Read the Docs for Technical Writers</A>
						</DL><p>
						<DT><H3 FOLDED>tools</H3>
						<DL><p>
							<DT><A HREF="https://readthedocs.org/">Home | Read the Docs</A>
							<DT><H3 FOLDED>Github pages</H3>
							<DL><p>
								<DT><A HREF="https://jekyllrb.com/">Jekyll ‚Ä¢ Simple, blog-aware, static sites | Transform your plain text into static websites and blogs</A>
								<DT><A HREF="https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/about-github-pages-and-jekyll">Set up site with Jekyll</A>
								<DT><A HREF="https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/creating-a-github-pages-site-with-jekyll">Creating a GitHub Pages site with Jekyll - GitHub Docs</A>
							</DL><p>
							<DT><H3 FOLDED>gitbooks</H3>
							<DL><p>
								<DT><A HREF="https://www.gitbook.com/">GitBook - Where software teams break knowledge silos.</A>
								<DT><A HREF="https://docs.gitbook.com/editing-content/editing-pages/change-requests">Change requests - GitBook Documentation</A>
								<DT><A HREF="https://docs.gitbook.com/integrations/git-sync/enabling-github-sync">Enabling GitHub Sync - GitBook Documentation</A>
								<DT><A HREF="https://www.garyng.xyz/gtil-gitbook/GitBook/relative-internal-links-in-gitbook.html">Relative Internal Links in GitBook ¬∑ Today I Learned...</A>
								<DT><A HREF="https://seadude.gitbooks.io/learn-gitbook/content/chapter1/internal.html">Internal Links ¬∑ Learn Gitbook</A>
								<DT><A HREF="https://docs.gitbook.com/editing-content/rich-text">Rich text - GitBook Documentation</A>
								<DT><A HREF="https://stackoverflow.com/questions/2822089/how-to-link-to-part-of-the-same-document-in-markdown">multimarkdown - How to link to part of the same document in Markdown? - Stack Overflow</A>
							</DL><p>
							<DT><A HREF="https://www.mkdocs.org/">MkDocs</A>
							<DT><H3 FOLDED>Jekyll</H3>
							<DL><p>
								<DT><A HREF="https://jekyllrb.com/">Home</A>
								<DT><A HREF="https://jekyllrb.com/docs/installation/macos/">Jekyll installation on macOS</A>
								<DT><A HREF="https://github.com/sighingnow/jekyll-gitbook">jekyll-gitbook</A>
								<DT><A HREF="https://www.youtube.com/watch?v=HVLl8GaduPQ">Video overview</A>
								<DT><A HREF="https://jekyllrb.com/docs/front-matter/">Front Matter</A>
								<DT><A HREF="https://mademistakes.com/mastering-jekyll/how-to-link/#how-to-link">How to use URLs and links in Jekyll - Made Mistakes</A>
								<DT><A HREF="https://github.com/benbalter/jekyll-relative-links">benbalter/jekyll-relative-links: A Jekyll plugin to convert relative links to markdown files to their rendered equivalents</A>
								<DT><A HREF="https://unruffled-ardinghelli-55d901.netlify.app/">Latex Jekyll</A>
								<DT><A HREF="http://jekyllthemes.org/page2/">Jekyll Themes</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>grammarly</H3>
						<DL><p>
							<DT><A HREF="https://support.grammarly.com/hc/en-us/articles/4412828867085-My-cursor-disappeared-in-the-Grammarly-Editor-for-Windows-and-Mac">My cursor disappeared in the Grammarly Editor</A>
						</DL><p>
						<DT><A HREF="https://towardsdatascience.com/how-to-easily-draw-neural-network-architecture-diagrams-a6b6138ed875">Diagrams: Neural Network</A>
						<DT><A HREF="https://github.com/kennethleungty/Neural-Network-Architecture-Diagrams">GitHub - kennethleungty/Neural-Network-Architecture-Diagrams: Diagrams for visualizing neural network architecture (Created with diagrams.net)</A>
					</DL><p>
				</DL><p>
				<DT><H3 FOLDED>Mathematics</H3>
				<DL><p>
					<DT><H3 FOLDED>institutions</H3>
					<DL><p>
						<DT><A HREF="https://www.ias.edu/video">Institute for Advanced Study</A>
						<DT><A HREF="https://simons.berkeley.edu/">Simons Institute for the Theory of Computing</A>
						<DT><A HREF="https://plato.stanford.edu/entries/hilbert-program/">Hilbert‚Äôs Program (Stanford Encyclopedia of Philosophy)</A>
					</DL><p>
					<DT><H3 FOLDED>computing</H3>
					<DL><p>
						<DT><A HREF="https://www.abelard.org/turpap2/tp2-ie.asp">On computable numbers, with an application to the Entscheidungsproblem - A. M. Turing, 1936</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/certified+programming">certified programming</A>
					</DL><p>
					<DT><H3 FOLDED>lambda calculus</H3>
					<DL><p>
					</DL><p>
					<DT><H3 FOLDED>calculus of constructions</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/calculus+of+constructions">calculus of constructions</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/pure+type+system">pure type system</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/lambda-calculus">lambda-calculus</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/computational+trinitarianism">computational trinitarianism</A>
					</DL><p>
					<DT><H3 FOLDED>algebra</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/algebraically+injective+object">algebraically injective object</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/injection">injection</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/homomorphism">homomorphism</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/disjoint+subsets">disjoint subsets</A>
						<DT><A HREF="https://linear.axler.net/">Linear Algebra Done Right (Sheldon Axler)</A>
					</DL><p>
					<DT><H3 FOLDED>logic</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/predicate+logic">predicate logic</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/ex+falso+quodlibet">ex falso quodlibet</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/inductive+reasoning">inductive reasoning</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/proposition">proposition</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/decidable+proposition">decidable proposition</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/modus+ponens">modus ponens</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/natural+deduction">natural deduction</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/relation">relation</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/proof">proof</A>
					</DL><p>
					<DT><H3 FOLDED>calculus on manifolds</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set in nLab</A>
					</DL><p>
					<DT><H3 FOLDED>type theory</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/preimage">preimage</A>
						<DT><A HREF="https://pjreddie.com/coq-tactics/">Coq Tactic Index</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Martin-L%C3%B6f+dependent+type+theory">Martin-L√∂f dependent type theory in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Phenomenology+of+Spirit">Phenomenology of Spirit in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/semantics">semantics</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/function+extensionality">function extensionality</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/identity+type">identity type</A>
						<DT><A HREF="https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F2699407&file=p75-wadler-supp.pdf">Propositions as Types-Philip Wadler</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/dependent+type+theory">dependent type theory</A>
						<DT><A HREF="http://www-sop.inria.fr/members/Yves.Bertot/tsinghua/tsinghua-1.pdf">Introduction to dependent types in Coq</A>
						<DT><A HREF="https://medium.com/background-thread/the-future-of-programming-is-dependent-types-programming-word-of-the-day-fcd5f2634878">The Future of Programming is Dependent Types</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/type">type</A>
					</DL><p>
					<DT><H3 FOLDED>proof assistant</H3>
					<DL><p>
						<DT><A HREF="https://coq.inria.fr/distrib/current/refman/">Introduction and Contents ‚Äî Coq 8.13.0 documentation</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/ML_(programming_language)">ML (programming language) - Wikipedia</A>
						<DT><A HREF="https://ocaml.org/">OCaml ‚Äì OCaml</A>
						<DT><A HREF="https://gmplib.org/list-archives/gmp-announce/2020-November/000049.html">GMP 6.2.1 released</A>
						<DT><A HREF="https://formulae.brew.sh/formula/coq#default">coq ‚Äî Homebrew Formulae</A>
						<DT><A HREF="https://formulae.brew.sh/cask/coqide#default">coqide ‚Äî Homebrew Formulae</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/separation+algebra">separation algebra</A>
						<DT><H3 FOLDED>Coq</H3>
						<DL><p>
							<DT><A HREF="https://ncatlab.org/nlab/show/definition">definition in nLab</A>
							<DT><A HREF="https://wiki.portal.chalmers.se/cse/pmwiki.php/ForMath/ForMath">ForMath: Formalisation of Mathematics</A>
							<DT><A HREF="https://drops.dagstuhl.de/opus/volltexte/2006/432/">DROPS - Introduction to the Flyspeck Project</A>
							<DT><A HREF="https://ncatlab.org/nlab/show/Archive+of+Formal+Proofs">Archive of Formal Proofs</A>
							<DT><A HREF="https://ncatlab.org/nlab/show/Coq">Coq</A>
							<DT><A HREF="https://coq.inria.fr/distrib/current/refman/language/core/basic.html">Coq 8.13.1 documentation</A>
							<DT><A HREF="ftp://mizar.uwb.edu.pl/pub/qed/manifesto">QED manifesto</A>
						</DL><p>
					</DL><p>
					<DT><H3 FOLDED>set theory</H3>
					<DL><p>
						<DT><A HREF="https://en.wikipedia.org/wiki/Groupoid">Groupoid - Wikipedia</A>
						<DT><A HREF="https://www.pinterest.es/pin/156077943317011320/">Number theory sets</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/function">function in nLab</A>
					</DL><p>
					<DT><H3 FOLDED>category theory</H3>
					<DL><p>
						<DT><A HREF="https://arxiv.org/abs/1809.05923">What is Applied Category Theory?</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Timeline+of+category+theory+and+related+mathematics">Timeline of category theory and related mathematics</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/generalized+element">generalized element</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/morphism">morphism</A>
					</DL><p>
					<DT><H3 FOLDED>proof theory</H3>
					<DL><p>
						<DT><A HREF="https://www.google.com/search?client=safari&rls=en&sxsrf=ALeKk02CUmoD_hscdezGf0jizv1UxWm0Zw%3A1595607125474&ei=VQgbX-C-HIGOlwSBh5r4Dg&q=David+Hilbert+proof+theory&oq=David+Hilbert+proof+theory&gs_lcp=CgZwc3ktYWIQAzoHCC4QJxCTAjoHCAAQFBCHAjoCCAA6AgguOgcIIxDqAhAnOgcILhDqAhAnOgQIIxAnOgQILhBDOggILhDHARCjAjoFCC4QkQI6BwguEBQQhwI6CAguEJECEJMCOgUILhDLAToFCAAQywE6CAguEMcBEK8BOgYIABAWEB46BwghEAoQoAE6CAghEBYQHRAeOgUIIRCgAVCo4kJYjq1DYIiwQ2gIcAB4AIABtQGIAc8YkgEENS4yM5gBAKABAaoBB2d3cy13aXqwAQrAAQE&sclient=psy-ab&ved=0ahUKEwigvajfo-bqAhUBx4UKHYGDBu8Q4dUDCAs&uact=5">David Hilbert proof theory - Google Search</A>
						<DT><A HREF="https://www.marxists.org/reference/subject/philosophy/works/ge/hilbert.htm">Foundations of Mathematics By David Hilbert (1927)</A>
					</DL><p>
					<DT><H3 FOLDED>univalence foundations</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/univalence+axiom#InSimplicialSets">univalence axiom in nLab</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence">Curry‚ÄìHoward correspondence - Wikipedia</A>
						<DT><A HREF="https://www.idris-lang.org/">Idris: A Language for Type-Driven Development</A>
						<DT><A HREF="https://www.fstar-lang.org/tutorial/">F* Tutorial</A>
						<DT><A HREF="http://goto.ucsd.edu/~ravi/research/oopsla12-djs.pdf">Dependent Types for JavaScript</A>
						<DT><A HREF="http://smallcultfollowing.com/babysteps/blog/2016/11/02/associated-type-constructors-part-1-basic-concepts-and-introduction/">Associated type constructors</A>
						<DT><A HREF="https://crates.io/crates/type_level_values">type_level_values - crates.io: Rust </A>
						<DT><A HREF="https://nalgebra.org/">nalgebra linear-algebra library | nalgebra</A>
						<DT><A HREF="https://webusers.imj-prg.fr/~leila.schneps/grothendieckcircle/Letters/GS.pdf">GROTHENDIECK-SERRE CORRESPONDENCE</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Groupoid">Groupoid - Wikipedia</A>
						<DT><A HREF="https://www.probabilitycourse.com/chapter1/1_2_3_cardinality.php">Cardinality | Finite Sets | Infinite Sets </A>
						<DT><A HREF="https://www.math.ias.edu/~vladimir/Site3/Univalent_Foundations.html">Univalent Foundations of Mathematics</A>
						<DT><A HREF="https://www.math.ias.edu/~vladimir/Foundations_library/toc.html">Table of contents</A>
						<DT><A HREF="https://medium.com/background-thread/the-future-of-programming-is-dependent-types-programming-word-of-the-day-fcd5f2634878">The Future of Programming is Dependent Types</A>
						<DT><A HREF="https://flint.cs.yale.edu/flint/software.html">Yale FLINT Group: Software</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Axiomatic_system">Axiomatic system - Wikipedia</A>
						<DT><A HREF="https://www.google.com/search?client=safari&rls=en&q=Martin+lof+type+theory&ie=UTF-8&oe=UTF-8">Martin lof type theory - Google Search</A>
						<DT><A HREF="https://www.google.com/search?client=safari&rls=en&q=formal+language&ie=UTF-8&oe=UTF-8">formal language - Google Search</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/HomePage">nLab</A>
						<DT><A HREF="https://github.com/UniMath/Foundations">UniMath/Foundations: Voevodsky's original development of the univalent foundations of mathematics in Coq</A>
						<DT><A HREF="https://github.com/UniMath/UniMath">UniMath/UniMath: This coq library aims to formalize a substantial body of mathematics using the univalent point of view.</A>
						<DT><A HREF="https://github.com/EgbertRijke/HoTT-Intro">EgbertRijke/HoTT-Intro: An introductory course to Homotopy Type Theory</A>
						<DT><A HREF="https://arxiv.org/abs/1401.0053">Experimental library of univalent formalization of mathematics</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set in nLab</A>
					</DL><p>
					<DT><H3 FOLDED>homotopy type theory</H3>
					<DL><p>
						<DT><A HREF="https://en.wikipedia.org/wiki/Homotopy">Homotopy - Wikipedia</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/simplicial+complex#RemarkOnTerminology">simplicial complex in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/simplicial+complex">simplicial complex in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/simplicial+set">simplicial set</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Modus_ponens">Modus ponens - Wikipedia</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/family+of+sets">family of sets in nLab</A>
						<DT><A HREF="https://golem.ph.utexas.edu/category/2013/06/the_hott_book.html#more">The HoTT Book | The n-Category Caf√©</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/homotopy.io">homotopy.io in nLab</A>
						<DT><A HREF="http://www.andrew.cmu.edu/user/erijke/hott/">Introduction to Homotopy Type Theory</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/homotopy">homotopy in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/function+extensionality">function extensionality</A>
					</DL><p>
					<DT><H3 FOLDED>Statistics</H3>
					<DL><p>
						<DT><A HREF="http://www.stat.cmu.edu/~larry/=stat705/">10-705 Intermediate Statistics, Fall 2020</A>
						<DT><A HREF="http://pub.math.leidenuniv.nl/~szabobt/STAN.html">Statistiek AN, 2018-2019</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/permutation">permutation</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/probability+distribution">probability distribution</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/supervised+learning">supervised learning</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/probability+density">probability density</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Banach+space">Banach space</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Wasserstein+metric">Wasserstein metric</A>
						<DT><A HREF="https://www.youtube.com/watch?v=R9aMV8QIEB0">The Best Book Ever Written on Mathematical Statistics - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>TeX</H3>
					<DL><p>
						<DT><A HREF="https://sourabhbajaj.com/mac-setup/LaTeX/">LaTeX ¬∑ macOS Setup Guide</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/195291/inserting-graph-from-a-software-in-latex">Inserting graph from a software in Latex - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="https://en.wikibooks.org/wiki/LaTeX/Mathematics">LaTeX/Mathematics - Wikibooks, open books for an open world</A>
						<DT><A HREF="http://tex.loria.fr/general/mil.pdf">‚Äétex.loria.fr/general/mil.pdf</A>
						<DT><A HREF="https://en.wikibooks.org/wiki/LaTeX/Basics">LaTeX/Basics - Wikibooks, open books for an open world</A>
						<DT><A HREF="https://support.typora.io/Math/#limitations">Math and Academic Functions</A>
						<DT><A HREF="http://nickgeorge.net/programming/latex_setup/">Setting up LaTeX on a Mac</A>
						<DT><A HREF="https://www.overleaf.com/project/5fb2ebb49f6f3f20dfb6e766">Example - Online LaTeX Editor Overleaf</A>
						<DT><A HREF="https://i0.wp.com/texblog.org/Wordpress/wp-content/uploads/2008/02/font-modifications-latex.png">font-modifications</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/115432/best-practice-for-typesetting-quantifiers">spacing - Best practice for typesetting quantifiers? - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols">List of LaTeX mathematical symbols - OeisWiki</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/351660/double-superscript">Double Superscript - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="http://www.personal.ceu.hu/tex/breaking.htm">LaTeX Line and Page Breaking</A>
						<DT><A HREF="https://www.overleaf.com/learn/latex/sections_and_chapters">Sections and chapters - Overleaf, Online LaTeX Editor</A>
						<DT><A HREF="https://www.overleaf.com/learn/latex/Creating_a_document_in_LaTeX">Creating a document in LaTeX - Overleaf, Online LaTeX Editor</A>
						<DT><A HREF="https://www.lipsum.com/">Lorem Ipsum - All the facts - Lipsum generator</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/539796/subsubsubsection-paragraph-not-showing-in-toc">table of contents - Subsubsubsection (paragraph) not showing in TOC - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="https://www.overleaf.com/learn/latex/Bold,_italics_and_underlining">Bold, italics and underlining - Overleaf, Online LaTeX Editor</A>
						<DT><A HREF="https://www.overleaf.com/learn/latex/Inserting_Images">Inserting Images - Overleaf, Online LaTeX Editor</A>
						<DT><A HREF="https://stackoverflow.com/questions/3175105/inserting-code-in-this-latex-document-with-indentation">Inserting code in this LaTeX document with indentation - Stack Overflow</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/85904/showcase-of-beautiful-title-page-done-in-tex">typography - Showcase of beautiful title page done in TeX - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="https://github.com/ajvondrak/coq/blob/master/coq.tex">coq/coq.tex at master ¬∑ ajvondrak/coq</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/8357/how-to-have-local-package-override-default-package/8359#8359">Internal directories structure</A>
						<DT><A HREF="https://github.com/amunn/make-local-texmf">texmf hierarchy</A>
						<DT><A HREF="https://coq.inria.fr/refman/using/tools/coqdoc.html">coqdoc</A>
					</DL><p>
					<DT><H3 FOLDED>basic algebra</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/injection">injection in nLab</A>
					</DL><p>
					<DT><H3 FOLDED>foundations</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/foundation+of+mathematics#EilenbergSteenrod">foundation of mathematics</A>
					</DL><p>
					<DT><A HREF="https://terrytao.wordpress.com/2009/01/01/245b-notes-0-a-quick-review-of-measure-and-integration-theory/">245B, notes 0: A quick review of measure and integration theory | What's new</A>
					<DT><A HREF="https://papers.labml.ai/paper/16a1cbf8f68f11ecb9b9d35608ee6155">Pen and Paper Exercises in Machine Learning</A>
					<DT><H3 FOLDED>Sheaf Theory</H3>
					<DL><p>
					</DL><p>
					<DT><H3 FOLDED>Differential Geometry</H3>
					<DL><p>
					</DL><p>
					<DT><H3 FOLDED>Algebraic Topology</H3>
					<DL><p>
					</DL><p>
					<DT><H3 FOLDED>Information Theory</H3>
					<DL><p>
						<DT><A HREF="https://en.wikipedia.org/wiki/Julia_set">Julia set - Wikipedia</A>
					</DL><p>
					<DT><H3 FOLDED>Chaos: Non-Linear Dynamical Systems</H3>
					<DL><p>
						<DT><A HREF="https://www.wolframalpha.com/widgets/view.jsp?id=c731077c04035ac9e92a3706288db18f">Wolfram|Alpha Widgets: "Logistic Map" - Free Mathematics Widget</A>
						<DT><A HREF="https://geoffboeing.com/2015/03/chaos-theory-logistic-map/">Chaos Theory and the Logistic Map ‚Äì Geoff Boeing</A>
						<DT><A HREF="https://github.com/brorson/FeigenbaumConstants/blob/master/CAJUNTalk.pdf">FeigenbaumConstants/CAJUNTalk.pdf at master ¬∑ brorson/FeigenbaumConstants</A>
						<DT><A HREF="https://github.com/gboeing/pynamical/blob/39478f13c98d86f5fafc23875e68c99dd15fc879/pynamical/pynamical.py#L186">pynamical/pynamical.py at 39478f13c98d86f5fafc23875e68c99dd15fc879 ¬∑ gboeing/pynamical</A>
						<DT><A HREF="https://github.com/gboeing/pynamical">gboeing/pynamical: Pynamical is a Python package for modeling and visualizing discrete nonlinear dynamical systems, chaos, and fractals.</A>
					</DL><p>
				</DL><p>
				<DT><H3 FOLDED>ai-jobs</H3>
				<DL><p>
					<DT><H3 FOLDED>ai-jobs-hiring</H3>
					<DL><p>
						<DT><H3 FOLDED>popular</H3>
						<DL><p>
							<DT><A HREF="https://www.indeed.com/q-Math-Software-Engineer-jobs.html">Math Software Engineer Jobs, Employment | Indeed.com</A>
							<DT><A HREF="https://www.simplyhired.com/search?q=mathematical+software+developer&job=RsU31mOh80Jmmo7WTgmur_p1hgEjCAW3Ni0kvM1M5qdfpIHKCj7tvw">SimplyHired</A>
							<DT><A HREF="https://www.glassdoor.com/Job/remote-mathematics-jobs-SRCH_IL.0,6_IS11047_KO7,18.htm">Glassdoor</A>
							<DT><A HREF="https://engineering.atspotify.com/">Spotify Engineering</A>
							<DT><A HREF="https://www.workatastartup.com/application/preview">Y Combinator's Work at a Startup</A>
							<DT><A HREF="https://jobs.lever.co/cohere/fab6d469-b35c-4c72-be88-30fe3effe570/apply">Cohere - Machine Learning Intern (Fall 2022)</A>
						</DL><p>
						<DT><A HREF="https://boards.greenhouse.io/spacex/jobs/4867945002?gh_jid=4867945002">Job Application for Summer 2021 Associate Engineer - SpaceX</A>
						<DT><A HREF="https://jobs.raytheonmissilesanddefense.com/search-jobs?orgIds=30457&ac=22805&ascf=%5B%7B%22key%22:%22job_type%22,%22value%22:%22College%20Jobs%22%7D%5D">Search our Job Opportunities at Raytheon Technologies</A>
						<DT><A HREF="https://www.works-hub.com/login">WorksHub</A>
						<DT><A HREF="https://www.isomorphiclabs.com/blog">Isomorphic Labs | Blog</A>
						<DT><A HREF="https://www.talentbyblind.com/profile">Talent By Blind | Get More Offers</A>
						<DT><A HREF="https://www.lifeatspotify.com/students">Students | Life at Spotify</A>
						<DT><A HREF="https://adobe.wd5.myworkdayjobs.com/en-US/external_university/userHome">Adobe careers</A>
						<DT><A HREF="https://hnhiring.com/">All Jobs From Hacker News 'Who is Hiring?' Posts | HNHIRING</A>
						<DT><A HREF="https://www.spinics.net/lists/pgsql-jobs/index.html#01055">Postgresql Jobs</A>
						<DT><A HREF="https://www.wecanbeheroes.io/startup/iomed-0">IOMED jobs: Your career at one of Europe's coolest startups</A>
						<DT><A HREF="https://wasmer.io/">Wasmer - The Universal WebAssembly Runtime</A>
						<DT><A HREF="https://angel.co/jobs">Startup Jobs | AngelList Talent</A>
						<DT><A HREF="https://ai-jobs.net/">Jobs in AI/ML and Big Data | ai-jobs.net</A>
						<DT><A HREF="https://x.company/">X, the moonshot factory</A>
						<DT><A HREF="https://read.cv/open-roles">Open Roles</A>
						<DT><A HREF="https://himalayas.app/onboarding/talent">Job Seeker Sign Up | Himalayas</A>
						<DT><A HREF="https://apply.workable.com/huggingface/?lng=en">Hugging Face - Current Openings</A>
						<DT><H3 FOLDED>careers</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>how we hire</H3>
						<DL><p>
							<DT><A HREF="https://careers.google.com/how-we-hire/#step-your-resume">How we hire - Google Careers</A>
							<DT><A HREF="https://www.google.com/about/careers/applications/jobs/results/">Search Jobs ‚Äî Google Careers</A>
							<DT><A HREF="https://scholar.google.com/citations?hl=en&user=aON3WZYAAAAJ&view_op=list_works&gmla=AOV7GLM2TckIKC7Mc7ZA3FSqKOLUKYf7X8mE915uFnfQ_2qb17xgxLafbFNftpTSWd_anXcnGVCsYkOw1KOcHcUL">‚Ä™Antonio J. Dominguez‚Ä¨ - ‚Ä™Google Scholar‚Ä¨</A>
							<DT><A HREF="https://wellfound.com/jobs/2708656-principal-software-engineer-c-cuda-raptor">Principal Software Engineer C++ / CUDA (Raptor) at SpaceX ‚Ä¢ Hawthorne | Wellfound (formerly AngelList Talent)</A>
						</DL><p>
						<DT><A HREF="https://wellfound.com/jobs">Find Startup Jobs Near You and Remote Jobs | Wellfound (formerly AngelList Talent)</A>
						<DT><A HREF="https://startup.jobs/?q=ai">Startup Jobs ‚Äì Developer, designer, marketing, sales jobs, and more</A>
					</DL><p>
					<DT><A HREF="https://engineering.atspotify.com/">Spotify Engineering</A>
					<DT><A HREF="https://www.workatastartup.com/application/preview">Y Combinator's Work at a Startup</A>
					<DT><A HREF="https://jobs.lever.co/cohere/fab6d469-b35c-4c72-be88-30fe3effe570/apply">Cohere - Machine Learning Intern (Fall 2022)</A>
					<DT><A HREF="https://www.levels.fyi/internships/Cohere-ai/Software-Engineer-Intern/">Cohere.ai Software Engineer Intern Salaries | $64.00 / hr | Levels.fyi</A>
					<DT><A HREF="https://www.metacareers.com/v2/jobs/2587807648062439/">Software Engineer, Systems ML - Frameworks / Compilers / Kernels | Meta Careers</A>
					<DT><A HREF="https://www.youtube.com/watch?v=3xD_aUGKk_4">May 2024: The tech layoffs have oversaturated the USA tech job market. - YouTube</A>
					<DT><A HREF="https://www.youtube.com/watch?v=blp7dRsSgBM">Open Data Engineering Q&amp;A - YouTube</A>
				</DL><p>
				<DT><H3 FOLDED>ai-tmp</H3>
				<DL><p>
					<DT><H3 FOLDED>gRPC / Protos / TGI / Bazel</H3>
					<DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=kerKXChDmsE&t=961s">Tonic makes gRPC in Rust stupidly simple - YouTube</A>
						<DT><A HREF="https://github.com/fullstorydev/grpcurl">fullstorydev/grpcurl: Like cURL, but for gRPC: Command-line tool for interacting with gRPC servers</A>
						<DT><A HREF="https://github.com/hyperium/tonic/blob/master/examples/helloworld-tutorial.md">tonic/examples/helloworld-tutorial.md at master ¬∑ hyperium/tonic</A>
						<DT><A HREF="https://github.com/search?q=repo%3Ahuggingface%2Ftext-generation-inference%20tonic&type=code">Code search results</A>
						<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/8a5bcba227fcddff40385e8707ef94c7d6b3b561/server/text_generation_server/server.py">text-generation-inference/server/text_generation_server/server.py at 8a5bcba227fcddff40385e8707ef94c7d6b3b561 ¬∑ huggingface/text-generation-inference</A>
						<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/Makefile">text-generation-inference/server/Makefile at main ¬∑ huggingface/text-generation-inference</A>
						<DT><A HREF="https://typer.tiangolo.com/">Typer</A>
						<DT><A HREF="https://protobuf.dev/overview/">Overview | Protocol Buffers Documentation</A>
						<DT><A HREF="https://github.com/grpc/grpc/tree/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/python/uds">grpc/examples/python/uds at d68161a64f191b8d8d5afe0507e7a2291f91ff1a ¬∑ grpc/grpc</A>
						<DT><A HREF="https://grpc.io/docs/languages/python/quickstart/">Quick start | Python | gRPC</A>
						<DT><A HREF="https://github.com/grpc/grpc/blob/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/protos/BUILD">grpc/examples/protos/BUILD at d68161a64f191b8d8d5afe0507e7a2291f91ff1a ¬∑ grpc/grpc</A>
						<DT><A HREF="https://chat.openai.com/c/f2b6f035-383c-4639-a4ee-0963305dee4f">ChatGPT</A>
						<DT><A HREF="https://bazel.build/run/bazelrc">Write bazelrc configuration files ¬†|¬† Bazel</A>
					</DL><p>
					<DT><H3 FOLDED>cutlass &amp; kernel opts / Bazel &amp; Starlark</H3>
					<DL><p>
						<DT><A HREF="https://docs.google.com/document/d/1EhHx1-vVOb8aTcreHIgmrU4JNNmqCsAmniA7j8Lj8Pc/edit">https://docs.google.com/document/d/1EhHx1-vVOb8aTcreHIgmrU4JNNmqCsAmniA7j8Lj8Pc/edit</A>
						<DT><A HREF="https://drive.google.com/drive/u/5/folders/1cWttjKQTTX1GsDEDy9KgG-yHNERuE0VH">Programming Model - Efficient AI - Google Drive</A>
						<DT><A HREF="https://research.colfax-intl.com/">Colfax Research ‚Äì Contributing to Innovations in Computing</A>
						<DT><A HREF="https://research.colfax-intl.com/wp-content/uploads/2024/01/layout_algebra.pdf">https://research.colfax-intl.com/wp-content/uploads/2024/01/layout_algebra.pdf</A>
						<DT><A HREF="https://research.colfax-intl.com/wp-content/uploads/2023/12/colfax-flashattention.pdf">https://research.colfax-intl.com/wp-content/uploads/2023/12/colfax-flashattention.pdf</A>
						<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/">https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance | by Pierre Lienhart | Feb, 2024 | Medium</A>
						<DT><A HREF="https://bazel.build/start/cpp">Bazel Tutorial: Build a C++ Project</A>
						<DT><A HREF="https://github.com/bazelbuild/rules_cc/blob/bbb0615a8728fe2ee790296f12b325b6d42583a7/examples/my_c_compile/my_c_compile.bzl">rules_cc/examples/my_c_compile/my_c_compile.bzl at bbb0615a8728fe2ee790296f12b325b6d42583a7 ¬∑ bazelbuild/rules_cc</A>
						<DT><A HREF="https://github.com/bazelbuild/starlark/blob/master/users.md">starlark/users.md at master ¬∑ bazelbuild/starlark</A>
						<DT><A HREF="https://github.com/bazelbuild/examples/blob/main/cpp-tutorial/stage1/main/hello-world.cc">examples/cpp-tutorial/stage1/main/hello-world.cc at main ¬∑ bazelbuild/examples</A>
						<DT><A HREF="https://bazel.build/tutorials/cpp-dependency">Review the dependency graph ¬†|¬† Bazel</A>
						<DT><A HREF="https://mail.google.com/mail/u/1/#inbox?compose=DmwnWrRlQhjWPQkzQHTzgSzxrcjhHfrsZMPWMKfGtPPHGFRMJNVZnsWwgVRcdQBvcdXSZdZHnkjQ">Inbox - antonio.jfdominguez1@gmail.com - Gmail</A>
					</DL><p>
				</DL><p>
			</DL><p>
			<DT><H3 FOLDED>software foundations</H3>
			<DL><p>
				<DT><H3 FOLDED>software-foundations-exercises</H3>
				<DL><p>
					<DT><A HREF="https://github.com/frankYaohua/software-foundations/blob/master/Basics.v">software-foundations/Basics.v at master ¬∑ frankYaohua/software-foundations</A>
					<DT><A HREF="https://www.seas.upenn.edu/~cis500/current/exams/index.html">Directory Index</A>
					<DT><A HREF="https://github.com/coq-community/coq-art/blob/v8.13.0/ch5_everydays_logic/SRC/class.v">coq-art/class.v at v8.13.0 ¬∑ coq-community/coq-art</A>
					<DT><A HREF="https://github.com/SatyendraBanjare/software-foundations">Repository 2</A>
					<DT><A HREF="https://github.com/paulkoerbitz/software-foundations">Repository 3</A>
					<DT><A HREF="https://github.com/StevenWongChess/software-foundations/blob/master/IndProp.v">Repository 4</A>
					<DT><A HREF="https://www.cs.uic.edu/~mansky/teaching/cs494sf/sp19/exams.html">CS 494 SF: Software Foundations ¬∑ Exams</A>
					<DT><A HREF="https://github.com/steven7woo/Coq-CIS500/blob/master/For_wenrui/Since_HW6/Imp.v">Coq-CIS500/Imp.v at master ¬∑ steven7woo/Coq-CIS500</A>
					<DT><A HREF="https://github.com/coq-community/coq-art/blob/v8.13.0/ch2_types_expressions/SRC/Zbtree.v">Binary tree</A>
					<DT><A HREF="https://github.com/bishboria/software-foundations/blob/master/Imp.v">Imp poset</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-environment</H3>
				<DL><p>
					<DT><A HREF="https://proofgeneral.github.io/">Proof General</A>
					<DT><A HREF="https://scholar.princeton.edu/scuellar/blog/how-i-set-my-mac-brew-opam-emacs-and-coq">How I set up my mac (brew, opam, emacs and coq)</A>
					<DT><A HREF="https://www.dc.fi.udc.es/staff/freire/coqdoc/pauillac.inria.fr/coq/doc/no15.htm">5.4¬†Compiled files</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-lectures</H3>
				<DL><p>
					<DT><A HREF="https://deepspec.org/event/dsss18/">DeepSpec: The Science of Deep Specification</A>
					<DT><A HREF="https://www.labri.fr/perso/casteran/CoqArt/">Coq'Art Home page</A>
					<DT><A HREF="https://github.com/plclub/cis670-16fa/blob/master/notes/IntuitionisticPropositionalLogicLecture.pdf">Introduction to Intuitionistic Propositional Logic</A>
					<DT><A HREF="https://www.nii.ac.jp/event/upload/talk-ShonanMeetings100th.pdf">Principles, examples and main applications</A>
					<DT><A HREF="https://www.ercim.eu/publication/Ercim_News/enw36/filiatre.html">Certification of Imperative Programs in Coq</A>
					<DT><A HREF="https://www.brics.dk/RS/97/18/BRICS-RS-97-18.pdf">How to Believe a Machine-Checked Proof</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-theory</H3>
				<DL><p>
					<DT><A HREF="https://ncatlab.org/nlab/show/certified+programming">certified programming</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-self-modified-code</H3>
				<DL><p>
					<DT><A HREF="https://authors.library.caltech.edu/">Welcome to CaltechAUTHORS - CaltechAUTHORS</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-resources</H3>
				<DL><p>
					<DT><A HREF="https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/fisher">Using Formal Methods to Eliminate Exploitable Bugs | USENIX</A>
					<DT><A HREF="https://madiot.fr/files/reportru.pdf">Specification of imperative languages using operational semantics in Coq</A>
					<DT><A HREF="http://why3.lri.fr/">Why3</A>
					<DT><A HREF="https://hal.inria.fr/hal-01094195/document">Intro to the Calculus of Inductive Constructions</A>
					<DT><A HREF="http://www.cse.chalmers.se/research/group/logic/TypesSS05/Extra/filliatre.pdf">HL</A>
					<DT><A HREF="http://nickbenton.name/coqasm.pdf">x86</A>
					<DT><A HREF="https://www.lri.fr/~paulin/LASER/course-notes.pdf">Coq overview</A>
					<DT><A HREF="https://www.dragonwasrobot.com/mathematics/2015/09/26/an-interpreter-a-compiler-and-a-virtual-machine.html">An interpreter, a compiler, and a virtual machine</A>
					<DT><A HREF="https://link.springer.com/chapter/10.1007/978-3-540-70594-9_3">Next generation programming languages</A>
				</DL><p>
				<DT><A HREF="https://softwarefoundations.cis.upenn.edu/">Software Foundations</A>
				<DT><A HREF="http://www.cs.cornell.edu/courses/cs4160/2020sp/sf/lf/terse/toc.html">Alternative version</A>
				<DT><A HREF="https://xavierleroy.org/courses/Eugene-2011/">Proving a Compiler</A>
				<DT><A HREF="https://nickdrozd.github.io/">Something Something Programming | Mostly thoughts about programming. Maybe other stuff too.</A>
			</DL><p>
		</DL><p>
		<DT><H3 FOLDED>Mathematics</H3>
		<DL><p>
			<DT><H3 FOLDED>institutions</H3>
			<DL><p>
				<DT><A HREF="https://www.ias.edu/video">Institute for Advanced Study</A>
				<DT><A HREF="https://simons.berkeley.edu/">Simons Institute for the Theory of Computing</A>
				<DT><A HREF="https://plato.stanford.edu/entries/hilbert-program/">Hilbert‚Äôs Program (Stanford Encyclopedia of Philosophy)</A>
			</DL><p>
			<DT><H3 FOLDED>computing</H3>
			<DL><p>
				<DT><A HREF="https://www.abelard.org/turpap2/tp2-ie.asp">On computable numbers, with an application to the Entscheidungsproblem - A. M. Turing, 1936</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/certified+programming">certified programming</A>
			</DL><p>
			<DT><H3 FOLDED>lambda calculus</H3>
			<DL><p>
			</DL><p>
			<DT><H3 FOLDED>calculus of constructions</H3>
			<DL><p>
				<DT><A HREF="https://ncatlab.org/nlab/show/calculus+of+constructions">calculus of constructions</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/pure+type+system">pure type system</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/lambda-calculus">lambda-calculus</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/computational+trinitarianism">computational trinitarianism</A>
			</DL><p>
			<DT><H3 FOLDED>algebra</H3>
			<DL><p>
				<DT><A HREF="https://ncatlab.org/nlab/show/algebraically+injective+object">algebraically injective object</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/injection">injection</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/homomorphism">homomorphism</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/disjoint+subsets">disjoint subsets</A>
				<DT><A HREF="https://linear.axler.net/">Linear Algebra Done Right (Sheldon Axler)</A>
			</DL><p>
			<DT><H3 FOLDED>logic</H3>
			<DL><p>
				<DT><A HREF="https://ncatlab.org/nlab/show/predicate+logic">predicate logic</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/ex+falso+quodlibet">ex falso quodlibet</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/inductive+reasoning">inductive reasoning</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/proposition">proposition</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/decidable+proposition">decidable proposition</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/modus+ponens">modus ponens</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/natural+deduction">natural deduction</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/relation">relation</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/proof">proof</A>
			</DL><p>
			<DT><H3 FOLDED>calculus on manifolds</H3>
			<DL><p>
				<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set in nLab</A>
			</DL><p>
			<DT><H3 FOLDED>type theory</H3>
			<DL><p>
				<DT><A HREF="https://ncatlab.org/nlab/show/preimage">preimage</A>
				<DT><A HREF="https://pjreddie.com/coq-tactics/">Coq Tactic Index</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/Martin-L%C3%B6f+dependent+type+theory">Martin-L√∂f dependent type theory in nLab</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/Phenomenology+of+Spirit">Phenomenology of Spirit in nLab</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/semantics">semantics</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/function+extensionality">function extensionality</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/identity+type">identity type</A>
				<DT><A HREF="https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F2699407&file=p75-wadler-supp.pdf">Propositions as Types-Philip Wadler</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/dependent+type+theory">dependent type theory</A>
				<DT><A HREF="http://www-sop.inria.fr/members/Yves.Bertot/tsinghua/tsinghua-1.pdf">Introduction to dependent types in Coq</A>
				<DT><A HREF="https://medium.com/background-thread/the-future-of-programming-is-dependent-types-programming-word-of-the-day-fcd5f2634878">The Future of Programming is Dependent Types</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/type">type</A>
			</DL><p>
			<DT><H3 FOLDED>proof assistant</H3>
			<DL><p>
				<DT><A HREF="https://coq.inria.fr/distrib/current/refman/">Introduction and Contents ‚Äî Coq 8.13.0 documentation</A>
				<DT><A HREF="https://en.wikipedia.org/wiki/ML_(programming_language)">ML (programming language) - Wikipedia</A>
				<DT><A HREF="https://ocaml.org/">OCaml ‚Äì OCaml</A>
				<DT><A HREF="https://gmplib.org/list-archives/gmp-announce/2020-November/000049.html">GMP 6.2.1 released</A>
				<DT><A HREF="https://formulae.brew.sh/formula/coq#default">coq ‚Äî Homebrew Formulae</A>
				<DT><A HREF="https://formulae.brew.sh/cask/coqide#default">coqide ‚Äî Homebrew Formulae</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/separation+algebra">separation algebra</A>
				<DT><H3 FOLDED>Coq</H3>
				<DL><p>
					<DT><A HREF="https://ncatlab.org/nlab/show/definition">definition in nLab</A>
					<DT><A HREF="https://wiki.portal.chalmers.se/cse/pmwiki.php/ForMath/ForMath">ForMath: Formalisation of Mathematics</A>
					<DT><A HREF="https://drops.dagstuhl.de/opus/volltexte/2006/432/">DROPS - Introduction to the Flyspeck Project</A>
					<DT><A HREF="https://ncatlab.org/nlab/show/Archive+of+Formal+Proofs">Archive of Formal Proofs</A>
					<DT><A HREF="https://ncatlab.org/nlab/show/Coq">Coq</A>
					<DT><A HREF="https://coq.inria.fr/distrib/current/refman/language/core/basic.html">Coq 8.13.1 documentation</A>
					<DT><A HREF="ftp://mizar.uwb.edu.pl/pub/qed/manifesto">QED manifesto</A>
				</DL><p>
			</DL><p>
			<DT><H3 FOLDED>set theory</H3>
			<DL><p>
				<DT><A HREF="https://en.wikipedia.org/wiki/Groupoid">Groupoid - Wikipedia</A>
				<DT><A HREF="https://www.pinterest.es/pin/156077943317011320/">Number theory sets</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set in nLab</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/function">function in nLab</A>
			</DL><p>
			<DT><H3 FOLDED>category theory</H3>
			<DL><p>
				<DT><A HREF="https://arxiv.org/abs/1809.05923">What is Applied Category Theory?</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/Timeline+of+category+theory+and+related+mathematics">Timeline of category theory and related mathematics</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/generalized+element">generalized element</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/morphism">morphism</A>
			</DL><p>
			<DT><H3 FOLDED>proof theory</H3>
			<DL><p>
				<DT><A HREF="https://www.google.com/search?client=safari&rls=en&sxsrf=ALeKk02CUmoD_hscdezGf0jizv1UxWm0Zw%3A1595607125474&ei=VQgbX-C-HIGOlwSBh5r4Dg&q=David+Hilbert+proof+theory&oq=David+Hilbert+proof+theory&gs_lcp=CgZwc3ktYWIQAzoHCC4QJxCTAjoHCAAQFBCHAjoCCAA6AgguOgcIIxDqAhAnOgcILhDqAhAnOgQIIxAnOgQILhBDOggILhDHARCjAjoFCC4QkQI6BwguEBQQhwI6CAguEJECEJMCOgUILhDLAToFCAAQywE6CAguEMcBEK8BOgYIABAWEB46BwghEAoQoAE6CAghEBYQHRAeOgUIIRCgAVCo4kJYjq1DYIiwQ2gIcAB4AIABtQGIAc8YkgEENS4yM5gBAKABAaoBB2d3cy13aXqwAQrAAQE&sclient=psy-ab&ved=0ahUKEwigvajfo-bqAhUBx4UKHYGDBu8Q4dUDCAs&uact=5">David Hilbert proof theory - Google Search</A>
				<DT><A HREF="https://www.marxists.org/reference/subject/philosophy/works/ge/hilbert.htm">Foundations of Mathematics By David Hilbert (1927)</A>
			</DL><p>
			<DT><H3 FOLDED>univalence foundations</H3>
			<DL><p>
				<DT><A HREF="https://ncatlab.org/nlab/show/univalence+axiom#InSimplicialSets">univalence axiom in nLab</A>
				<DT><A HREF="https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence">Curry‚ÄìHoward correspondence - Wikipedia</A>
				<DT><A HREF="https://www.idris-lang.org/">Idris: A Language for Type-Driven Development</A>
				<DT><A HREF="https://www.fstar-lang.org/tutorial/">F* Tutorial</A>
				<DT><A HREF="http://goto.ucsd.edu/~ravi/research/oopsla12-djs.pdf">Dependent Types for JavaScript</A>
				<DT><A HREF="http://smallcultfollowing.com/babysteps/blog/2016/11/02/associated-type-constructors-part-1-basic-concepts-and-introduction/">Associated type constructors</A>
				<DT><A HREF="https://crates.io/crates/type_level_values">type_level_values - crates.io: Rust </A>
				<DT><A HREF="https://nalgebra.org/">nalgebra linear-algebra library | nalgebra</A>
				<DT><A HREF="https://webusers.imj-prg.fr/~leila.schneps/grothendieckcircle/Letters/GS.pdf">GROTHENDIECK-SERRE CORRESPONDENCE</A>
				<DT><A HREF="https://en.wikipedia.org/wiki/Groupoid">Groupoid - Wikipedia</A>
				<DT><A HREF="https://www.probabilitycourse.com/chapter1/1_2_3_cardinality.php">Cardinality | Finite Sets | Infinite Sets </A>
				<DT><A HREF="https://www.math.ias.edu/~vladimir/Site3/Univalent_Foundations.html">Univalent Foundations of Mathematics</A>
				<DT><A HREF="https://www.math.ias.edu/~vladimir/Foundations_library/toc.html">Table of contents</A>
				<DT><A HREF="https://medium.com/background-thread/the-future-of-programming-is-dependent-types-programming-word-of-the-day-fcd5f2634878">The Future of Programming is Dependent Types</A>
				<DT><A HREF="https://flint.cs.yale.edu/flint/software.html">Yale FLINT Group: Software</A>
				<DT><A HREF="https://en.wikipedia.org/wiki/Axiomatic_system">Axiomatic system - Wikipedia</A>
				<DT><A HREF="https://www.google.com/search?client=safari&rls=en&q=Martin+lof+type+theory&ie=UTF-8&oe=UTF-8">Martin lof type theory - Google Search</A>
				<DT><A HREF="https://www.google.com/search?client=safari&rls=en&q=formal+language&ie=UTF-8&oe=UTF-8">formal language - Google Search</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/HomePage">nLab</A>
				<DT><A HREF="https://github.com/UniMath/Foundations">UniMath/Foundations: Voevodsky's original development of the univalent foundations of mathematics in Coq</A>
				<DT><A HREF="https://github.com/UniMath/UniMath">UniMath/UniMath: This coq library aims to formalize a substantial body of mathematics using the univalent point of view.</A>
				<DT><A HREF="https://github.com/EgbertRijke/HoTT-Intro">EgbertRijke/HoTT-Intro: An introductory course to Homotopy Type Theory</A>
				<DT><A HREF="https://arxiv.org/abs/1401.0053">Experimental library of univalent formalization of mathematics</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set in nLab</A>
			</DL><p>
			<DT><H3 FOLDED>homotopy type theory</H3>
			<DL><p>
				<DT><A HREF="https://en.wikipedia.org/wiki/Homotopy">Homotopy - Wikipedia</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/simplicial+complex#RemarkOnTerminology">simplicial complex in nLab</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/simplicial+complex">simplicial complex in nLab</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/simplicial+set">simplicial set</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set</A>
				<DT><A HREF="https://en.wikipedia.org/wiki/Modus_ponens">Modus ponens - Wikipedia</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/family+of+sets">family of sets in nLab</A>
				<DT><A HREF="https://golem.ph.utexas.edu/category/2013/06/the_hott_book.html#more">The HoTT Book | The n-Category Caf√©</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/homotopy.io">homotopy.io in nLab</A>
				<DT><A HREF="http://www.andrew.cmu.edu/user/erijke/hott/">Introduction to Homotopy Type Theory</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/homotopy">homotopy in nLab</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/function+extensionality">function extensionality</A>
			</DL><p>
			<DT><H3 FOLDED>Statistics</H3>
			<DL><p>
				<DT><A HREF="http://www.stat.cmu.edu/~larry/=stat705/">10-705 Intermediate Statistics, Fall 2020</A>
				<DT><A HREF="http://pub.math.leidenuniv.nl/~szabobt/STAN.html">Statistiek AN, 2018-2019</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/permutation">permutation</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/probability+distribution">probability distribution</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/supervised+learning">supervised learning</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/probability+density">probability density</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/Banach+space">Banach space</A>
				<DT><A HREF="https://ncatlab.org/nlab/show/Wasserstein+metric">Wasserstein metric</A>
				<DT><A HREF="https://www.youtube.com/watch?v=R9aMV8QIEB0">The Best Book Ever Written on Mathematical Statistics - YouTube</A>
			</DL><p>
			<DT><H3 FOLDED>TeX</H3>
			<DL><p>
				<DT><A HREF="https://sourabhbajaj.com/mac-setup/LaTeX/">LaTeX ¬∑ macOS Setup Guide</A>
				<DT><A HREF="https://tex.stackexchange.com/questions/195291/inserting-graph-from-a-software-in-latex">Inserting graph from a software in Latex - TeX - LaTeX Stack Exchange</A>
				<DT><A HREF="https://en.wikibooks.org/wiki/LaTeX/Mathematics">LaTeX/Mathematics - Wikibooks, open books for an open world</A>
				<DT><A HREF="http://tex.loria.fr/general/mil.pdf">‚Äétex.loria.fr/general/mil.pdf</A>
				<DT><A HREF="https://en.wikibooks.org/wiki/LaTeX/Basics">LaTeX/Basics - Wikibooks, open books for an open world</A>
				<DT><A HREF="https://support.typora.io/Math/#limitations">Math and Academic Functions</A>
				<DT><A HREF="http://nickgeorge.net/programming/latex_setup/">Setting up LaTeX on a Mac</A>
				<DT><A HREF="https://www.overleaf.com/project/5fb2ebb49f6f3f20dfb6e766">Example - Online LaTeX Editor Overleaf</A>
				<DT><A HREF="https://i0.wp.com/texblog.org/Wordpress/wp-content/uploads/2008/02/font-modifications-latex.png">font-modifications</A>
				<DT><A HREF="https://tex.stackexchange.com/questions/115432/best-practice-for-typesetting-quantifiers">spacing - Best practice for typesetting quantifiers? - TeX - LaTeX Stack Exchange</A>
				<DT><A HREF="https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols">List of LaTeX mathematical symbols - OeisWiki</A>
				<DT><A HREF="https://tex.stackexchange.com/questions/351660/double-superscript">Double Superscript - TeX - LaTeX Stack Exchange</A>
				<DT><A HREF="http://www.personal.ceu.hu/tex/breaking.htm">LaTeX Line and Page Breaking</A>
				<DT><A HREF="https://www.overleaf.com/learn/latex/sections_and_chapters">Sections and chapters - Overleaf, Online LaTeX Editor</A>
				<DT><A HREF="https://www.overleaf.com/learn/latex/Creating_a_document_in_LaTeX">Creating a document in LaTeX - Overleaf, Online LaTeX Editor</A>
				<DT><A HREF="https://www.lipsum.com/">Lorem Ipsum - All the facts - Lipsum generator</A>
				<DT><A HREF="https://tex.stackexchange.com/questions/539796/subsubsubsection-paragraph-not-showing-in-toc">table of contents - Subsubsubsection (paragraph) not showing in TOC - TeX - LaTeX Stack Exchange</A>
				<DT><A HREF="https://www.overleaf.com/learn/latex/Bold,_italics_and_underlining">Bold, italics and underlining - Overleaf, Online LaTeX Editor</A>
				<DT><A HREF="https://www.overleaf.com/learn/latex/Inserting_Images">Inserting Images - Overleaf, Online LaTeX Editor</A>
				<DT><A HREF="https://stackoverflow.com/questions/3175105/inserting-code-in-this-latex-document-with-indentation">Inserting code in this LaTeX document with indentation - Stack Overflow</A>
				<DT><A HREF="https://tex.stackexchange.com/questions/85904/showcase-of-beautiful-title-page-done-in-tex">typography - Showcase of beautiful title page done in TeX - TeX - LaTeX Stack Exchange</A>
				<DT><A HREF="https://github.com/ajvondrak/coq/blob/master/coq.tex">coq/coq.tex at master ¬∑ ajvondrak/coq</A>
				<DT><A HREF="https://tex.stackexchange.com/questions/8357/how-to-have-local-package-override-default-package/8359#8359">Internal directories structure</A>
				<DT><A HREF="https://github.com/amunn/make-local-texmf">texmf hierarchy</A>
				<DT><A HREF="https://coq.inria.fr/refman/using/tools/coqdoc.html">coqdoc</A>
			</DL><p>
			<DT><H3 FOLDED>basic algebra</H3>
			<DL><p>
				<DT><A HREF="https://ncatlab.org/nlab/show/injection">injection in nLab</A>
			</DL><p>
			<DT><H3 FOLDED>foundations</H3>
			<DL><p>
				<DT><A HREF="https://ncatlab.org/nlab/show/foundation+of+mathematics#EilenbergSteenrod">foundation of mathematics</A>
			</DL><p>
			<DT><A HREF="https://terrytao.wordpress.com/2009/01/01/245b-notes-0-a-quick-review-of-measure-and-integration-theory/">245B, notes 0: A quick review of measure and integration theory | What's new</A>
			<DT><A HREF="https://papers.labml.ai/paper/16a1cbf8f68f11ecb9b9d35608ee6155">Pen and Paper Exercises in Machine Learning</A>
			<DT><H3 FOLDED>Sheaf Theory</H3>
			<DL><p>
			</DL><p>
			<DT><H3 FOLDED>Differential Geometry</H3>
			<DL><p>
			</DL><p>
			<DT><H3 FOLDED>Algebraic Topology</H3>
			<DL><p>
			</DL><p>
			<DT><H3 FOLDED>Information Theory</H3>
			<DL><p>
				<DT><A HREF="https://en.wikipedia.org/wiki/Julia_set">Julia set - Wikipedia</A>
			</DL><p>
			<DT><H3 FOLDED>Chaos: Non-Linear Dynamical Systems</H3>
			<DL><p>
				<DT><A HREF="https://www.wolframalpha.com/widgets/view.jsp?id=c731077c04035ac9e92a3706288db18f">Wolfram|Alpha Widgets: "Logistic Map" - Free Mathematics Widget</A>
				<DT><A HREF="https://geoffboeing.com/2015/03/chaos-theory-logistic-map/">Chaos Theory and the Logistic Map ‚Äì Geoff Boeing</A>
				<DT><A HREF="https://github.com/brorson/FeigenbaumConstants/blob/master/CAJUNTalk.pdf">FeigenbaumConstants/CAJUNTalk.pdf at master ¬∑ brorson/FeigenbaumConstants</A>
				<DT><A HREF="https://github.com/gboeing/pynamical/blob/39478f13c98d86f5fafc23875e68c99dd15fc879/pynamical/pynamical.py#L186">pynamical/pynamical.py at 39478f13c98d86f5fafc23875e68c99dd15fc879 ¬∑ gboeing/pynamical</A>
				<DT><A HREF="https://github.com/gboeing/pynamical">gboeing/pynamical: Pynamical is a Python package for modeling and visualizing discrete nonlinear dynamical systems, chaos, and fractals.</A>
			</DL><p>
		</DL><p>
		<DT><H3 FOLDED>Saved Tabs</H3>
		<DL><p>
			<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt/tree/master">cloneofsimo/min-max-gpt: Minimal (400 LOC) implementation Maximum (multi-node, FSDP) GPT training</A>
			<DT><A HREF="https://people.cs.umass.edu/~miyyer/cs685/schedule.html">https://people.cs.umass.edu/~miyyer/cs685/schedule.html</A>
			<DT><A HREF="https://stanford-cs324.github.io/winter2022/lectures/introduction/">https://stanford-cs324.github.io/winter2022/lectures/introduction/</A>
			<DT><A HREF="https://phontron.com/class/anlp2024/lectures/#sequence-modeling-jan-25">Lectures | 11-711 ANLP</A>
			<DT><A HREF="https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf">https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf</A>
			<DT><A HREF="https://www.jasonwei.net/blog/common-arguments-regarding-emergent-abilities">Common arguments regarding emergent abilities ‚Äî Jason Wei</A>
			<DT><A HREF="https://www.youtube.com/watch?v=3gb-ZkVRemQ&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=27">Stanford CS25: V4 I Jason Wei &amp; Hyung Won Chung of OpenAI - YouTube</A>
			<DT><A HREF="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g2885e521b53_0_0">Language Language Models (in 2023) - Google Slides</A>
			<DT><A HREF="https://x.com/hwchung27/status/1800676312916656592">(1) Hyung Won Chung en X: "I gave a lecture at @Stanford CS 25. Lecture video: https://t.co/0mexY32eLI AI is moving so fast that it's hard to keep up. Instead of spending all our energy catching up with the latest development, we should study the change itself. First step is to identify and understand... https://t.co/no218Rx3Yb" / X</A>
			<DT><A HREF="https://pytorch.org/blog/maximizing-training-throughput/?utm_content=293931524&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Maximizing Training Throughput Using PyTorch FSDP and Torch.compile | PyTorch</A>
			<DT><A HREF="https://github.com/pytorch/torchtitan/tree/main">pytorch/torchtitan: A native PyTorch Library for large model training</A>
		</DL><p>
	</DL><p>
</HTML>
