<!DOCTYPE NETSCAPE-Bookmark-file-1>
	<HTML>
	<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=UTF-8">
	<Title>Bookmarks</Title>
	<H1>Bookmarks</H1>
	<DT><H3 FOLDED>Favourites</H3>
	<DL><p>
		<DT><A HREF="https://www.google.com/?client=safari&channel=mac_bm">Google</A>
		<DT><A HREF="https://www.wikipedia.org/">Wikipedia</A>
		<DT><A HREF="https://www.apple.com/es">Apple</A>
		<DT><A HREF="https://www.bing.com/">Bing</A>
		<DT><A HREF="https://www.google.com/?client=safari&channel=ipad_bm">Google</A>
		<DT><A HREF="https://es.yahoo.com/">Yahoo</A>
	</DL><p>
	<DT><H3 FOLDED>Bookmarks Menu</H3>
	<DL><p>
	</DL><p>
	<DT><H3 FOLDED>Tab Group Favourites</H3>
	<DL><p>
	</DL><p>
	<DT><H3 FOLDED id="com.apple.ReadingList">Reading List</H3>
	<DL><p>
		<DT><A HREF="https://github.com/pytorch-labs/tritonbench">TritonBench</A>
		<DT><A HREF="https://github.com/S-LoRA/S-LoRA">S-LoRA: Serving Thousands of Concurrent LoRA Adapters [paper]</A>
		<DT><A HREF="https://github.com/triton-lang/triton/issues/4868">[PROTON][CUPTI_PCSAMPLING]: RuntimeError: Failed to execute cuptiGetContextId with error 35 · Issue #4868 · triton-lang/triton</A>
		<DT><A HREF="https://github.com/Jokeren/triton-samples/blob/main/Triton_Tools_Tutorial.ipynb">triton-samples/Triton_Tools_Tutorial.ipynb at main · Jokeren/triton-samples</A>
		<DT><A HREF="https://github.com/pytorch/pytorch/blob/20af56d4359c3f5fed2e8f94e111a8502f2ebeb3/test/test_flop_counter.py#L736">pytorch/test/test_flop_counter.py at 20af56d4359c3f5fed2e8f94e111a8502f2ebeb3 · pytorch/pytorch</A>
		<DT><A HREF="https://github.com/pytorch/ao/blob/6b529961bd0b41953d26cdde5851f460839d5cf6/torchao/profiler/device_spec.py#L111">ao/torchao/profiler/device_spec.py at 6b529961bd0b41953d26cdde5851f460839d5cf6 · pytorch/ao</A>
		<DT><A HREF="https://github.com/microsoft/microxcaling?tab=readme-ov-file#Spec-Configuration">MX Pytorch Emulation Library</A>
		<DT><A HREF="https://astralord.github.io/posts/transformer-inference-optimization-toolset/">Transformers Inference Optimization Toolset</A>
		<DT><A HREF="https://colab.research.google.com/drive/1XQwio7DsqB5LP2D574f_uIb8G7KhirNa?usp=sharing#scrollTo=P5sS86KpdaTI">Google Colab</A>
		<DT><A HREF="https://github.com/pytorch/workshops/blob/master/ASPLOS_2024/inductor.pdf">workshops/ASPLOS_2024/inductor.pdf at master · pytorch/workshops</A>
		<DT><A HREF="https://github.com/ai-compiler-study/flash-attention/blob/main/extra/benchmark_flash_attention_v3.py#L116">flash-attention/extra/benchmark_flash_attention_v3.py at main · ai-compiler-study/flash-attention</A>
		<DT><A HREF="https://github.com/facebookresearch/xformers/tree/4a9dd7ec079e0c935db10daa2a1a89fd19cfa231">facebookresearch/xformers at 4a9dd7ec079e0c935db10daa2a1a89fd19cfa231</A>
		<DT><A HREF="https://gist.github.com/cloneofsimo/af610ff8aa11a3f57956e7d7f578409c">FlashAttention comparison</A>
		<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/main/kernels/triton/inference/gptq/a100_qlinear.py">applied-ai/kernels/triton/inference/gptq/a100_qlinear.py at main · pytorch-labs/applied-ai</A>
		<DT><A HREF="https://carpedm30.notion.site/FLUX-Triton-Implementation-7f021f6de14f4fe8a5331bc6e778168a">Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.</A>
		<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/blob/master/pipeline-gemm/README.md">CUTLASS Hopper GEMMs with pipelined kernel design</A>
		<DT><A HREF="https://github.com/search?q=repo%3ANVIDIA%2FTensorRT-Model-Optimizer%20FLUX&type=code">Code search results</A>
		<DT><A HREF="https://fkong.tech/posts/">Posts · fkong' tech blog</A>
		<DT><A HREF="https://discuss.pytorch.org/t/getting-triton-to-generate-all-kernels/189173/4">https://discuss.pytorch.org/t/getting-triton-to-generate-all-kernels/189173/4</A>
		<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html#benchmark-individual-triton-kernel">TorchInductor GPU Profiling</A>
		<DT><A HREF="https://github.com/sgl-project/sglang/blob/a7c47e0f028c2a9e67cbc99ab67692ec765d3dd0/python/sglang/srt/layers/prefill_attention.py#L147">sglang/python/sglang/srt/layers/prefill_attention.py at a7c47e0f028c2a9e67cbc99ab67692ec765d3dd0 · sgl-project/sglang</A>
		<DT><A HREF="https://docs.google.com/document/u/3/d/1ZcxkW1FRNVm9AvDuTthRHvM09XnbkXOQzOE0yqSVMQI/edit">torch_custom_operators - Google Docs</A>
		<DT><A HREF="https://www.youtube.com/watch?v=d0gS5TXarXc&t=80s">Signals. I spent 2 years to understand this part. - YouTube</A>
		<DT><A HREF="https://user-images.githubusercontent.com/3841370/253379177-38ba1531-ea0d-4851-b31a-a6d4ddc944b0.png">253379177-38ba1531-ea0d-4851-b31a-a6d4ddc944b0.png 7.178×4.390 pixels</A>
		<DT><A HREF="https://colab.research.google.com/github/corolla-johnson/mkultra/blob/master/tuning_finetune.ipynb#scrollTo=cE2jNS5UMXKh">tuning_finetune_alice.ipynb - Colaboratory</A>
		<DT><A HREF="https://www.amazon.es/dp/1452182787/?tag=halcyonreal0b-21">Descripción del producto</A>
		<DT><A HREF="https://www.ebay.com/itm/153542159104?chn=ps&_trkparms=ispr%3D1&amdata=enc%3A1sq8JBcRtT6KcLZ-t4nHwlA91&norover=1&mkevt=1&mkrid=711-117182-37290-0&mkcid=2&itemid=153542159104&targetid=1262843335169&device=c&mktype=&googleloc=1005421&poi=&campaignid=15275224983&mkgroupid=131097072938&rlsatarget=pla-1262843335169&abcId=9300697&merchantid=137762152&gclid=CjwKCAiAiKuOBhBQEiwAId_sK4Vo04TQeVRAMajvPpCiWSkXUUfG3ndA9DARYgVeIuHrVnJj3IawnRoCimQQAvD_BwE">Studio Ghibli Layout Design Exhibition Hayao Miyazaki Art Book | eBay</A>
		<DT><A HREF="https://www.google.com/search?q=Studio+Ghibli+Layout+Design+Exhibition+Art+Book+Hayao+Miyazaki+From&sxsrf=AOaemvLdkPamZzJFeOFzCcVI5u7vYqv05w:1640740904393&source=lnms&tbm=shop&sa=X&ved=2ahUKEwjryJiW7If1AhWOHhQKHXJPCDwQ_AUoA3oECAEQBQ&biw=2240&bih=1180&dpr=2">Studio Ghibli Layout Design Exhibition Art Book Hayao Miyazaki From - Google Shopping</A>
		<DT><A HREF="https://www.amazon.com/Art-Pixar-Complete-Scripts-Animation/dp/0811879631">Editorial Reviews</A>
		<DT><A HREF="https://www.google.com/search?q=pixar+layouts+and+background+art+book+by&source=lmns&tbm=shop&bih=1180&biw=2240&client=safari&hl=en&sa=X&ved=2ahUKEwj09tjS6of1AhUN2-AKHb9hAj4Q_AUoAnoECAEQAg">pixar layouts and background art book by - Google Shopping</A>
		<DT><A HREF="https://www.tatteredcover.com/book/9781423138662">Page not found | Tattered Cover Book Store</A>
		<DT><A HREF="https://www.tatteredcover.com/contact-us-1">Contact Us | Tattered Cover Book Store</A>
		<DT><A HREF="https://www.tatteredcover.com/search/site/Walt%20Disney%20Animation%20Studios%20The%20Archive%20Series%20Layout%20%26%20Background">Search | Tattered Cover Book Store</A>
		<DT><A HREF="https://www.google.com/search?q=They+Drew+as+They+Pleased+Volume+6:+The+Hidden+Art+of+Disney%27s+New+Golden+Age+(Disney+x+Chronicle+Books)&client=safari&rls=en&sxsrf=AOaemvJRzA-THgHGQjRiX72En-FKx8brUw:1640740384480&source=lnms&tbm=shop&sa=X&ved=2ahUKEwid_aOe6of1AhWB0eAKHcyWBVcQ_AUoA3oECAEQBQ&biw=2240&bih=1180&dpr=2">They Drew as They Pleased Volume 6: The Hidden Art of Disney's New Golden Age (Disney x Chronicle Books) - Google Shopping</A>
		<DT><A HREF="https://www.amazon.com/They-Drew-Pleased-Hidden-Disneys/dp/1797200933/ref=pd_sbs_3/145-2891899-2697556?pd_rd_w=JgRmc&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=Q0FP94JPPJA1B04P7AYN&pd_rd_r=b5b842be-d34d-482b-8b35-c39120b1a464&pd_rd_wg=vvyJu&pd_rd_i=1797200933&psc=1">They Drew as They Pleased Volume 6: The Hidden Art of Disney's New Golden Age (Disney x Chronicle Books) Hardcover – August 4, 2020</A>
		<DT><A HREF="https://www.creativecavepublishers.com/books/layout-background-walt-disney-animation-archives/">Layout &amp; Background (Walt Disney Animation Archives)</A>
		<DT><A HREF="https://www.amazon.com/gp/product/1423134206?ref_=dbs_m_mng_rwt_calw_thcv_1&storeType=ebooks">Walt Disney Animation Studios The Archive Series #3: Design: Walt Disney Animation Research Libr: 9781423134206: Amazon.com: Books</A>
		<DT><A HREF="https://www.amazon.com/Layout-Background-Disney-Animation-Archives/dp/142313866X">Walt Disney Animation Studios The Archive Series #4: Layout &amp; Background Hardcover – October 25, 2011</A>
		<DT><A HREF="https://www.google.com/search?q=Walt+Disney+Animation+Studios+The+Archive:+Layout+%26+Background+pdf&client=safari&rls=en&sxsrf=AOaemvLEx_g8hNo7U9hbzLTYYHeOjWStnw:1640739669128&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiMm5bJ54f1AhXRA2MBHSiVAMsQ_AUoAXoECBUQAw&biw=2240&bih=1180&dpr=2#imgrc=oPV_tLZoS1wjdM&imgdii=aUZ42t3Hxy4KdM">Walt Disney Animation Studios The Archive: Layout &amp; Background pdf - Google Search</A>
		<DT><A HREF="https://www.amazon.com/Paper-Dreams-Artists-Disney-Storyboards/dp/0786863072">Amazon.com: Paper Dreams: The Art And Artists Of Disney Storyboards: 9780786863075: Canemaker, John: Books</A>
		<DT><A HREF="https://www.karts.ac.kr/en/main.do">Korea National University of Arts</A>
		<DT><A HREF="https://www.mmca.go.kr/eng/publication/publicationMain.do">국립현대미술관</A>
		<DT><A HREF="http://mmcashop-en.co.kr/goods/goods_view.php?goodsNo=1000001313">MMCA SHOP</A>
		<DT><A HREF="https://www.google.com/search?q=south+korean+illustration+university+books&client=safari&rls=en&sxsrf=AOaemvIK65RkA4i92os9bBTTqD-dsb9aGA%3A1640738171253&ei=e63LYYzyDqi5gweJu7TwCg&ved=0ahUKEwjMm_f-4Yf1AhWo3OAKHYkdDa4Q4dUDCA0&uact=5&oq=south+korean+illustration+university+books&gs_lcp=Cgdnd3Mtd2l6EAM6BAgAEEc6BQgAEM0COgQIIRAKSgQIQRgASgQIRhgAUJEHWMQQYMUSaAFwAngAgAFriAGbBZIBAzUuMpgBAKABAcgBBMABAQ&sclient=gws-wiz">south korean illustration university books - Google Search</A>
		<DT><A HREF="https://www.e-flux.com/announcements/421733/book-release-korean-art-1900-2020/">Book release: Korean Art 1900-2020 - Announcements - e-flux</A>
		<DT><A HREF="https://www.phaidon.com/store/art/korean-art-from-1953-collision-innovation-and-interaction-9780714878331/">Korean Art from 1953: Collision, Innovation, Interaction</A>
		<DT><A HREF="https://github.com/gii-is-DP1/dp1-2020-g3-09/blob/master/docker-compose.yml">Page not found · GitHub</A>
		<DT><A HREF="https://www.reddit.com/r/OpenAI/comments/u831xb/dalle_2_waitlist_length/">DALL-E 2 waitlist length : OpenAI</A>
		<DT><A HREF="https://medium.com/@DMeechan/fixing-the-installation-failed-virtualbox-error-on-mac-high-sierra-7c421362b5b5">Fixing ‘The Installation Failed’ VirtualBox Error on Mac High Sierra</A>
		<DT><A HREF="https://stackoverflow.com/questions/40523307/brew-install-docker-does-not-include-docker-engine/43365425#43365425">Brew install docker does not include docker engine?</A>
		<DT><A HREF="https://medium.com/crowdbotics/a-complete-one-by-one-guide-to-install-docker-on-your-mac-os-using-homebrew-e818eb4cfc3">A complete one-by-one guide to install Docker on your Mac OS using Homebrew</A>
		<DT><A HREF="https://formulae.brew.sh/formula/docker-compose#default">docker-compose — Homebrew Formulae</A>
		<DT><A HREF="https://ragunathrajasekaran.medium.com/getting-started-with-spring-data-jpa-hibernate-orm-repository-part-4-95a6ef2af513">Spring Data JPA, Hibernate, ORM &amp; Repository (Part 4)</A>
		<DT><A HREF="https://docs.google.com/document/d/1x3TRnRXz8PCHAWAgb7pzNfgAfBuKPxsyvBVZKXKeBFc/edit#heading=h.7ye3de504xy7">Experimentation Prompts: Infographics - Google Docs</A>
		<DT><A HREF="https://docs.google.com/spreadsheets/d/1Og2u4LExfCgTckBDKte54tdBuI2JPdK3YN6v2L6scn8/edit#gid=2074678152">openai-humaneval - Google Sheets</A>
		<DT><A HREF="https://docs.google.com/spreadsheets/d/1zuQQnOzC_zrd4MBPQg4W0A7Ab5k0jEnPRSwy8AxiC4c/edit#gid=0">Function header - Google Sheets</A>
		<DT><A HREF="https://drive.google.com/drive/folders/1l6yNtgD-Xx0Awb0D5VZ-nuMG1ea-WwAr">Folder - Google Drive</A>
		<DT><A HREF="https://huggingface.co/settings/tokens">Hugging Face – The AI community building the future.</A>
		<DT><A HREF="https://huggingface.co/datasets/bigcode/the-stack">bigcode/the-stack · Datasets at Hugging Face</A>
		<DT><A HREF="https://huggingface.co/docs/datasets/repository_structure">Structure your repository</A>
		<DT><A HREF="https://huggingface.co/datasets/diversoailab/standard_humaneval/tree/main/data">diversoailab/standard_humaneval at main</A>
		<DT><A HREF="http://localhost:8888/notebooks/notebook/dataset.ipynb#2.1-Debugging-Tests">dataset - Jupyter Notebook</A>
		<DT><A HREF="http://localhost:8888/tree/notebook">notebook/</A>
		<DT><A HREF="https://mail.google.com/mail/u/1/#inbox?compose=GTvVlcSPFdJllBGQkSkhDnrBpFzLrGXDHVsZsQTLCtzPLdPbCJZhZjQhgpjDzgbWtdHRZkmhNrtlg">Inbox (2) - antonio.jfdominguez1@gmail.com - Gmail</A>
		<DT><A HREF="https://docs.google.com/document/d/1x3TRnRXz8PCHAWAgb7pzNfgAfBuKPxsyvBVZKXKeBFc/edit#">Experimentation Prompts: Infographics - Google Docs</A>
	</DL><p>
	<DT><H3 FOLDED>15-04-2025</H3>
	<DL><p>
		<DT><H3 FOLDED>Computational Research</H3>
		<DL><p>
			<DT><H3 FOLDED>Artificial Intelligence</H3>
			<DL><p>
				<DT><H3 FOLDED>Companies</H3>
				<DL><p>
					<DT><H3 FOLDED>Deep tech</H3>
					<DL><p>
						<DT><H3 FOLDED>fundig-rounds</H3>
						<DL><p>
							<DT><A HREF="https://hackmd.io/@0RrvBhdwQya4V6lCDWvmew/BJScF5y2n?utm_source=preview-mode&utm_medium=rec">Investment Memo - CoreWeave - HackMD</A>
							<DT><A HREF="https://artificialanalysis.ai/speech-to-text">Speech to Text (ASR) Providers Leaderboard &amp; Comparison | Artificial Analysis</A>
							<DT><A HREF="https://www.replicated.com/blog/series-c-announcement">Replicated $50M Series C To Advance Multi-Prem Software Adoption</A>
							<DT><A HREF="https://fireworks.ai/blog/why-gpus-on-demand">GPUs on-demand: Not serverless, not reserved, but some third thing</A>
							<DT><A HREF="https://www.mercatopartners.com/editorials/why-we-invested-in-lambda">Why Mercato Led Lambda’s $44M Series B</A>
							<DT><A HREF="https://artificialanalysis.ai/models/deepseek-v2">DeepSeek-V2 - Quality, Performance &amp; Price Analysis | Artificial Analysis</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ptGDaGUXInw">Mark Russinovich | Generative AI in the Cloud: Inside Microsoft AI Innovation - YouTube</A>
							<DT><A HREF="https://gwern.net/complement">Laws of Tech: Commoditize Your Complement · Gwern.net</A>
							<DT><A HREF="https://drive.google.com/file/d/1gquqRqiT-2Be85p_5w0izGQGgHvVzncQ/view">mistral.ai strategic memo.pdf - Google Drive</A>
							<DT><A HREF="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">Building Meta’s GenAI Infrastructure - Engineering at Meta</A>
							<DT><A HREF="https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness">Training great LLMs entirely from ground up in the wilderness as a startup — Yi Tay</A>
						</DL><p>
						<DT><A HREF="https://gwern.net/complement">Laws of Tech: Commoditize Your Complement · Gwern.net</A>
						<DT><A HREF="https://www.flagshippioneering.com/stories/explainer-artificial-intelligence-and-medicine">Artificial Intelligence and Medicine | Flagship Pioneering</A>
					</DL><p>
					<DT><H3 FOLDED>xAI</H3>
					<DL><p>
						<DT><H3 FOLDED>xai-people</H3>
						<DL><p>
							<DT><A HREF="https://people.eecs.berkeley.edu/~hendrycks/">Dan Hendrycks</A>
							<DT><A HREF="https://twitter.com/ibab_ml">(1) Igor Babuschkin (@ibab_ml) / X</A>
							<DT><A HREF="https://twitter.com/thegregyang">(1) Greg Yang (@TheGregYang) / X</A>
							<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437/photo/1">(1) Guodong Zhang (@Guodzh) / X</A>
							<DT><A HREF="https://thegregyang.com/">Greg Yang | Professional page</A>
							<DT><A HREF="https://github.com/kykosic">kykosic (Kyle Kosic)</A>
						</DL><p>
						<DT><A HREF="https://x.ai/">xAI: Understand the Universe</A>
						<DT><A HREF="https://x.com/ibab/status/1827047684714463603">(1) ibab en X: "Grok 2 mini is now 2x faster than it was yesterday. In the last three days @lm_zheng and @MalekiSaeed rewrote our inference stack from scratch using SGLang (https://t.co/M1M8BlXosH). This has also allowed us to serve the big Grok 2 model, which requires multi-host inference, at a https://t.co/G9iXTV8o0z" / X</A>
					</DL><p>
					<DT><H3 FOLDED>Reka</H3>
					<DL><p>
						<DT><A HREF="https://www.yitay.net/">Yi Tay</A>
					</DL><p>
					<DT><H3 FOLDED>NVIDIA</H3>
					<DL><p>
						<DT><A HREF="https://www.nvidia.com/en-us/data-center/dgx-cloud/">DGX Cloud | AI Supercomputer in the Cloud | NVIDIA</A>
						<DT><A HREF="https://www.hpcwire.com/2023/03/21/nvidias-ai-factory-services-start-at-37000/">DGX Cloud Is Here: Nvidia's AI Factory Services Start at $37,000</A>
					</DL><p>
					<DT><H3 FOLDED>Imbue</H3>
					<DL><p>
						<DT><A HREF="https://imbue.com/">imbue (General Intelligence):  AI system that can reason (agents)</A>
					</DL><p>
					<DT><H3 FOLDED>EU</H3>
					<DL><p>
						<DT><H3 FOLDED>EuroHPC</H3>
						<DL><p>
							<DT><H3 FOLDED>LUMI</H3>
							<DL><p>
								<DT><A HREF="https://www.lumi-supercomputer.eu/">Front Page - LUMI</A>
							</DL><p>
							<DT><H3 FOLDED>Jupiter</H3>
							<DL><p>
								<DT><A HREF="https://www.fz-juelich.de/en/ias/jsc/jupiter">JUPITER - Exascale for Europe</A>
								<DT><A HREF="https://www.datacenterdynamics.com/en/news/europes-first-exascale-supercomputer-will-feature-24000-nvidia-gh200-superchips/">Europe's first exascale supercomputer will feature 24,000 Nvidia GH200 Superchips - DCD</A>
							</DL><p>
							<DT><H3 FOLDED>AI Act</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/BertuzLuca/status/1722997599932678450">AI Act (November 2023)</A>
							</DL><p>
							<DT><A HREF="https://euro-stack.eu/">EuroStack</A>
						</DL><p>
						<DT><H3 FOLDED>Germany</H3>
						<DL><p>
							<DT><A HREF="https://aleph-alpha.com/">ALEPH ALPHA - AI for Enterprises and Governments</A>
							<DT><A HREF="https://www.cnbc.com/2023/11/06/aleph-alpha-raises-500-million-to-build-a-european-rival-to-openai.html">Aleph Alpha raises $500 million to build a European rival to OpenAI</A>
						</DL><p>
						<DT><H3 FOLDED>France</H3>
						<DL><p>
							<DT><H3 FOLDED>Mistral AI</H3>
							<DL><p>
								<DT><A HREF="https://mistral.ai/">Mistral AI | Open source models</A>
							</DL><p>
							<DT><H3 FOLDED>Kyutai</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/kyutai_labs">(1) kyutai (@kyutai_labs) / X</A>
								<DT><A HREF="https://twitter.com/kyutai_labs/status/1725483922652283211">Founding Research Team</A>
								<DT><A HREF="http://kyutai.org/">kyutai: open science AI lab</A>
							</DL><p>
						</DL><p>
						<DT><A HREF="https://cip.org/whitepaper">Whitepaper — The Collective Intelligence Project</A>
						<DT><A HREF="https://twitter.com/scienceisstrat1/status/1693355056634958128">economic consequences of tech stagnation for Europe’s prosperity</A>
						<DT><A HREF="https://proposal.eu-inc.org/14d076fd79c581199761dd7e8b020774?v=14d076fd79c58146b048000caeed686a">A Blueprint for implementing the EU Inc</A>
					</DL><p>
					<DT><H3 FOLDED>YC</H3>
					<DL><p>
						<DT><A HREF="https://www.ycombinator.com/library/4r-yc-and-hard-tech-startups">YC and Hard Tech Startups : YC Startup Library | Y Combinator</A>
					</DL><p>
					<DT><H3 FOLDED>Financial</H3>
					<DL><p>
						<DT><A HREF="https://www.jpmorgan.com/technology/artificial-intelligence">artificial-intelligence</A>
						<DT><A HREF="https://x.com/techstartups/status/1872614059725488400">techstartups en X: "•General Catalyst: This renowned Silicon Valley venture capital firm raised $8 billion, marking the largest sum by a U.S. venture capital group in over two years. The firm intends to allocate $4.5 billion to its core VC funds, $1.5 billion to initiating new start-ups, and $2" / X</A>
					</DL><p>
					<DT><H3 FOLDED>MS: GeneralAI</H3>
					<DL><p>
						<DT><A HREF="https://thegenerality.com/agi/index.html">Advancing AI for humanity | Foundation of AI</A>
						<DT><A HREF="https://github.com/microsoft/unilm">microsoft/unilm: Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities</A>
					</DL><p>
					<DT><H3 FOLDED>token-economy</H3>
					<DL><p>
						<DT><A HREF="https://x.com/simonw/status/1841942172310110343">Gemini Flash-8B cost 0.00097</A>
						<DT><A HREF="https://www.ovhcloud.com/en/public-cloud/prices/">Price list: A comparison of our Public Cloud offers | OVHcloud Worldwide</A>
						<DT><A HREF="https://x.com/rwang07/status/1905207610606760014">Barclays on AI compute and Agentic AI</A>
					</DL><p>
					<DT><H3 FOLDED>flop-economy</H3>
					<DL><p>
						<DT><H3 FOLDED>neoclouds</H3>
						<DL><p>
							<DT><A HREF="https://x.com/PytorchToAtoms/status/1860915799122211022">(1) Pytorch To Atoms en X: "When GPU neocloud tell you that you need to "self-manage" and install yourself CUDA and IB drivers, slurm and their DC techs are eating Cheetos while plugging in transceivers https://t.co/yD9jN8Uwk6" / X</A>
							<DT><A HREF="https://www.latent.space/p/enterprise">The Most Dangerous Thing An AI Startup Can Do Is Build For Other AI Startups</A>
							<DT><A HREF="https://semianalysis.com/2025/03/26/the-gpu-cloud-clustermax-rating-system-how-to-rent-gpus/">The GPU Cloud ClusterMAX™ Rating System | How to Rent GPUs – SemiAnalysis</A>
						</DL><p>
						<DT><A HREF="https://cloud.hypertec.com/pricing/">Cost-Effective Cloud Pricing Tailored to Your Workloads | Hypertec Cloud</A>
						<DT><A HREF="https://www.together.ai/blog/nvidia-gb200-together-gpu-cluster-36k">Together AI to Co-Build Turbocharged NVIDIA GB200 Cluster with 36K Blackwell GPUs in Partnership with Hypertec Cloud</A>
						<DT><A HREF="https://www.together.ai/blog/introducing-the-together-enterprise-platform">Introducing The Together Enterprise Platform: Run GenAI securely in any environment, with 2x faster inference and continuous model optimization</A>
						<DT><A HREF="https://www.together.ai/blog/nvidia-h200-and-h100-gpu-cluster-performance-together-kernel-collection">Supercharging NVIDIA H200 and H100 GPU Cluster Performance With Together Kernel Collection</A>
						<DT><A HREF="https://x.com/satyanadella/status/1883753899255046301">Jevons paradox</A>
						<DT><A HREF="https://docs.google.com/spreadsheets/d/1WJNEAcCnCdscawEaGIOZkQvH_KyDEgRyjK99Rg82VW8/edit?gid=0#gid=0">Wavespeed-DataCrunch cost model - Google Sheets</A>
					</DL><p>
					<DT><H3 FOLDED>DC-research</H3>
					<DL><p>
						<DT><H3 FOLDED>sk-research-hub</H3>
						<DL><p>
							<DT><A HREF="https://cvlab.kaist.ac.kr/home">CVLAB</A>
							<DT><A HREF="https://sihyun.me/">Sihyun Yu</A>
							<DT><A HREF="https://mingukkang.github.io/">Minguk Kang</A>
							<DT><A HREF="https://yangspace.co.kr/">Sejong Yang</A>
							<DT><A HREF="https://seominjoon.github.io/">Minjoon Seo</A>
						</DL><p>
						<DT><H3 FOLDED>1X</H3>
						<DL><p>
							<DT><A HREF="https://www.1x.tech/discover/1x-world-model-sampling-challenge">1X World Model: Sampling Challenge Update</A>
							<DT><A HREF="https://github.com/thu-ml/RoboticsDiffusionTransformer">thu-ml/RoboticsDiffusionTransformer: RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation</A>
							<DT><A HREF="https://github.com/1x-technologies/1xgpt?tab=readme-ov-file">1x-technologies/1xgpt: world modeling challenge for humanoid robots</A>
						</DL><p>
						<DT><H3 FOLDED>replicate</H3>
						<DL><p>
						</DL><p>
						<DT><A HREF="https://carpedm30.notion.site/DataCrunch-Project-12d60168829780d5ab7cfad7ce6d083e">[DataCrunch] Project</A>
						<DT><A HREF="https://gwern.net/complement">Laws of Tech: Commoditize Your Complement · Gwern.net</A>
						<DT><A HREF="https://github.com/genmoai/models/tree/main">genmoai/models: The best OSS video generation models</A>
						<DT><A HREF="https://docs.google.com/document/d/1BzbjwGsCMO-X2A0pgAZTTAT022jgwxM20SDpB4gP52I/edit?tab=t.0">Industrial AI Research Collaboration Early Statement - Google Docs</A>
						<DT><A HREF="https://crusoe.ai/cloud/">Crusoe Cloud</A>
					</DL><p>
					<DT><H3 FOLDED>AI labs</H3>
					<DL><p>
						<DT><H3 FOLDED>Georgia Tech</H3>
						<DL><p>
							<DT><A HREF="https://www.gatech.edu/">Georgia Institute of Technology</A>
						</DL><p>
						<DT><H3 FOLDED>SHI Labs</H3>
						<DL><p>
							<DT><H3 FOLDED>shi-labs-efficient-GenAI-inference</H3>
							<DL><p>
								<DT><A HREF="https://github.com/SHI-Labs/NATTEN">SHI-Labs/NATTEN: Neighborhood Attention Extension. Bringing attention to a neighborhood near you!</A>
							</DL><p>
							<DT><A HREF="https://shi-labs.com/">SHI Labs</A>
							<DT><A HREF="https://github.com/alihassanijr">alihassanijr (Ali Hassani)</A>
							<DT><A HREF="https://github.com/SHI-Labs/NATTEN">SHI-Labs/NATTEN: Neighborhood Attention Extension. Bringing attention to a neighborhood near you!</A>
							<DT><A HREF="https://github.com/SHI-Labs">SHI Labs github</A>
						</DL><p>
						<DT><H3 FOLDED>KAUST</H3>
						<DL><p>
							<DT><A HREF="https://www.kaust.edu.sa/en/">King Abdullah University of Science and Technology | KAUST</A>
							<DT><A HREF="https://www.kaust.edu.sa/en/research/generative-ai">Generative AI</A>
							<DT><A HREF="https://www.kaust.edu.sa/en/research/generative-ai/gen-ai-factory">GenAI Factory: ML sys</A>
						</DL><p>
						<DT><H3 FOLDED>NVlabs</H3>
						<DL><p>
							<DT><H3 FOLDED>nvlabs-applied-research</H3>
							<DL><p>
								<DT><A HREF="https://research.nvidia.com/labs/adlr/">NVIDIA Applied Deep Learning Research - NVIDIA ADLR</A>
								<DT><A HREF="https://research.nvidia.com/labs/adlr/projects/">Projects - NVIDIA ADLR</A>
								<DT><A HREF="https://research.nvidia.com/labs/adlr/projects/bigvgan/">BigVGAN: A Universal Neural Vocoder with Large-Scale Training - NVIDIA ADLR</A>
								<DT><A HREF="https://blog.shi-labs.com/distributed-gemm-88be6a481e2b">Distributed GEMM: CUTLASS-native Tensor Parallelism | SHI Labs</A>
							</DL><p>
							<DT><H3 FOLDED>nvlabs-robotics</H3>
							<DL><p>
								<DT><A HREF="https://research.nvidia.com/labs/srl/">NVIDIA Seattle Robotics Lab</A>
							</DL><p>
							<DT><H3 FOLDED>nvlabs-genai</H3>
							<DL><p>
								<DT><A HREF="https://research.nvidia.com/labs/genair/">Fundamental Generative AI Research</A>
							</DL><p>
							<DT><A HREF="https://github.com/NVlabs">NVIDIA Research Projects</A>
							<DT><A HREF="https://www.nvidia.com/en-us/research/">Research at NVIDIA | Advancing the Latest Technology | NVIDIA</A>
						</DL><p>
						<DT><A HREF="https://blog.shi-labs.com/distributed-gemm-88be6a481e2b">Distributed GEMM: CUTLASS-native Tensor Parallelism | SHI Labs</A>
					</DL><p>
					<DT><A HREF="https://github.com/run-ai">run:ai</A>
					<DT><A HREF="https://www.youtube.com/watch?v=j3MPhl7HcGw">The Race to Build in Antarctica - YouTube</A>
				</DL><p>
				<DT><H3 FOLDED>Papers</H3>
				<DL><p>
					<DT><H3 FOLDED>Requests For Research</H3>
					<DL><p>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772472408559206798">Clive Chan</A>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772370709274038694">Sholto Douglas</A>
						<DT><A HREF="https://www.youtube.com/watch?v=cPu3SecmgUU&list=LL&index=10">How They Became Leading AI Researchers in Just 1 Year – Sholto Douglas &amp; Trenton Bricken - YouTube</A>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1774583930500632917">Chip Huyen</A>
						<DT><A HREF="https://openai.com/research/requests-for-research-2">OpenAI: Requests for Research 2.0</A>
						<DT><A HREF="https://www.youtube.com/watch?v=UTuuTTnjxMQ&list=PLd7-bHaQwnthaNDpZ32TtYONGVk95-fhF">Sholto Douglas &amp; Trenton Bricken - How to Build &amp; Understand GPT-7's Mind - YouTube</A>
						<DT><A HREF="https://x.com/_sholtodouglas/status/1772370709274038694">Sholto Douglas: Language models</A>
						<DT><A HREF="https://x.com/itsclivetime/status/1772472408559206798">Clive Chan</A>
					</DL><p>
					<DT><H3 FOLDED>Reading</H3>
					<DL><p>
						<DT><H3 FOLDED>Reading Groups</H3>
						<DL><p>
							<DT><A HREF="https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/">probabilistic-machine-learning</A>
							<DT><A HREF="https://www.youtube.com/@SanDiegoMachineLearning/videos">videos</A>
							<DT><A HREF="https://mlcollective.org/events/">ML Collective</A>
						</DL><p>
						<DT><H3 FOLDED>reading-news</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=fGF3nPClUT0">LLaMA3 400B to beat GPT4? (&amp; more) | Trends in AI - May 2024 - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Papers With Code</H3>
						<DL><p>
							<DT><A HREF="https://paperswithcode.com/paper/the-distributed-information-bottleneck">The Distributed Information Bottleneck</A>
						</DL><p>
						<DT><H3 FOLDED>Books</H3>
						<DL><p>
							<DT><A HREF="https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/">Probabilistic Machine Learning</A>
							<DT><A HREF="https://mitpressbookstore.mit.edu/book/9780262048439">Probabilistic Machine Learning: Advanced Topics (Adaptive Computation and Machine Learning series) | mitpressbookstore</A>
							<DT><A HREF="https://www.amazon.com/dp/0262048434/">Probabilistic Machine Learning: Advanced Topics (Adaptive Computation and Machine Learning series): Murphy, Kevin P.: 9780262048439: Amazon.com: Books</A>
							<DT><A HREF="https://www.amazon.com/Obstacle-Way-Timeless-Turning-Triumph/dp/1591846358">The Obstacle Is the Way: The Timeless Art of Turning Trials into Triumph: Holiday, Ryan: 8601411257797: Amazon.com: Books</A>
						</DL><p>
						<DT><H3 FOLDED>Papers implementations</H3>
						<DL><p>
							<DT><A HREF="https://nn.labml.ai/?_gl=1*v934qz*_ga*MTE0MzE1Mjg1My4xNzE5NjU5MzYx*_ga_PDCL9PHMHT*MTcxOTY1OTM2MS4xLjAuMTcxOTY1OTM2MS4wLjAuMA..">Annotated Research Paper Implementations: Transformers, StyleGAN, Stable Diffusion, DDPM/DDIM, LayerNorm, Nucleus Sampling and more</A>
							<DT><A HREF="https://github.com/kmohan321/Research_Papers">kmohan321/Research_Papers</A>
						</DL><p>
						<DT><A HREF="https://scispace.com/">SCISPACE</A>
						<DT><A HREF="https://chatdoc.com/">ChatDOC - Chat with your documents</A>
						<DT><A HREF="https://kipp.ly/sept-oct-2023/">Things Read | Sept/Oct 2023 (kipply's blog) (MUST)</A>
						<DT><A HREF="https://www.simonwenkel.com/index.html">Summary &amp; Update Papers AI/ML</A>
						<DT><A HREF="https://search.zeta-alpha.com/?similar_to=CO_3527012&sort_by=relevance&q=&retrieval_method=knn">Zeta Alpha</A>
						<DT><A HREF="https://www.youtube.com/watch?v=QQIwfpOY-qA">20 papers to master Language modeling?</A>
						<DT><A HREF="https://kipp.ly/nov-dec-2023/">Kipply's blog: nov-dec-2023</A>
						<DT><A HREF="https://nn.labml.ai/unet/index.html">U-Net</A>
						<DT><A HREF="https://twitter.com/TheGregYang/status/1680358832789155842">Greg Yang: Books to read</A>
						<DT><A HREF="https://gwern.net/complement">Laws of Tech: Commoditize Your Complement · Gwern.net</A>
						<DT><A HREF="https://gist.github.com/matijagrcic/ae8353eb1e6be84a7c85d9fdc2f9631f">John_Carmack_Ilya_Sutskever.md</A>
						<DT><A HREF="https://papers.cool/">Cool Papers - Immersive Paper Discovery</A>
						<DT><A HREF="https://rentry.org/LocalModelsLinks">Local Models Related Links</A>
						<DT><A HREF="https://x.com/GCResearchTeam/status/1864322846777897103">(1) Graphcore Research en X: "Our November Papers of the Month is now live. This edition covers 4 papers, on "super weights", context-parallelism, scaling laws for precision and critical batch sizes. We provide our summaries and analysis of each. (🧵1/n) https://t.co/CkQySUiNpO" / X</A>
					</DL><p>
					<DT><H3 FOLDED>Research tools</H3>
					<DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=hSTy_BInQs8">Obsidian: The King of Learning Tools (FULL GUIDE + SETUP) - YouTube</A>
						<DT><A HREF="https://www.connectedpapers.com/">Connected Papers | Find and explore academic papers</A>
					</DL><p>
					<DT><H3 FOLDED>people</H3>
					<DL><p>
						<DT><H3 FOLDED>High context people</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772472408559206798">Clive Chan</A>
							<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772370709274038694">Sholto Douglas</A>
							<DT><A HREF="https://x.com/MajmudarAdam/status/1794190796411027791">(3) adammaj en X: "I've spent the past ~3 weeks going through the entire history of deep learning and reimplementing all the core breakthroughs. It has completely changed my beliefs about deep learning progress and where we're headed. Progress tracker in thread (all resources at the end) 👇 https://t.co/BjcEA2iv3f" / X</A>
							<DT><A HREF="https://www.youtube.com/watch?v=cPu3SecmgUU&list=LL&index=10">How They Became Leading AI Researchers in Just 1 Year – Sholto Douglas &amp; Trenton Bricken - YouTube</A>
							<DT><A HREF="https://twitter.com/dwarkesh_sp">Dwarkesh Patel (main technical AI podcast)</A>
							<DT><A HREF="https://scholar.google.co.uk/citations?user=DaFHynwAAAAJ&hl=en">‪Alex Graves‬ - ‪Google Scholar‬</A>
							<DT><A HREF="https://twitter.com/kipperrii/status/1777814429838573861">Alex Graves: 8 years rule</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GU2K0kiHE1Q">AI Reading List (by Ilya Sutskever) - Part 1 - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GxjEjy5UYJU">AI Reading List (by Ilya Sutskever) - Part 2 - YouTube</A>
							<DT><A HREF="https://x.com/NoamShazeer">(1) Noam Shazeer (@NoamShazeer) / X</A>
							<DT><A HREF="https://x.com/papers_anon/status/1805084174510162112">(1) PapersAnon en X: "https://t.co/CJC3YWPoB6 Various links for ML and local models (not just LLMs) that's kept fairly updated. https://t.co/5pLfM330hp ML papers I've read that I think are interesting. Also keep a text file at the top of all the abstracts for easy searching." / X</A>
							<DT><A HREF="https://rentry.org/LocalModelsLinks">Local Models Related Links</A>
							<DT><A HREF="https://www.jasonwei.net/thoughts">Thoughts — Jason Wei</A>
							<DT><A HREF="https://x.com/ludwigABAP/status/1821107246538924407">(1) ludwig en X: ""If you constantly unpack everything for deeper understanding you never get anything done. If you don't unpack and understand it when you need to, you'll do the wrong thing" I found this quote that I wrote down (apparently from Jim Keller) and it's the best way to describe what" / X</A>
							<DT><A HREF="https://x.com/spikedoanz/status/1819435570495737988">(1) spike en X: "open sourcing said list: Joscha Bach Marvin Minsky Geohot Karpathy Paul Graham Wittgenstein Jim Keller John Carmack Aaron Swartz Robert Sapolsky Gwern Scott Alexander Nietzche Feynman Tim Rogers Jonathan Blow" / X</A>
							<DT><A HREF="https://x.com/_clashluke">(1) Lucas Nestler (@_clashluke) / X AGI</A>
						</DL><p>
						<DT><H3 FOLDED>historical</H3>
						<DL><p>
							<DT><A HREF="https://en.wikipedia.org/wiki/Hermann_Weyl">Hermann Weyl - (Symmetry) Mathematician</A>
						</DL><p>
						<DT><A HREF="https://tacocohen.wordpress.com/">Taco Cohen-Qualcomm</A>
						<DT><A HREF="https://www.cs.ox.ac.uk/people/michael.bronstein/">Michael Bronstein-Imperial College/Twitter</A>
						<DT><A HREF="https://petar-v.com/">Petar Veličković-Deepmind</A>
						<DT><A HREF="https://arxiv.org/search/cs?searchtype=author&query=Bruna%2C+J">Joan Bruna</A>
						<DT><A HREF="https://www.stellabiderman.com/">Stella Biderman</A>
						<DT><A HREF="https://www.stellabiderman.com/">Stella Biderman (EleutherAI)</A>
						<DT><A HREF="https://people.csail.mit.edu/zhonge/index.html">Ellen D. Zhong</A>
						<DT><A HREF="https://scholar.google.com/citations?hl=en&user=7Fxbm0AAAAAJ&view_op=list_works">‪Phitchaya Mangpo Phothilimthana‬ - ‪Google Scholar‬</A>
					</DL><p>
					<DT><H3 FOLDED>Information Theory &amp; Communication</H3>
					<DL><p>
						<DT><H3 FOLDED>representation theory</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=IwYiETZEGY0&list=LL&index=11&t=25s">What does AI have to do with Plato's Allegory of the Cave? - YouTube</A>
							<DT><A HREF="https://arxiv.org/abs/2405.07987">[2405.07987] The Platonic Representation Hypothesis</A>
							<DT><A HREF="https://www.youtube.com/watch?v=1_xH2mUFpZw&t=802s">The Platonic Representation Hypothesis - YouTube</A>
						</DL><p>
						<DT><A HREF="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">(SHANNON) A Mathematical Theory of Communication</A>
						<DT><A HREF="https://twitter.com/realGeorgeHotz/status/1738013824987656635">(1) George Hotz 🌑 en X: ".@__tinygrad__ is really an entropics research group. Distilling neural networks down to their purest essence. The compression of the compressor. Someday net size will be measured in gates instead of FLOPS or weights." / X</A>
						<DT><A HREF="https://twitter.com/realGeorgeHotz/status/1690831164431585280">Thermodynamics is to Energy as &gt;???&gt;is to Intelligence</A>
						<DT><A HREF="https://www.youtube.com/watch?v=Z_sQg-Alg7E">Estimating the Information Flow in Deep Neural Networks - YouTube</A>
						<DT><A HREF="https://irhum.github.io/blog/spherical-harmonics/">irhum.github.io - Visual Notes on Spherical Harmonics</A>
						<DT><A HREF="https://x.com/din0s_/status/1801271625309937771">(1) dinos en X: Awesome Information Retrieval</A>
						<DT><A HREF="https://arxiv.org/pdf/1703.00810">Opening the balck box of Deep Neural Networks</A>
						<DT><A HREF="https://github.com/ravidziv/IDNNs">ravidziv/IDNNs: explore DNNs via Infomration</A>
						<DT><A HREF="https://www.youtube.com/@szymonozog7862/videos">Simon Oz - YouTube</A>
						<DT><A HREF="https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/">Anatomize Deep Learning with Information Theory | Lil'Log</A>
						<DT><A HREF="https://x.com/tanishqkumar07/status/1856045600355352753">(1) Tanishq Kumar @ NeurIPS 2024 en X: "[1/7] New paper alert! Heard about the BitNet hype or that Llama-3 is harder to quantize? Our new work studies both! We formulate scaling laws for precision, across both pre and post-training https://t.co/8FC4g2fSRb. TLDR; - Models become harder to post-train quantize as they https://t.co/m212U9eJd7" / X</A>
					</DL><p>
					<DT><H3 FOLDED>Signal processing</H3>
					<DL><p>
						<DT><A HREF="https://www.jezzamon.com/fourier/index.html">An Interactive Introduction to Fourier Transforms</A>
						<DT><A HREF="https://www.youtube.com/watch?v=2EZap2EoMR0">Recitation 1 - Why Vibrations and Waves Matter - YouTube</A>
						<DT><A HREF="https://twitter.com/taiyasaki/status/1782856728973045843">Band-limited Neural Fields for Levels of Detail Reconstruction</A>
						<DT><A HREF="https://irhum.github.io/blog/spherical-harmonics/">irhum.github.io - Visual Notes on Spherical Harmonics</A>
						<DT><A HREF="https://x.com/Ethan_smith_20/status/1767570870598279181">Ethan en X: "Created a new method of generative model (although kinda crappy lol) that works by autoregressive sequencing fourier coefficients. Inspired by the coarse to fine generation by Diffusion models. Full write up here: https://t.co/voD78qVwIB and tldr in thread 🧵 (flowers102) https://t.co/VyhR0WA4vs" / X</A>
						<DT><A HREF="https://www.youtube.com/watch?v=Cx5Z-OslNWE&list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k">Course Introduction of 18.065 by Professor Strang - YouTube</A>
						<DT><A HREF="https://www.youtube.com/watch?v=6xaaeop7gJ8&list=PLADC1A1B7FA7FF7B6">Lecture 2, Signals and Systems: Part 1 | MIT RES.6.007 Signals and Systems, Spring 2011 - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>Computational Complexity</H3>
					<DL><p>
						<DT><A HREF="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov–Arnold representation theorem</A>
						<DT><A HREF="https://hadrien-montanelli.github.io/2019-06-25.html">Deep networks and the Kolmogorov–Arnold theorem</A>
						<DT><A HREF="https://math.stackexchange.com/questions/2518664/are-there-any-simple-examples-of-kolmogorov-arnold-representation">Are there any simple examples of Kolmogorov-Arnold representation?</A>
					</DL><p>
					<DT><H3 FOLDED>Numerical linear algebra</H3>
					<DL><p>
						<DT><H3 FOLDED>Random Matrix Theory</H3>
						<DL><p>
							<DT><A HREF="https://math.mit.edu/~edelman/publications/random_matrix_theory.pdf">MIT: Introduction</A>
							<DT><A HREF="https://www.youtube.com/watch?v=oPQ4mNcqY7k&t=9s">Terence Tao's central limit theorem, Double Factorials (!!) and the Moment Method #SoME3 - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=1aXOXHA7Jcw&t=3032s">Greg Yang | Large N Limits: Random Matrices &amp; Neural Networks</A>
							<DT><A HREF="https://www.youtube.com/watch?v=u32UEaFevNY">Math Reading Group - Random Matrix Theory III (12/11/23) - YouTube</A>
							<DT><A HREF="https://ericauld.github.io/2023/05/20/clt.html">Eric Auld | Math, Economics, and Computing</A>
							<DT><A HREF="https://rmt-fall2019.s3.amazonaws.com/rmt-fall2019.pdf">https://rmt-fall2019.s3.amazonaws.com/rmt-fall2019.pdf</A>
						</DL><p>
						<DT><H3 FOLDED>Rand NLA</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2302.11474">[2302.11474] Randomized Numerical Linear Algebra : A Perspective on the Field With an Eye to Software</A>
							<DT><A HREF="https://www.youtube.com/watch?v=6htbyY3rH1w&t=1699s">Is the Future of Linear Algebra.. Random? - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=fJ2EyvR85ro">Randomized Singular Value Decomposition (SVD) - YouTube</A>
						</DL><p>
						<DT><A HREF="https://archive.org/details/gantmacher-the-theory-of-matrices-vol-1-1959/page/12/mode/2up">The Theory Of Matrices Vol 1 : F. R. Gantmacher : Free Download, Borrow, and Streaming : Internet Archive</A>
						<DT><A HREF="https://www.maths.ed.ac.uk/~v1ranick/papers/gantmacher1.pdf">https://www.maths.ed.ac.uk/~v1ranick/papers/gantmacher1.pdf</A>
						<DT><A HREF="https://www.youtube.com/watch?v=Cx5Z-OslNWE&list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k">Course Introduction of 18.065 by Professor Strang - YouTube</A>
						<DT><A HREF="https://x.com/yaroslavvb/status/1822036160115617975">Where do eigenvalues come from?</A>
					</DL><p>
					<DT><H3 FOLDED>Statistics</H3>
					<DL><p>
						<DT><H3 FOLDED>probability theory</H3>
						<DL><p>
							<DT><A HREF="https://terrytao.wordpress.com/2015/09/29/275a-notes-0-foundations-of-probability-theory/">275A, Notes 0: Foundations of probability theory | What's new</A>
							<DT><A HREF="https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf">Probability: Theory and Examples (Rick Durrett)</A>
						</DL><p>
						<DT><H3 FOLDED>optimal transport</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=C12o-60fmgE">IFML SEMINAR: 2/2/24 - Gromov-Wasserstein Alignment: Statistical and Computational Advancements... - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=EauDdCzxphE&t=3s">Optimal Transport and Information Geometry for Machine Learning and Data Science - YouTube</A>
							<DT><A HREF="https://x.com/gabrielpeyre/status/1863564944207175888">(1) Gabriel Peyré en X: "Optimal transport computes an interpolation between two distributions using an optimal coupling. Flow matching, on the other hand, uses a simpler “independent” coupling, which is the product of the marginals. https://t.co/ILrKMPwLJh" / X</A>
						</DL><p>
						<DT><H3 FOLDED>statistics-physics-informed-learning</H3>
						<DL><p>
							<DT><A HREF="https://mitmath.github.io/18337/lecture15/diffeq_machine_learning">Mixing Differential Equations and Neural Networks for Physics-Informed Learning</A>
						</DL><p>
						<DT><H3 FOLDED>statistics-bayes</H3>
						<DL><p>
							<DT><A HREF="https://ermongroup.github.io/cs228-notes/representation/directed/">Probabilistic Graphical Models: Cascades and Nets</A>
						</DL><p>
						<DT><A HREF="https://matjaz.substack.com/p/why-are-there-complete-problems-really?s=r">Why are there complete problems, really?</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/permutation">permutation</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/probability+distribution">probability distribution</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/supervised+learning">supervised learning</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/probability+density">probability density</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Banach+space">Banach space</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Wasserstein+metric">Wasserstein metric</A>
						<DT><A HREF="https://simons.berkeley.edu/workshops/deep-learning-theory-workshop">Deep Learning Theory Workshop and Summer School | Simons Institute for the Theory of Computing</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Domain_adaptation#Domain_shift">Domain adaptation - (Distribution Shift)</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Log_probability">Log probability</A>
						<DT><A HREF="https://starai.cs.ucla.edu/papers/ZhangIJCAI23.pdf">On the Paradox of Learning to Reason from Data</A>
						<DT><A HREF="https://twitter.com/sirbayes/status/1765905163020325296">(1) Kevin Patrick Murphy en X: "@fchollet claimed that "learning to reason" (robustly across problem instances) is hard because of the nature of the MLE objective. This paper proves that claim. The reason is "statistical features inherently exist in data distributions, but can hinder model generalization..." / X</A>
						<DT><A HREF="https://www.youtube.com/watch?v=xkS9GyujiqQ">03 – Naïve Bayes parameters estimation and Laplace smoothing - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>Deep Learning</H3>
					<DL><p>
						<DT><H3 FOLDED>automatic differentiation</H3>
						<DL><p>
							<DT><A HREF="https://x.com/karpathy/status/1803963383018066272">These 94 lines of code are everything that is needed to train a neural network. Everything else is just efficiency.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=_ds0-daMESY">Automatic Differentiation - A Revisionist History and the State of the Art - AD meets SDG and PLT - YouTube</A>
							<DT><A HREF="https://kexue.fm/archives/9902">让炼丹更科学一些（一）：SGD的平均损失收敛 - 科学空间|Scientific Spaces</A>
							<DT><A HREF="https://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">https://www.cs.toronto.edu/~hinton/absps/naturebp.pdf</A>
							<DT><A HREF="https://arogozhnikov.github.io/2023/12/28/fastest-autograd.html">Fastest Autograd in the West</A>
							<DT><A HREF="https://www.youtube.com/watch?v=S7VG-0Tw6a4">Building an autograd engine with only Triton GPU kernels - live 2025.1.28 - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>optimization</H3>
						<DL><p>
							<DT><H3 FOLDED>convex optimization</H3>
							<DL><p>
								<DT><A HREF="https://web.stanford.edu/~boyd/cvxbook/">Convex Optimization</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Convex_optimization">Convex optimization - Wikipedia</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=NE88eqLngkg">Optimization for Deep Learning (Momentum, RMSprop, AdaGrad, Adam)</A>
							<DT><A HREF="https://www.youtube.com/watch?v=6CaUxbFX8Oc&list=PLiCLbsFQNFAxOmVeqPhI5er1LGf2-L9I4">Lecture 1 Part 1: Approximate Dynamic Programming Lectures</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mHO2ocZxqQM">IFML Seminar: 3/8/2024 - An Lyapunov Analysis of the Lion Optimizer - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GM6XPEQbkS4">Provably Faster Gradient Descent via Long Steps</A>
							<DT><A HREF="https://www.youtube.com/watch?v=AKMuA_TVz3A">An observation on Generalization</A>
							<DT><A HREF="https://web.stanford.edu/~boyd/cvxbook/">Convex Optimization – Boyd and Vandenberghe</A>
							<DT><A HREF="https://www.youtube.com/watch?v=78vq6kgsTa8">Tom Goldstein: "What do neural loss surfaces look like?" - YouTube</A>
							<DT><A HREF="https://machine-learning-etc.ghost.io/">Generating functions approach to gradient descent analysis</A>
							<DT><A HREF="https://nn.labml.ai/optimizers/index.html">Optimizers implementations</A>
						</DL><p>
						<DT><H3 FOLDED>neural networks</H3>
						<DL><p>
							<DT><H3 FOLDED>nn-equivariant-invariant</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?app=desktop&v=kTvow5-eCCQ&list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd&index=5">Group Equivariant Deep Learning</A>
								<DT><A HREF="https://twitter.com/JustinMSolomon/status/1538780379062075392">I know nothing about "equivariant" NNs</A>
							</DL><p>
							<DT><H3 FOLDED>Learning Dynamics</H3>
							<DL><p>
								<DT><H3 FOLDED>gradient</H3>
								<DL><p>
									<DT><H3 FOLDED>Vanishing Gradients</H3>
									<DL><p>
										<DT><A HREF="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits: Transformers Residual Stream</A>
										<DT><A HREF="https://www.youtube.com/watch?v=P6sfmUTpUmc">Building makemore Part 3: Activations &amp; Gradients, BatchNorm - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=ncTHBi8a9uA">The Fundamental Problem with Neural Networks - Vanishing Gradients - YouTube</A>
										<DT><A HREF="https://kexue.fm/archives/7888">Let’s also talk about the vanishing/exploding gradient problem of RNN</A>
									</DL><p>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1800585478539981094">What the gradient seems to really like: a thread</A>
									<DT><A HREF="https://arxiv.org/abs/2403.02241">[2403.02241] Neural Redshift: Random Networks are not Random Functions</A>
									<DT><A HREF="https://arxiv.org/abs/2004.01461">[2004.01461] Gradient Centralization: A New Optimization Technique for Deep Neural Networks</A>
								</DL><p>
								<DT><A HREF="https://transformer-circuits.pub/2022/toy_model/index.html#learning">Toy Models of Superposition</A>
								<DT><A HREF="https://www.fast.ai/posts/2023-09-04-learning-jumps/">fast.ai – Can LLMs learn from a single example?</A>
								<DT><A HREF="https://arxiv.org/abs/1712.09913">[1712.09913] Visualizing the Loss Landscape of Neural Nets</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1806547801779863697">language model layer permutation</A>
								<DT><A HREF="https://kexue.fm/archives/9902">Making Alchemy More Scientific (I): SGD Average Loss Convergence</A>
								<DT><A HREF="https://www.lesswrong.com/posts/2JJtxitp6nqu6ffak/basic-facts-about-language-models-during-training-1">Basic facts about language models during training — LessWrong</A>
								<DT><A HREF="https://x.com/kellerjordan0/status/1866333331857780873">Keller Jordan en X: "Here's an interpretability method for neural net trainings that is strong but expensive: Run the training a few thousand times, and then for pairs of inputs, measure the correlation between their final predicted outputs across runs of training. This yields a highly... 1/8🧵 https://t.co/4X2Lny0AIP" / X</A>
								<DT><A HREF="https://x.com/aaron_defazio/status/1868406236494074195/photo/1">Training recovers from loss spikes because spikes occur in only a few latent dimensions.</A>
							</DL><p>
							<DT><H3 FOLDED>backpropagation</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=VMj-3S1tku0&t=4186s">The spelled-out intro to neural networks and backpropagation: building micrograd - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=P6sfmUTpUmc">Building makemore Part 3: Activations &amp; Gradients, BatchNorm - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=q8SA3rM6ckI">Building makemore Part 4: Becoming a Backprop Ninja - YouTube</A>
								<DT><A HREF="https://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">https://www.cs.toronto.edu/~hinton/absps/naturebp.pdf</A>
							</DL><p>
							<DT><H3 FOLDED>MLP</H3>
							<DL><p>
								<DT><H3 FOLDED>KAN</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=-PFIkkwWdnM">Kolmogorov-Arnold Networks: MLP vs KAN, Math, B-Splines, Universal Approximation Theorem - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>mlp-scaling</H3>
								<DL><p>
									<DT><A HREF="https://github.com/gregorbachmann/scaling_mlps">gregorbachmann/scaling_mlps</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=TCH_1BHY58I">Building makemore Part 2: MLP - YouTube</A>
								<DT><A HREF="https://x.com/thomasahle/status/1798408687981297844">Using 𝚝𝚘𝚛𝚌𝚑.𝚌𝚘𝚖𝚙𝚒𝚕𝚎 makes KANs as fast as MLPs!</A>
								<DT><A HREF="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits</A>
								<DT><A HREF="https://www.youtube.com/watch?v=OmudSvOQhCg">Why do we need activation functions? - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>SGD</H3>
							<DL><p>
								<DT><A HREF="https://kexue.fm/archives/9902">Making Alchemy More Scientific (I): SGD Average Loss Convergence</A>
							</DL><p>
							<DT><H3 FOLDED>proxy model</H3>
							<DL><p>
								<DT><A HREF="https://kexue.fm/archives/8444">Can we losslessly upscale a Transformer model? (Part 1)</A>
							</DL><p>
							<DT><H3 FOLDED>circuits</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=9EN_HoEk3KY&t=2s">Ilya Sutskever: OpenAI Meta-Learning and Self-Play | Why Do Neural Networks Work?</A>
								<DT><A HREF="https://distill.pub/2020/circuits/zoom-in/">Zoom In: An Introduction to Circuits</A>
								<DT><A HREF="https://distill.pub/2020/circuits/">Thread: Circuits</A>
								<DT><A HREF="https://distill.pub/2020/circuits/visualizing-weights/">Visualizing Weights</A>
								<DT><A HREF="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">interpreting GPT: the logit lens — LessWrong</A>
							</DL><p>
							<DT><H3 FOLDED>latent-space</H3>
							<DL><p>
								<DT><A HREF="https://sander.ai/2025/04/15/latents.html">Generative modelling in latent space – Sander Dieleman</A>
							</DL><p>
							<DT><H3 FOLDED>Feature learning</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=DuBQCBWcq4M&t=340s">Greg Yang — Feature Learning in Infinite-Width Neural Networks - YouTube</A>
								<DT><A HREF="https://arxiv.org/abs/2011.14522">[2011.14522] Feature Learning in Infinite-Width Neural Networks</A>
								<DT><A HREF="https://arxiv.org/abs/2310.17813">[2310.17813] A Spectral Condition for Feature Learning</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=9EN_HoEk3KY&t=2s">Ilya Sutskever: OpenAI Meta-Learning and Self-Play | Why Do Neural Networks Work?</A>
							<DT><A HREF="https://www.youtube.com/watch?v=BjyZcSiVg5A">Deep Learning Theory Session. Ilya SutskeverIlya Sutskever - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M74WKIh0ciI&list=LL&index=24">Three weight matrices in a neural network growing connections as two parallel networks are merged. - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=g0gREwiDbis">Dr. Geoffrey Hinton: A brief study of neural networks</A>
							<DT><A HREF="https://www.youtube.com/watch?v=nL7zpkfKbPs">Human MNIST Challenge: Can you recognize the Fourier transform of handwritten digits? - YouTube</A>
							<DT><A HREF="https://github.com/cloneofsimo/insightful-nn-papers">cloneofsimo/insightful-nn-papers: These papers will provide unique insightful concepts that will broaden your perspective on neural networks and deep learning</A>
							<DT><A HREF="https://ncatlab.org/nlab/show/neural+network">neural network in nLab</A>
							<DT><A HREF="https://hagan.okstate.edu/NNDesign.pdf">Neural Network Design</A>
							<DT><A HREF="https://www.youtube.com/watch?v=L3-WFKCW-tY&t=2624s">33. Neural Nets and the Learning Function - YouTube</A>
							<DT><A HREF="https://blog.janestreet.com/visualizing-piecewise-linear-neural-networks/">Jane Street Tech Blog - Visualizing piecewise linear neural networks</A>
							<DT><A HREF="https://arxiv.org/abs/1312.6098">[1312.6098] On the number of response regions of deep feed forward networks with piece-wise linear activations</A>
							<DT><A HREF="https://www.youtube.com/watch?v=AKMuA_TVz3A&t=2612s">An Observation on Generalization - YouTube</A>
							<DT><A HREF="https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/">Anatomize Deep Learning with Information Theory | Lil'Log</A>
							<DT><A HREF="https://arxiv.org/abs/2310.19956">[2310.19956] The Impact of Depth on Compositional Generalization in Transformer Language Models</A>
							<DT><A HREF="https://x.com/i/bookmarks?post_id=1863100717868654951">Analyzing the global convergence of Newton is hard. Attraction basins are fractals whose boundaries are points which do not converge.</A>
							<DT><A HREF="https://arxiv.org/abs/2411.07191">[2411.07191] The Super Weight in Large Language Models</A>
							<DT><A HREF="https://www.youtube.com/watch?v=YpFaPKOeNME">NEURAL NETWORKS ARE REALLY WEIRD... - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>deep-learning-people</H3>
						<DL><p>
							<DT><H3 FOLDED>Michael Bronstein</H3>
							<DL><p>
								<DT><A HREF="https://towardsdatascience.com/predictions-and-hopes-for-geometric-graph-ml-in-2022-aa3b8b79f5cc">What does 2022 hold for Geometric &amp; Graph ML?</A>
								<DT><A HREF="https://arxiv.org/abs/2104.13478">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</A>
								<DT><A HREF="https://medium.com/stanford-cs224w/tackling-the-traveling-salesman-problem-with-graph-neural-networks-b86ef4300c6e">Tackling the Traveling Salesman Problem with Graph Neural Networks | by Michael Atkin | Stanford CS224W GraphML Tutorials | May, 2023 | Medium</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/agikoala/status/1641838208823468032/photo/1">(1) jason en X: "Three starter pokemon in deep learning</A>
							<DT><A HREF="https://scholar.google.com/citations?user=x04W_mMAAAAJ">‪Ilya Sutskever‬ - ‪Google Scholar‬</A>
							<DT><A HREF="https://scholar.google.com/citations?user=NkzyCvUAAAAJ&hl=en">‪Oriol Vinyals‬ - ‪Google Scholar‬</A>
							<DT><A HREF="https://scholar.google.com/citations?user=vfT6-XIAAAAJ&hl=en">‪Quoc V. Le‬ - ‪Google Scholar‬</A>
							<DT><A HREF="https://scholar.google.co.uk/citations?user=DaFHynwAAAAJ&hl=en">‪Alex Graves‬ - ‪Google Scholar‬</A>
							<DT><A HREF="https://www.yitay.net/">Yi Tay</A>
							<DT><A HREF="https://www.jasonwei.net/thoughts">Thoughts — Jason Wei</A>
							<DT><A HREF="https://hwchung27.github.io/">Hyung Won Chung</A>
							<DT><A HREF="https://twitter.com/ilyasut/status/1790517455628198322">(1) Ilya Sutskever en X: "After almost a decade, I have made the decision to leave OpenAI.  The company’s trajectory has been nothing short of miraculous, and I’m confident that OpenAI will build AGI that is both safe and beneficial under the leadership of @sama, @gdb, @miramurati and now, under the" / X</A>
							<DT><A HREF="https://twitter.com/merettm">akub Pachocki (OpenAI dota bot)</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Ckz8XA2hW84&list=LL&index=14&t=1s">Ilya Sutskever (OpenAI) and Jensen Huang (NVIDIA CEO) : AI Today and Vision of the Future (3/2023) - YouTube</A>
							<DT><A HREF="https://www.linkedin.com/in/alecradford/">Alec Radford</A>
							<DT><A HREF="https://www.linkedin.com/in/markchen90/">Mark Chen: Head of Fronteirs Research at OpenAI</A>
							<DT><A HREF="https://x.com/peterthedecent">(1) Peter (@peterthedecent) / X</A>
							<DT><A HREF="https://www.youtube.com/watch?v=AJV4WLSc4pw">Ilya Sutskever |Neuroscience has brought many very important great ideas to AI|The first year of AGI - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>deep-learning-lectures</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=bZF4N8HR1cc">09 – AE, DAE, and VAE with PyTorch; generative adversarial networks (GAN) and code - YouTube</A>
							<DT><A HREF="https://mlcollective.org/dlct/">ML Collective</A>
							<DT><A HREF="https://informationisbeautiful.net/">Information is Beautiful</A>
							<DT><A HREF="https://www.youtube.com/watch?v=v7arCzjQk38">Robust Gradient Descent: Agnostically Estimating an Unknown Affine Transformation... - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=MRNrbLRxNyg">Toward a Grand Unified Theory of Accelerations in Optimization and Machine Learning - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M74WKIh0ciI&list=LL&index=24">Three weight matrices in a neural network growing connections as two parallel networks are merged. - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=EI6k2g9CUHo">Baharan Mirzasoleiman - How Structure Helps in Machine Learning - YouTube</A>
							<DT><A HREF="https://nn.labml.ai/unet/index.html">U-Net</A>
							<DT><A HREF="https://github.com/Atcold/NYU-DLSP21">Atcold/NYU-DLSP21: NYU Deep Learning Spring 2021</A>
							<DT><A HREF="https://atcold.github.io/NYU-DLSP21/">DEEP LEARNING · Deep Learning</A>
							<DT><A HREF="https://uvadlc.github.io/">UvA Deep Learning Course (Amsterdam)</A>
							<DT><A HREF="https://mathematical-tours.github.io/maths-ia-course/">The Mathematics of IA - Mathematical Tours of Data Sciences</A>
							<DT><A HREF="https://cs229.stanford.edu/lectures-spring2022/main_notes.pdf">CS229 Lecture Notes</A>
							<DT><A HREF="https://www.youtube.com/watch?v=UOvPeC8WOt8">The Neural Network, A Visual Introduction - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ys-G9uEW3O0">02 – Discrete probability recap, Naïve Bayes classification - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=BjyZcSiVg5A">Deep Learning Theory Session. Ilya SutskeverIlya Sutskever - YouTube</A>
							<DT><A HREF="https://github.com/EurekaLabsAI">EurekaLabsAI</A>
						</DL><p>
						<DT><H3 FOLDED>deep-learning-books</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=kuvFoXzTK3E">Prof. Chris Bishop's NEW Deep Learning Textbook (Microsoft)</A>
							<DT><A HREF="https://www.cis.upenn.edu/~jean/math-deep.pdf">Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning</A>
						</DL><p>
						<DT><H3 FOLDED>Machine Learning</H3>
						<DL><p>
							<DT><H3 FOLDED>Physics-Informed Learning</H3>
							<DL><p>
								<DT><A HREF="https://mitmath.github.io/18337/lecture15/diffeq_machine_learning">Mixing Differential Equations and Neural Networks</A>
								<DT><A HREF="https://www.youtube.com/channel/UCDtsHjkOEMHYPGgpKX8VOPg/videos">Parallel Computing and Scientific Machine Learning (MIT 2021 Spring)</A>
								<DT><A HREF="https://twitter.com/martinmbauer/status/1529705943386275842">Protons are not fundamental particles, but dynamical systems made of quarks and gluons.</A>
								<DT><A HREF="https://twitter.com/thuereyGroup/status/1529177918479511552">How much does training with a differentiable physics simulator really improve things?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5P19hROy9vk">Why Quantum Mechanics Uses the Physics of SPRINGS - Quantum Harmonic Oscillators EXPLAINED - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=GCz6afDVy5Y">AI/ML+Physics: Recap and Summary [Physics Informed Machine Learning] - YouTube</A>
								<DT><A HREF="https://x.com/oharub/status/1863616236497633596">(1) Ruben Ohana en X: "Generating cat videos is nice, but what if you could tackle real scientific problems with the same methods? 🧪🌌 Introducing The Well: 16 datasets (15TB) for Machine Learning, from astrophysics to fluid dynamics and biology. 🐙: https://t.co/PMAHK7i2lG 📜: https://t.co/6XLJA5lJnI https://t.co/1uqb0XUV4s" / X</A>
							</DL><p>
							<DT><H3 FOLDED>ml-cs</H3>
							<DL><p>
								<DT><A HREF="http://cs229.stanford.edu/syllabus.html">CS229: Machine Learning</A>
								<DT><A HREF="http://cs229.stanford.edu/#info">CS229: Machine Learning</A>
								<DT><A HREF="https://www.youtube.com/@machinelearningatberkeley8868">Machine Learning at Berkeley - YouTube</A>
							</DL><p>
							<DT><A HREF="https://www.simonwenkel.com/index.html">SimonWenkel.com</A>
							<DT><A HREF="https://developers.google.com/machine-learning/glossary/">Machine Learning Glossary  |  Google Developers</A>
							<DT><A HREF="https://github.com/ageron/handson-ml2">handson-ml2: A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.</A>
							<DT><A HREF="https://www.eleuther.ai/beginners.pdf">A Beginner’s Guide to Machine Learning</A>
							<DT><A HREF="https://github.com/ageron/handson-ml2">Fundamentals of Machine Learning and Deep Learning in Python (Sklearn, Keras, TensorFlow v2)</A>
							<DT><A HREF="https://snap-research.github.io/BitsFusion/">Snap.Inc: BitsFusion</A>
						</DL><p>
						<DT><H3 FOLDED>Constractive Learning</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</A>
							<DT><A HREF="https://arxiv.org/abs/1911.05722">Momentum Contrast for Unsupervised Visual Representation Learning</A>
							<DT><A HREF="https://crfm.stanford.edu/2022/04/14/contrastive-learning.html">Understanding Deep Learning with Unlabeled Data: Contrastive Learning</A>
						</DL><p>
						<DT><H3 FOLDED>Self-Supervised Learning</H3>
						<DL><p>
							<DT><H3 FOLDED>DINO</H3>
							<DL><p>
								<DT><A HREF="https://dinov2.metademolab.com/">DINOv2 by Meta AI</A>
								<DT><A HREF="https://github.com/facebookresearch/dinov2">facebookresearch/dinov2: PyTorch code and models for the DINOv2 self-supervised learning method.</A>
								<DT><A HREF="https://github.com/facebookresearch/dino">facebookresearch/dino: PyTorch code for Vision Transformers training with the Self-Supervised learning method DINO</A>
								<DT><A HREF="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/dino.py">vit-pytorch/vit_pytorch/dino.py at main · lucidrains/vit-pytorch</A>
								<DT><A HREF="https://x.com/Almorgand/status/1872598770179035191">DINO-X: A unified vision model for open-world object detection and understanding</A>
								<DT><A HREF="https://x.com/TongPetersb/status/1907477153202971022">Peter Tong en X: "Vision models have been smaller than language models; what if we scale them up? Introducing Web-SSL: A family of billion-scale SSL vision models (up to 7B parameters) trained on billions of images without language supervision, using VQA to evaluate the learned representation. https://t.co/TF0llXGGWf" / X</A>
							</DL><p>
							<DT><H3 FOLDED>JEPA</H3>
							<DL><p>
								<DT><H3 FOLDED>I-JEPA</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/docs/transformers/en/model_doc/ijepa">I-JEPA</A>
								</DL><p>
								<DT><H3 FOLDED>V-JEPA</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2404.08471">[2404.08471] Revisiting Feature Prediction for Learning Visual Representations from Video</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=jSdHmImyUjk&ab_channel=YannicKilcher">JEPA - A Path Towards Autonomous Machine Intelligence (Paper Explained) - YouTube</A>
								<DT><A HREF="https://arxiv.org/abs/2307.12698">[2307.12698] MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</A>
								<DT><A HREF="https://github.com/facebookresearch/jepa">facebookresearch/jepa: PyTorch code and models for V-JEPA self-supervised learning from video.</A>
								<DT><A HREF="https://openreview.net/forum?id=BZ5a1r-kVsf">A Path Towards Autonomous Machine Intelligence | OpenReview</A>
							</DL><p>
							<DT><H3 FOLDED>DoRA</H3>
							<DL><p>
								<DT><A HREF="https://shashankvkt.github.io/dora">Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video</A>
							</DL><p>
							<DT><A HREF="https://openreview.net/pdf?id=Yen1lGns2o">IS IMAGENET WORTH 1 VIDEO? LEARNING STRONG IMAGE ENCODERS FROM 1 LONG UNLABELLED VIDEO</A>
							<DT><A HREF="https://arxiv.org/abs/2103.00020">CLIP: Learning Transferable Visual Models From Natural Language Supervision</A>
							<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1744716166290096161">(Yann LeCun) Adaptive linear classifiers</A>
							<DT><A HREF="https://github.com/imbue-ai/self_supervised">imbue-ai/self_supervised: A Pytorch-Lightning implementation of self-supervised algorithms</A>
							<DT><A HREF="https://github.com/microsoft/unilm">microsoft/unilm: Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities</A>
							<DT><A HREF="https://kexue.fm/archives/9359">Using the heat equation to guide self-supervised learning</A>
							<DT><A HREF="https://x.com/nrehiew_/status/1864677276731777136">(1) wh en X: "1 of the top papers from ICLR 2024 (honourable mention) tldr: introduce a new self-supervised learning method for vision. Training on a single video matches models trained on imagenet https://t.co/kF84CoBPy2" / X</A>
							<DT><A HREF="https://x.com/TongPetersb/status/1907477153202971022">Peter Tong en X: "Vision models have been smaller than language models; what if we scale them up? Introducing Web-SSL: A family of billion-scale SSL vision models (up to 7B parameters) trained on billions of images without language supervision, using VQA to evaluate the learned representation. https://t.co/TF0llXGGWf" / X</A>
						</DL><p>
						<DT><H3 FOLDED>blueprint</H3>
						<DL><p>
							<DT><H3 FOLDED>Tensor Programs</H3>
							<DL><p>
								<DT><A HREF="https://thegregyang.com/">Greg Yang | Professional page</A>
								<DT><A HREF="https://arxiv.org/abs/2310.02244">[2310.02244] Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks</A>
								<DT><A HREF="https://www.youtube.com/watch?v=1aXOXHA7Jcw&t=7148s">Greg Yang | Large N Limits: Random Matrices &amp; Neural Networks | The Cartesian Cafe w/ Timothy Nguyen - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>blueprint-geometric-deep-learninng</H3>
							<DL><p>
								<DT><H3 FOLDED>geometric-deep-learninng-papers</H3>
								<DL><p>
									<DT><A HREF="https://geometricdeeplearning.com/">Geometric Deep Learning - Grids, Groups, Graphs, Geodesics, and Gauges</A>
									<DT><A HREF="https://www.youtube.com/watch?v=e5233FYNfNQ">Math Reading Group - Geometric Deep Learning I (06/08/23) - YouTube</A>
									<DT><A HREF="https://geometricdeeplearning.com/lectures/">GDL Course</A>
									<DT><A HREF="https://arxiv.org/abs/2408.00949">[2408.00949] Equivariant neural networks and piecewise linear representation theory</A>
									<DT><A HREF="https://thegradient.pub/towards-geometric-deep-learning/">Towards Geometric Deep Learning</A>
								</DL><p>
								<DT><H3 FOLDED>geometric-deep-learninng-lectures</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=PtA0lg_e5nA&list=PLn2-dEmQeTfQ8YVuHBOvAhUlnIPYxkeu3">youtube</A>
									<DT><A HREF="https://medium.com/towards-data-science/geometric-deep-learning-da09e7c17aa3">Introduction to Geometric Deep Learning | by Ahmed A. A. Elhag</A>
								</DL><p>
								<DT><H3 FOLDED>geometric-deep-learninng-software</H3>
								<DL><p>
									<DT><A HREF="https://blog.paperspace.com/geometric-deep-learning-framework-comparison/">Geometric Deep Learning Library Comparison | Paperspace Blog</A>
									<DT><A HREF="https://medium.com/towards-data-science/graph-neural-networks-a-learning-journey-since-2008-from-python-to-jax-graph-attention-networks-692e4d6d7637">Graph Neural Networks: A learning journey since 2008 — From Python to JAX: Graph Attention Networks | by Stefano Bosisio | Mar, 2022 | Towards Data Science</A>
									<DT><A HREF="https://twitter.com/phillip_lippe/status/1536340878960173057">"Are you interested in learning JAX with Flax? We have translated our popular Deep Learning tutorials on CNNs, GNNs, (Vision) Transformers, and more from PyTorch to JAX+Flax, with considerable speedups for smaller models!</A>
									<DT><A HREF="https://github.com/phlippe/uvadlc_notebooks">phlippe/uvadlc_notebooks: Repository of Jupyter notebook tutorials for teaching the Deep Learning Course at the University of Amsterdam (MSc AI), Fall 2021/Spring 2022</A>
									<DT><A HREF="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html">Tutorial 2 (JAX): Introduction to JAX+Flax — UvA DL Notebooks v1.2 documentation</A>
									<DT><A HREF="https://uvadlc-notebooks.readthedocs.io/en/latest/index.html">Welcome to the UvA Deep Learning Tutorials! — UvA DL Notebooks v1.2 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>Graph Neural Networks</H3>
								<DL><p>
									<DT><A HREF="http://snap.stanford.edu/graphsage/">GraphSAGE: embedding framework</A>
									<DT><A HREF="https://towardsdatascience.com/transformers-are-graph-neural-networks-bca9f75412aa">Transformers are Graph Neural Networks</A>
									<DT><A HREF="https://www.youtube.com/watch?v=7KMcXHwQzZs">Michael Bronstein | Neural diffusion PDEs, differential geometry, and graph neural networks</A>
									<DT><A HREF="https://towardsdatascience.com/manifold-learning-2-99a25eeb677d">Latent graph neural networks: Manifold learning 2.0?</A>
									<DT><A HREF="https://distill.pub/2021/gnn-intro/">A Gentle Introduction to Graph Neural Networks</A>
									<DT><A HREF="https://bookdown.org/omarlizardo/_main/4-6-network-composition-homophily-measures.html">4.6 Network Composition-Homophily Measures</A>
									<DT><H3 FOLDED>theory</H3>
									<DL><p>
										<DT><A HREF="https://ncatlab.org/nlab/show/Lipschitz+map">Lipschitz map</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/Banach+space">Banach space</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/permutation">permutation</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/linear%20order">linear order</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/cartesian+product">cartesian product</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/hypergraph">hypergraph in nLab</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/span">span in nLab</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/Bayesian+reasoning">Bayesian reasoning in nLab</A>
										<DT><A HREF="https://ncatlab.org/nlab/show/probability+space">probability space in nLab</A>
										<DT><A HREF="https://en.wikipedia.org/wiki/Network_homophily">homophily</A>
									</DL><p>
									<DT><H3 FOLDED>PDE</H3>
									<DL><p>
										<DT><A HREF="https://blog.twitter.com/engineering/en_us/topics/insights/2021/graph-neural-networks-as-neural-diffusion-pdes">Graph Neural Networks as Neural Diffusion PDEs</A>
										<DT><A HREF="https://towardsdatascience.com/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a">Graph Neural Networks beyond Weisfeiler-Lehman and vanilla Message Passing</A>
										<DT><A HREF="https://www.youtube.com/watch?v=9SMbH18nMUg">Graph Neural Networks and Diffusion PDEs | Benjamin Chamberlain &amp; James Rowbottom - YouTube</A>
									</DL><p>
									<DT><A HREF="https://twitter.com/n_keriven/status/1529404489832308736">Theoretical analysis of graph (over)smoothing</A>
									<DT><A HREF="https://twitter.com/rampasek/status/1536534947825176577">Graph Transformers at scale</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Network_homophily">homophily</A>
									<DT><A HREF="https://towardsdatascience.com/predictions-and-hopes-for-geometric-graph-ml-in-2022-aa3b8b79f5cc">What does 2022 hold for Geometric &amp; Graph ML?</A>
									<DT><A HREF="https://towardsdatascience.com/using-subgraphs-for-more-expressive-gnns-8d06418d5ab">Using Subgraphs for More Expressive GNNs | by Michael Bronstein | Towards Data Science</A>
									<DT><A HREF="https://graphdeeplearning.github.io/post/benchmarking-gnns/">Benchmarking Graph Neural Networks | NTU Graph Deep Learning Lab</A>
									<DT><A HREF="https://distill.pub/2021/understanding-gnns/">Understanding Convolutions on Graphs</A>
								</DL><p>
								<DT><A HREF="https://michael-bronstein.medium.com/">Michael Bronstein – Medium (Geometrical Deep Learning)</A>
								<DT><A HREF="https://www.quantamagazine.org/researchers-build-ai-that-builds-ai-20220125/">Researchers Build AI That Builds AI | Quanta Magazine</A>
								<DT><A HREF="https://www.youtube.com/watch?v=rie-9AEhYdY">WE MUST ADD STRUCTURE TO DEEP LEARNING BECAUSE... - YouTube</A>
								<DT><A HREF="https://x.com/PetarV_93/status/1821543305274167767">(1) Petar Veličković en X: "I don't think I've been this excited about a geometric deep learning paper in a long time 🤯 Gibson, Tubbenhauer and Williamson: Equivariant neural networks and piecewise linear representation theory https://t.co/9DSlEnke9H" / X</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/TheGregYang/status/1711803863177855033">Infinite Depth Neural Networks (Greg Yang)</A>
							<DT><A HREF="https://twitter.com/TheGregYang/status/1709700104951931077">Neural Nets: What if depth -&gt; inf as well?</A>
							<DT><A HREF="https://twitter.com/hayou_soufiane/status/1622594202042499073">Q: What happens to the neural covariance when both Width and Depth are taken to infinity?</A>
							<DT><A HREF="https://irhum.github.io/blog/spherical-harmonics/">irhum.github.io - Visual Notes on Spherical Harmonics</A>
							<DT><A HREF="https://github.com/P2333/Bag-of-Tricks-for-AT">P2333/Bag-of-Tricks-for-AT: Empirical tricks for training robust models (ICLR 2021)</A>
						</DL><p>
						<DT><H3 FOLDED>The Bitter Leasson</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/polynoamial/status/1789381426187546644">(1) Noam Brown en X: "@jxmnop The point of the Bitter Lesson is that research and clever ideas are important, but people should think about how their ideas scale with data and compute rather than just relying on One Weird Trick to get them a little farther than SOTA." / X</A>
							<DT><A HREF="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</A>
							<DT><A HREF="https://twitter.com/polynoamial">(1) Noam Brown (@polynoamial) / X</A>
						</DL><p>
						<DT><H3 FOLDED>SDE vs ODE</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/wgilpin0/status/1737934963365056543">William Gilpin: "Can machine learning predict chaos?</A>
						</DL><p>
						<DT><H3 FOLDED>Neural Architecture Search</H3>
						<DL><p>
							<DT><A HREF="https://blog.research.google/2022/02/unlocking-full-potential-of-datacenter.html">Unlocking the Full Potential of Datacenter ML Accelerators with Platform-Aware Neural Architecture Search – Google Research Blog</A>
							<DT><A HREF="https://x.com/antferdom/status/1799474799661691014">xLSTM won't replace the Transformer. Two bitter lessons</A>
						</DL><p>
						<DT><H3 FOLDED>differentiable neural computers</H3>
						<DL><p>
							<DT><A HREF="https://jaspock.github.io/funicular/dnc.html">A bit-by-bit guide to the equations governing differentiable neural computers</A>
							<DT><A HREF="https://gwern.net/doc/reinforcement-learning/model-free/2016-graves.pdf">https://gwern.net/doc/reinforcement-learning/model-free/2016-graves.pdf</A>
							<DT><A HREF="https://deepmind.google/discover/blog/differentiable-neural-computers/">Differentiable neural computers - Google DeepMind</A>
						</DL><p>
						<DT><H3 FOLDED>deep-learning-debugging</H3>
						<DL><p>
							<DT><H3 FOLDED>silent data corruption (SDC)</H3>
							<DL><p>
								<DT><A HREF="https://dl.acm.org/doi/abs/10.1145/3620666.3651349">Dr. DNA: Combating Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations | Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</A>
								<DT><A HREF="https://dl.acm.org/doi/pdf/10.1145/3620666.3651349">Dr. DNA: Combating Silent Data Corruptions in Deep Learning using Distribution of Neuron Activations</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>deep double descent</H3>
						<DL><p>
							<DT><A HREF="https://openai.com/index/deep-double-descent/">Deep double descent | OpenAI</A>
							<DT><A HREF="https://www.youtube.com/watch?v=W_TAKJRgrbs">Ilya Sutskever on Deep Double Descent - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=5I7-ItaOZFU&t=7s">Double Descent explained by Yann LeCun - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=qRHdQz_P_Lo&t=1s">Statistical Learning: 10.7 Interpolation and Double Descent - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Kih-VPHL3gA">Deep Double Descent and Overparameterization: Classical Machine Learning vs. Modern Deep Learning - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=7L9wVeStNPA">Deep Double Descent: Where Bigger Models and More Data Hurts - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Grokking</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=dND-7llwrpw&t=230s">Grokking: Generalization beyond Overfitting on small algorithmic datasets (Paper Explained) - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Nvb_4Jj5kBo">Why "Grokking" AI Would Be A Key To AGI - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Meta-Learning and Self-Play</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=9EN_HoEk3KY">Ilya Sutskever: OpenAI Meta-Learning and Self-Play | MIT Artificial General Intelligence (AGI) - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>GFlowNets</H3>
						<DL><p>
							<DT><A HREF="https://machinelearning.apple.com/research/improving-gflownets">Improving GFlowNets for Text-to-Image Diffusion Alignment - Apple Machine Learning Research</A>
						</DL><p>
						<DT><H3 FOLDED>inverse problems</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=038QxC5kTLk&list=LL&index=16&t=666s">Giannis Daras - Generative Models and Comp. Imaging: Soft Diffusion and Learning from Corrupted Data - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=9Y8NKkuPhC0&list=LL&index=17&t=4s">Hyungjin Chung - Adapting and Regularizing Diffusion Models for Inverse Problems - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Time-Series</H3>
						<DL><p>
							<DT><A HREF="https://huggingface.co/ibm-granite/granite-timeseries-patchtst">ibm-granite/granite-timeseries-patchtst · Hugging Face</A>
							<DT><A HREF="https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/">A decoder-only foundation model for time-series forecasting</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Time_series">Time series - Wikipedia</A>
							<DT><A HREF="https://github.com/google-research/timesfm">google-research/timesfm: TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.</A>
							<DT><A HREF="https://huggingface.co/google/timesfm-1.0-200m">google/timesfm-1.0-200m · Hugging Face</A>
							<DT><A HREF="https://arxiv.org/pdf/2310.10688">A DECODER-ONLY FOUNDATION MODEL FOR TIME-SERIES FORECASTING</A>
							<DT><A HREF="https://deepmind.google/discover/blog/gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-sota-accuracy/">GenCast predicts weather and the risks of extreme conditions with state-of-the-art accuracy - Google DeepMind</A>
							<DT><A HREF="https://www.kaggle.com/competitions/jane-street-real-time-market-data-forecasting">Jane Street Real-Time Market Data Forecasting | Kaggle</A>
						</DL><p>
						<DT><A HREF="https://x.com/MajmudarAdam/status/1794190796411027791">History of deep learning (core)</A>
						<DT><A HREF="https://github.com/adam-maj/deep-learning">adam-maj/deep-learning: A deep-dive on the entire history of deep-learning</A>
						<DT><A HREF="https://gist.github.com/matijagrcic/ae8353eb1e6be84a7c85d9fdc2f9631f">Ilya Sutskever reading list.md</A>
						<DT><A HREF="https://github.com/google-research/tuning_playbook">google-research/tuning_playbook: A playbook for systematically maximizing the performance of DNN</A>
						<DT><A HREF="https://twitter.com/agikoala/status/1641838208823468032/photo/1">(1) jason en X: "Three starter pokemon in deep learning https://t.co/u6RZqrvsAJ" / X</A>
						<DT><A HREF="https://twitter.com/prateeky2806/status/1665759148380758022">Mergin diff task-specific models into a multi-task model</A>
						<DT><A HREF="https://www.youtube.com/watch?v=pA-azVdihQc&t=1890s">What is Machine Learning Good For? - Alex Davies - YouTube</A>
						<DT><A HREF="https://arxiv.org/pdf/2002.09398.pdf">It’s Not What Machines Can Learn, It’s What We Cannot Teach (DeepMind Math Reasoning)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=5exL8UYxpsI&t=63s">IFML SEMINAR: 1/26/24 - Meta Optimization (Google DeepMind)</A>
						<DT><A HREF="https://starai.cs.ucla.edu/papers/ZhangIJCAI23.pdf">On the Paradox of Learning to Reason from Data</A>
						<DT><A HREF="https://github.com/gregorbachmann/scaling_mlps">gregorbachmann/scaling_mlps</A>
						<DT><A HREF="https://www.youtube.com/watch?v=1menqhfNzzo">Stanford EE364A Convex Optimization I Stephen Boyd I 2023 I Lecture 3 - YouTube</A>
						<DT><A HREF="https://www.youtube.com/watch?v=6_v9Ogi7P6Q">Topology for Energy-Based models</A>
						<DT><A HREF="https://arxiv.org/abs/2403.14606">[2403.14606] The Elements of Differentiable Programming</A>
						<DT><A HREF="https://scholar.google.co.uk/citations?view_op=view_citation&hl=en&user=DaFHynwAAAAJ&citation_for_view=DaFHynwAAAAJ:isC4tDSrTZIC">Neural turing machines (main)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=yZ-HT81qYuE">What is Differentiable Programming - YouTube</A>
						<DT><A HREF="https://twitter.com/BenTheEgg">Each new layer adds residual information,  so simple concepts are nailed early, complex ones late</A>
						<DT><A HREF="https://www.youtube.com/watch?v=EvSe0ktD95k&t=7s">A Path Towards Autonomous Machine Intelligence with Dr. Yann LeCun - YouTube</A>
						<DT><A HREF="https://www.youtube.com/watch?v=GyKlMcsl72w">00 – Course introduction - YouTube</A>
						<DT><A HREF="https://stanford-cs336.github.io/spring2024/">Stanford CS336 | Language Modeling from Scratch</A>
						<DT><A HREF="https://rentry.org/LocalModelsPapers">Local Models Related Papers</A>
						<DT><A HREF="https://www.youtube.com/watch?v=GxjEjy5UYJU&t=25s">AI Reading List (by Ilya Sutskever) - Part 2 - YouTube</A>
						<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples/tree/master">NVIDIA/DeepLearningExamples: State-of-the-Art Deep Learning scripts organized by models</A>
					</DL><p>
					<DT><H3 FOLDED>Reinforcement Learning</H3>
					<DL><p>
						<DT><H3 FOLDED>search-and-learning</H3>
						<DL><p>
							<DT><H3 FOLDED>Game Theory</H3>
							<DL><p>
								<DT><A HREF="https://pub.towardsai.net/deepminds-clever-idea-to-master-asymmetric-games-79c3461ef6e">DeepMind’s Clever Idea to Master Asymmetric Games</A>
							</DL><p>
							<DT><H3 FOLDED>self-play</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=EY9iHSe82Hc">Self-Play by Noam Brown - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>dynamic programming</H3>
							<DL><p>
								<DT><A HREF="https://developer.nvidia.com/blog/boosting-dynamic-programming-performance-using-nvidia-hopper-gpu-dpx-instructions/">Boosting Dynamic Programming Performance Using NVIDIA Hopper GPU DPX Instructions | NVIDIA Technical Blog</A>
								<DT><A HREF="https://web.mit.edu/dimitrib/www/abstractdp_MIT.html">LESSONS FROM ALPHAZERO FOR OPTIMAL, MODEL PREDICTIVE, AND ADAPTIVE CONTROL</A>
							</DL><p>
							<DT><H3 FOLDED>Monte Carlo Tree Search</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=Fbs4lnGLS8M">Monte Carlo Tree Search (MCTS) Tutorial - YouTube</A>
							</DL><p>
							<DT><A HREF="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</A>
							<DT><A HREF="https://web.mit.edu/dimitrib/www/abstractdp_MIT.html">LESSONS FROM ALPHAZERO FOR OPTIMAL, MODEL PREDICTIVE, AND ADAPTIVE CONTROL</A>
							<DT><A HREF="https://www.umut-acar.org/self-adjusting-computation">Umut A. Acar - Self-Adjusting Computation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=BJi6N4tDupk">OpenAI: Teaching AI Through Self-Play - Ilya Sutskever</A>
							<DT><A HREF="https://www.youtube.com/watch?v=uz83G-2ny8Q">DeepMind’s New AI Saw 15,000,000,000 Chess Boards! - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=EvSe0ktD95k&t=7s">A Path Towards Autonomous Machine Intelligence with Dr. Yann LeCun - YouTube</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/b36a7273c61c4315d5d01925a0d2a67534bcb965/extra/mcts_search.py#L34">tinygrad/extra/mcts_search.py at b36a7273c61c4315d5d01925a0d2a67534bcb965 · tinygrad/tinygrad</A>
						</DL><p>
						<DT><H3 FOLDED>MuZero</H3>
						<DL><p>
							<DT><A HREF="https://github.com/kaesve/muzero">A clean implementation of MuZero and AlphaZero following the AlphaZero General framework. Train and Pit both algorithms against each other, and investigate reliability of learned MuZero MDP models.</A>
							<DT><A HREF="https://github.com/chiamp/muzero-cartpole">Cartpole</A>
							<DT><A HREF="https://twitter.com/DrJimFan/status/1627354160529285120?lang=en">(Jim Fan)"What’s next after open source and open model? Open training</A>
							<DT><A HREF="https://github.com/google-deepmind/mctx">google-deepmind/mctx: Monte Carlo tree search in JAX</A>
							<DT><A HREF="https://www.youtube.com/watch?v=yIrFIOx4VP8&t=21939s">George Hotz | Programming | Can MuZero play Tic Tac Toe? | Part1 | DeepMind AI - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=xc0jGZYFQLQ&t=63799s">George Hotz | Programming | Fun with MuZero and MCTS on a lovely Sunday | CartPole | DeepMind AI - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=62nq4Zsn8vc&t=1027s">Alpha Zero and Monte Carlo Tree Search - YouTube</A>
							<DT><A HREF="https://lczero.org/">Leela Chess Zero</A>
							<DT><A HREF="https://twitter.com/DrJimFan/status/1625538305889820673">mctx</A>
							<DT><A HREF="https://arxiv.org/abs/1911.08265">[1911.08265] Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</A>
							<DT><A HREF="https://deepmind.google/technologies/alphazero-and-muzero/?_gl=1*1ai5qs2*_up*MQ..*_ga*MTk2ODY0MzQ4Ny4xNjk5MjE2MzUz*_ga_LS8HVHCNQ0*MTY5OTIxNjM1My4xLjAuMTY5OTIxNjM3My4wLjAuMA..">AlphaZero and MuZero - Google DeepMind</A>
							<DT><A HREF="https://www.youtube.com/watch?v=xc0jGZYFQLQ&t=63800s">George Hotz | Programming | Fun with MuZero and MCTS on a lovely Sunday | CartPole | DeepMind AI - YouTube</A>
							<DT><A HREF="https://github.com/xjdr-alt/muzero_sketch">xjdr-alt/muzero_sketch</A>
						</DL><p>
						<DT><H3 FOLDED>AlphaGo</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>AlphaZero</H3>
						<DL><p>
							<DT><A HREF="https://web.mit.edu/dimitrib/www/abstractdp_MIT.html">LESSONS FROM ALPHAZERO FOR OPTIMAL, MODEL PREDICTIVE, AND ADAPTIVE CONTROL</A>
						</DL><p>
						<DT><H3 FOLDED>Entity-Based Reinforcement Learning</H3>
						<DL><p>
							<DT><A HREF="https://clemenswinter.com/2023/04/14/entity-based-reinforcement-learning/">Entity-Based Reinforcement Learning | Clemens' Blog</A>
						</DL><p>
						<DT><A HREF="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">A Taxonomy of RL Algorithms</A>
						<DT><A HREF="https://arxiv.org/abs/2412.05265">[2412.05265] Reinforcement Learning: A Comprehensive Overview</A>
						<DT><A HREF="https://arxiv.org/abs/1707.03497">(Deepmind) Value Prediction Network: predictions of FUTURE VALUES rather than of future observations.</A>
						<DT><A HREF="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">Introduction to Reinforcement Learning with David Silver (Deepmind)</A>
						<DT><A HREF="http://www.incompleteideas.net/book/the-book.html">Sutton &amp; Barto Book: Reinforcement Learning: An Introduction</A>
						<DT><A HREF="https://twitter.com/TheSequenceAI/status/1530591453243744256">Let's talk about Exploration-Exploitation Dilemma</A>
						<DT><A HREF="https://www.amazon.com/dp/0262039249/">Reinforcement Learning, second edition: An Introduction Sutton, Richard S., Barto, Andrew G</A>
						<DT><A HREF="https://arxiv.org/pdf/2201.02135.pdf">2022 June Arxiv Nature book</A>
						<DT><A HREF="http://www.incompleteideas.net/book/RLbook2020.pdf">Reinforcement Learning: An Introduction 2nd edition (Richard S. Sutton)</A>
						<DT><A HREF="https://github.com/lvwerra/rl-implementations">lvwerra/rl-implementations: This repo contains a set of notebooks to reproduce reinforcement learning algorithms.</A>
						<DT><A HREF="https://github.com/sotetsuk/pgx">sotetsuk/pgx: 🎲 Vectorized RL game environments written in JAX with end-to-end AlphaZero examples</A>
						<DT><A HREF="https://github.com/google-deepmind/dqn_zoo">google-deepmind/dqn_zoo: DQN Zoo is a collection of reference implementations of reinforcement learning agents developed at DeepMind based on the Deep Q-Network (DQN) agent.</A>
						<DT><A HREF="https://www.youtube.com/watch?v=-tZkb0vgaDk">George Hotz | Programming | RL is dumb and doesn't work | Reinforcement Learning LunarLander Part 2 - YouTube</A>
						<DT><A HREF="https://shield.ai/">Shield AI - Building The World’s Best AI Pilot</A>
						<DT><A HREF="https://x.com/RobertTLange">Rejax: JAX RL algorithms</A>
						<DT><A HREF="https://github.com/kvfrans/rlbase_stable">kvfrans/rlbase_stable</A>
						<DT><A HREF="https://github.com/openai/baselines">openai/baselines: OpenAI Baselines: high-quality implementations of reinforcement learning algorithms</A>
						<DT><A HREF="https://github.com/imbue-ai/garage">imbue-ai/garage: A toolkit for reproducible reinforcement learning research.</A>
						<DT><A HREF="https://pufferai.github.io/build/html/rst/landing.html">Emulation - PufferLib 1.0.0 documentation</A>
						<DT><A HREF="https://github.com/rl-tools/rl-tools">rl-tools/rl-tools: The Fastest Deep Reinforcement Learning Library</A>
						<DT><A HREF="https://x.com/i/bookmarks?post_id=1863436864411341112">Reward Hacking (OpenAI)</A>
						<DT><A HREF="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">Reward Hacking in Reinforcement Learning | Lil'Log</A>
						<DT><A HREF="https://naklecha.notion.site/a-reinforcement-learning-guide">a reinforcement learning guide</A>
						<DT><A HREF="https://www.youtube.com/watch?v=FgzM3zpZ55o">Stanford CS234: Reinforcement Learning | Winter 2019 | Lecture 1 - Introduction - Emma Brunskill - YouTube</A>
						<DT><A HREF="http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/richsprinciples.html">RLAI open web page</A>
						<DT><A HREF="https://x.com/tuzhaopeng/status/1906975869538914570">(1) Zhaopeng Tu en X: "Can reinforcement learning scale beyond math and coding tasks? Introducing Reinforcement Learning with Verifiable Rewards (RLVR) across diverse, less-structured domains (e.g., medicine, chemistry, psychology, economics, and education), where well-structured reference answers https://t.co/yFXnBkXdyC" / X</A>
						<DT><A HREF="https://www.youtube.com/watch?v=CNA1sb2fJS4">Inference Time Scaling for Generalist Reward Modeling - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>Diffusion</H3>
					<DL><p>
						<DT><H3 FOLDED>diffusion-theory</H3>
						<DL><p>
							<DT><H3 FOLDED>discrete-diffusion</H3>
							<DL><p>
								<DT><A HREF="https://x.com/cloneofsimo/status/1874911222459662525">discrete diffusion</A>
								<DT><A HREF="https://x.com/cloneofsimo/status/1876159854509019574">DDPD: planner decides which tokens to denoise and denoiser decides waht to replace it with</A>
								<DT><A HREF="https://github.com/fal-ai-community/minDDPD">fal-ai-community/minDDPD</A>
							</DL><p>
							<DT><A HREF="https://kexue-fm.translate.goog/archives/10567?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en-US&_x_tr_pto=wapp">A brief talk on generative diffusion model (Part 26): Distillation based on id</A>
							<DT><A HREF="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models? | Lil'Log</A>
							<DT><A HREF="https://github.com/stars/Yaofang-Liu/lists/diffusion-mdels">Yaofang-Liu's list / Diffusion Mdels</A>
							<DT><A HREF="https://sander.ai/2025/04/15/latents.html">Generative modelling in latent space – Sander Dieleman</A>
							<DT><A HREF="https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/">Building Diffusion Model's theory from ground up | ICLR Blogposts 2024</A>
							<DT><A HREF="https://kexue.fm/archives/9164">A talk on generative diffusion model (Part 3): DDPM = Bayesian + denoising</A>
							<DT><A HREF="https://kexue.fm/archives/10047">A brief talk on generative diffusion model (Part 22): Signal-to-noise ratio</A>
							<DT><A HREF="https://github.com/tmabraham/diffusion_reading_group">tmabraham/diffusion_reading_group: Diffusion Reading Group at EleutherAI</A>
							<DT><A HREF="https://astralord.github.io/posts/power-of-diffusion-models/">Power of Diffusion Models | AstraBlog</A>
							<DT><A HREF="https://sander.ai/2024/09/02/spectral-autoregression.html">Diffusion is spectral autoregression – Sander Dieleman</A>
							<DT><A HREF="https://sander.ai/2024/06/14/noise-schedules.html">Noise schedules considered harmful – Sander Dieleman</A>
							<DT><A HREF="https://sander.ai/2023/01/09/diffusion-language.html">Diffusion language models – Sander Dieleman</A>
							<DT><A HREF="https://sander.ai/2023/08/28/geometry.html">The geometry of diffusion guidance – Sander Dieleman</A>
							<DT><A HREF="https://sander.ai/2023/07/20/perspectives.html">Perspectives on diffusion – Sander Dieleman</A>
							<DT><A HREF="https://sander.ai/2022/05/26/guidance.html">Guidance: a cheat code for diffusion models – Sander Dieleman</A>
							<DT><A HREF="https://sander.ai/2022/01/31/diffusion.html">Diffusion models are autoencoders – Sander Dieleman</A>
							<DT><A HREF="https://sander.ai/2024/02/28/paradox.html">The paradox of diffusion distillation – Sander Dieleman</A>
							<DT><A HREF="https://imagen.research.google/">Imagen: Text-to-Image Diffusion Models</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-lectures</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=i2qSxMVeVLI&t=430s">How I Understand Diffusion Models - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=yxVcnuRrKqQ&t=53s">Generative Modeling - Normalizing Flows - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=038QxC5kTLk&list=LL&index=4&t=661s">Giannis Daras - Generative Models and Comp. Imaging: Soft Diffusion and Learning from Corrupted Data - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=9Y8NKkuPhC0&list=LL&index=5">Hyungjin Chung - Adapting and Regularizing Diffusion Models for Inverse Problems - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=1d4r19GEVos&list=LL&index=7&t=2025s">CVPR #18546 - Denoising Diffusion Models: A Generative Learning Big Bang - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=038QxC5kTLk&list=LL&index=16&t=666s">Giannis Daras - Generative Models and Comp. Imaging: Soft Diffusion and Learning from Corrupted Data - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=9Y8NKkuPhC0&list=LL&index=17&t=4s">Hyungjin Chung - Adapting and Regularizing Diffusion Models for Inverse Problems - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=NJ72iEPRXFk">Sanjay Shakkottai: Tutorial on the Mathematical Foundations of Diffusion Models for Image Generation - YouTube</A>
							<DT><A HREF="https://github.com/ChenHsing/Awesome-Video-Diffusion-Models">ChenHsing/Awesome-Video-Diffusion-Models: [CSUR] A Survey on Video Diffusion Models</A>
							<DT><A HREF="https://madebyoll.in/posts/dino_diffusion/#why-it-works">Bare-bones Diffusion Models</A>
							<DT><A HREF="https://github.com/helblazer811/diffusion-visualizations">helblazer811/diffusion-visualizations: Visualizations of the theory behind diffusion models.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=0V96wE7lY4w">Denoising Autoencoders | Deep Learning Animated - YouTube</A>
							<DT><A HREF="https://x.com/i/bookmarks?post_id=1863080363792408864">(1) X</A>
							<DT><A HREF="https://x.com/i/bookmarks?post_id=1862843110247563343">The link between diffusion models and optimal transport</A>
							<DT><A HREF="https://github.com/wangkai930418/awesome-diffusion-categorized">wangkai930418/awesome-diffusion-categorized: collection of diffusion model papers categorized by their subareas</A>
							<DT><A HREF="https://x.com/rmsnorm/status/1864452411118514369">(1) Nick Stracke en X: "🤔 Why do we extract diffusion features from noisy images? Isn’t that destroying information? Yes, it is - but we found a way to do better. 🚀 Here’s how we unlock better features, no noise, no hassle 🧵👇 https://t.co/xPj1xsytn9" / X</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-autoregression</H3>
						<DL><p>
							<DT><H3 FOLDED>language-models-diffusion</H3>
							<DL><p>
								<DT><H3 FOLDED>LLaDA</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2502.09992">[2502.09992] Large Language Diffusion Models</A>
									<DT><A HREF="https://x.com/cloneofsimo/status/1897275066628665542">(1) Simo Ryu en X: "Will be speedrunning LLaDA (MDM) in 3 hours on Twitch https://t.co/doV679xISo https://t.co/d2LNZdPFaG" / X</A>
									<DT><A HREF="https://www.youtube.com/watch?v=hnYn8QHR8UI">llada repro cloneofsimo - YouTube</A>
									<DT><A HREF="https://x.com/cloneofsimo/status/1897563505160851658">Simo Ryu en X: "LLaDA with muP. it just works, again. Im so tired of saying it works. Just use it, and thank me later https://t.co/gJimDN2IHj" / X</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>4o-image-generation</H3>
							<DL><p>
								<DT><H3 FOLDED>transfusion</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2408.11039">[2408.11039] Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</A>
								</DL><p>
								<DT><H3 FOLDED>cm3leon</H3>
								<DL><p>
									<DT><A HREF="https://ai.meta.com/blog/generative-ai-text-images-cm3leon/">Introducing CM3leon, a more efficient, state-of-the-art generative model for text and images</A>
									<DT><A HREF="https://github.com/kyegomez/CM3Leon">kyegomez/CM3Leon: An open source implementation of "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning", an all-new multi modal AI that uses just a decoder to generate both text and images</A>
									<DT><A HREF="https://github.com/facebookresearch/chameleon">facebookresearch/chameleon: Repository for Meta Chameleon, a mixed-modal early-fusion foundation model from FAIR.</A>
								</DL><p>
								<DT><H3 FOLDED>autoregressive-distillation-dit</H3>
								<DL><p>
									<DT><A HREF="https://ai.meta.com/research/publications/autoregressive-distillation-of-diffusion-transformers/">Autoregressive Distillation of Diffusion Transformers | Research - AI at Meta</A>
									<DT><A HREF="https://scontent.fsvq1-1.fna.fbcdn.net/v/t39.2365-6/489921800_654822090742241_4867106051645455876_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=FJVUjlh6Kx0Q7kNvwE5NHi4&_nc_oc=AdmUSk3zqZprFi2186ZSkR8RpIGMeJmXCc7eMBc_L4qoNUsU4WgcgxSN126Po9YoIes&_nc_zt=14&_nc_ht=scontent.fsvq1-1.fna&_nc_gid=vNO4Ueq_YVDbyNWlU33gsQ&oh=00_AfEP6Ak5T7tle6ySM2N2HDqzdk9iIvFQI5dRGfD9bahXBA&oe=6804525A">utoregressive Distillation of Diffusion Transformers</A>
									<DT><A HREF="https://github.com/alsdudrla10/ARD">alsdudrla10/ARD: [CVPR 2025 Oral] PyTorch re-implementation for Autoregressive Distillation of Diffusion Transformers (ARD).</A>
									<DT><A HREF="https://github.com/facebookresearch/DiT">facebookresearch/DiT: Official PyTorch Implementation of "Scalable Diffusion Models with Transformers"</A>
								</DL><p>
								<DT><A HREF="https://openai.com/index/introducing-4o-image-generation/">Introducing 4o Image Generation | OpenAI</A>
								<DT><A HREF="https://openai.com/index/image-gpt/">Image GPT | OpenAI</A>
							</DL><p>
							<DT><H3 FOLDED>deepseek-janus</H3>
							<DL><p>
								<DT><A HREF="https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf">Janus/janus_pro_tech_report.pdf at main · deepseek-ai/Janus</A>
								<DT><A HREF="https://github.com/deepseek-ai/Janus">deepseek-ai/Janus: Janus-Series: Unified Multimodal Understanding and Generation Models</A>
								<DT><A HREF="https://github.com/deepseek-ai/Janus?tab=readme-ov-file#text-to-image-generation">deepseek-ai/Janus: Janus-Series: Unified Multimodal Understanding and Generation Models</A>
								<DT><A HREF="https://huggingface.co/deepseek-ai/Janus-Pro-7B/tree/main">deepseek-ai/Janus-Pro-7B at main</A>
							</DL><p>
							<DT><H3 FOLDED>emu</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>JetFormer</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2411.19722">[2411.19722] JetFormer: An Autoregressive Generative Model of Raw Images and Text</A>
								<DT><A HREF="https://x.com/giffmana/status/1863657113953657129">(1) Lucas Beyer (bl16) en X: "The first of the cool things we* got this week! Typically, you'd train a VQ-VAE/GAN tokenizer first, and then use its tokens for your LLM/DiT/... But we all know eventually end-to-end wins over pipelines. With flow models, you can actually learn pixel-LLM-pixel end-to-end!" / X</A>
							</DL><p>
							<DT><H3 FOLDED>hart</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>llamagen</H3>
							<DL><p>
								<DT><A HREF="https://github.com/FoundationVision/LlamaGen">FoundationVision/LlamaGen: Autoregressive Model Beats Diffusion: 🦙 Llama for Scalable Image Generation</A>
							</DL><p>
							<DT><H3 FOLDED>diffusion-autoregression-parallel-decoding</H3>
							<DL><p>
								<DT><A HREF="https://x.com/supremeZhuang/status/1868839299707502943">(1) Bohan Zhuang en X: "We'd like to share our latest simple and effective work, ZipAR, which reduces 91% of auto-regressive image generation overhead without any training. Addressing the issue of slow decoding in AR image generation models (such as Emu3, Lumina-mGPT, LlamaGen, and Janus), we propose a https://t.co/ZFUW3fy4g6" / X</A>
								<DT><A HREF="https://arxiv.org/abs/2412.04062">[2412.04062] ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality</A>
								<DT><A HREF="https://github.com/ThisisBillhe/ZipAR">ThisisBillhe/ZipAR: This is the official PyTorch implementation of "ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality"</A>
							</DL><p>
							<DT><A HREF="https://x.com/sedielem/status/1820233922287919263">(1) Sander Dieleman en X: "The interpretation of diffusion as autoregression in the frequency domain seems to be stirring up a lot of thought! (I may or may not have a new blog post in the works 🧐) https://t.co/XSxP27pKSt" / X</A>
							<DT><A HREF="https://sander.ai/2024/09/02/spectral-autoregression.html">Diffusion is spectral autoregression – Sander Dieleman</A>
							<DT><A HREF="https://x.com/nrehiew_/status/1832412663273464152">(1) wh en X: "A visualization of how I think of diffusion in frequency space Diffusion often generates the low frequencies in the earlier steps before generating the higher frequencies in the later steps https://t.co/rPebHNxozZ" / X</A>
							<DT><A HREF="https://emu.baai.ac.cn/about">Emu3</A>
							<DT><A HREF="https://arxiv.org/pdf/2409.18869">Emu3: Next-Token Prediction is All You Need</A>
							<DT><A HREF="https://github.com/FoundationVision/LlamaGen">FoundationVision/LlamaGen: Autoregressive Model Beats Diffusion: 🦙 Llama for Scalable Image Generation</A>
							<DT><A HREF="https://x.com/nrehiew_/status/1863591609834615070">(1) wh en X: "The 6th highest scored paper (7,8,8,8) going into Neurips 2024 tldr: They introduce a new image generation approach that uses an autoregressive transformer to predict increasingly larger latent feature maps starting from a single pixel feature map to the final latent/image https://t.co/56m0LhV6dV" / X</A>
							<DT><A HREF="https://openreview.net/pdf?id=gojL67CfS8">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</A>
							<DT><A HREF="https://github.com/FoundationVision/VAR">FoundationVision/VAR: [NeurIPS 2024 Oral][GPT beats diffusion🔥] [scaling laws in visual generation📈] Official impl. of "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction". An *ultra-simple, user-friendly yet state-of-the-art* codebase for autoregressive image generation!</A>
							<DT><A HREF="https://x.com/thjashin/status/1866916736290697421">google-deepmind/md4: Simplified and Generalized Masked Diffusion for Discrete Data</A>
							<DT><A HREF="https://arxiv.org/abs/2406.04329">[2406.04329] Simplified and Generalized Masked Diffusion for Discrete Data</A>
							<DT><A HREF="https://neurips.cc/virtual/2024/poster/93071">NeurIPS Poster Simplified and Generalized Masked Diffusion for Discrete Data</A>
							<DT><A HREF="https://arxiv.org/pdf/2404.02905">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</A>
							<DT><A HREF="https://openai.com/index/image-gpt/">Image GPT | OpenAI</A>
							<DT><A HREF="https://x.com/Dorialexander/status/1891408227171975375">Large Language Diffusion Models</A>
							<DT><A HREF="https://x.com/DimitrisPapail/status/1906524049053827289">(1) Dimitris Papailiopoulos en X: "OK something VERY COOL is happening, and 4o+imagen CAN generate VALID mazes when they are in the shape of a rhombus, i.e., 45° rotated square Some examples below (the red squiggly line is mine). The prompt is "generate an image of a valid maze in a shape of a rhombus." https://t.co/F3EimKsBhl" / X</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-inference-optimization</H3>
						<DL><p>
							<DT><A HREF="https://github.com/vdesai2014/inference-optimization-blog-post">vdesai2014/inference-optimization-blog-post</A>
							<DT><A HREF="https://www.vrushankdes.ai/diffusion-inference-optimization">Diffusion Inference Optimization</A>
							<DT><A HREF="https://github.com/PipeFusion/PipeFusion">PipeFusion/PipeFusion: A Suite of Parallel Approaches for Inference of Diffusion Transformer Models on GPU Clusters</A>
							<DT><A HREF="https://github.com/PipeFusion/PatchVAE">PipeFusion/PatchVAE: A patch parallelism VAE implement for high resolution generation</A>
							<DT><A HREF="https://github.com/mit-han-lab/distrifuser">mit-han-lab/distrifuser: [CVPR 2024 Highlight] DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</A>
							<DT><A HREF="https://arxiv.org/abs/2402.19481">[2402.19481] DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</A>
							<DT><A HREF="https://github.com/czg1225/AsyncDiff">czg1225/AsyncDiff: Official implementation of "AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising"</A>
							<DT><A HREF="https://ar5iv.labs.arxiv.org/html/2312.09608">[2312.09608] Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models</A>
							<DT><A HREF="https://github.com/hutaiHang/Faster-Diffusion">hutaiHang/Faster-Diffusion: Official implementation of "Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models"</A>
							<DT><A HREF="https://machinelearning.apple.com/research/efficient-diffusion-models">Efficient Diffusion Models without Attention - Apple Machine Learning Research</A>
							<DT><A HREF="https://gist.github.com/Chillee/41baf11aac8036d25d637321c48dad20">You Could Have Invented Flash-Attention!</A>
							<DT><A HREF="https://github.com/siliconflow/onediff/blob/main/onediff_diffusers_extensions/examples/sd3/text_to_image_sd3.py">onediff/onediff_diffusers_extensions/examples/sd3/text_to_image_sd3.py at main · siliconflow/onediff</A>
							<DT><A HREF="https://www.felixsanz.dev/articles/ultimate-guide-to-optimizing-stable-diffusion-xl">Ultimate guide to optimizing Stable Diffusion XL - Félix Sanz</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-training</H3>
						<DL><p>
							<DT><H3 FOLDED>diffusion-training-amp</H3>
							<DL><p>
								<DT><H3 FOLDED>diffusion-amp-fp8</H3>
								<DL><p>
									<DT><A HREF="https://github.com/graphcore-research/out-of-the-box-fp8-training/blob/main/out_of_the_box_fp8_training.ipynb">out-of-the-box-fp8-training/out_of_the_box_fp8_training.ipynb at main · graphcore-research/out-of-the-box-fp8-training</A>
									<DT><A HREF="https://github.com/PaddlePaddle/PaddleMIX/blob/dc863a4655debeba1a6d664125dfcf3ab2238fc9/ppdiffusers/examples/class_conditional_image_generation/DiT/diffusion/dit.py#L371">PaddleMIX/ppdiffusers/examples/class_conditional_image_generation/DiT/diffusion/dit.py at dc863a4655debeba1a6d664125dfcf3ab2238fc9 · PaddlePaddle/PaddleMIX</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>mosaicml-diffusion</H3>
							<DL><p>
								<DT><A HREF="https://github.com/mosaicml/diffusion">mosaicml/diffusion</A>
							</DL><p>
							<DT><H3 FOLDED>DiT-MoE</H3>
							<DL><p>
								<DT><A HREF="https://github.com/feizc/DiT-MoE">feizc/DiT-MoE: Scaling Diffusion Transformers with Mixture of Experts</A>
							</DL><p>
							<DT><H3 FOLDED>diffusion-lora</H3>
							<DL><p>
								<DT><A HREF="https://github.com/cloneofsimo/lora">cloneofsimo/lora: Using Low-rank adaptation to quickly fine-tune diffusion models.</A>
							</DL><p>
							<DT><H3 FOLDED>REPA</H3>
							<DL><p>
								<DT><A HREF="https://sihyun.me/REPA/">Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think</A>
								<DT><A HREF="https://github.com/cloneofsimo/repa-rf">cloneofsimo/repa-rf</A>
							</DL><p>
							<DT><H3 FOLDED>WebDataset</H3>
							<DL><p>
								<DT><A HREF="https://github.com/webdataset/webdataset">webdataset/webdataset: A high-performance Python-based I/O system for large (and small) deep learning problems, with strong support for PyTorch.</A>
							</DL><p>
							<DT><A HREF="https://gist.github.com/cloneofsimo/73db5aaa6d74b1b3eebce31333083afa">SDXL_trainer</A>
							<DT><A HREF="https://arxiv.org/abs/2407.15811">[2407.15811] Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget</A>
							<DT><A HREF="https://snap-research.github.io/BitsFusion/">BitsFusion</A>
							<DT><A HREF="https://github.com/PaddlePaddle/PaddleMIX/tree/dc863a4655debeba1a6d664125dfcf3ab2238fc9">PaddlePaddle/PaddleMIX</A>
							<DT><A HREF="https://github.com/cloneofsimo/minRF">cloneofsimo/minRF: Minimal implementation of scalable rectified flow transformers, based on SD3's approach</A>
							<DT><A HREF="https://github.com/cloneofsimo/minDiffusion">cloneofsimo/minDiffusion: Self-contained, minimalistic implementation of diffusion models with Pytorch.</A>
							<DT><A HREF="https://github.com/Kwai-Kolors/Kolors">Kwai-Kolors/Kolors: Kolors Team</A>
							<DT><A HREF="https://github.com/Anima-Lab/MaskDiT">Anima-Lab/MaskDiT: Code for Fast Training of Diffusion Models with Masked Transformers</A>
							<DT><A HREF="https://github.com/bghira/SimpleTuner?tab=readme-ov-file#flux1">bghira/SimpleTuner: A general fine-tuning kit geared toward diffusion models.</A>
							<DT><A HREF="https://github.com/fal-ai/diffusion-speedrun">fal-ai/diffusion-speedrun</A>
							<DT><A HREF="https://github.com/fal-ai-community/nano-mdm">fal-ai-community/nano-mdm: Tiny re-implementation of MDM in style of LLaDA and nano-gpt speedrun</A>
							<DT><A HREF="https://github.com/fal-ai-community/minDDPD">fal-ai-community/minDDPD</A>
						</DL><p>
						<DT><H3 FOLDED>Diffusion-distillation</H3>
						<DL><p>
							<DT><H3 FOLDED>Imagine Flash (Meta AI)</H3>
							<DL><p>
								<DT><A HREF="https://ai.meta.com/research/publications/imagine-flash-accelerating-emu-diffusion-models-with-backward-distillation/?utm_source=linkedin&utm_medium=organic_social&utm_content=image&utm_campaign=imagineflash">Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation | Research - AI at Meta</A>
								<DT><A HREF="https://scontent.fsvq5-1.fna.fbcdn.net/v/t39.2365-6/10000000_1650715635757274_3499348867570367129_n.pdf?_nc_cat=111&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=E__oAKOcVaIQ7kNvgG2Odel&_nc_ht=scontent.fsvq5-1.fna&oh=00_AfABQIIxviDeDBA4t-0_pSaK-QGovi86kWT62uEJom-57w&oe=66355F6B">Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation</A>
							</DL><p>
							<DT><H3 FOLDED>SDXL-Lightning: Progressive Adversarial Diffusion Distillation (ByteDance)</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2402.13929">[2402.13929] SDXL-Lightning: Progressive Adversarial Diffusion Distillation</A>
								<DT><A HREF="https://arxiv.org/html/2402.13929v1">SDXL-Lightning: Progressive Adversarial Diffusion Distillation</A>
								<DT><A HREF="https://developers.cloudflare.com/workers-ai/models/stable-diffusion-xl-lightning/">stable-diffusion-xl-lightning · Cloudflare Workers AI docs</A>
								<DT><A HREF="https://huggingface.co/ByteDance/SDXL-Lightning">ByteDance/SDXL-Lightning · Hugging Face</A>
							</DL><p>
							<DT><A HREF="https://scontent.fsvq5-1.fna.fbcdn.net/v/t39.2365-6/10000000_1650715635757274_3499348867570367129_n.pdf?_nc_cat=111&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=E__oAKOcVaIQ7kNvgG2Odel&_nc_ht=scontent.fsvq5-1.fna&oh=00_AfABQIIxviDeDBA4t-0_pSaK-QGovi86kWT62uEJom-57w&oe=66355F6B">Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation</A>
							<DT><A HREF="https://arxiv.org/abs/2311.17042">[2311.17042] Adversarial Diffusion Distillation</A>
							<DT><A HREF="https://hyper-sd.github.io/">Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis</A>
						</DL><p>
						<DT><H3 FOLDED>Latent Consistency Models</H3>
						<DL><p>
							<DT><A HREF="https://gsunshine.notion.site/Consistency-Models-Made-Easy-954205c0b4a24c009f78719f43b419cc?pvs=4">💫 Consistency Models Made Easy</A>
							<DT><A HREF="https://twitter.com/ZhengyangGeng/status/1773069621211415003">Zhengyang Geng: fast Consistency Models inference</A>
							<DT><A HREF="https://twitter.com/DrYangSong">(1) Yang Song (@DrYangSong) / X</A>
							<DT><A HREF="https://sander.ai/2024/02/28/paradox.html">The paradox of diffusion distillation – Sander Dieleman</A>
							<DT><A HREF="https://github.com/locuslab/ect">locuslab/ect: Consistency Models Made Easy</A>
							<DT><A HREF="https://arxiv.org/pdf/2410.11081">simplifying, stabilizing and scaling continous tiem consistency models</A>
							<DT><A HREF="https://www.youtube.com/watch?v=gHYy0zalMSA">MAJOR inference efficiency gain for diffusion models - YouTube</A>
							<DT><A HREF="https://huggingface.co/OPPOer/TLCMFlux">OPPOer/TLCMFlux · Hugging Face</A>
							<DT><A HREF="https://github.com/luosiallen/latent-consistency-model?tab=readme-ov-file">luosiallen/latent-consistency-model: Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference</A>
						</DL><p>
						<DT><H3 FOLDED>DiT</H3>
						<DL><p>
							<DT><H3 FOLDED>DiT-optimization</H3>
							<DL><p>
								<DT><H3 FOLDED>diffusers</H3>
								<DL><p>
									<DT><H3 FOLDED>diffusers-imports</H3>
									<DL><p>
										<DT><A HREF="https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py">optuna/optuna/integration/__init__.py at master</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/42cae93b942ec904ead46c26c42be24422adc92c/src/diffusers/utils/import_utils.py#L760">diffusers/src/diffusers/utils/import_utils.py</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/67bef2027cc461af5bbe73b3c0f35bb1350f5aa8/src/diffusers/pipelines/consistency_models/__init__.py">diffusers/src/diffusers/pipelines/consistency_models/__init__.py</A>
									</DL><p>
									<DT><H3 FOLDED>diffusers-patch</H3>
									<DL><p>
										<DT><A HREF="https://github.com/chengzeyi/piflux/blob/main/src/piflux/adapters/diffusers.py">piflux/src/piflux/adapters/diffusers.py at main · chengzeyi/piflux</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>xelerate</H3>
								<DL><p>
									<DT><H3 FOLDED>xelerate-blackwell</H3>
									<DL><p>
										<DT><A HREF="https://github.com/datacrunch-research/blogs/blob/main/wavespeed-flux-blackwell-technical-report/flux_blackwell_b200_technical_report.md">blogs/wavespeed-flux-blackwell-technical-report/flux_blackwell_b200_technical_report.md at main · datacrunch-research/blogs</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>dit-quantization</H3>
								<DL><p>
									<DT><H3 FOLDED>diffusion-quantization</H3>
									<DL><p>
										<DT><H3 FOLDED>diffusion-quantization-people</H3>
										<DL><p>
											<DT><A HREF="https://htqin.github.io/">Haotong Qin</A>
											<DT><A HREF="https://github.com/LeiWang1999">Lei Wang: BitBLAS</A>
										</DL><p>
										<DT><A HREF="https://arxiv.org/abs/2302.04304">[2302.04304] Q-Diffusion: Quantizing Diffusion Models</A>
										<DT><A HREF="https://arxiv.org/abs/2305.10657">[2305.10657] PTQD: Accurate Post-Training Quantization for Diffusion Models</A>
										<DT><A HREF="https://github.com/ziplab/PTQD">ziplab/PTQD: The official implementation of PTQD: Accurate Post-Training Quantization for Diffusion Models</A>
										<DT><A HREF="https://arxiv.org/pdf/2310.03270">EFFICIENTDM: EFFICIENT QUANTIZATION-AWARE FINE-TUNING OF LOW-BIT DIFFUSION MODELS</A>
										<DT><A HREF="https://github.com/IST-DASLab/QUIK">IST-DASLab/QUIK: Repository for the QUIK project, enabling the use of 4bit kernels for generative inference</A>
										<DT><A HREF="https://github.com/ThisisBillhe/torch_quantizer?tab=readme-ov-file">ThisisBillhe/torch_quantizer: torch_quantizer is a out-of-box quantization tool for PyTorch models on CUDA backend, specially optimized for Diffusion Models.</A>
										<DT><A HREF="https://arxiv.org/abs/2310.09259">[2310.09259] QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language Models</A>
										<DT><A HREF="https://github.com/IST-DASLab/marlin">IST-DASLab/marlin: FP16xINT4 LLM inference kernel that can achieve near-ideal ~4x speedups up to medium batchsizes of 16-32 tokens.</A>
										<DT><A HREF="https://github.com/snap-research/BitsFusion">snap-research/BitsFusion</A>
										<DT><A HREF="https://github.com/microsoft/BitBLAS">microsoft/BitBLAS: BitBLAS is a library to support mixed-precision matrix multiplications, especially for quantized LLM deployment.</A>
										<DT><A HREF="https://github.com/htqin/awesome-model-quantization">htqin/awesome-model-quantization: A list of papers, docs, codes about model quantization. This repo is aimed to provide the info for model quantization research, we are continuously improving the project. Welcome to PR the works (papers, repositories) that are missed by the repo.</A>
										<DT><A HREF="https://github.com/bytedance/decoupleQ">bytedance/decoupleQ: A quantization algorithm for LLM</A>
										<DT><A HREF="https://github.com/bytedance/AffineQuant">bytedance/AffineQuant: Official implementation of the ICLR 2024 paper AffineQuant</A>
										<DT><A HREF="https://github.com/Xiuyu-Li/q-diffusion">Xiuyu-Li/q-diffusion: [ICCV 2023] Q-Diffusion: Quantizing Diffusion Models.</A>
										<DT><A HREF="https://github.com/aredden/flux-fp8-api/blob/main/float8_quantize.py">flux-fp8-api/float8_quantize.py at main · aredden/flux-fp8-api</A>
									</DL><p>
									<DT><H3 FOLDED>diffusers-torchao</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sayakpaul/diffusers-torchao">sayakpaul/diffusers-torchao: End-to-end recipes for optimizing diffusion models with torchao and diffusers (inference and FP8 training).</A>
									</DL><p>
									<DT><H3 FOLDED>Nunchaku</H3>
									<DL><p>
										<DT><A HREF="https://github.com/mit-han-lab/nunchaku">mit-han-lab/nunchaku: SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</A>
									</DL><p>
									<DT><H3 FOLDED>flux-fp4</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/mit-han-lab/svdq-fp4-flux.1-dev">mit-han-lab/svdq-fp4-flux.1-dev · Hugging Face</A>
									</DL><p>
									<DT><A HREF="https://hanlab.mit.edu/projects/svdquant">SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</A>
									<DT><A HREF="https://github.com/mit-han-lab/deepcompressor">mit-han-lab/deepcompressor: Model Compression Toolbox for Large Language Models and Diffusion Models</A>
								</DL><p>
								<DT><H3 FOLDED>dit-cache</H3>
								<DL><p>
									<DT><H3 FOLDED>AdaCache</H3>
									<DL><p>
										<DT><A HREF="https://github.com/AdaCache-DiT/AdaCache">AdaCache-DiT/AdaCache: Adaptive Caching for Faster Video Generation with Diffusion Transformers</A>
										<DT><A HREF="https://adacache-dit.github.io/clarity/adacache_meta.pdf">Adaptive Caching for Faster Video Generation with Diffusion Transformers</A>
									</DL><p>
									<DT><H3 FOLDED>ToCa</H3>
									<DL><p>
										<DT><A HREF="https://github.com/Shenyi-Z/ToCa">Shenyi-Z/ToCa: Accelerating Diffusion Transformers with Token-wise Feature Caching</A>
										<DT><A HREF="https://github.com/Shenyi-Z/ToCa/commit/d0ef7e114c7bcc825c3aac809f22a25b6758df86">New version of DiT-ToCa · Shenyi-Z/ToCa@d0ef7e1</A>
										<DT><A HREF="https://arxiv.org/pdf/2410.05317#page=19">ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHING</A>
									</DL><p>
									<DT><H3 FOLDED>FORA</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2407.01425">[2407.01425] FORA: Fast-Forward Caching in Diffusion Transformer Acceleration</A>
										<DT><A HREF="https://github.com/prathebaselva/FORA">prathebaselva/FORA: FORA introduces simple yet effective caching mechanism in Diffusion Transformer Architecture for faster inference sampling.</A>
									</DL><p>
									<DT><H3 FOLDED>FasterCache</H3>
									<DL><p>
										<DT><A HREF="https://github.com/Vchitect/FasterCache">Vchitect/FasterCache: FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality</A>
										<DT><A HREF="https://arxiv.org/abs/2410.19355">[2410.19355] FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality</A>
										<DT><A HREF="https://vchitect.github.io/FasterCache/">FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality</A>
									</DL><p>
									<DT><H3 FOLDED>ParaAttention-caching</H3>
									<DL><p>
										<DT><H3 FOLDED>first-block-cache</H3>
										<DL><p>
											<DT><A HREF="https://github.com/alexarmbr/ParaAttention/pull/1/commits/db3309716ac2d8f2f10c8fa2ea72aeb41f5f76d3">Fbcache logging by alexarmbr · Pull Request #1 · alexarmbr/ParaAttention</A>
											<DT><A HREF="https://github.com/chengzeyi/ParaAttention/tree/5ae75fe7201528e97fbef5a377b80d12496fb499/src/para_attn/first_block_cache">ParaAttention/src/para_attn/first_block_cache</A>
										</DL><p>
										<DT><A HREF="https://huggingface.co/blog/NagaSaiAbhinay/transformer-layers-as-painters-dit">Extending *Transformer layers as Painters* to DiT's</A>
									</DL><p>
									<DT><H3 FOLDED>TeaCache</H3>
									<DL><p>
										<DT><A HREF="https://github.com/LiewFeng/TeaCache">LiewFeng/TeaCache: Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model</A>
										<DT><A HREF="https://github.com/MingXiangL/Teacache-xDiT">MingXiangL/Teacache-xDiT: Combining Teacache with xDiT to Accelerate Visual Generation Models</A>
										<DT><A HREF="https://github.com/MingXiangL/Teacache-xDiT/blob/c86f95a36f7727f8d0c41705474dca559595a99d/examples/teacache/flux_teacache.py">Teacache-xDiT/examples/teacache/flux_teacache.py at c86f95a36f7727f8d0c41705474dca559595a99d · MingXiangL/Teacache-xDiT</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2411.13588">[2411.13588] Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study</A>
								</DL><p>
								<DT><H3 FOLDED>dit-linear-attention</H3>
								<DL><p>
									<DT><H3 FOLDED>Sana</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2410.10629">SANA: EFFICIENT HIGH-RESOLUTION IMAGE SYNTHESIS WITH LINEAR DIFFUSION TRANSFORMERS</A>
										<DT><A HREF="https://www.linkedin.com/feed/update/urn:li:activity:7251843706310275072/">NVIDIA Research, Efficient AI</A>
										<DT><A HREF="https://github.com/NVlabs/Sana">NVlabs/Sana: SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer</A>
										<DT><A HREF="https://arxiv.org/html/2411.10958v1">SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration</A>
										<DT><A HREF="https://x.com/nrehiew_/status/1861605525558812880">(2) wh en X: "9th highest scored ICLR 2025 paper 8,8,8,10. Worth noting all reviewers increased their scores by 2 after rebuttals tldr: they introduce a bunch of architectural changes to a diffusion transformer, getting 100x speed improvements with no real quality impacts https://t.co/SKNMLiV6rL" / X</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/pull/9982">[Sana] Add Sana, including `SanaPipeline`, `SanaPAGPipeline`, `LinearAttentionProcessor`, `Flow-based DPM-sovler` and so on. by lawrence-cj · Pull Request #9982 · huggingface/diffusers</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/pull/9982/files#diff-d908866bff59e8dee0188bcff59bd36ca51ecb8f262e5f6893b9224f1b4dde89">[Sana] Add Sana, including `SanaPipeline`, `SanaPAGPipeline`, `LinearAttentionProcessor`, `Flow-based DPM-sovler` and so on. by lawrence-cj · Pull Request #9982 · huggingface/diffusers</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/html/2501.12976v1">LiT: Delving into a Simplified Linear Diffusion Transformer for Image Generation</A>
								</DL><p>
								<DT><H3 FOLDED>dit-distributed-inference</H3>
								<DL><p>
									<DT><H3 FOLDED>ParaAttention</H3>
									<DL><p>
										<DT><H3 FOLDED>_templated_ring_attention</H3>
										<DL><p>
										</DL><p>
										<DT><H3 FOLDED>op_replace</H3>
										<DL><p>
											<DT><A HREF="https://github.com/Vchitect/Vchitect-2.0/blob/master/op_replace.py">Vchitect-2.0/op_replace.py at master · Vchitect/Vchitect-2.0</A>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-interface</H3>
										<DL><p>
											<DT><A HREF="https://github.com/chengzeyi/ParaAttention/blob/main/src/para_attn/para_attn_interface.py">ParaAttention/src/para_attn/para_attn_interface.py at main · chengzeyi/ParaAttention</A>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-dynamo</H3>
										<DL><p>
											<DT><A HREF="https://github.com/chengzeyi/ParaAttention/pull/25/files">fix compability with torch 2.6.0 dynamo by chengzeyi · Pull Request #25 · chengzeyi/ParaAttention</A>
											<DT><A HREF="https://github.com/chengzeyi/ParaAttention/blob/9ac562bc9b4a2a3f0f7c12fffda869516bb45d9d/src/para_attn/para_attn_interface.py#L304">ParaAttention/src/para_attn/para_attn_interface.py at 9ac562bc9b4a2a3f0f7c12fffda869516bb45d9d · chengzeyi/ParaAttention</A>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-inductor</H3>
										<DL><p>
											<DT><A HREF="https://github.com/chengzeyi/ParaAttention/issues/22">cannot torch.compile the HunyuanVideo video transformer · Issue #22 · chengzeyi/ParaAttention</A>
											<DT><A HREF="https://github.com/chengzeyi/ParaAttention/commit/46bf9e762a8470fbe1adedb2c3add1e662cdc170">fix hunyuanvideo context parallel with compile · chengzeyi/ParaAttention@46bf9e7</A>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-kernels</H3>
										<DL><p>
											<DT><H3 FOLDED>QuantumAttention</H3>
											<DL><p>
												<DT><H3 FOLDED>QuantumAttention-TK</H3>
												<DL><p>
													<DT><H3 FOLDED>exp2</H3>
													<DL><p>
														<DT><A HREF="https://github.com/deepseek-ai/FlashMLA/blob/18e32770cc719687e869af6a7df4686dee67e041/csrc/softmax.h#L111">softmax.h#L111 This allows the compiler to use the ffma instruction instead of fadd and fmul separately.</A>
														<DT><A HREF="https://github.com/WaveSpeedAI/QuantumAttention/blob/0baf4b3fd3c568964acd4f176c35e8f073c70c20/src/quantum_attn/tk/attention.py#L31">QuantumAttention/src/quantum_attn/tk/attention.py at 0baf4b3fd3c568964acd4f176c35e8f073c70c20 · WaveSpeedAI/QuantumAttention</A>
													</DL><p>
													<DT><A HREF="https://github.com/WaveSpeedAI/QuantumAttention/blob/e7b647eb86596609ea68d372c9291a2e63e16d16/src/quantum_attn/tk/attention.py">QuantumAttention/src/quantum_attn/tk/attention.py at e7b647eb86596609ea68d372c9291a2e63e16d16 · WaveSpeedAI/QuantumAttention</A>
												</DL><p>
												<DT><A HREF="https://github.com/chengzeyi/QuantumAttention/commit/35abd08f042314e01bad999a0521faee24ca4d6a">Dev kernel (#1) · chengzeyi/QuantumAttention@35abd08</A>
												<DT><A HREF="https://github.com/chengzeyi/QuantumAttention/blob/main/src/quantum_attn/config.py">QuantumAttention/src/quantum_attn/config.py at main env vars patch</A>
												<DT><A HREF="https://github.com/search?q=repo%3Achengzeyi%2FQuantumAttention%20attention.force_eager_fallback&type=code">config patching (e.g. attention.force_eager_fallback)</A>
												<DT><A HREF="https://github.com/WaveSpeedAI/QuantumAttention/commit/e7b647eb86596609ea68d372c9291a2e63e16d16">Dev fp8 attn per head (#3) · WaveSpeedAI/QuantumAttention@e7b647e</A>
											</DL><p>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-server</H3>
										<DL><p>
											<DT><H3 FOLDED>ParaAttentio-distributed-server</H3>
											<DL><p>
												<DT><A HREF="https://github.com/chengzeyi/ParaAttention/pull/21/files">http server for distributed inference by alexarmbr · Pull Request #21 · chengzeyi/ParaAttention</A>
											</DL><p>
											<DT><A HREF="https://github.com/antferdom/ParaAttention/tree/main/server">ParaAttention/server at main · antferdom/ParaAttention</A>
											<DT><A HREF="https://github.com/xdit-project/xDiT/tree/main/http-service">xDiT/http-service at main · xdit-project/xDiT</A>
											<DT><A HREF="https://github.com/WaveSpeedAI/Step-Video-T2V-ParaAttention/blob/main/api/call_remote_server.py">Step-Video-T2V-ParaAttention/api/call_remote_server.py at main · WaveSpeedAI/Step-Video-T2V-ParaAttention</A>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-benchmark</H3>
										<DL><p>
											<DT><A HREF="https://docs.google.com/spreadsheets/d/18araPIxeeIGbS9BNooUCYsC4RkfnhIe4kcAqkscQGL4/edit?gid=0#gid=0">attention roofline - Google Sheets</A>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-test</H3>
										<DL><p>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-hf-checkpoint</H3>
										<DL><p>
											<DT><A HREF="https://huggingface.co/docs/huggingface_hub/en/guides/cli">Command Line Interface (CLI)</A>
											<DT><A HREF="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B">huggingface-cli download Wan-AI/Wan2.1-T2V-14B --local-dir ./Wan2.1-T2V-14B</A>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-stepVideo</H3>
										<DL><p>
											<DT><A HREF="https://github.com/WaveSpeedAI/Step-Video-T2V-ParaAttention/blob/main/api/call_remote_server.py">Step-Video-T2V-ParaAttention/api/call_remote_server.py at main · WaveSpeedAI/Step-Video-T2V-ParaAttention</A>
											<DT><A HREF="https://github.com/WaveSpeedAI/Step-Video-T2V-ParaAttention">WaveSpeedAI/Step-Video-T2V-ParaAttention</A>
										</DL><p>
										<DT><H3 FOLDED>ParaAttention-Wan</H3>
										<DL><p>
										</DL><p>
										<DT><A HREF="https://github.com/chengzeyi/ParaAttention/blob/main/examples/run_flux.py">ParaAttention/examples/run_flux.py at main · chengzeyi/ParaAttention</A>
										<DT><A HREF="https://github.com/chengzeyi/ParaAttention">chengzeyi/ParaAttention: [WIP] Context parallel attention that works with torch.compile</A>
										<DT><A HREF="https://github.com/chengzeyi/ParaAttention/blob/main/src/para_attn/para_attn_interface.py">ParaAttention/src/para_attn/para_attn_interface.py:  from torch.distributed.tensor.experimental._attention</A>
										<DT><A HREF="https://github.com/Tencent/HunyuanVideo/pull/68/commits/d227cb7c034f246251359a50ff292e300e692927">parallel inference using xdit by feifeibear · Pull Request #68: test_attention.py</A>
										<DT><A HREF="https://github.com/chengzeyi/ParaAttention/commit/9ac562bc9b4a2a3f0f7c12fffda869516bb45d9d">fix compability with torch 2.6.0 dynamo (#25) · chengzeyi/ParaAttention@9ac562b</A>
										<DT><A HREF="https://github.com/chengzeyi/ParaAttention/blob/9ac562bc9b4a2a3f0f7c12fffda869516bb45d9d/src/para_attn/para_attn_interface.py#L304">ParaAttention/src/para_attn/para_attn_interface.py at 9ac562bc9b4a2a3f0f7c12fffda869516bb45d9d · chengzeyi/ParaAttention</A>
									</DL><p>
									<DT><H3 FOLDED>piflux</H3>
									<DL><p>
										<DT><A HREF="https://github.com/chengzeyi/piflux">chengzeyi/piflux: (WIP) Parallel inference for black-forest-labs' FLUX model.</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>xDiT</H3>
								<DL><p>
									<DT><H3 FOLDED>xdit-dev</H3>
									<DL><p>
										<DT><H3 FOLDED>xdit-docker</H3>
										<DL><p>
											<DT><A HREF="https://hub.docker.com/r/thufeifeibear/xdit-dev/tags">thufeifeibear/xdit-dev Tags | Docker Hub</A>
											<DT><A HREF="https://github.com/xdit-project/HunyuanVideo/blob/main/docker/Dockerfile_xDiT">HunyuanVideo/docker/Dockerfile_xDiT at main · xdit-project/HunyuanVideo</A>
										</DL><p>
										<DT><A HREF="https://github.com/xdit-project/xDiT/blob/main/docs/developer/The_implement_design_of_xdit_framework.md">xDiT/docs/developer/The_implement_design_of_xdit_framework.md at main · xdit-project/xDiT</A>
										<DT><A HREF="https://github.com/xdit-project/xDiT/blob/main/docs/developer/Http_Service.md">xDiT/docs/developer/Http_Service.md at main · xdit-project/xDiT</A>
										<DT><A HREF="https://github.com/xdit-project/xDiT/blob/main/docs/developer/Manual_for_Adding_New_Models.md">xDiT/docs/developer/Manual_for_Adding_New_Models.md at main · xdit-project/xDiT</A>
										<DT><A HREF="https://github.com/xdit-project/xDiT/commit/251a7ba111252f48f53f3cc9ac323e61688c812c#diff-11e309e9b0185cf90cff5f7e301cccd5ec80b9ca75de069df2417ea07e15d3e3">[Tests]: Context and Sequence Parallel (#438) · xdit-project/xDiT@251a7ba</A>
										<DT><A HREF="https://github.com/xdit-project/xDiT/commit/a4757c9b169f9f10e87875e4171bb57af90e588f">[Bugfix] Fix for only one DIT worker and a separate VAE worker. (#443) · xdit-project/xDiT@a4757c9</A>
									</DL><p>
									<DT><H3 FOLDED>xdit-attention</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ai-compiler-study/flux/blob/main/src/flux/math.py">flux/src/flux/math.py at main · ai-compiler-study/flux</A>
										<DT><A HREF="https://github.com/facebookresearch/xformers/blob/77c1da7fe5c9a25f613bd4045bb1da4856310aab/xformers/ops/fmha/flash3.py#L57">xformers/xformers/ops/fmha/flash3.py: torch custom op wrapper logic FA3</A>
										<DT><A HREF="https://github.com/xdit-project/xDiT/commit/0cbdf4a00122a63ca654a6eb898f48901474e9cf">closes #334; prefer newer version of flash-attn (#394) · xdit-project/xDiT@0cbdf4a</A>
									</DL><p>
									<DT><H3 FOLDED>xdit-sequence-parallelism</H3>
									<DL><p>
										<DT><H3 FOLDED>YunChang</H3>
										<DL><p>
											<DT><H3 FOLDED>deepspeed-ulysses</H3>
											<DL><p>
												<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md">DeepSpeed/blogs/deepspeed-ulysses/README.md at master · microsoft/DeepSpeed</A>
											</DL><p>
											<DT><A HREF="https://github.com/feifeibear/long-context-attention">feifeibear/long-context-attention: USP: Unified (a.k.a. Hybrid, 2D) Sequence Parallel Attention for Long Context Transformers Model Training and Inference</A>
											<DT><A HREF="https://github.com/zhuzilin/ring-flash-attention">zhuzilin/ring-flash-attention: Ring attention implementation with flash attention</A>
											<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md">DeepSpeed/blogs/deepspeed-ulysses/README.md at master · microsoft/DeepSpeed</A>
											<DT><A HREF="https://arxiv.org/pdf/2405.07719">USP: A Unified Sequence Parallelism Approach for Long Context Generative AI</A>
										</DL><p>
										<DT><H3 FOLDED>xdit-usp</H3>
										<DL><p>
											<DT><A HREF="https://github.com/xdit-project/xDiT/commit/ca94011acb0a1699222431bfafce8720ef9d743d#diff-f5328dbf6a5ced8a82574ec233924b8ee3fa7ee69d56706dee83314ca38f2bd8">Support optimized USP in Flux (#368) · xdit-project/xDiT@ca94011</A>
										</DL><p>
										<DT><A HREF="https://github.com/xdit-project/xDiT/commit/ca94011acb0a1699222431bfafce8720ef9d743d#diff-f5328dbf6a5ced8a82574ec233924b8ee3fa7ee69d56706dee83314ca38f2bd8">Support optimized USP in Flux (#368) · xdit-project/xDiT@ca94011</A>
										<DT><A HREF="https://github.com/xdit-project/xDiT/blob/main/xfuser/model_executor/layers/usp_legacy.py">xDiT/xfuser/model_executor/layers/usp_legacy.py at main</A>
										<DT><A HREF="https://github.com/xdit-project/xDiT/blob/main/xfuser/model_executor/layers/usp.py">xDiT/xfuser/model_executor/layers/usp.py at main</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2411.01738">[2411.01738] xDiT: an Inference Engine for Diffusion Transformers (DiTs) with Massive Parallelism</A>
									<DT><A HREF="https://arxiv.org/abs/2405.07719">[2405.07719] USP: A Unified Sequence Parallelism Approach for Long Context Generative AI</A>
									<DT><A HREF="https://arxiv.org/abs/2405.14430">[2405.14430] PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference</A>
									<DT><A HREF="https://github.com/xdit-project/xDiT/tree/main">xdit-project/xDiT: xDiT: A Scalable Inference Engine for Diffusion Transformers (DiTs) on multi-GPU Clusters</A>
									<DT><A HREF="https://github.com/xdit-project/xDiT/blob/main/xfuser/model_executor/layers/attention_processor.py">xDiT/xfuser/model_executor/layers/attention_processor.py at main · xdit-project/xDiT</A>
									<DT><A HREF="https://gist.github.com/gradjitta/9afcdf70fee07a172cdf0de30e67ca82">report_hotswap.md</A>
									<DT><A HREF="https://www.youtube.com/watch?v=7DXnGrARqys">(1) TACO-DiT: Accelerating Your ComfyUI Generation Experience - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>dit-torch-compile</H3>
								<DL><p>
									<DT><A HREF="https://github.com/aredden/flux-fp8-api/blob/49c776c9185a2c87e38c57be6e4d41cd20e86021/flux_pipeline.py#L166">flux_pipeline.py#L166</A>
									<DT><A HREF="https://github.com/discus0434/faster-flux/blob/main/src/faster_flux/pipeline_wrapper.py">faster-flux/src/faster_flux/pipeline_wrapper.py at main · discus0434/faster-flux</A>
								</DL><p>
								<DT><H3 FOLDED>dit-distillation</H3>
								<DL><p>
									<DT><A HREF="https://github.com/TencentARC/FluxKits">TencentARC/FluxKits</A>
								</DL><p>
								<DT><H3 FOLDED>thu-nics</H3>
								<DL><p>
									<DT><A HREF="https://github.com/thu-nics">NICS-EFC Lab of Tsinghua University</A>
									<DT><A HREF="https://github.com/thu-nics/ViDiT-Q">thu-nics/ViDiT-Q: ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation</A>
									<DT><A HREF="https://github.com/thu-nics/DiTFastAttn">thu-nics/DiTFastAttn</A>
								</DL><p>
								<DT><A HREF="https://github.com/xdit-project/xDiT/tree/main">xdit-project/xDiT: xDiT: A Scalable Inference Engine for Diffusion Transformers (DiTs) on multi-GPU Clusters</A>
								<DT><A HREF="https://github.com/siliconflow/onediff/blob/main/onediff_diffusers_extensions/examples/sd3/text_to_image_sd3.py">onediff/onediff_diffusers_extensions/examples/sd3/text_to_image_sd3.py at main · siliconflow/onediff</A>
								<DT><A HREF="https://github.com/thu-nics/DiTFastAttn">thu-nics/DiTFastAttn</A>
								<DT><A HREF="https://github.com/aredden/flux-fp8-api/blob/main/float8_quantize.py#L273">flux-fp8-api/float8_quantize.py at main · aredden/flux-fp8-api</A>
								<DT><A HREF="https://github.com/aredden/flux-fp8-api/blob/main/float8_quantize.py">flux-fp8-api/float8_quantize.py at main · aredden/flux-fp8-api</A>
								<DT><A HREF="https://github.com/discus0434/comfyui-flux-accelerator">discus0434/comfyui-flux-accelerator: Accelerates Flux.1 image generation, just by using this node.</A>
								<DT><A HREF="https://github.com/discus0434/faster-flux/blob/main/src/faster_flux/pipeline_wrapper.py">faster-flux/src/faster_flux/pipeline_wrapper.py at main · discus0434/faster-flux</A>
								<DT><A HREF="https://arxiv.org/html/2406.01125v1">Δ-DiT: A Training-Free Acceleration Method Tailored for Diffusion Transformers</A>
								<DT><A HREF="https://github.com/discus0434/comfyui-flux-accelerator/blob/main/__init__.py">comfyui-flux-accelerator/__init__.py at main</A>
								<DT><A HREF="https://github.com/hustvl/DiG">hustvl/DiG: DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention</A>
								<DT><A HREF="https://github.com/gojasper/flash-diffusion">gojasper/flash-diffusion: Official implementation of ⚡ Flash Diffusion ⚡: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation</A>
								<DT><A HREF="https://github.com/kabachuha/OpenMMDiT">kabachuha/OpenMMDiT: Open(MM)DiT: An Easy, Fast and Memory-Efficient System for (MM)DiT Training and Inference</A>
								<DT><A HREF="https://github.com/NUS-HPC-AI-Lab/VideoSys">NUS-HPC-AI-Lab/VideoSys: VideoSys: An easy and efficient system for video generation</A>
								<DT><A HREF="https://github.com/sayakpaul/diffusers-torchao/blob/main/inference/benchmark_image.py">diffusers-torchao/inference/benchmark_image.py at main · sayakpaul/diffusers-torchao</A>
								<DT><A HREF="https://www.outerport.com/blog/fast-flux-load">How fast can you load a FLUX (LoRA) model?</A>
							</DL><p>
							<DT><H3 FOLDED>DiT-serving</H3>
							<DL><p>
								<DT><H3 FOLDED>xdit-serving</H3>
								<DL><p>
									<DT><A HREF="https://github.com/xdit-project/xDiT/commit/68328955f79300ac0ed7f243bf6cbb2bb045a5c9#diff-b335630551682c19a781afebcf4d07bf978fb1f8ac04c6bf87428ed5106870f5">add a http service demo. (#257) · xdit-project/xDiT@6832895</A>
									<DT><A HREF="https://github.com/xdit-project/xDiT/blob/main/entrypoints/launch.py">xDiT/entrypoints/launch.py at main · xdit-project/xDiT</A>
									<DT><A HREF="https://github.com/xdit-project/xDiT/commit/fb5c55aab778e95f538f38e5936ac2c1bf542dec">refactor http service (#465) · xdit-project/xDiT@fb5c55a</A>
								</DL><p>
								<DT><H3 FOLDED>dit-serving-lora</H3>
								<DL><p>
									<DT><A HREF="https://github.com/aredden/flux-fp8-api/blob/main/lora_loading.py">flux-fp8-api/lora_loading.py at main · aredden/flux-fp8-api</A>
								</DL><p>
								<DT><A HREF="https://github.com/aredden/flux-fp8-api/tree/main">aredden/flux-fp8-api: Flux diffusion model implementation using quantized fp8 matmul &amp; remaining layers use faster half precision accumulate, which is ~2x faster on consumer devices.</A>
								<DT><A HREF="https://github.com/cccntu/vFLUX">cccntu/vFLUX</A>
								<DT><A HREF="https://jonathanc.net/blogs/maximizing_pytorch_throughput">Maximizing PyTorch Throughput with FastAPI – Jonathan Chang’s Blog</A>
								<DT><A HREF="https://blog.vllm.ai/2024/09/05/perf-update.html">vLLM v0.6.0: 2.7x Throughput Improvement and 5x Latency Reduction | vLLM Blog</A>
							</DL><p>
							<DT><H3 FOLDED>image-generation</H3>
							<DL><p>
								<DT><H3 FOLDED>Seedream 3.0</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>Flux</H3>
								<DL><p>
									<DT><H3 FOLDED>flux-impl</H3>
									<DL><p>
										<DT><A HREF="https://github.com/black-forest-labs/flux">black-forest-labs/flux: Official inference repo for FLUX.1 models</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/pull/6334">Flux.1 by samm393 · Pull Request #6334 · tinygrad/tinygrad</A>
										<DT><A HREF="https://x.com/i/bookmarks?post_id=1820404554795802684">Flux's Architecture diagram</A>
									</DL><p>
									<DT><H3 FOLDED>flux-lora</H3>
									<DL><p>
										<DT><A HREF="https://github.com/aredden/flux-fp8-api/blob/main/lora_loading.py#L332">flux-fp8-api/lora_loading.py at main · aredden/flux-fp8-api</A>
										<DT><A HREF="https://github.com/siliconflow/onediff/tree/7c325253d4e280e470613be43fa3e582a476923e/onediff_diffusers_extensions">onediff/onediff_diffusers_extensions at 7c325253d4e280e470613be43fa3e582a476923e · siliconflow/onediff</A>
									</DL><p>
									<DT><H3 FOLDED>flux-tools</H3>
									<DL><p>
										<DT><A HREF="https://blackforestlabs.ai/flux-1-tools/">Introducing FLUX.1 Tools - Black Forest Labs</A>
										<DT><A HREF="https://x.com/EHuanglu/status/1859949254107902169">(1) el.cine en X: "Why FLUX.1 Tools Are So Important! I’ve been using these tools since they were released, I knew it’s super powerful, but honestly, I never thought about it this way. A great explanation by Heather: “I used depth maps generated with FLUX Depth Pro, and asked Claude how I could https://t.co/OFc8expjB7" / X</A>
										<DT><A HREF="https://x.com/kuer5ord/status/1861040066170335492">(1) StableKirito en X: "🎉 Excited to open source Qwen2vl-Flux! A powerful image generation model combining FLUX with Qwen2VL's vision-language understanding. Create, transform, and control images like never before! https://t.co/M3D1Hjcb0j" / X</A>
										<DT><A HREF="https://huggingface.co/Djrango/Qwen2vl-Flux">Djrango/Qwen2vl-Flux · Hugging Face</A>
										<DT><A HREF="https://x.com/bdsqlsz/status/1861007594024050824">(1) 青龍聖者 en X: "OminiControl: Minimal and Universal Control for Diffusion Transformer Flux 1. model Code:https://t.co/8jVB1KSrpP Demo:https://t.co/RP1IgGDYlh https://t.co/51P3TqHWqp" / X</A>
									</DL><p>
									<DT><A HREF="https://x.com/cloneofsimo/status/1820379637937275380">(1) Simo Ryu en X: "Flux uses AuraFlow's arch btw (mixture of MMDiT -&amp;gt; single large DiT) feels good to make contributions without actual paper https://t.co/RXrvOYHIKh" / X</A>
									<DT><A HREF="https://x.com/ErwannMillon/status/1821467403982917674">(1) Erwann Millon en X: "flux + flash attention 3 go brrr ~11% speedup on H100, but output images are slightly different. may be because of using bf16 which fa3 hopper beta doesn't officially support to use this build flash attention 3 hopper from src and change below https://t.co/NQ8Uug6zCK" / X</A>
									<DT><A HREF="https://gist.github.com/drisspg/16c060c93c069d779958ef1438dfc813">flux torch native benchmarking</A>
								</DL><p>
								<DT><H3 FOLDED>HiDream-I1</H3>
								<DL><p>
									<DT><A HREF="https://github.com/HiDream-ai/HiDream-I1">HiDream-ai/HiDream-I1</A>
									<DT><A HREF="https://huggingface.co/HiDream-ai/HiDream-I1-Full">HiDream-ai/HiDream-I1-Full · Hugging Face</A>
									<DT><A HREF="https://x.com/linoy_tsaban/status/1909570114309308539">HiDream text encoder dependency test: Llama 3 8B</A>
								</DL><p>
								<DT><H3 FOLDED>SD3</H3>
								<DL><p>
									<DT><H3 FOLDED>SD-3.5</H3>
									<DL><p>
										<DT><A HREF="https://stabilityai.notion.site/Stable-Diffusion-3-5-fine-tuning-guide-11a61cdcd1968027a15bdbd7c40be8c6">Stable Diffusion 3.5 fine-tuning guide</A>
										<DT><A HREF="https://github.com/Stability-AI/sd3.5">Stability-AI/sd3.5</A>
									</DL><p>
									<DT><A HREF="https://gist.github.com/cloneofsimo/809b036a65be317163dd81ba824d67aa">MM DiT model that was proposed by SD3 paper.</A>
									<DT><A HREF="https://github.com/cloneofsimo/minRF">cloneofsimo/minRF: Minimal implementation of scalable rectified flow transformers, based on SD3's approach</A>
									<DT><A HREF="https://www.youtube.com/watch?v=yTXMK2TZOZc">Stable Diffusion 3: Architectural analsys</A>
									<DT><A HREF="https://github.com/Stability-AI/sd3-ref/blob/master/sd3_impls.py#L173">sd3-ref/sd3_impls.py at master · Stability-AI/sd3-ref</A>
								</DL><p>
								<DT><H3 FOLDED>CogView4</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/diffusers/pull/10959/files">Fix Graph Breaks When Compiling CogView4 by chengzeyi · Pull Request #10959 · huggingface/diffusers</A>
									<DT><A HREF="https://github.com/THUDM/CogView4">THUDM/CogView4: CogView4, CogView3-Plus and CogView3(ECCV 2024)</A>
								</DL><p>
								<DT><A HREF="https://github.com/haoosz/BiGR?tab=readme-ov-file">haoosz/BiGR: The official code for "BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities"</A>
								<DT><A HREF="https://github.com/ziqihuangg/Awesome-Evaluation-of-Visual-Generation">ziqihuangg/Awesome-Evaluation-of-Visual-Generation: A list of works on evaluation of visual generation models, including evaluation metrics, models, and systems</A>
							</DL><p>
							<DT><H3 FOLDED>MM DiT</H3>
							<DL><p>
								<DT><A HREF="https://gist.github.com/cloneofsimo/809b036a65be317163dd81ba824d67aa">MM DiT model that was proposed by SD3 paper.</A>
								<DT><A HREF="https://carpedm30.notion.site/Diffusion-Optimization-276fde0aed9745f99000a7fea6e9c40b?p=cab3bc1752084addbbe18b68a8edf8ba&pm=s">MM DiT Analysis</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1822271687649546274">How it handles attention compared to traditional Cross attention (diagram)</A>
								<DT><A HREF="https://github.com/Stability-AI/sd3-ref">Stability-AI/sd3-ref</A>
							</DL><p>
							<DT><H3 FOLDED>diffusion-multimodal</H3>
							<DL><p>
								<DT><H3 FOLDED>qwen2vl-flux</H3>
								<DL><p>
									<DT><A HREF="https://chatgpt.com/c/67e5cc46-6074-800c-80e0-953c72f3adcd">Qwen-2.5-VL Text Encoder Analysis</A>
									<DT><A HREF="https://github.com/erwold/qwen2vl-flux/blob/main/technical-report.pdf">qwen2vl-flux/technical-report.pdf at main · erwold/qwen2vl-flux</A>
									<DT><A HREF="https://github.com/erwold/qwen2vl-flux">erwold/qwen2vl-flux</A>
								</DL><p>
								<DT><H3 FOLDED>X2I</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/OPPOer/X2I">OPPOer/X2I · Hugging Face</A>
									<DT><A HREF="https://arxiv.org/html/2503.06134v2#S6">X2I: Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation</A>
									<DT><A HREF="https://chatgpt.com/c/67e5cc46-6074-800c-80e0-953c72f3adcd">Qwen-2.5-VL Text Encoder Analysis</A>
									<DT><A HREF="https://github.com/OpenBMB/MiniCPM-o">OpenBMB/MiniCPM-o: MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech and Multimodal Live Streaming on Your Phone</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://github.com/thu-nics/DiTFastAttn">thu-nics/DiTFastAttn</A>
							<DT><A HREF="https://github.com/Tencent/HunyuanDiT/">Tencent/HunyuanDiT: Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding</A>
							<DT><A HREF="https://dit.hunyuan.tencent.com/">腾讯混元DiT</A>
							<DT><A HREF="https://github.com/kvfrans/jax-diffusion-transformer/?tab=readme-ov-file">kvfrans/jax-diffusion-transformer: Implementation of Diffusion Transformer (DiT) in JAX</A>
							<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/models/dit/model.py">TensorRT-LLM/tensorrt_llm/models/dit/model.py at main · NVIDIA/TensorRT-LLM</A>
							<DT><A HREF="https://github.com/black-forest-labs/flux/blob/main/src/flux/model.py">flux/src/flux/model.py at main · black-forest-labs/flux</A>
							<DT><A HREF="https://github.com/facebookresearch/DiT/blob/main/models.py">DiT/models.py at main · facebookresearch/DiT</A>
							<DT><A HREF="https://github.com/openai/glide-text2im">openai/glide-text2im: GLIDE: a diffusion-based text-conditional image synthesis model</A>
							<DT><A HREF="https://github.com/facebookresearch/mae/blob/main/models_mae.py">mae/models_mae.py at main · facebookresearch/mae</A>
							<DT><A HREF="https://github.com/xdit-project/xDiT">xdit-project/xDiT: xDiT: A Scalable Inference Engine for Diffusion Transformers (DiTs) on multi-GPU Clusters</A>
							<DT><A HREF="https://github.com/oahzxl/OpenDiT">oahzxl/OpenDiT: OpenDiT: An Easy, Fast and Memory-Efficient System for DiT Training and Inference</A>
							<DT><A HREF="https://github.com/NUS-HPC-AI-Lab/VideoSys">NUS-HPC-AI-Lab/VideoSys: VideoSys: An easy and efficient system for video generation</A>
							<DT><A HREF="https://github.com/Anima-Lab/MaskDiT">Anima-Lab/MaskDiT: Code for Fast Training of Diffusion Models with Masked Transformers</A>
							<DT><A HREF="https://github.com/facebookresearch/DiT">facebookresearch/DiT: Official PyTorch Implementation of "Scalable Diffusion Models with Transformers"</A>
							<DT><A HREF="https://github.com/willisma/SiT">willisma/SiT: Official PyTorch Implementation of "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers"</A>
							<DT><A HREF="https://x.com/JXQNHZr1yUAj5Be/status/1854317183960309995">Scaling Laws For Diffusion Transformers</A>
						</DL><p>
						<DT><H3 FOLDED>video-generation</H3>
						<DL><p>
							<DT><H3 FOLDED>videogen-inference-optimization</H3>
							<DL><p>
								<DT><H3 FOLDED>mochi-xdit</H3>
								<DL><p>
									<DT><A HREF="https://github.com/xdit-project/mochi-xdit?tab=readme-ov-file">xdit-project/mochi-xdit: faster parallel inference of mochi video generation model</A>
									<DT><A HREF="https://medium.com/@xditproject/exploring-the-power-of-xdit-in-speeding-up-mochis-video-generation-19f71d4f8f97">Enhancing Parallelism and Speedup for xDiT in Serving the Mochi-1 Video Generation Model | by xdit-project | Nov, 2024 | Medium</A>
								</DL><p>
								<DT><H3 FOLDED>videogen-realtime</H3>
								<DL><p>
									<DT><H3 FOLDED>LTX-Video</H3>
									<DL><p>
										<DT><A HREF="https://www.lightricks.com/">Lightricks: LTXV</A>
										<DT><A HREF="https://github.com/Lightricks/LTX-Video">Lightricks/LTX-Video: Official repository for LTX-Video: DiT-based video generation model that can generate high-quality videos in real-time</A>
										<DT><A HREF="https://x.com/yoavhacohen/status/1897295194426355815">LTX-Video upgrade (03-03-25)</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>video-distillation</H3>
								<DL><p>
									<DT><H3 FOLDED>FastVideo</H3>
									<DL><p>
										<DT><A HREF="https://github.com/hao-ai-lab/FastVideo">hao-ai-lab/FastVideo: FastVideo is an open-source framework for accelerating large video diffusion model.</A>
										<DT><A HREF="https://x.com/haoailab/status/1891943972442079545">Sliding Tile Attention (STA)</A>
									</DL><p>
									<DT><A HREF="https://github.com/hao-ai-lab/FastVideo/blob/main/docs/distillation.md">FastVideo/docs/distillation.md at main · hao-ai-lab/FastVideo</A>
								</DL><p>
								<DT><H3 FOLDED>sliding-tile-attention</H3>
								<DL><p>
									<DT><A HREF="https://x.com/haoailab/status/1891943986245533993">Sliding Tile Attention (STA): notes</A>
									<DT><A HREF="https://hao-ai-lab.github.io/blogs/sta/">Fast Video Generation with Sliding Tile Attention | Hao AI Lab @ UCSD</A>
									<DT><A HREF="https://arxiv.org/abs/2502.04507">[2502.04507] Fast Video Generation with Sliding Tile Attention</A>
									<DT><A HREF="https://github.com/hao-ai-lab/FastVideo/blob/a3ec969397a0c87124ca8d958e57b6d2960eef61/csrc/sliding_tile_attention/README.md">FastVideo/csrc/sliding_tile_attention/README.md at a3ec969397a0c87124ca8d958e57b6d2960eef61 · hao-ai-lab/FastVideo</A>
									<DT><A HREF="https://github.com/fal-ai-community/NativeSparseAttention">fal-ai-community/NativeSparseAttention: research impl of Native Sparse Attention (2502.11089)</A>
								</DL><p>
								<DT><A HREF="https://github.com/THUDM/CogVideo/issues/48">Collaboration with xDiT, where xDiT is a DiT parallel inference framework: To achieve real-time generation, the DiT model must be deployed in parallel across multiple devices, and xDiT is designed to address this challenge.</A>
								<DT><A HREF="https://github.com/Vchitect/VEnhancer">Vchitect/VEnhancer: Official codes of VEnhancer: Generative Space-Time Enhancement for Video Generation</A>
								<DT><A HREF="https://github.com/hao-ai-lab/FastVideo">hao-ai-lab/FastVideo: FastVideo is an open-source framework for accelerating large video diffusion model.</A>
								<DT><A HREF="https://github.com/G-U-N/Phased-Consistency-Model">G-U-N/Phased-Consistency-Model: [NeurIPS 2024] Boosting the performance of consistency models with PCM!</A>
								<DT><H3 FOLDED>video-distillatio</H3>
								<DL><p>
								</DL><p>
								<DT><A HREF="https://github.com/LiewFeng/TeaCache">LiewFeng/TeaCache: Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model</A>
							</DL><p>
							<DT><H3 FOLDED>videogen-training</H3>
							<DL><p>
								<DT><H3 FOLDED>Pusa</H3>
								<DL><p>
									<DT><A HREF="https://github.com/Yaofang-Liu/Pusa-VidGen">Yaofang-Liu/Pusa-VidGen: Pusa: Thousands-handed Video Diffusion Model</A>
									<DT><A HREF="https://huggingface.co/RaphaelLiu/Pusa-V0.5">RaphaelLiu/Pusa-V0.5 · Hugging Face</A>
									<DT><A HREF="https://huggingface.co/datasets/RaphaelLiu/PusaV0.5_Training">RaphaelLiu/PusaV0.5_Training · Datasets at Hugging Face</A>
									<DT><A HREF="https://github.com/stars/Yaofang-Liu/lists/diffusion-mdels">Yaofang-Liu's list / Diffusion Mdels</A>
								</DL><p>
								<DT><H3 FOLDED>VideoX-Fun</H3>
								<DL><p>
									<DT><A HREF="https://github.com/aigc-apps/VideoX-Fun">aigc-apps/VideoX-Fun: 📹 A more flexible framework that can generate videos at any resolution and creates videos from images.</A>
								</DL><p>
								<DT><A HREF="https://github.com/VideoVerses/VideoTuna">VideoVerses/VideoTuna: Let's finetune video generation models!</A>
								<DT><A HREF="https://github.com/Vchitect/LiteGen">Vchitect/LiteGen: A light-weight and high-efficient training framework for accelerating diffusion tasks.</A>
								<DT><A HREF="https://github.com/sihyun-yu/REPA">sihyun-yu/REPA: Official Pytorch Implementation of Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think</A>
								<DT><A HREF="https://apollo-lmms.github.io/">Apollo: An Exploration of Video Understanding in Large Multimodal Models</A>
								<DT><A HREF="https://github.com/aigc-apps/VideoX-Fun">aigc-apps/VideoX-Fun: 📹 A more flexible framework that can generate videos at any resolution and creates videos from images.</A>
							</DL><p>
							<DT><H3 FOLDED>bytedance-seed-Seaweed-7B</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2504.08685v1">Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</A>
								<DT><A HREF="https://seaweed.video/">Seaweed</A>
							</DL><p>
							<DT><H3 FOLDED>Wan</H3>
							<DL><p>
								<DT><H3 FOLDED>wan-I2V</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>wan-V2V</H3>
								<DL><p>
									<DT><A HREF="https://x.com/Kijaidesign/status/1897019853883760777">(1) Jukka Seppänen en X: "WanVideo2video fun, 129 frames with sliding context windows using the 14B model. https://t.co/a1U4pc0Slm" / X</A>
									<DT><A HREF="https://github.com/kijai/ComfyUI-WanVideoWrapper">kijai/ComfyUI-WanVideoWrapper</A>
									<DT><A HREF="https://github.com/aigc-apps/VideoX-Fun/blob/main/examples/wan2.1_fun/predict_v2v_control.py">VideoX-Fun/examples/wan2.1_fun/predict_v2v_control.py at main · aigc-apps/VideoX-Fun</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2503.20314">[2503.20314] Wan: Open and Advanced Large-Scale Video Generative Models</A>
								<DT><A HREF="https://x.com/Alibaba_WanX">(1) WanX (@Alibaba_WanX) / X</A>
								<DT><A HREF="https://tongyi.aliyun.com/wanxiang/">通义万相_AI创意作画_AI绘画_人工智能-阿里云</A>
								<DT><A HREF="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B">Wan-AI/Wan2.1-T2V-14B · Hugging Face</A>
								<DT><A HREF="https://github.com/Wan-Video/Wan2.1">Wan-Video/Wan2.1: Wan: Open and Advanced Large-Scale Video Generative Models</A>
								<DT><A HREF="https://wanxai.com/">Wan_AI Creative Drawing_AI Painting_Artificial Intelligence_Large Model</A>
							</DL><p>
							<DT><H3 FOLDED>HunyuanVideo</H3>
							<DL><p>
								<DT><H3 FOLDED>HunyuanVideo-distributed</H3>
								<DL><p>
									<DT><H3 FOLDED>xDiT-HunyuanVideo</H3>
									<DL><p>
										<DT><A HREF="https://github.com/xdit-project/xDiT/blob/172905f8894668c8588c2a98ffc415f02d84ea86/docs/performance/hunyuanvideo.md">xDiT/docs/performance/hunyuanvideo.md</A>
										<DT><A HREF="https://github.com/xdit-project/xDiT/commit/1b589b76b6503278de3e18285f0df838d60bee53">add hunyuan_video_usp_example.py (#401) · xdit-project/xDiT@1b589b7</A>
									</DL><p>
									<DT><A HREF="https://github.com/Tencent/HunyuanVideo/pull/68/files">Commits · Tencent/HunyuanVideo: parallel inference using xdit #68</A>
								</DL><p>
								<DT><H3 FOLDED>HunyuanVideo-fp8</H3>
								<DL><p>
									<DT><A HREF="https://github.com/Tencent/HunyuanVideo/pull/136/files">update fp8 infer by mboboGO · Pull Request #136 · Tencent/HunyuanVideo</A>
								</DL><p>
								<DT><H3 FOLDED>Hunyuan-diffusers</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/diffusers/issues/10106">Add HunyuanVideo · Issue #10106 · huggingface/diffusers</A>
									<DT><A HREF="https://huggingface.co/docs/diffusers/main/api/pipelines/hunyuan_video">HunyuanVideo pipeline</A>
								</DL><p>
								<DT><A HREF="https://github.com/Tencent/HunyuanVideo">Tencent/HunyuanVideo</A>
								<DT><A HREF="https://arxiv.org/abs/2412.03603">[2412.03603] HunyuanVideo: A Systematic Framework For Large Video Generative Models</A>
								<DT><A HREF="https://aivideo.hunyuan.tencent.com/">Tencent Hunyuan video</A>
								<DT><A HREF="https://github.com/kijai/ComfyUI-HunyuanVideoWrapper">kijai/ComfyUI-HunyuanVideoWrapper</A>
								<DT><A HREF="https://github.com/Saiyan-World/goku">Saiyan-World/goku: Video Generation Foundation Models: https://saiyan-world.github.io/goku/</A>
							</DL><p>
							<DT><H3 FOLDED>mochi</H3>
							<DL><p>
								<DT><H3 FOLDED>AsymmDiT-optimization</H3>
								<DL><p>
									<DT><A HREF="https://github.com/cccntu/mochi-1">hide communication overhead with compute pipelining</A>
									<DT><A HREF="https://github.com/xdit-project/mochi-xdit">xdit-project/mochi-xdit: parallel and faster mochi video generation model</A>
								</DL><p>
								<DT><H3 FOLDED>mochi-finetuning</H3>
								<DL><p>
									<DT><A HREF="https://github.com/Yaofang-Liu/Mochi-Full-Finetuner">Yaofang-Liu/Mochi-Full-Finetuner: Code for full fintuing Mochi model with FSDP (and CP)</A>
								</DL><p>
								<DT><A HREF="https://github.com/genmoai/models/blob/main/src/genmo/lib/attn_imports.py">models/src/genmo/lib/attn_imports.py at main · genmoai/models</A>
								<DT><A HREF="https://github.com/genmoai/models">genmoai/models: The best OSS video generation models</A>
								<DT><A HREF="https://github.com/kijai/ComfyUI-MochiWrapper">kijai/ComfyUI-MochiWrapper</A>
							</DL><p>
							<DT><H3 FOLDED>StepFun</H3>
							<DL><p>
								<DT><H3 FOLDED>StepVideo</H3>
								<DL><p>
									<DT><H3 FOLDED>ParaAttention-StepVideo</H3>
									<DL><p>
										<DT><A HREF="https://github.com/WaveSpeedAI/Step-Video-T2V-ParaAttention/pull/1/files">support para_attn by chengzeyi · Pull Request #1 · WaveSpeedAI/Step-Video-T2V-ParaAttention</A>
									</DL><p>
									<DT><A HREF="https://x.com/bdsqlsz/status/1891348664460714425">tepFun open-sources Step-Video-T2V, SOTA 30B text-to-video model Step-Video-T2V 544px992px204frame Step-Video-T2V-turbo 544px992px136frame 80G VRAM needs</A>
									<DT><A HREF="https://arxiv.org/abs/2502.10248">[2502.10248] Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</A>
									<DT><A HREF="https://arxiv.org/pdf/2502.10248">Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</A>
									<DT><A HREF="https://www.stepfun.com/">阶跃星辰</A>
								</DL><p>
								<DT><A HREF="https://github.com/stepfun-ai/Step-Audio?tab=readme-ov-file">stepfun-ai/Step-Audio</A>
							</DL><p>
							<DT><H3 FOLDED>Cosmos</H3>
							<DL><p>
								<DT><A HREF="https://x.com/DrJimFan/status/1876516972512559170">(1) Jim Fan en X: "Introducing NVIDIA Cosmos, an open-source, open-weight Video World Model. It's trained on 20M hours of videos and weighs from 4B to 14B. Cosmos offers two flavors: diffusion (continuous tokens) and autoregressive (discrete tokens); and two generation modes: text-&amp;gt;video and https://t.co/onplT711Fy" / X</A>
								<DT><A HREF="https://github.com/NVIDIA/Cosmos-Tokenizer">NVIDIA/Cosmos-Tokenizer: A suite of image and video neural tokenizers</A>
								<DT><A HREF="https://github.com/NVIDIA/Cosmos">NVIDIA/Cosmos: Cosmos is a world model development platform that consists of world foundation models, tokenizers and video processing pipeline to accelerate the development of Physical AI at Robotics &amp; AV labs. Cosmos is purpose built for physical AI. The Cosmos repository will enable end users to run the Cosmos models, run inference scripts and generate videos.</A>
							</DL><p>
							<DT><H3 FOLDED>Goku</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Saiyan-World/goku">Saiyan-World/goku: Video Generation Foundation Models: https://saiyan-world.github.io/goku/</A>
								<DT><A HREF="https://saiyan-world.github.io/goku/">Goku</A>
								<DT><A HREF="https://huggingface.co/datasets/saiyan-world/Goku-MovieGenBench">saiyan-world/Goku-MovieGenBench · Datasets at Hugging Face</A>
								<DT><A HREF="https://arxiv.org/abs/2502.04896">[2502.04896] Goku: Flow Based Video Generative Foundation Models</A>
								<DT><A HREF="https://github.com/Saiyan-World/goku/commit/e29a217bdb9195fa26b2db6add9f1848472a1bf9">:waxing_crescent_moon: model · Saiyan-World/goku@e29a217</A>
							</DL><p>
							<DT><H3 FOLDED>MovieGen</H3>
							<DL><p>
								<DT><A HREF="https://x.com/AIatMeta/status/1842188252541043075">Meta Movie Gen</A>
								<DT><A HREF="https://github.com/kyegomez/movie-gen">kyegomez/movie-gen: An open source community implementation of the model from the paper: "Movie Gen: A Cast of Media Foundation Models". Join our community to help implement this model!</A>
							</DL><p>
							<DT><H3 FOLDED>Vchitect</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Vchitect">Vchitect</A>
								<DT><A HREF="https://vchitect.intern-ai.org.cn/">Vchitect 2.0: text-to-video 20-second video generation, flexible aspect ratios, generative space-time enhancent, long video evaluation</A>
								<DT><A HREF="https://github.com/Vchitect/VEnhancer">Vchitect/VEnhancer: Official codes of VEnhancer: Generative Space-Time Enhancement for Video Generation</A>
								<DT><A HREF="https://huggingface.co/Vchitect">Vchitect (Vchitect) Huggingface</A>
								<DT><A HREF="https://github.com/Vchitect/Vchitect-2.0/blob/master/op_replace.py">Vchitect-2.0/op_replace.py at master · Vchitect/Vchitect-2.0</A>
							</DL><p>
							<DT><H3 FOLDED>videogen-eval</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Vchitect/VBench">Vchitect/VBench: [CVPR2024 Highlight] VBench - We Evaluate Video Generation</A>
							</DL><p>
							<DT><H3 FOLDED>videoSys</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>video-understanding</H3>
							<DL><p>
								<DT><H3 FOLDED>apollo</H3>
								<DL><p>
									<DT><A HREF="https://apollo-lmms.github.io/">Apollo: An Exploration of Video Understanding in Large Multimodal Models</A>
									<DT><A HREF="https://arxiv.org/abs/2412.10360">[2412.10360] Apollo: An Exploration of Video Understanding in Large Multimodal Models</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>video-world-models</H3>
							<DL><p>
								<DT><H3 FOLDED>DecartAI</H3>
								<DL><p>
									<DT><A HREF="https://x.com/DecartAI">A new era of real-time generative experiences, enabled by cutting-edge AI efficiency.</A>
								</DL><p>
								<DT><H3 FOLDED>Genie 2</H3>
								<DL><p>
									<DT><A HREF="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2: A large-scale foundation world model - Google DeepMind</A>
									<DT><A HREF="https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/">A generalist AI agent for 3D virtual environments - Google DeepMind</A>
									<DT><A HREF="https://github.com/lucidrains/genie2-pytorch">lucidrains/genie2-pytorch: Implementation of a framework for Genie2 in Pytorch</A>
								</DL><p>
								<DT><A HREF="https://github.com/eloialonso/diamond">eloialonso/diamond: DIAMOND (DIffusion As a Model Of eNvironment Dreams) is a reinforcement learning agent trained in a diffusion world model. NeurIPS 2024 Spotlight.</A>
								<DT><A HREF="https://www.1x.tech/discover/1x-world-model-sampling-challenge">1X World Model: Sampling Challenge Update</A>
								<DT><A HREF="https://x.com/___Harald___/status/1866345938639548513">Harald Schäfer en X: "Our worldmodel at @comma_ai doesn't make the prettiest videos. But it may be the most physically accurate ML-based driving simulator in existence today. Here is what a deviation and recovery of exactly 1m to the left and right looks like. https://t.co/ZBN8KpoZWR" / X</A>
								<DT><A HREF="https://x.com/zhou_xian_/status/1869511650782658846">(1) Zhou Xian en X: "Everything you love about generative models — now powered by real physics! Announcing the Genesis project — after a 24-month large-scale research collaboration involving over 20 research labs — a generative physics engine able to generate 4D dynamical worlds powered by a physics https://t.co/4HRaTp5Gbs" / X</A>
								<DT><A HREF="https://github.com/Genesis-Embodied-AI/Genesis">Genesis-Embodied-AI/Genesis: A generative world for general-purpose robotics &amp; embodied AI learning.</A>
								<DT><A HREF="https://github.com/NJU-PCALab/STAR">NJU-PCALab/STAR: STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</A>
								<DT><A HREF="https://x.com/JackMonas/status/1836257177453105445">(1) Jack Monas en X: "Some initial results from our @1x_tech world model, a learned simulator that can predict how the world responds to our robot's actions. Below are comparisons of real videos vs our generations, both of which start from the same initial frames. See our blog post for more details! https://t.co/l27rYNphcZ" / X</A>
							</DL><p>
							<DT><H3 FOLDED>video-test-time-training</H3>
							<DL><p>
								<DT><H3 FOLDED>ttt-thunderkittens</H3>
								<DL><p>
									<DT><A HREF="https://github.com/test-time-training/ttt-video-dit">test-time-training/ttt-video-dit</A>
									<DT><A HREF="https://x.com/karansdalal/status/1909393559981375574">TP at SM level</A>
								</DL><p>
								<DT><A HREF="https://test-time-training.github.io/video-dit/">One-Minute Video Generation with Test-Time Training</A>
								<DT><A HREF="https://arxiv.org/abs/2504.05298">[2504.05298] One-Minute Video Generation with Test-Time Training</A>
								<DT><A HREF="https://x.com/karansdalal/status/1909393559981375574">TT layers for training and video generation</A>
								<DT><A HREF="https://github.com/test-time-training/ttt-tk/tree/353b6c8346544458fb38bf1fe8de4f1675299ed1">test-time-training/ttt-tk at 353b6c8346544458fb38bf1fe8de4f1675299ed1</A>
							</DL><p>
							<DT><A HREF="https://github.com/NUS-HPC-AI-Lab/VideoSys">NUS-HPC-AI-Lab/VideoSys: VideoSys: An easy and efficient system for video generation</A>
							<DT><A HREF="https://github.com/NUS-HPC-AI-Lab/Awesome-Efficient-Video-Generation">NUS-HPC-AI-Lab/Awesome-Efficient-Video-Generation</A>
							<DT><A HREF="https://x.com/AIatMeta/status/1842188252541043075">Meta Movie Gen</A>
							<DT><A HREF="https://ai.meta.com/static-resource/movie-gen-research-paper">Movie Gen: A Cast of Media Foundation Models</A>
							<DT><A HREF="https://arxiv.org/abs/2410.19355">[2410.19355] FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality</A>
							<DT><A HREF="https://github.com/Vchitect/FasterCache">Vchitect/FasterCache: FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality</A>
							<DT><A HREF="https://github.com/VectorSpaceLab/OmniGen">VectorSpaceLab/OmniGen: OmniGen: Unified Image Generation. https://arxiv.org/pdf/2409.11340</A>
							<DT><A HREF="https://github.com/pytorch/torchcodec">pytorch/torchcodec: PyTorch video decoding</A>
							<DT><A HREF="https://github.com/Tencent/DepthCrafter">Tencent/DepthCrafter: DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos</A>
							<DT><A HREF="https://x.com/papers_anon/status/1858754606601564459">PapersAnon en X: "Everything is a Video: Unifying Modalities through Next-Frame Prediction Reformulate diverse multimodal tasks into a unified next-frame prediction. Evaluated on a range of tasks, including text-to-text, image-to-text, video-to-video, video-to-text, and audio-to-text</A>
							<DT><A HREF="https://arxiv.org/abs/2411.10503">[2411.10503] Everything is a Video: Unifying Modalities through Next-Frame Prediction</A>
							<DT><A HREF="https://sihyun.me/CMD/">Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition</A>
							<DT><A HREF="https://github.com/Tencent/HunyuanVideo">Tencent/HunyuanVideo</A>
							<DT><A HREF="https://x.com/nrehiew_/status/1864677276731777136">(1) wh en X: "1 of the top papers from ICLR 2024 (honourable mention) tldr: introduce a new self-supervised learning method for vision. Training on a single video matches models trained on imagenet https://t.co/kF84CoBPy2" / X</A>
							<DT><A HREF="https://openreview.net/pdf?id=Yen1lGns2o">IS IMAGENET WORTH 1 VIDEO? LEARNING STRONG IMAGE ENCODERS FROM 1 LONG UNLABELLED VIDEO</A>
							<DT><A HREF="https://research.google/blog/extending-video-masked-autoencoders-to-128-frames/">Extending video masked autoencoders to 128 frames</A>
							<DT><A HREF="https://arxiv.org/abs/2410.20280">[2410.20280] MarDini: Masked Autoregressive Diffusion for Video Generation at Scale</A>
							<DT><A HREF="https://lilianweng.github.io/posts/2024-04-12-diffusion-video/">Diffusion Models for Video Generation | Lil'Log</A>
							<DT><A HREF="https://storydiffusion.github.io/">StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</A>
						</DL><p>
						<DT><H3 FOLDED>Denoising Diffusion Probabilistic Models</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2006.11239">[2006.11239] Denoising Diffusion Probabilistic Models</A>
							<DT><A HREF="https://github.com/SonicCodes/hyperada">SonicCodes/hyperada: Linear hypernetwork ada</A>
							<DT><A HREF="https://github.com/cloneofsimo/minDiffusion">cloneofsimo/minDiffusion: Self-contained, minimalistic implementation of diffusion models with Pytorch.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=0V96wE7lY4w">Denoising Autoencoders | Deep Learning Animated - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>cloneofsimo</H3>
						<DL><p>
							<DT><H3 FOLDED>minRF</H3>
							<DL><p>
								<DT><A HREF="https://github.com/cloneofsimo/minRF">cloneofsimo/minRF: Minimal implementation of scalable rectified flow transformers, based on SD3's approach</A>
								<DT><A HREF="https://arxiv.org/abs/2406.07480">[2406.07480] Image Neural Field Diffusion Models</A>
								<DT><A HREF="https://cdn.openai.com/papers/dall-e-3.pdf">https://cdn.openai.com/papers/dall-e-3.pdf</A>
								<DT><A HREF="https://arxiv.org/abs/2310.20550">[2310.20550] CapsFusion: Rethinking Image-Text Data at Scale</A>
								<DT><A HREF="https://google.github.io/imageinwords/">ImageInWords</A>
								<DT><A HREF="https://huggingface.co/datasets/graph-based-captions/GBC10M">graph-based-captions/GBC10M · Datasets at Hugging Face</A>
								<DT><A HREF="https://arxiv.org/abs/1812.06162">[1812.06162] An Empirical Model of Large-Batch Training</A>
								<DT><A HREF="https://x.com/_xjdr/status/1801063628298412239">(2) xjdr en X: "Megablox, splash attention, pallas and automatically sharded named axis come free and built in with Jax and y'all are still using pytorch in production?!?!" / X</A>
								<DT><A HREF="https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py">jax/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py at main · google/jax</A>
							</DL><p>
							<DT><A HREF="https://x.com/cloneofsimo/status/1803802198348103781">(1) Simo Ryu en X: "Lavenderflow-pretrained-256x256-6.8B Hybrid MMDiT just reached 0.597 on GenEval! 🥳 It took me and @isidentical less than 7 weeks of part-time effort + 4k h100 hours to get to SDXL-level (and this is just pretrained model) Does two of us worth 1B valuation? https://t.co/mD3cSpOyvM" / X</A>
							<DT><A HREF="https://github.com/cloneofsimo/efae">cloneofsimo/efae</A>
							<DT><A HREF="https://gligen.github.io/">GLIGEN:Open-Set Grounded Text-to-Image Generation.</A>
							<DT><A HREF="https://arxiv.org/html/2311.12342v2">LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis</A>
							<DT><A HREF="https://github.com/lllyasviel/Omost">lllyasviel/Omost: Your image is almost there!</A>
							<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
							<DT><A HREF="https://github.com/cloneofsimo/minSDXL/blob/master/sdxl_rewrite.py">minSDXL/sdxl_rewrite.py at master · cloneofsimo/minSDXL</A>
						</DL><p>
						<DT><H3 FOLDED>imgdataset_process</H3>
						<DL><p>
							<DT><A HREF="https://github.com/cloneofsimo/imgdataset_process/tree/main">cloneofsimo/imgdataset_process</A>
							<DT><A HREF="https://stackoverflow.com/questions/35392594/getting-facebook-original-image-url">python - Getting Facebook original image url - Stack Overflow</A>
							<DT><A HREF="https://meyerweb.com/eric/tools/dencoder/">URL Decoder/Encoder</A>
							<DT><A HREF="https://gist.github.com/cloneofsimo/d31eee8a5352655cb45869694adf0880">MDS-Multiprocessed-datamerging to NFS, because writing is async this is faster</A>
							<DT><A HREF="https://github.com/allenai/cached_path">allenai/cached_path: A file utility for accessing both local and remote files through a unified interface.</A>
							<DT><A HREF="https://github.com/carpedm20/dreamsim">DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</A>
						</DL><p>
						<DT><H3 FOLDED>image-captioning</H3>
						<DL><p>
							<DT><H3 FOLDED>LAION-POP</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/datasets/Ejafa/ye-pop">Ejafa/ye-pop · Datasets at Hugging Face</A>
								<DT><A HREF="https://laion.ai/blog/laion-pop/">LAION POP: 600,000 high-resolution images with detailed descriptions | LAION</A>
							</DL><p>
							<DT><H3 FOLDED>better-captions</H3>
							<DL><p>
								<DT><A HREF="https://cdn.openai.com/papers/dall-e-3.pdf">Improving Image Generation with Better Captions</A>
								<DT><A HREF="https://gligen.github.io/">GLIGEN:Open-Set Grounded Text-to-Image Generation.</A>
								<DT><A HREF="https://arxiv.org/html/2311.12342v2">LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis</A>
								<DT><A HREF="https://github.com/lllyasviel/Omost">lllyasviel/Omost: Your image is almost there!</A>
							</DL><p>
							<DT><A HREF="https://huggingface.co/datasets/google/imageinwords">google/imageinwords · Datasets at Hugging Face</A>
							<DT><A HREF="https://www.microsoft.com/en-us/research/publication/florence-2-advancing-a-unified-representation-for-a-variety-of-vision-tasks/">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks - Microsoft Research</A>
							<DT><A HREF="https://x.com/gytdau/status/1804266728781943003">(1) Gytis Daujotas en X: "Excited to release a research preview of Feature Lab, a playground where you can sculpt images by editing their features. It uses a SAE to extract the sparse features from an image embeddings model, and allows us to finely sculpt and tweak images. https://t.co/0Nv6f7dX2P" / X</A>
							<DT><A HREF="https://gist.github.com/cloneofsimo/b50f159ad7588ef41f2abeee9aee0df3">Set of wildly complex image descriptions &amp; atomic factual statements.</A>
							<DT><A HREF="https://gist.github.com/cloneofsimo/87cfa71a7c4e2a5f9748cb8812eb0536">siglip_mds.py</A>
							<DT><A HREF="https://github.com/rom1504/img2dataset/">rom1504/img2dataset: Easily turn large sets of image urls to an image dataset. Can download, resize and package 100M urls in 20h on one machine.</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-data</H3>
						<DL><p>
							<DT><H3 FOLDED>diffusion-data-loader</H3>
							<DL><p>
								<DT><H3 FOLDED>mds</H3>
								<DL><p>
									<DT><H3 FOLDED>mds-examples</H3>
									<DL><p>
										<DT><H3 FOLDED>mds-LAION-400M</H3>
										<DL><p>
											<DT><A HREF="https://github.com/mosaicml/diffusion-benchmark/blob/main/data.py">diffusion-benchmark/data.py at main · mosaicml/diffusion-benchmark</A>
											<DT><A HREF="https://github.com/mosaicml/streaming/tree/main/streaming/multimodal/convert/laion/laion400m">streaming/streaming/multimodal/convert/laion/laion400m at main · mosaicml/streaming</A>
										</DL><p>
										<DT><H3 FOLDED>mds-WebVid</H3>
										<DL><p>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/multimodal/webvid.py">streaming/streaming/multimodal/webvid.py at main · mosaicml/streaming</A>
										</DL><p>
										<DT><H3 FOLDED>mds-C4</H3>
										<DL><p>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/text/c4.py">streaming/streaming/text/c4.py at main · mosaicml/streaming</A>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/text/convert/c4.py">streaming/streaming/text/convert/c4.py at main · mosaicml/streaming</A>
										</DL><p>
										<DT><H3 FOLDED>mds-EnWiki</H3>
										<DL><p>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/text/enwiki.py">streaming/streaming/text/enwiki.py at main · mosaicml/streaming</A>
											<DT><A HREF="https://github.com/mosaicml/streaming/tree/main/streaming/text/convert/enwiki">streaming/streaming/text/convert/enwiki at main · mosaicml/streaming</A>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/text/pile.py">streaming/streaming/text/pile.py at main · mosaicml/streaming</A>
										</DL><p>
										<DT><H3 FOLDED>mds-Pile</H3>
										<DL><p>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/text/pile.py">streaming/streaming/text/pile.py at main · mosaicml/streaming</A>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/text/convert/pile.py">streaming/streaming/text/convert/pile.py at main · mosaicml/streaming</A>
										</DL><p>
										<DT><H3 FOLDED>mds-ADE20K</H3>
										<DL><p>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/vision/ade20k.py">streaming/streaming/vision/ade20k.py at main · mosaicml/streaming</A>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/vision/convert/ade20k.py">streaming/streaming/vision/convert/ade20k.py at main · mosaicml/streaming</A>
										</DL><p>
										<DT><H3 FOLDED>mds-CIFAR10</H3>
										<DL><p>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/vision/cifar10.py">streaming/streaming/vision/cifar10.py at main · mosaicml/streaming</A>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/vision/convert/cifar10.py">streaming/streaming/vision/convert/cifar10.py at main · mosaicml/streaming</A>
										</DL><p>
										<DT><H3 FOLDED>mds-COCO</H3>
										<DL><p>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/vision/coco.py">streaming/streaming/vision/coco.py at main · mosaicml/streaming</A>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/vision/convert/coco.py">streaming/streaming/vision/convert/coco.py at main · mosaicml/streaming</A>
										</DL><p>
										<DT><H3 FOLDED>mds-ImageNet</H3>
										<DL><p>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/vision/imagenet.py">streaming/streaming/vision/imagenet.py at main · mosaicml/streaming</A>
											<DT><A HREF="https://github.com/mosaicml/streaming/blob/main/streaming/vision/convert/imagenet.py">streaming/streaming/vision/convert/imagenet.py at main · mosaicml/streaming</A>
										</DL><p>
										<DT><A HREF="https://github.com/cloneofsimo/imgdataset_process/tree/main">cloneofsimo/imgdataset_process</A>
										<DT><A HREF="https://gist.github.com/cloneofsimo/87cfa71a7c4e2a5f9748cb8812eb0536">siglip_mds.py</A>
										<DT><A HREF="https://gist.github.com/cloneofsimo/e41d5718905a5023df7bab494cede051">vae_preprocess</A>
										<DT><A HREF="https://gist.github.com/cloneofsimo/5830ed223b94d55f1609270d129c623b">hr.py</A>
									</DL><p>
									<DT><A HREF="https://gist.github.com/cloneofsimo/d31eee8a5352655cb45869694adf0880">MDS-Multiprocessed-datamerging to NFS, because writing is async this is faster</A>
									<DT><A HREF="https://github.com/allenai/cached_path">allenai/cached_path: A file utility for accessing both local and remote files through a unified interface.</A>
									<DT><A HREF="https://github.com/mosaicml/streaming">mosaicml/streaming: A Data Streaming Library for Efficient Neural Network Training</A>
									<DT><A HREF="https://github.com/geohot/minikeyvalue">geohot/minikeyvalue: A distributed key value store in under 1000 lines. Used in production at comma.ai</A>
									<DT><A HREF="https://github.com/search?q=repo%3Acoreweave%2Ftensorizer+mmap&type=code">mmap: coreweave tensorizer</A>
								</DL><p>
								<DT><H3 FOLDED>WebDataset</H3>
								<DL><p>
									<DT><A HREF="https://webdataset.github.io/webdataset/">webdataset</A>
									<DT><A HREF="https://gist.github.com/cloneofsimo/e41d5718905a5023df7bab494cede051">vae_preprocess</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=Ayt0PTaoovI">AWS re:Invent 2021 - Large-scale distributed training of media ML models with Amazon FSx - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>lance</H3>
							<DL><p>
								<DT><A HREF="https://gist.github.com/cloneofsimo/cefe61fb4b437ce366e4f25f37fbb7b8">lance dataset concurrent writes?</A>
							</DL><p>
							<DT><A HREF="https://gist.github.com/cloneofsimo/6d1d8e98ce25fb88cd95565cef18ddf4">Got confused by Unfold operation, yet again LOL</A>
						</DL><p>
						<DT><H3 FOLDED>VAE</H3>
						<DL><p>
							<DT><A HREF="https://github.com/PipeFusion/PatchVAE">PipeFusion/PatchVAE: A patch parallelism VAE implement for high resolution generation</A>
							<DT><A HREF="https://github.com/xdit-project/DistVAE">xdit-project/DistVAE: A parallelism VAE avoids OOM for high resolution image generation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=0V96wE7lY4w">Denoising Autoencoders | Deep Learning Animated - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=qJeaCHQ1k2w&t=397s">Variational Autoencoders | Generative AI Animated - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=hZ4a4NgM3u0">Autoencoders | Deep Learning Animated - YouTube</A>
							<DT><A HREF="https://x.com/mtschannen/status/1863622784376586499">(1) Michael Tschannen en X: "Have you ever wondered how to train an autoregressive generative transformer on text and raw pixels, without a pretrained visual tokenizer (e.g. VQ-VAE)? We have been pondering this during summer and developed a new model: JetFormer 🌊🤖 https://t.co/ngvPzZvUYW A thread 👇 1/ https://t.co/04R4a1nbMu" / X</A>
							<DT><A HREF="https://www.youtube.com/watch?v=fnULFOyNZn8">Variational Autoencoders For Image Generation - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Rectified Flow</H3>
						<DL><p>
							<DT><A HREF="https://www.cs.utexas.edu/~lqiang/rectflow/html/intro.html">Rectified Flow — Rectified Flow</A>
							<DT><A HREF="https://github.com/cloneofsimo/rectified-flow">cloneofsimo/rectified-flow</A>
							<DT><A HREF="https://github.com/TongTong313/rectified-flow">TongTong313/rectified-flow: 从零手搓Flow Matching（Rectified Flow）</A>
						</DL><p>
						<DT><H3 FOLDED>Flow matching</H3>
						<DL><p>
							<DT><H3 FOLDED>Conditional Flow Matching</H3>
							<DL><p>
								<DT><A HREF="https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/">A Visual Dive into Conditional Flow Matching | ICLR Blogposts 2025</A>
								<DT><A HREF="https://drive.google.com/file/d/1-QKAT8IPbqOpCq42DUeEqrgIP7f7f4TH/view">fm_tutotial_combined.pdf - Google Drive</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=5ZSwYogAxYg">Flow Matching: Simplifying and Generalizing Diffusion Models | Yaron Lipman - YouTube</A>
							<DT><A HREF="https://x.com/i/bookmarks?post_id=1803004045084197186">code snipet: conditional_flow_matching_loss</A>
							<DT><A HREF="https://bm371613.github.io/conditional-flow-matching/">conditional-flow-matching</A>
							<DT><A HREF="https://arxiv.org/abs/2210.02747">[2210.02747] Flow Matching for Generative Modeling</A>
							<DT><A HREF="https://www.youtube.com/watch?v=DDq_pIfHqLs">How I Understand Flow Matching - YouTube</A>
							<DT><A HREF="https://drscotthawley.github.io/blog/posts/FlowModels.html">blog - Flow With What You Know</A>
							<DT><A HREF="https://x.com/RickyTQChen/status/1866470185307459722">Flow Matching Guide and Code (Meta, Yaron, Ricky)</A>
							<DT><A HREF="https://github.com/facebookresearch/flow_matching">facebookresearch/flow_matching: A PyTorch library for implementing flow matching algorithms, featuring continuous and discrete flow matching implementations. It includes practical examples for both text and image modalities.</A>
							<DT><A HREF="https://x.com/cgarciae88/status/1867340873136038293">Flow Matching Facebook Research's Standalone Flow Matching notebook 50 lines of JAX</A>
							<DT><A HREF="https://x.com/dome_271/status/1878082477064880565">(1) dome | Outlier en X: "Tomorrow! :D https://t.co/zAG2931FSH" / X</A>
							<DT><A HREF="https://www.youtube.com/watch?v=7cMzfkWFWhI&t=344s">Flow Matching | Explanation + PyTorch Implementation - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GCoP2w-Cqtg">MIT 6.S184: Flow Matching and Diffusion Models - Lecture 01 - Generative AI with SDEs - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>guidances-schedules</H3>
						<DL><p>
							<DT><A HREF="https://x.com/_clashluke/status/1815672877259051392">(1) Lucas Nestler en X: "Seems like @nico_dufour tested guidances schedules very rigorously at https://t.co/l1TehowhgL. If you're interested in different schedules and want to better understand their induced differences, or if you would like to see thorough evaluations with actionable advice, read the https://t.co/FpHIu7XZ2d" / X</A>
							<DT><A HREF="https://arxiv.org/abs/2404.13040">[2404.13040] Analysis of Classifier-Free Guidance Weight Schedulers</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-evaluation</H3>
						<DL><p>
							<DT><H3 FOLDED>GRADE</H3>
							<DL><p>
								<DT><A HREF="https://x.com/RoyiRassin/status/1854576025679577098">(1) Royi Rassin @ EMNLP 2024 en X: "How diverse are the outputs of text-to-image models and how can we measure that? In our new work, we propose a measure based on LLMs and Visual-QA (VQA), and show NONE of the 12 models we experiment with are diverse. 🧵 1/11 https://t.co/PyWkodbrVZ" / X</A>
								<DT><A HREF="https://arxiv.org/abs/2410.22592">[2410.22592] GRADE: Quantifying Sample Diversity in Text-to-Image Models</A>
							</DL><p>
							<DT><H3 FOLDED>FID</H3>
							<DL><p>
								<DT><A HREF="https://github.com/GaParmar/clean-fid">GaParmar/clean-fid: PyTorch - FID calculation with proper image resizing and quantization steps [CVPR 2022]</A>
								<DT><A HREF="https://github.com/xdit-project/xDiT/commit/be43f787ec0ac6b764ff08913539b4decafcfa1a#diff-ba47e7474645cbefcfdb7cb5f554d376a0610c28e2d112620ae50e00d3e4e207R4">FID metrics evalutions for Pixart and Flux.1 (#327) · xdit-project/xDiT@be43f78</A>
							</DL><p>
							<DT><H3 FOLDED>DreamSim</H3>
							<DL><p>
								<DT><A HREF="https://github.com/carpedm20/dreamsim">DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</A>
								<DT><A HREF="https://github.com/ssundaram21/dreamsim">ssundaram21/dreamsim: DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data (NeurIPS 2023 Spotlight) / / / / When Does Perceptual Alignment Benefit Vision Representations? (NeurIPS 2024)</A>
								<DT><A HREF="https://github.com/replicate/img-quality-eval">replicate/img-quality-eval: Automatic quality evaluation web app for generative text-to-image models</A>
							</DL><p>
							<DT><A HREF="https://github.com/carpedm20/dreamsim">DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</A>
							<DT><A HREF="https://gist.github.com/cloneofsimo/525bcbf30b6f0185a30181e6ef46d562">vibecheckgen.py</A>
							<DT><A HREF="https://github.com/thu-nics/FlashEval">thu-nics/FlashEval</A>
							<DT><A HREF="https://github.com/rosinality/stylegan2-pytorch/blob/master/inception.py">stylegan2-pytorch/inception.py at master · rosinality/stylegan2-pytorch</A>
							<DT><A HREF="https://blbadger.github.io/feature-visualization.html">Feature Visualization I: Feature Maps | Form and Formula</A>
							<DT><A HREF="https://github.com/blbadger/nnetworks">blbadger/nnetworks: Deep learning model implementations and interpretations.</A>
							<DT><A HREF="https://github.com/THUDM/ImageReward">THUDM/ImageReward: [NeurIPS 2023] ImageReward: Learning and Evaluating Human Preferences for Text-to-image Generation</A>
							<DT><A HREF="https://github.com/discus0434/aesthetic-predictor-v2-5">discus0434/aesthetic-predictor-v2-5: SigLIP-based Aesthetic Score Predictor</A>
							<DT><A HREF="https://huggingface.co/blog/burtenshaw/image-preferences">Let’s make a generation of amazing image generation models</A>
							<DT><A HREF="https://imgsys.org/">imgsys.org | an image model arena by fal.ai</A>
							<DT><A HREF="https://arxiv.org/abs/2306.04675">[2306.04675] Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models</A>
							<DT><A HREF="https://github.com/djghosh13/geneval">djghosh13/geneval: GenEval: An object-focused framework for evaluating text-to-image alignment</A>
							<DT><A HREF="https://arxiv.org/abs/2310.11513">[2310.11513] GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment</A>
							<DT><A HREF="https://img-quality-eval.onrender.com/">Image quality evaluation - generate and evaluate</A>
						</DL><p>
						<DT><H3 FOLDED>comfyui</H3>
						<DL><p>
							<DT><H3 FOLDED>WaveSpeed</H3>
							<DL><p>
								<DT><A HREF="https://github.com/chengzeyi/Comfy-WaveSpeed">chengzeyi/Comfy-WaveSpeed: [WIP] The all in one inference optimization solution for ComfyUl, universal, flexible, and fast.</A>
							</DL><p>
							<DT><A HREF="https://github.com/PaddlePaddle/PaddleMIX/blob/dc863a4655debeba1a6d664125dfcf3ab2238fc9/comfyui/ComfyUI_ppdiffusers/sdxl_pipe_nodes.py">PaddleMIX/comfyui/ComfyUI_ppdiffusers/sdxl_pipe_nodes.py</A>
							<DT><A HREF="https://github.com/siliconflow/onediff_comfy_nodes">siliconflow/onediff_comfy_nodes: Just a subfolder of https://github.com/siliconflow/onediff</A>
							<DT><A HREF="https://github.com/siliconflow/onediff/tree/main/onediff_comfy_nodes">onediff/onediff_comfy_nodes at main · siliconflow/onediff</A>
							<DT><A HREF="https://github.com/chengzeyi/ComfyUI_stable_fast">chengzeyi/ComfyUI_stable_fast</A>
							<DT><A HREF="https://github.com/Chaoses-Ib/ComfyScript">Chaoses-Ib/ComfyScript: A Python frontend and library for ComfyUI</A>
							<DT><A HREF="https://github.com/ali1234/comfyui-node-decorator/blob/main/registry.py">comfyui-node-decorator/registry.py at main · ali1234/comfyui-node-decorator</A>
							<DT><A HREF="https://github.com/discus0434/comfyui-flux-accelerator">discus0434/comfyui-flux-accelerator: Accelerates Flux.1 image generation, just by using this node.</A>
							<DT><A HREF="https://github.com/kijai?tab=repositories">kijai (kijai) / Repositories</A>
							<DT><A HREF="https://github.com/camenduru?tab=stars">camenduru / Starred</A>
							<DT><A HREF="https://www.youtube.com/watch?v=7DXnGrARqys">(1) TACO-DiT: Accelerating Your ComfyUI Generation Experience - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>SHIFTUP</H3>
						<DL><p>
							<DT><A HREF="https://www.linkedin.com/feed/update/urn:li:activity:7242467995803906048/">Diffusion, optimization, debuggin, infrastructure, diffusion engineering</A>
							<DT><A HREF="https://carpedm30.notion.site/Diffusion-880a4bda0b1b465a85774fe22f3a2f5a">Diffusion 사이드 프로젝트 함께하실 분 찾습니다!</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-people</H3>
						<DL><p>
							<DT><A HREF="https://www.linkedin.com/in/sanderdieleman/">Sander Dieleman | LinkedIn</A>
							<DT><A HREF="https://x.com/Ethan_smith_20/status/1767570870598279181">Ethan Smith</A>
							<DT><A HREF="https://x.com/cloneofsimo">(1) Simo Ryu (@cloneofsimo) / X</A>
							<DT><A HREF="https://x.com/allnoteson">(1) Andreas Jansson (@allnoteson) / X</A>
							<DT><A HREF="https://www.linkedin.com/in/carpedm20/">Taehoon Kim | LinkedIn</A>
							<DT><A HREF="https://github.com/cszy98">cszy98 (Lyu Zhengyao)</A>
						</DL><p>
						<DT><H3 FOLDED>image-edit</H3>
						<DL><p>
							<DT><H3 FOLDED>controlnet</H3>
							<DL><p>
								<DT><A HREF="https://github.com/liming-ai/ControlNet_Plus_Plus">liming-ai/ControlNet_Plus_Plus: Official PyTorch implementation of ECCV 2024 Paper: ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback.</A>
							</DL><p>
							<DT><H3 FOLDED>Stable Flow: Vital Layers for Training-Free Image Editing</H3>
							<DL><p>
								<DT><A HREF="https://x.com/OmriAvr/status/1859838000143925678">(1) Omri Avrahami en X: "[1/10] 🚨 We present our recent @Snap project: Stable Flow --- A training-free method that performs various types of image editing operations (e.g., non-rigid editing, object addition and replacement) using flow models. Project page: https://t.co/g0Naul2hoQ https://t.co/G6t4yyGgbT" / X</A>
								<DT><A HREF="https://omriavrahami.com/stable-flow/">Stable Flow</A>
								<DT><A HREF="https://arxiv.org/abs/2411.14430">[2411.14430] Stable Flow: Vital Layers for Training-Free Image Editing</A>
							</DL><p>
							<DT><H3 FOLDED>FLUX-tools</H3>
							<DL><p>
								<DT><A HREF="https://blackforestlabs.ai/flux-1-tools/">Introducing FLUX.1 Tools - Black Forest Labs</A>
							</DL><p>
							<DT><A HREF="https://omriavrahami.com/stable-flow/">Stable Flow: Vital Layers for Training-Free Image Editing</A>
						</DL><p>
						<DT><H3 FOLDED>difusion-guidance</H3>
						<DL><p>
							<DT><A HREF="https://x.com/haotian_yeee/status/1859672842658512908">(1) Haotian Ye en X: "💡 From prediction to generation: Training-Free Guidance for Diffusion (NeurIPS Spotlight) How do we use any off-the-shelf predictor and unconditional diffusion sampler for conditional generation without any training? Our framework, TFG, boosts performance works across 7 models, https://t.co/6i5qIq2ecJ" / X</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-test-time-compute</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/html/2501.09732v1">Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps</A>
							<DT><A HREF="https://github.com/sayakpaul/tt-scale-flux">sayakpaul/tt-scale-flux: Inference-time scaling of Flux beyond denoising steps.</A>
						</DL><p>
						<DT><H3 FOLDED>diffusion-high-resolution</H3>
						<DL><p>
							<DT><H3 FOLDED>RectifiedHR</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=VcykV0Cdo7U">High Resolution Image Generation via Energy Rectification - YouTube</A>
								<DT><A HREF="https://colab.research.google.com/drive/1Ls6xfJOHbWkTW1ftynuesKw5CV0h0zGM?usp=sharing#scrollTo=9AS0L283btwY">RectifiedHR Diffusion - Colab</A>
								<DT><A HREF="https://arxiv.org/abs/2503.02537">[2503.02537] RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification</A>
								<DT><A HREF="https://zhenyangcs.github.io/RectifiedHR-Diffusion/">RectifiedHR: Enable Efficient High Resolution Image Generation via Energy Rectification</A>
								<DT><A HREF="https://github.com/EnVision-Research/RectifiedHR">EnVision-Research/RectifiedHR: Official PyTorch/Diffusers implementation of "RectifiedHR: Enable Efficient High Resolution Image Generation via Energy Rectification"</A>
							</DL><p>
						</DL><p>
						<DT><A HREF="https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/">Building Diffusion Model's theory from ground up | ICLR Blogposts 2024</A>
						<DT><A HREF="https://kexue.fm/archives/9164">A talk on generative diffusion model (Part 3): DDPM = Bayesian + denoising</A>
						<DT><A HREF="https://kexue.fm/archives/10047">A brief talk on generative diffusion model (Part 22): Signal-to-noise ratio</A>
						<DT><A HREF="https://github.com/tmabraham/diffusion_reading_group">tmabraham/diffusion_reading_group: Diffusion Reading Group at EleutherAI</A>
						<DT><A HREF="https://astralord.github.io/posts/power-of-diffusion-models/">Power of Diffusion Models | AstraBlog</A>
						<DT><A HREF="https://sander.ai/2024/09/02/spectral-autoregression.html">Diffusion is spectral autoregression – Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2024/06/14/noise-schedules.html">Noise schedules considered harmful – Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2023/01/09/diffusion-language.html">Diffusion language models – Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2023/08/28/geometry.html">The geometry of diffusion guidance – Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2023/07/20/perspectives.html">Perspectives on diffusion – Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2022/05/26/guidance.html">Guidance: a cheat code for diffusion models – Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2022/01/31/diffusion.html">Diffusion models are autoencoders – Sander Dieleman</A>
						<DT><A HREF="https://sander.ai/2024/02/28/paradox.html">The paradox of diffusion distillation – Sander Dieleman</A>
						<DT><A HREF="https://imagen.research.google/">Imagen: Text-to-Image Diffusion Models</A>
						<DT><A HREF="https://arxiv.org/pdf/2205.09991.pdf">Planning with Diffusion for Flexible Behavior Synthesis</A>
						<DT><A HREF="https://arxiv.org/abs/2206.01714">Compositional Visual Generation with Composable Diffusion Models</A>
						<DT><A HREF="https://twitter.com/finbarrtimbers/status/1643283063017971713">diffusion-based models into 11 modular parts</A>
						<DT><A HREF="https://arxiv.org/abs/2206.00364">[2206.00364] Elucidating the Design Space of Diffusion-Based Generative Models</A>
						<DT><A HREF="https://github.com/NVIDIA/TensorRT/blob/3aaa97b91ee1dd61ea46f78683d9a3438f26192e/demo/experimental/HuggingFace-Diffusers/TensorRT-diffusers-txt2img.ipynb">TensorRT/demo/experimental/HuggingFace-Diffusers/TensorRT-diffusers-txt2img.ipynb</A>
						<DT><A HREF="https://twitter.com/_akhaliq/status/1742255547741544602">DIffusion Model with Perceptual Loss (ByteDance)</A>
						<DT><A HREF="https://huggingface.co/papers/2312.02696">Paper page - Analyzing and Improving the Training Dynamics of Diffusion Models</A>
						<DT><A HREF="https://www.youtube.com/watch?v=HoKDTa5jHvg">Diffusion Models | Paper Explanation | Math Explained - YouTube</A>
						<DT><A HREF="https://www.youtube.com/watch?v=ogJsCPqgFMk">Efficient Text-to-Image Training (16x cheaper than Stable Diffusion) | Paper Explained - YouTube</A>
						<DT><A HREF="https://github.com/neelnanda-io/Stable-Diffusion-Interp/blob/main/w3d5_part2_stablediff_solution.py">Stable-Diffusion-Interp/w3d5_part2_stablediff_solution.py at main · neelnanda-io/Stable-Diffusion-Interp</A>
						<DT><A HREF="https://github.com/neelnanda-io/Stable-Diffusion-Interp/tree/main">neelnanda-io/Stable-Diffusion-Interp</A>
						<DT><A HREF="https://www.slideshare.net/slideshow/embed_code/key/BoSrT1r6h4kDTJ">ex-OpenAI slides</A>
						<DT><A HREF="https://twitter.com/PreetumNakkiran/status/1767268109784940867">Projection onto manifold perspective</A>
						<DT><A HREF="https://chenyang.co/diffusion.html">Chenyyang Yuan: Diffusion models from scratch, from a new theoretical perspective</A>
						<DT><A HREF="https://github.com/yuanchenyang/smalldiffusion">yuanchenyang/smalldiffusion: Simple and readable code for training and sampling from diffusion models</A>
						<DT><A HREF="https://arxiv.org/abs/2403.18103">[2403.18103] Tutorial on Diffusion Models for Imaging and Vision</A>
						<DT><A HREF="https://github.com/neelnanda-io/Stable-Diffusion-Interp/blob/main/w3d5_part2_stablediff_solution.py">Stable-Diffusion-Interp/w3d5_part2_stablediff_solution.py</A>
						<DT><A HREF="https://twitter.com/rm_rafailov/status/1781145364810350689">hybrid auto-regressive + diffusion video generation models</A>
						<DT><A HREF="https://twitter.com/BenTheEgg/status/1783972218772373708">(1) Benjamin Lefaudeux en X: "And.. we're back in business ! Possibly placebo, but I can feel the heat just looking at the screen https://t.co/s6bO10acjA" / X</A>
						<DT><A HREF="https://github.com/bytedance">Bytedance Inc.</A>
						<DT><A HREF="https://github.com/cloneofsimo/imagenet.int8">cloneofsimo/imagenet.int8</A>
						<DT><A HREF="https://carpedm30.notion.site/SHIFT-UP-AI-Labs-2cc71f48eb1140d09a439ab0b10bdb7b?p=642900de802c444caf4e3b51d34079aa&pm=s">SHIFT UP AI Labs</A>
						<DT><A HREF="https://www.youtube.com/watch?v=zc5NTeJbk-k&t=124s">Why Does Diffusion Work Better than Auto-Regression? - YouTube</A>
						<DT><A HREF="https://github.com/snap-research/BitsFusion">snap-research/BitsFusion</A>
						<DT><A HREF="https://github.com/snap-research/HyperHuman">snap-research/HyperHuman: [ICLR 2024] Github Repo for "HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion"</A>
						<DT><A HREF="https://www.microsoft.com/en-us/research/publication/florence-2-advancing-a-unified-representation-for-a-variety-of-vision-tasks/">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks - Microsoft Research</A>
						<DT><A HREF="https://mettyz.github.io/DiffusionEngine/">DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection</A>
						<DT><A HREF="https://github.com/diff-usion/Awesome-Diffusion-Models">diff-usion/Awesome-Diffusion-Models: A collection of resources and papers on Diffusion Models</A>
						<DT><A HREF="https://blog.csdn.net/xd_wjc/article/details/134441396">Stable Diffusion1.5网络结构-超详细原创_stable diffusion unet 结构-CSDN博客</A>
						<DT><A HREF="https://machinelearning.apple.com/research/autoregressive-image-models">Scalable Pre-training of Large Autoregressive Image Models - Apple Machine Learning Research</A>
						<DT><A HREF="https://github.com/apple/ml-aim">apple/ml-aim: This repository provides the code and model checkpoints of the research paper: Scalable Pre-training of Large Autoregressive Image Models</A>
						<DT><A HREF="https://github.com/SonicCodes/hyperada">SonicCodes/hyperada: Linear hypernetwork ada</A>
						<DT><A HREF="https://arxiv.org/abs/2006.11239">[2006.11239] Denoising Diffusion Probabilistic Models</A>
						<DT><A HREF="https://huggingface.co/papers/2408.07009">Imagen 3</A>
						<DT><A HREF="https://x.com/Ethan_smith_20/status/1767570870598279181">Ethan (SF til 8th) en X: "Created a new method of generative model (although kinda crappy lol) that works by autoregressive sequencing fourier coefficients. Inspired by the coarse to fine generation by Diffusion models. Full write up here: https://t.co/voD78qVwIB and tldr in thread 🧵 (flowers102) https://t.co/VyhR0WA4vs" / X</A>
						<DT><A HREF="https://github.com/lucidrains/transfusion-pytorch">lucidrains/transfusion-pytorch: Pytorch implementation of Transfusion, "Predict the Next Token and Diffuse Images with One Multi-Modal Model", from MetaAI</A>
						<DT><A HREF="https://huggingface.co/rain1011">rain1011 (Yang Jin)</A>
						<DT><A HREF="https://github.com/eloialonso/diamond">eloialonso/diamond: DIAMOND (DIffusion As a Model Of eNvironment Dreams) is a reinforcement learning agent trained in a diffusion world model. NeurIPS 2024 Spotlight.</A>
						<DT><A HREF="https://www.markov.bio/research/mech-interp-path-to-e2e-biology">Through a Glass Darkly | Markov Bio</A>
						<DT><A HREF="https://x.com/i/bookmarks?post_id=1862843110247563343">The link between diffusion models and optimal transport</A>
						<DT><A HREF="https://huggingface.co/papers/2411.16318">Paper page - One Diffusion to Generate Them All</A>
						<DT><A HREF="https://x.com/thjashin/status/1800870255146979595">(1) Jiaxin Shi en X: "Discrete diffusion models made simple &amp;amp; competitive on both language and pixel-level image modeling! https://t.co/G3AnUVQzOD ✅New variational objective (integrate cross-entropy!) ✅Beating prior diffusion language models &amp;amp; matching best AR on pixel-level image modeling ...(1/n) https://t.co/Aw8R9mgJ5l" / X</A>
						<DT><A HREF="https://arxiv.org/abs/2406.04329">[2406.04329] Simplified and Generalized Masked Diffusion for Discrete Data</A>
						<DT><A HREF="https://www.youtube.com/watch?v=VcykV0Cdo7U">High Resolution Image Generation via Energy Rectification - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>Vision</H3>
					<DL><p>
						<DT><H3 FOLDED>clip-based-models</H3>
						<DL><p>
							<DT><H3 FOLDED>CLIP</H3>
							<DL><p>
								<DT><A HREF="https://x.com/JinaAI_/status/1859659764281782420">(1) Jina AI en X: "Jina-CLIP-v2: a 0.9B multilingual multimodal embedding model that supports 89 languages, 512x512 image resolution, 8192 token-length, and Matryoshka representations down to 64-dim for both images and text. https://t.co/TpUnvamf0z With of course strong performance on retrieval &amp;amp;" / X</A>
							</DL><p>
							<DT><H3 FOLDED>DINO</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>sigLIP</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2303.15343">[2303.15343] Sigmoid Loss for Language Image Pre-Training</A>
							</DL><p>
							<DT><H3 FOLDED>PaliGemma</H3>
							<DL><p>
								<DT><A HREF="https://x.com/papers_anon/status/1859807437081149928">Multimodal Autoregressive Pre-training of Large Vision Encoders From Apple. V2 of the AIM generalist vision encoders (L/H/1B/3B). Pairs the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Outperforms CLIP, SigLIP</A>
								<DT><A HREF="https://x.com/A_K_Nain/status/1865949459563082009">(1) Aakash Kumar Nain en X: "Google DeepMind announced PaliGemma 2 last week. It is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. What does this generation of PaliGemma bring to the table? I finished reading the technical report, and here is a https://t.co/30evk6VTIo" / X</A>
								<DT><A HREF="https://arxiv.org/abs/2412.03555">[2412.03555] PaliGemma 2: A Family of Versatile VLMs for Transfer</A>
								<DT><A HREF="https://huggingface.co/blog/paligemma2">Welcome PaliGemma 2 – New vision language models by Google</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2103.00020">CLIP: Learning Transferable Visual Models From Natural Language Supervision</A>
							<DT><A HREF="https://arxiv.org/abs/2310.09199">[2310.09199] PaLI-3 Vision Language Models: Smaller, Faster, Stronger</A>
							<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/imagen">JAX-Toolbox/rosetta/rosetta/projects/imagen</A>
							<DT><A HREF="https://x.com/BenTheEgg/status/1788596702380769616">MobileCLIP (Apple)</A>
							<DT><A HREF="https://x.com/i/bookmarks?post_id=1805149804059607366">SigLIP SO400</A>
							<DT><A HREF="https://x.com/olivierhenaff/status/1805995802352910557">(1) Olivier Hénaff en X: "Thrilled to announce our latest work on active data curation: joint example selection (JEST) drastically accelerates large-scale multimodal pretraining, surpassing previous SoTA (SigLIP) with 10x fewer iterations and FLOPS: https://t.co/NFPSrPIs4C https://t.co/Ggkg2Pn5qS" / X</A>
							<DT><A HREF="https://github.com/mlfoundations/open_clip">mlfoundations/open_clip: An open source implementation of CLIP.</A>
							<DT><A HREF="https://github.com/LAION-AI/CLIP-based-NSFW-Detector">LAION-AI/CLIP-based-NSFW-Detector</A>
							<DT><A HREF="https://vinija.ai/models/CLIP/">Vinija's Notes • Models • CLIP</A>
							<DT><A HREF="https://x.com/cloneofsimo/status/1827964161311363240">(1) Simo Ryu en X: "HF PALIGEMMA SNEAKY BUG?? If you ever wanted to do continued-generation with paligemma-3b-224, it adds EOS token at end of suffix by default. (I guess suffix was only intended for fine-tuning) Therefore unless you temporarily make that not-eos, it will generate gibberish! https://t.co/iZJEivVfSV" / X</A>
						</DL><p>
						<DT><H3 FOLDED>ViT</H3>
						<DL><p>
							<DT><H3 FOLDED>NATTEN</H3>
							<DL><p>
								<DT><A HREF="https://x.com/AliHassaniJr/status/1636558575421263872">(1) Ali Hassani en X: "We upgraded NATTEN today to extend support to @PyTorch 2.0 (+ SM89/SM90), allowing users to enjoy all the goodies in the new release. We're seeing up to 20% throughput improvement in Neighborhood Attention based models with torch.compile! https://t.co/CnmXAj4R2R https://t.co/P40giqHlP0" / X</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=ma3NYVo8Im0">All Things ViTs || CVPR 2023 Tutorial || Hila Chefer and Sayak Paul - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=vsqKGZT8Qn8">Vision Transformer Basics - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=j3VNqtJUoz0">Vision Transformer Quick Guide</A>
							<DT><A HREF="https://github.com/mit-han-lab/efficientvit">mit-han-lab/efficientvit: EfficientViT is a new family of vision models for efficient high-resolution vision.</A>
							<DT><A HREF="https://x.com/AliHassaniJr/status/1785686867758718991">(1) Ali Hassani en X: "Fused neighborhood attention now supports backward pass! Upgrade today to get all the features and all the speed. Get up to 844%, 385% and 447% improvement in 1D/2D/3D forward+backward pass at the op level. Training w/ spatio-temporal attention should be a breeze now. https://t.co/yBdWPT1eyp" / X</A>
							<DT><A HREF="https://x.com/WenhaoLi29/status/1846217448678207556">(1) Wenhao Li en X: "We trained a Vision Transformer to solve ONE single task from @fchollet and @mikeknoop’s @arcprize. Unexpectedly, it failed to produce the test output, even when using 1 MILLION examples! Why is this the case? 🤔 https://t.co/lsS6PKiHa3" / X</A>
							<DT><A HREF="https://alessiodevoto.github.io/ViT-in-pure-JAX/">Vision Transformer in pure JAX. - Alessio Devoto</A>
							<DT><A HREF="https://x.com/rchoudhury997/status/1857514021014081583">(1) Rohan Choudhury en X: "Excited to finally release our NeurIPS 2024 (spotlight) paper! We introduce Run-Length Tokenization (RLT), a simple way to significantly speed up your vision transformer on video with no loss in performance! https://t.co/jERT4LxOJw" / X</A>
							<DT><A HREF="https://x.com/MistralAI/status/1858565550450065920">Pixtral Large: new SOTA vision model</A>
							<DT><A HREF="https://github.com/AmericanPresidentJimmyCarter/test-torch-bfloat16-vit-training">AmericanPresidentJimmyCarter/test-torch-bfloat16-vit-training</A>
							<DT><A HREF="https://www.artfintel.com/p/papers-ive-read-this-week-vision">Papers I've read this week: vision language models</A>
							<DT><A HREF="https://x.com/tommiekerssies/status/1906793027911250027">(1) Tommie Kerssies en X: "Image segmentation doesn’t have to be rocket science. 🚀 Why build a rocket engine full of bolted-on subsystems when one elegant unit does the job? 💡 That’s what we did for segmentation. ✅ Meet the Encoder-only Mask Transformer (EoMT): https://t.co/LWZXyeR4by (CVPR 2025) (1/6) https://t.co/gqyQ70qldp" / X</A>
						</DL><p>
						<DT><H3 FOLDED>GAN</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/anand_bhattad/status/1664798414318518274">(1) Anand Bhattad en X: "1/ SUPER excited to share our latest paper! Would you believe it if I told you that a StyleGAN, trained only to generate images, can also generate normals, depth, albedo, shading &amp;amp; segmentation? We show that it can! 🤯 w/ @danielbmckee , @HoiemDerek &amp;amp; David Forsyth @IllinoisCS https://t.co/ae6pNn9r03" / X</A>
							<DT><A HREF="https://arxiv.org/abs/2306.00987">[2306.00987] StyleGAN knows Normal, Depth, Albedo, and More</A>
							<DT><A HREF="https://blog.fal.ai/introducing-aurasr-an-open-reproduction-of-the-gigagan-upscaler-2/">Introducing AuraSR - An open reproduction of the GigaGAN Upscaler</A>
							<DT><A HREF="https://lilianweng.github.io/posts/2017-08-20-gan/">From GAN to WGAN | Lil'Log</A>
						</DL><p>
						<DT><H3 FOLDED>image-tokens</H3>
						<DL><p>
							<DT><A HREF="https://github.com/lucidrains/titok-pytorch">lucidrains/titok-pytorch: Implementation of TiTok, proposed by Bytedance in "An Image is Worth 32 Tokens for Reconstruction and Generation"</A>
						</DL><p>
						<DT><H3 FOLDED>Florence-2</H3>
						<DL><p>
							<DT><A HREF="https://huggingface.co/microsoft/Florence-2-large-ft">microsoft/Florence-2-large-ft · Hugging Face</A>
							<DT><A HREF="https://arxiv.org/abs/2311.06242">[2311.06242] Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</A>
						</DL><p>
						<DT><A HREF="https://arxiv.org/abs/2310.05737">[2310.05737] Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation</A>
						<DT><A HREF="https://segment-anything.com/demo#">Segment Anything | Meta AI</A>
						<DT><A HREF="https://towardsdatascience.com/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b">META’s Hiera: reduce complexity to increase accuracy</A>
						<DT><A HREF="https://twitter.com/skalskip92/status/1728867156681732229">GPT-4V &amp; SLAM</A>
						<DT><A HREF="https://arxiv.org/html/2404.00498v1">94% on CIFAR-10 in 3.29 Seconds on a Single GPU</A>
						<DT><A HREF="https://arxiv.org/abs/2309.16588">[2309.16588] Vision Transformers Need Registers</A>
						<DT><A HREF="https://x.com/borisdayma/status/1806092526572503533">attention registers</A>
						<DT><A HREF="https://machinelearning.apple.com/research/autoregressive-image-models">Scalable Pre-training of Large Autoregressive Image Models - Apple Machine Learning Research</A>
						<DT><A HREF="https://x.com/bfl_ml/status/1819003686011449788">(1) Black Forest Labs en X: "We are excited to announce the launch of Black Forest Labs. Our mission is to develop and advance state-of-the-art generative deep learning models for media and to push the boundaries of creativity, efficiency and diversity. https://t.co/ilcWvJgmsX" / X</A>
						<DT><A HREF="https://github.com/PKU-YuanGroup/LLaVA-o1">PKU-YuanGroup/LLaVA-o1</A>
						<DT><A HREF="https://github.com/salesforce/LAVIS">salesforce/LAVIS: LAVIS - A One-stop Library for Language-Vision Intelligence</A>
						<DT><A HREF="https://x.com/giffmana/status/1863739511638774095">BIG_VISION GPU tutorial</A>
						<DT><A HREF="https://lucasb.eyer.be/articles/bv_tuto.html">Using big_vision on GPUs</A>
					</DL><p>
					<DT><H3 FOLDED>Large Transformer Model Inference Optimization</H3>
					<DL><p>
						<DT><H3 FOLDED>model compression</H3>
						<DL><p>
							<DT><H3 FOLDED>model distillation</H3>
							<DL><p>
								<DT><A HREF="https://x.com/giffmana/status/1402836863954599936">(1) Lucas Beyer (bl16) en X: "So you think you know distillation; it's easy, right?</A>
								<DT><A HREF="https://arxiv.org/pdf/2408.11796">LLM Pruning and Distillation in Practice: The Minitron Approach</A>
								<DT><A HREF="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</A>
								<DT><A HREF="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#gemini-model-updates">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</A>
								<DT><A HREF="https://www.youtube.com/watch?v=FKsARHV3ZTI">Model Distilation: Student Models</A>
								<DT><A HREF="https://github.com/HuangOwen/Awesome-LLM-Compression">HuangOwen/Awesome-LLM-Compression: Awesome LLM compression research papers and tools.</A>
								<DT><A HREF="https://github.com/microsoft/TransformerCompression">microsoft/TransformerCompression: For releasing code related to compression methods for transformers, accompanying our publications</A>
								<DT><A HREF="https://arxiv.org/abs/2106.05237">[2106.05237] Knowledge distillation: A good teacher is patient and consistent</A>
								<DT><A HREF="https://x.com/PavloMolchanov/status/1815721057203925180">Minitron-4B/8B</A>
								<DT><A HREF="https://github.com/NVlabs/Minitron?tab=readme-ov-file">NVlabs/Minitron: A family of compressed models obtained via pruning and knowledge distillation</A>
								<DT><A HREF="https://x.com/AIatMeta/status/1824133790224224291">(1) AI at Meta en X: "Using structured weight pruning and knowledge distillation, the @NVIDIAAI research team refined Llama 3.1 8B into a new Llama-3.1-Minitron 4B. They're releasing the new models on @huggingface and shared a deep dive on how they did it ➡️ https://t.co/fJTrcfzx9m https://t.co/hTNVFwGjjh" / X</A>
								<DT><A HREF="https://pytorch.org/blog/llama-into-torchtune/?utm_content=316721961&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Distilling Llama3.1 8B into 1B in torchtune | PyTorch</A>
								<DT><A HREF="https://www.youtube.com/watch?v=jrf76uNs77k">The Unreasonable Effectiveness of Reasoning Distillation: using DeepSeek R1 to beat OpenAI o1 - YouTube</A>
								<DT><A HREF="https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation">Bespoke-Stratos: The unreasonable effectiveness of reasoning distillation</A>
								<DT><A HREF="https://github.com/horus-ai-labs/DistillFlow">horus-ai-labs/DistillFlow</A>
							</DL><p>
							<DT><A HREF="https://x.com/karpathy/status/1814038096218083497">(1) Andrej Karpathy en X: "LLM model size competition is intensifying... backwards! My bet is that we'll see models that "think" very well and reliably that are very very small. There is most likely a setting even of GPT-2 parameters for which most people will consider GPT-2 "smart". The reason current" / X</A>
							<DT><A HREF="https://github.com/zipnn/zipnn/blob/main/images/table2.png">zipnn/images/table2.png at main · zipnn/zipnn</A>
							<DT><A HREF="https://github.com/zipnn/zipnn">zipnn/zipnn: A lossless and near-lossless compression method optimized for numbers/tensors in the Foundation Models environment</A>
							<DT><A HREF="https://github.com/neuralmagic/compressed-tensors">neuralmagic/compressed-tensors: A safetensors extension to efficiently store sparse quantized tensors on disk</A>
							<DT><A HREF="https://github.com/neuralmagic/AutoFP8">neuralmagic/AutoFP8</A>
							<DT><A HREF="https://arxiv.org/abs/2410.05437">[2410.05437] ESPACE: Dimensionality Reduction of Activations for Model Compression</A>
						</DL><p>
						<DT><H3 FOLDED>throughput-bandwidth-latency</H3>
						<DL><p>
							<DT><H3 FOLDED>Continuity equation</H3>
							<DL><p>
								<DT><A HREF="https://www.fisimat.com.mx/ecuacion-de-continuidad/">Ecuación de Continuidad - Ejercicios Resueltos - Fisimat</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=jlMAX2Oaht0">Text-generation-inference (TGI) deployment optimization and benchmarking - YouTube</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Continuity_equation">Continuity equation - Wikipedia</A>
							<DT><A HREF="https://cursor.sh/blog/instant-apply?utm_source=ainews&utm_medium=email&utm_campaign=ainews-to-be-named-9199">Cursor: Near-Instant Full-File Edits (OpenAI fund)</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mYRqvB1_gRk&t=835s">Exploring the Latency/Throughput &amp; Cost Space for LLM Inference // Timothée Lacroix // CTO Mistral - YouTube</A>
							<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62031/">Speeding up LLM Inference With TensorRT-LLM | NVIDIA On-Demand</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-quantization</H3>
						<DL><p>
							<DT><H3 FOLDED>model-quantization</H3>
							<DL><p>
								<DT><A HREF="https://leimao.github.io/article/Neural-Networks-Quantization/">Quantization for Neural Networks - Lei Mao's Log Book</A>
								<DT><A HREF="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization">A Visual Guide to Quantization - by Maarten Grootendorst</A>
								<DT><A HREF="https://github.com/htqin/awesome-model-quantization">htqin/awesome-model-quantization: A list of papers, docs, codes about model quantization. This repo is aimed to provide the info for model quantization research, we are continuously improving the project. Welcome to PR the works (papers, repositories) that are missed by the repo.</A>
								<DT><A HREF="https://github.com/htqin">htqin (Haotong Qin)</A>
								<DT><A HREF="https://github.com/bytedance/decoupleQ">bytedance/decoupleQ: A quantization algorithm for LLM</A>
								<DT><A HREF="https://github.com/bytedance/AffineQuant">bytedance/AffineQuant: Official implementation of the ICLR 2024 paper AffineQuant</A>
								<DT><A HREF="https://arxiv.org/abs/2302.04304">[2302.04304] Q-Diffusion: Quantizing Diffusion Models</A>
								<DT><A HREF="https://github.com/microsoft/BitBLAS">microsoft/BitBLAS: BitBLAS is a library to support mixed-precision matrix multiplications, especially for quantized LLM deployment.</A>
								<DT><A HREF="https://github.com/htqin/cnn-quantization">htqin/cnn-quantization: Quantization of Convolutional Neural networks.</A>
								<DT><A HREF="https://github.com/Macaronlin/LLaMA3-Quantization">Macaronlin/LLaMA3-Quantization: A repository dedicated to evaluating the performance of quantizied LLaMA3 using various quantization methods..</A>
								<DT><A HREF="https://x.com/realGeorgeHotz/status/1819963680739512550">(1) George Hotz 🌑 en X: "This is one of the coolest papers I've seen in a while. "Self-Compressing Neural Networks" is dynamic quantization-aware training that puts size (in bytes) of the model in the loss! Paper: https://t.co/2nnOb0m8kz My implementation (in @__tinygrad__): https://t.co/ChW4kdGygE" / X</A>
								<DT><A HREF="https://arxiv.org/pdf/2301.13142">Self-Compressing Neural Networks</A>
								<DT><A HREF="https://github.com/geohot/ai-notebooks/blob/master/mnist_self_compression.ipynb">ai-notebooks/mnist_self_compression.ipynb at master · geohot/ai-notebooks</A>
								<DT><A HREF="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization?utm_campaign=post&utm_medium=web">A Visual Guide to Quantization - by Maarten Grootendorst</A>
								<DT><A HREF="https://static.sched.com/hosted_files/pytorch2024/c3/dmx-compressor.pdf">https://static.sched.com/hosted_files/pytorch2024/c3/dmx-compressor.pdf</A>
								<DT><A HREF="https://github.com/microsoft/microxcaling?tab=readme-ov-file#Spec-Configuration">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
								<DT><A HREF="https://github.com/microsoft/VPTQ">microsoft/VPTQ: VPTQ, A Flexible and Extreme low-bit quantization algorithm</A>
								<DT><A HREF="https://github.com/mit-han-lab/nunchaku">mit-han-lab/nunchaku: SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</A>
								<DT><A HREF="https://arxiv.org/abs/2405.16406">[2405.16406] SpinQuant: LLM quantization with learned rotations</A>
								<DT><A HREF="https://www.together.ai/blog/even-better-even-faster-quantized-llms-with-qtip">Even Better, Even Faster Quantized LLMs with QTIP</A>
								<DT><A HREF="https://arxiv.org/abs/2404.00456">[2404.00456] QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs</A>
								<DT><A HREF="https://github.com/spcl/QuaRot">spcl/QuaRot: Code for Neurips24 paper: QuaRot, an end-to-end 4-bit inference of large language models.</A>
								<DT><A HREF="https://github.com/facebookresearch/SpinQuant">facebookresearch/SpinQuant: Code repo for the paper "SpinQuant LLM quantization with learned rotations"</A>
								<DT><A HREF="https://github.com/usyd-fsalab/fp6_llm">usyd-fsalab/fp6_llm: An efficient GPU support for LLM inference with x-bit quantization (e.g. FP6,FP5).</A>
								<DT><A HREF="https://github.com/chengzeyi/AdaptiveFloat4">chengzeyi/AdaptiveFloat4: A novel high-precision 4bit quantization format</A>
								<DT><A HREF="https://arxiv.org/abs/2411.02355">[2411.02355] "Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization</A>
								<DT><A HREF="https://github.com/sgl-project/sglang/blob/55842eb81a782da7e522ec0210c3fa1f3f74dc0a/python/sglang/srt/layers/quantization/__init__.py#L100">sglang/python/sglang/srt/layers/quantization/__init__.py: QUANTIZATION_METHODS</A>
							</DL><p>
							<DT><H3 FOLDED>transformer-inference-quantization-people</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/Tim_Dettmers">Tim Dettmers (@Tim_Dettmers) / Twitter</A>
							</DL><p>
							<DT><H3 FOLDED>Post-training quantization (PTQ)</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2106.09685">[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models</A>
								<DT><A HREF="https://github.com/TimDettmers/bitsandbytes">TimDettmers/bitsandbytes: 8-bit CUDA functions for PyTorch</A>
								<DT><A HREF="https://arxiv.org/pdf/2004.09602.pdf">Integer Quantization For Deep Learning Inference: Principles and Empirical Evaluation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=AlASZb93rrc">Lecture 05 - Quantization (Part I) | MIT 6.S965 - YouTube</A>
								<DT><A HREF="https://twitter.com/atiorh/status/1684659953716584452">Atila @ ICML en Twitter: "Applying 1, 2, 4, 6 and 8-bit quantization via palettization yields much better results, e.g. We can use 1, 2, 4, 6 or 8-bits palettes to achieve the same compression rate as linear 8-bit quant (50%) but maintain correctness as high as 80dB (2dB loss vs 17dB for linear 8-bit) https://t.co/lA6ldVXYSW" / X</A>
								<DT><A HREF="https://twitter.com/amanrsanger/status/1690828443699847168">Quantitative quantization analysis at scale (bfp16 vs int)</A>
								<DT><A HREF="https://arxiv.org/pdf/2206.01861.pdf">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</A>
								<DT><A HREF="https://github.com/spcl/QuaRot">spcl/QuaRot: Code for QuaRot, an end-to-end 4-bit inference of large language models.</A>
							</DL><p>
							<DT><H3 FOLDED>transformer-inference-quantization-1-BitNet</H3>
							<DL><p>
								<DT><A HREF="https://mobiusml.github.io/1bit_blog/">Towards 1-bit Machine Learning Models</A>
								<DT><A HREF="https://github.com/rafacelente/bllama?tab=readme-ov-file">rafacelente/bllama: 1.58-bit LLaMa model</A>
								<DT><A HREF="https://github.com/microsoft/BitBLAS">microsoft/BitBLAS: BitBLAS is a library to support mixed-precision matrix multiplications, especially for quantized LLM deployment.</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2212.09720">[2212.09720] The case for 4-bit precision: k-bit Inference Scaling Laws</A>
							<DT><A HREF="https://arxiv.org/abs/2305.19268?utm_source=Cohere_For_AI&utm_medium=LinkedIn">[2305.19268] Intriguing Properties of Quantization at Scale</A>
							<DT><A HREF="https://twitter.com/awnihannun/status/1773185727997825406">MLX low-RAM lazy quantiztion procedure</A>
							<DT><A HREF="https://github.com/Vahe1994/AQLM">Vahe1994/AQLM: Official Pytorch repository for Extreme Compression of Large Language Models via Additive Quantization</A>
							<DT><A HREF="https://arxiv.org/pdf/2401.06118.pdf">Extreme Compression of Large Language Models via Additive Quantization</A>
							<DT><A HREF="https://github.com/snap-research/BitsFusion">snap-research/BitsFusion</A>
							<DT><A HREF="https://github.com/HanGuo97/flute">HanGuo97/flute: Fast Matrix Multiplications for Lookup Table-Quantized LLMs</A>
							<DT><A HREF="https://www.databricks.com/blog/serving-quantized-llms-nvidia-h100-tensor-core-gpus">Serving Quantized LLMs on NVIDIA H100 Tensor Core GPUs | Databricks Blog</A>
							<DT><A HREF="https://x.com/casper_hansen_/status/1861457676346998791">AutoAWQ to use Flash Decoding: flash_attn_with_kvcache</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-sparsity</H3>
						<DL><p>
							<DT><H3 FOLDED>transformer-inference-sparsity-deepspeed</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>Sparse Tensor Computation</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/SparTA/tree/nmsparse_artifact">microsoft/SparTA at nmsparse_artifact</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2302.02596">[2302.02596] Ten Lessons We Have Learned in the New "Sparseland": A Short Handbook for Sparse Neural Network Researchers</A>
							<DT><A HREF="https://arxiv.org/abs/1902.09574">[1902.09574] The State of Sparsity in Deep Neural Networks</A>
							<DT><A HREF="https://twitter.com/lzcemma15/status/1683916730052268032">Contextual Sparsity for Efficient LLMs at Inference Time</A>
							<DT><A HREF="https://www.youtube.com/watch?v=0g351WQTaf8">[REFAI Seminar 04/20/23] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</A>
							<DT><A HREF="https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/">What Is Sparsity in AI Inference and Machine Learning?</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mGDnOLcfE8g">Lecture 11: Sparsity - YouTube</A>
							<DT><A HREF="https://github.com/neuralmagic/sparseml">neuralmagic/sparseml: Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models</A>
							<DT><A HREF="https://github.com/neuralmagic/sparsezoo">neuralmagic/sparsezoo: Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes</A>
							<DT><A HREF="https://github.com/google-research/jaxpruner">google-research/jaxpruner</A>
							<DT><A HREF="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/">Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog</A>
							<DT><A HREF="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/">Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT</A>
							<DT><A HREF="https://arxiv.org/abs/2302.02596">[2302.02596] Ten Lessons We Have Learned in the New "Sparseland"</A>
							<DT><A HREF="https://twitter.com/_akhaliq/status/1734046582805344492">SparQ Attention: Bandwidth-Efficient LLM Inference</A>
							<DT><A HREF="https://arxiv.org/abs/2405.15743">[2405.15743] Sparse maximal update parameterization: A holistic approach to sparse training dynamics</A>
							<DT><A HREF="https://x.com/_EldarKurtic/status/1861084914327658865">2:4 Sparsity: Sparase-LLaMA-3.1-8B-2of4</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-pruning</H3>
						<DL><p>
							<DT><A HREF="https://openreview.net/pdf?id=0GRBKLBjJE">A Fast Post-Training Pruning Framework for Transformers</A>
							<DT><A HREF="https://www.youtube.com/watch?v=sZzc6tAtTrM">Lecture 03 - Pruning and Sparsity (Part I) | MIT 6.S965 - YouTube</A>
							<DT><A HREF="https://github.com/google-research/jaxpruner">google-research/jaxpruner</A>
							<DT><A HREF="https://x.com/AIatMeta/status/1824133790224224291">Minitron 4B</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-architectural-optimization</H3>
						<DL><p>
							<DT><H3 FOLDED>Attention</H3>
							<DL><p>
								<DT><H3 FOLDED>MHA (Multi-Head Attention)</H3>
								<DL><p>
									<DT><A HREF="https://kexue.fm/archives/10091">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA - 科学空间|Scientific Spaces</A>
								</DL><p>
								<DT><H3 FOLDED>MQA (Multi-Query Attention)</H3>
								<DL><p>
									<DT><A HREF="https://kexue.fm/archives/10091">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA - 科学空间|Scientific Spaces</A>
									<DT><A HREF="https://papers.cool/arxiv/1911.02150">Fast Transformer Decoding: One Write-Head is All You Need | Cool Papers - Immersive Paper Discovery</A>
									<DT><A HREF="https://arxiv.org/abs/1911.02150">[1911.02150] Fast Transformer Decoding: One Write-Head is All You Need</A>
									<DT><A HREF="https://gist.github.com/eqy/24246e2c70072aa5f3e3a803ef98f58f">cuDNN GQA</A>
								</DL><p>
								<DT><H3 FOLDED>GQA (Grouped-Query Attention)</H3>
								<DL><p>
									<DT><A HREF="https://kexue.fm/archives/10091">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA - 科学空间|Scientific Spaces</A>
									<DT><A HREF="https://arxiv.org/abs/2305.13245">[2305.13245] GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</A>
								</DL><p>
								<DT><H3 FOLDED>MLA (Multi-head Latent Attention)</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/html/2405.04434v2">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</A>
									<DT><A HREF="https://kexue.fm/archives/10091">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA - 科学空间|Scientific Spaces</A>
									<DT><A HREF="https://github.com/google/maxtext/blob/f7ee8c636fd500995e76c227b351d48680ab7890/MaxText/layers/attentions.py#L435">maxtext/MaxText/layers/attentions.py at f7ee8c636fd500995e76c227b351d48680ab7890 · google/maxtext</A>
									<DT><A HREF="https://gist.github.com/Chillee/2e270fc5413dbbce58c779f8c4eac66c">flex_attention_tutorial.py</A>
									<DT><A HREF="https://pytorch.org/blog/flexattention/">FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention | PyTorch</A>
									<DT><A HREF="https://docs.google.com/presentation/d/1wB_Ul0LZwIDL47qFl64b8hVhH1_ya-1YPAPSSv0cKMs/edit#slide=id.g3031bdfe4db_0_8">SGLang DeepSeek MLA - Google Slides</A>
									<DT><A HREF="https://github.com/madsys-dev/deepseekv2-profile/tree/924174cb5dc11fad24bdaad3fd820ebf87506368/mla">deepseekv2-profile/mla at 924174cb5dc11fad24bdaad3fd820ebf87506368 · madsys-dev/deepseekv2-profile</A>
									<DT><A HREF="https://zhuanlan.zhihu.com/p/23574608727">SGLang DP MLA 特性解读 - 知乎</A>
									<DT><A HREF="https://github.com/deepseek-ai/FlashMLA">deepseek-ai/FlashMLA</A>
									<DT><A HREF="https://arxiv.org/abs/2502.14837">[2502.14837] Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs</A>
									<DT><A HREF="https://github.com/JT-Ushio/MHA2MLA">JT-Ushio/MHA2MLA: Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs</A>
									<DT><A HREF="https://www.youtube.com/watch?v=0VLAoVGf_74">The Genius of DeepSeek’s 57X Efficiency Boost [MLA] - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=s9R5s4U1WH8">Gentle Introduction to Causal Transformer Optimization and Multi-Head Latent Attention! - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>Ring attention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/mgmalek/ring-attention">mgmalek/ring-attention</A>
									<DT><A HREF="https://github.com/zhuzilin/ring-flash-attention">zhuzilin/ring-flash-attention: Ring attention implementation with flash attention</A>
									<DT><A HREF="https://github.com/gpu-mode/ring-attention">gpu-mode/ring-attention: ring-attention experiments</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/experimental/_attention.py">pytorch/torch/distributed/tensor/experimental/_attention.py: _templated_ring_attention</A>
								</DL><p>
								<DT><H3 FOLDED>Linear Attention</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/1812.01243">[1812.01243] Efficient Attention: Attention with Linear Complexities</A>
									<DT><A HREF="https://arxiv.org/abs/2006.04768">[2006.04768] Linformer: Self-Attention with Linear Complexity</A>
									<DT><A HREF="https://arxiv.org/abs/2001.04451">[2001.04451] Reformer: The Efficient Transformer</A>
									<DT><A HREF="https://openai.com/research/requests-for-research-2">Transformers with Linear Attention (OpenAI)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=dVH1dRoMPBc">Do we need Attention? A Mamba Primer - YouTube</A>
									<DT><A HREF="https://x.com/simran_s_arora/status/1845909074774475125">LLaMA 405B linear-time inference, KV-state</A>
									<DT><A HREF="https://haileyschoelkopf.github.io/blog/2024/linear-attn/">Linear Attention Fundamentals | Hailey Schoelkopf</A>
									<DT><A HREF="https://www.youtube.com/watch?v=d0HJvGSWw8A">Linear Attention and Beyond (Interactive Tutorial with Songlin Yang) - YouTube</A>
									<DT><A HREF="https://github.com/sustcsonglin/linear-attention-and-beyond-slides">sustcsonglin/linear-attention-and-beyond-slides</A>
								</DL><p>
								<DT><H3 FOLDED>Adaptive Attention</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2207.07061.pdf">Confident Adaptive Language Modeling (CALM)</A>
								</DL><p>
								<DT><H3 FOLDED>Attention Free</H3>
								<DL><p>
									<DT><H3 FOLDED>Mamba</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=dVH1dRoMPBc&t=10s">Do we need Attention? A Mamba Primer - YouTube</A>
										<DT><A HREF="https://x.com/TXhunyuan/status/1899105803073958010">Hunyuan-TurboS</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2212.10544">[2212.10544] Pretraining Without Attention</A>
									<DT><A HREF="https://arxiv.org/abs/2105.14103">[2105.14103] An Attention Free Transformer</A>
									<DT><A HREF="https://github.com/rish-16/aft-pytorch">rish-16/aft-pytorch: Unofficial PyTorch implementation of Attention Free Transformer (AFT) layers by Apple Inc.</A>
								</DL><p>
								<DT><H3 FOLDED>RVKV</H3>
								<DL><p>
									<DT><A HREF="https://github.com/BlinkDL/RWKV-LM">BlinkDL/RWKV-LM: RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, "infinite" ctx_len, and free sentence embedding.</A>
								</DL><p>
								<DT><H3 FOLDED>PagedAttention</H3>
								<DL><p>
									<DT><A HREF="https://vllm.ai/">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</A>
									<DT><A HREF="https://github.com/microsoft/vattention">microsoft/vattention: Dynamic Memory Management for Serving LLMs without PagedAttention</A>
								</DL><p>
								<DT><H3 FOLDED>SageAttention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/thu-ml/SageAttention">thu-ml/SageAttention: Quantized Attention that achieves speedups of 2.1x and 2.7x compared to FlashAttention2 and xformers, respectively, without lossing end-to-end metrics across various models.</A>
									<DT><A HREF="https://arxiv.org/abs/2410.02367">[2410.02367] SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration</A>
									<DT><A HREF="https://github.com/feifeibear/ChituAttention">feifeibear/ChituAttention: Quantized Attention on GPU</A>
									<DT><A HREF="https://arxiv.org/html/2411.10958v1">SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration</A>
								</DL><p>
								<DT><H3 FOLDED>DuoAttention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/mit-han-lab/duo-attention/tree/main">mit-han-lab/duo-attention: DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</A>
								</DL><p>
								<DT><H3 FOLDED>KV-cache-efficient</H3>
								<DL><p>
									<DT><H3 FOLDED>RadixAttention</H3>
									<DL><p>
										<DT><A HREF="https://lmsys.org/blog/2024-01-17-sglang/">Fast and Expressive LLM Inference with RadixAttention and SGLang | LMSYS Org</A>
										<DT><A HREF="https://arxiv.org/pdf/2312.07104">SGLang: Efficient Execution of Structured Language Model Programs</A>
										<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8">LLM Inference Series: 4. KV caching, a deeper look | by Pierre Lienhart | Medium</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/blob/7310aede97a0fdacc0be3219b6f8174b53351075/python/sglang/srt/layers/radix_attention.py#L21">sglang/python/sglang/srt/layers/radix_attention.py at 7310aede97a0fdacc0be3219b6f8174b53351075 · sgl-project/sglang</A>
										<DT><A HREF="https://www.youtube.com/watch?v=Ny4xxErgFgQ">Efficient LLM Inference with SGLang, Lianmin Zheng, xAI - YouTube</A>
									</DL><p>
									<DT><A HREF="https://github.com/chengzeyi/AsyncKVAttention">chengzeyi/AsyncKVAttention</A>
									<DT><A HREF="https://github.com/shadowpa0327/Palu">shadowpa0327/Palu: Code for Palu: Compressing KV-Cache with Low-Rank Projection</A>
									<DT><A HREF="https://github.com/JerryYin777/Cross-Layer-Attention">JerryYin777/Cross-Layer-Attention: Self Reproduction Code of Paper "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention (MIT CSAIL)</A>
									<DT><A HREF="https://x.com/Ar_Douillard/status/1845791796569260202">KV Prediction for Improved Time to First Token</A>
									<DT><A HREF="https://x.com/hkproj/status/1828778411814236173">Writing in the Margins (WiM)</A>
									<DT><A HREF="https://github.com/NVIDIA/kvpress">NVIDIA/kvpress: LLM KV cache compression made easy</A>
									<DT><A HREF="https://github.com/jzhang38/EasyContext">jzhang38/EasyContext: Memory optimization and training recipes to extrapolate language models' context length to 1 million tokens, with minimal hardware.</A>
									<DT><A HREF="https://qwenlm.github.io/blog/qwen2.5-turbo/">Extending the Context Length to 1M Tokens! | Qwen</A>
									<DT><A HREF="https://github.com/NVIDIA/Star-Attention">NVIDIA/Star-Attention: Efficient LLM Inference over Long Sequences</A>
									<DT><A HREF="https://x.com/iofu728/status/1868588111246283047">(1) Huiqiang Jiang en X: "🕸️ KV cache has its own lifecycle, but previous benchs focus solely on single-req, ignore full lifecycle. 🪻SCBench fills the gap with a KV-cache-centric angle, covering 4 kv cache stage across 12 tasks, 4 capability, and 2 shared-context mode https://t.co/lVeuNwpfyf https://t.co/YZiN2Skctr" / X</A>
								</DL><p>
								<DT><H3 FOLDED>torch-SDPA</H3>
								<DL><p>
									<DT><H3 FOLDED>sdpa-cudnn</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/4d9b5a87e4cc1e2ff81dce5123ee98a7c1b2d6a8/test/test_transformers.py#L3366">pytorch/test/test_transformers.py: torch.ops.aten._scaled_dot_product_cudnn_attention</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml#L14817C9-L14817C44">pytorch/aten/src/ATen/native/native_functions.yaml: _scaled_dot_product_cudnn_attention</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/commit/9a9a0abc2818d40d06eda6c0b6fdbc949474f12e">[SDPA-CUDNN] Make CuDNN Attention Opt in (#138522) · pytorch/pytorch@9a9a0ab</A>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/nn/attention/__init__.py#L30">pytorch/torch/nn/attention/__init__.py at main · pytorch/pytorch</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/4d9b5a87e4cc1e2ff81dce5123ee98a7c1b2d6a8/torch/_meta_registrations.py#L5022">pytorch/torch/_meta_registrations.py: meta__scaled_dot_product_cudnn_attention</A>
									<DT><A HREF="https://gist.github.com/cloneofsimo/af610ff8aa11a3f57956e7d7f578409c">FlashAttention comparison</A>
									<DT><A HREF="https://docs.google.com/spreadsheets/d/18araPIxeeIGbS9BNooUCYsC4RkfnhIe4kcAqkscQGL4/edit?gid=0#gid=0">attention roofline - Google Sheets</A>
									<DT><A HREF="https://gist.github.com/drisspg/bef40a41a3f2b6faedf4a5d625616bda">sdpa.py: Torch SDPA benchmarking</A>
								</DL><p>
								<DT><H3 FOLDED>ChituAttention: Quantized Attention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/feifeibear/ChituAttention">feifeibear/ChituAttention: Quantized Attention on GPU</A>
								</DL><p>
								<DT><H3 FOLDED>Pyramid Attention Broadcast</H3>
								<DL><p>
									<DT><A HREF="https://oahzxl.github.io/PAB/">Real-Time Video Generation with Pyramid Attention Broadcast</A>
									<DT><A HREF="https://github.com/NUS-HPC-AI-Lab/VideoSys">NUS-HPC-AI-Lab/VideoSys: VideoSys: An easy and efficient system for video generation</A>
									<DT><A HREF="https://arxiv.org/abs/2408.12588">[2408.12588] Real-Time Video Generation with Pyramid Attention Broadcast</A>
									<DT><A HREF="https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/pab.md">VideoSys/docs/pab.md at master · NUS-HPC-AI-Lab/VideoSys</A>
								</DL><p>
								<DT><H3 FOLDED>Stick-breaking Attention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/shawntan/stickbreaking-attention?tab=readme-ov-file">shawntan/stickbreaking-attention: Stick-breaking attention</A>
									<DT><A HREF="https://arxiv.org/abs/2403.08245">[2403.08245] Scattered Mixture-of-Experts Implementation</A>
								</DL><p>
								<DT><H3 FOLDED>Star Attention</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2411.17116">[2411.17116] Star Attention: Efficient LLM Inference over Long Sequences</A>
								</DL><p>
								<DT><H3 FOLDED>attention-language</H3>
								<DL><p>
									<DT><A HREF="https://x.com/VictorTaelin/status/1865562732457566287">turing complete attention language</A>
									<DT><A HREF="https://srush.github.io/raspy/">attention as a programming language</A>
									<DT><A HREF="https://arxiv.org/pdf/2106.06981">Thinking Like Transformers</A>
									<DT><A HREF="https://github.com/microsoft/AttentionEngine">microsoft/AttentionEngine</A>
								</DL><p>
								<DT><H3 FOLDED>QuantumAttention</H3>
								<DL><p>
									<DT><H3 FOLDED>QuantumAttention-TK</H3>
									<DL><p>
										<DT><H3 FOLDED>exp2</H3>
										<DL><p>
											<DT><A HREF="https://github.com/deepseek-ai/FlashMLA/blob/18e32770cc719687e869af6a7df4686dee67e041/csrc/softmax.h#L111">softmax.h#L111 This allows the compiler to use the ffma instruction instead of fadd and fmul separately.</A>
											<DT><A HREF="https://github.com/WaveSpeedAI/QuantumAttention/blob/0baf4b3fd3c568964acd4f176c35e8f073c70c20/src/quantum_attn/tk/attention.py#L31">QuantumAttention/src/quantum_attn/tk/attention.py at 0baf4b3fd3c568964acd4f176c35e8f073c70c20 · WaveSpeedAI/QuantumAttention</A>
										</DL><p>
										<DT><A HREF="https://github.com/WaveSpeedAI/QuantumAttention/blob/e7b647eb86596609ea68d372c9291a2e63e16d16/src/quantum_attn/tk/attention.py">QuantumAttention/src/quantum_attn/tk/attention.py at e7b647eb86596609ea68d372c9291a2e63e16d16 · WaveSpeedAI/QuantumAttention</A>
									</DL><p>
									<DT><A HREF="https://github.com/chengzeyi/QuantumAttention/commit/35abd08f042314e01bad999a0521faee24ca4d6a">Dev kernel (#1) · chengzeyi/QuantumAttention@35abd08</A>
									<DT><A HREF="https://github.com/chengzeyi/QuantumAttention/blob/main/src/quantum_attn/config.py">QuantumAttention/src/quantum_attn/config.py at main env vars patch</A>
									<DT><A HREF="https://github.com/search?q=repo%3Achengzeyi%2FQuantumAttention%20attention.force_eager_fallback&type=code">config patching (e.g. attention.force_eager_fallback)</A>
									<DT><A HREF="https://github.com/WaveSpeedAI/QuantumAttention/commit/e7b647eb86596609ea68d372c9291a2e63e16d16">Dev fp8 attn per head (#3) · WaveSpeedAI/QuantumAttention@e7b647e</A>
								</DL><p>
								<DT><H3 FOLDED>sparse-attention</H3>
								<DL><p>
									<DT><H3 FOLDED>MoBA</H3>
									<DL><p>
										<DT><A HREF="https://x.com/Kimi_Moonshot/status/1891825059599352259">Introducing MoBA: Mixture of Block Attention for Long-Context LLMs release notes</A>
										<DT><A HREF="https://github.com/MoonshotAI/MoBA">MoonshotAI/MoBA: MoBA: Mixture of Block Attention for Long-Context LLMs</A>
										<DT><A HREF="https://github.com/MoonshotAI/MoBA/blob/master/MoBA_Tech_Report.pdf">MoBA/MoBA_Tech_Report.pdf at master · MoonshotAI/MoBA</A>
									</DL><p>
									<DT><H3 FOLDED>deepseek-nsa</H3>
									<DL><p>
										<DT><A HREF="https://x.com/deepseek_ai/status/1891745487071609327">(1) DeepSeek en X: "🚀 Introducing NSA: A Hardware-Aligned and Natively Trainable Sparse Attention mechanism for ultra-fast long-context training &amp;amp; inference! Core components of NSA: • Dynamic hierarchical sparse strategy • Coarse-grained token compression • Fine-grained token selection 💡 With https://t.co/zjXuBzzDCp" / X</A>
										<DT><A HREF="https://arxiv.org/abs/2502.11089">[2502.11089] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</A>
										<DT><A HREF="https://github.com/lucidrains/native-sparse-attention-pytorch">lucidrains/native-sparse-attention-pytorch: Implementation of the sparse attention pattern proposed by the Deepseek team in their "Native Sparse Attention" paper</A>
										<DT><A HREF="https://arxiv.org/abs/2209.04881">[2209.04881] On The Computational Complexity of Self-Attention</A>
										<DT><A HREF="https://github.com/XunhaoLai/native-sparse-attention-triton">XunhaoLai/native-sparse-attention-triton: This repo provides an efficient Triton implementation of the sparse attention mechanism from the paper [Native Sparse Attention](https://arxiv.org/abs/2502.11089).</A>
										<DT><A HREF="https://github.com/mdy666/qwen-nsa">mdy666/qwen-nsa: qwen-nsa</A>
										<DT><A HREF="https://github.com/fla-org/native-sparse-attention">fla-org/native-sparse-attention: 🐳 Efficient Triton implementations for "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention"</A>
									</DL><p>
									<DT><H3 FOLDED>SpargeAttn</H3>
									<DL><p>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>Slim-attention</H3>
								<DL><p>
									<DT><A HREF="https://x.com/TheTuringPost/status/1905205969925357934">(1) TuringPost en X: "What is Slim Attention? It's a new attention mechanism that allows models to be 2x faster and cut memory use by 32 times! ▪️ What's the secret? It uses the same math as Multi-Head Attention (MHA) but applies one clever trick: Instead of storing both keys (K) and values (V) in https://t.co/TOPoylrZrA" / X</A>
								</DL><p>
								<DT><H3 FOLDED>prefix cache</H3>
								<DL><p>
									<DT><A HREF="https://github.com/allwefantasy/auto-coder/blob/b509271626e0bc0c511dcd510f6cb6c9d28b3267/src/autocoder/rag/long_context_rag.py#L170">auto-coder/src/autocoder/rag/long_context_rag.py at b509271626e0bc0c511dcd510f6cb6c9d28b3267 · allwefantasy/auto-coder</A>
								</DL><p>
								<DT><H3 FOLDED>local-attention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/lucidrains/local-attention">lucidrains/local-attention: An implementation of local windowed attention for language modeling</A>
								</DL><p>
								<DT><H3 FOLDED>attention-theory</H3>
								<DL><p>
									<DT><A HREF="https://liyuan24.github.io/writings/attention_backprop.html">Attention Backpropagation | Liyuan’s Log</A>
								</DL><p>
								<DT><A HREF="https://kexue.fm/archives/10091">缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA - 科学空间|Scientific Spaces</A>
								<DT><A HREF="https://github.com/feifeibear/long-context-attention">feifeibear/long-context-attention: Sequence Parallel Attention for Long Context LLM Model Training and Inference</A>
								<DT><A HREF="https://kexue.fm/archives/9812">Scale operation of Attention from the perspective of gradient maximization</A>
								<DT><A HREF="https://kexue.fm/archives/8823">Analyzing the Scale Operation of Attention from the Perspective of Entropy</A>
								<DT><A HREF="https://www.youtube.com/watch?v=fEVyfT-gLqQ">10 – Self / cross, hard / soft attention and the Transformer - YouTube</A>
								<DT><A HREF="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">The Transformer Family Version 2.0 | Lil'Log</A>
								<DT><A HREF="https://atcold.github.io/NYU-DLSP20/en/week12/12-3/">Attention and the Transformer · Deep Learning</A>
								<DT><A HREF="https://twitter.com/charles_irl/status/1724110196744835193">PagedAttention, Virtual Context, Speculative Decoding, Register Tokens</A>
								<DT><A HREF="https://twitter.com/giffmana/status/1659512770100973572/photo/1">Self-Attention Does Not Need O(n^2) Memory</A>
								<DT><A HREF="https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html">Rethinking Attention with Performers – Google Research Blog</A>
								<DT><A HREF="https://twitter.com/ZihangDai/status/1281349893873897478/photo/1">Funnel-Transformer</A>
								<DT><A HREF="https://arxiv.org/abs/2404.07143">[2404.07143] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/gpt-attention.md">TensorRT-LLM/docs/source/advanced/gpt-attention.md</A>
								<DT><A HREF="https://www.youtube.com/watch?v=3a0_hAiFKag">TransformerFAM: Feedback attention is working memory - YouTube</A>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1764717117881094582">Based attention</A>
								<DT><A HREF="https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py">jax/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py at main · google/jax</A>
								<DT><A HREF="https://github.com/mit-han-lab/duo-attention/tree/main">mit-han-lab/duo-attention: DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</A>
								<DT><A HREF="https://www.youtube.com/watch?v=qR56cyMdDXg">Every attention head explained - YouTube</A>
								<DT><A HREF="https://github.com/xdit-project/xDiT/blob/main/xfuser/model_executor/layers/attention_processor.py">xDiT/xfuser/model_executor/layers/attention_processor.py at main · xdit-project/xDiT</A>
								<DT><A HREF="https://github.com/mit-han-lab/Block-Sparse-Attention">mit-han-lab/Block-Sparse-Attention: A sparse attention kernel supporting mix sparse patterns</A>
								<DT><A HREF="https://github.com/pytorch-labs/attention-gym">pytorch-labs/attention-gym: Helpful tools and examples for working with flex-attention</A>
								<DT><A HREF="https://arxiv.org/abs/1910.10683">[1910.10683] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer: Figure 3 Attention patterns</A>
								<DT><A HREF="https://www.youtube.com/watch?v=eMlx5fFNoYc">Attention in transformers, visually explained | DL6 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=KJtZARuO3JY">Visualizing transformers and attention | Talk for TNG Big Tech Day '24 - YouTube</A>
								<DT><A HREF="https://github.com/microsoft/AttentionEngine">microsoft/AttentionEngine</A>
								<DT><A HREF="https://sayak.dev/posts/attn-diffusion.html">Flavors of attention in modern diffusion models – Sayak Paul</A>
							</DL><p>
							<DT><H3 FOLDED>Adaptive Computation</H3>
							<DL><p>
								<DT><H3 FOLDED>adaptive-computation-calm</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2207.07061.pdf">Confident Adaptive Language Modeling (CALM)</A>
									<DT><A HREF="https://blog.research.google/2022/12/accelerating-text-generation-with.html?m=1">Accelerating text generation with Confident Adaptive Language Modeling (CALM) – Google Research Blog</A>
								</DL><p>
								<DT><H3 FOLDED>speculative decoding</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/">A Hitchhiker’s Guide to Speculative Decoding | PyTorch</A>
									<DT><A HREF="https://github.com/feifeibear/LLMSpeculativeSampling">feifeibear/LLMSpeculativeSampling: Fast inference from large lauguage models via speculative decoding</A>
								</DL><p>
								<DT><A HREF="https://github.com/hao-ai-lab/LookaheadDecoding">hao-ai-lab/LookaheadDecoding</A>
								<DT><A HREF="https://blog.research.google/2024/01/introducing-aspire-for-selective.html">Introducing ASPIRE for selective prediction in LLMs – Google Research Blog</A>
								<DT><A HREF="https://arxiv.org/abs/2404.16710">[2404.16710] LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding</A>
								<DT><A HREF="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">The Transformer Family Version 2.0: Universal Transformer</A>
							</DL><p>
							<DT><H3 FOLDED>MoE</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/Tutel">microsoft/Tutel: Tutel MoE: An Optimized Mixture-of-Experts Implementation</A>
								<DT><A HREF="https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/">Demystifying AI Inference Deployments for Trillion Parameter Large Language Models | NVIDIA Technical Blog</A>
								<DT><A HREF="https://pytorch.org/blog/training-moes/?utm_content=298456196&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Training MoEs at Scale with PyTorch | PyTorch</A>
								<DT><A HREF="https://www.artfintel.com/p/more-on-mixture-of-experts-models">More on Mixture of Experts models - by Finbarr Timbers</A>
								<DT><A HREF="https://www.artfintel.com/p/papers-ive-read-this-week-mixture">Papers I’ve read this week, Mixture of Experts edition</A>
								<DT><A HREF="https://arxiv.org/html/2404.02258v1">Mixture-of-Depths: Dynamically allocating compute in transformer-based language models</A>
								<DT><A HREF="https://github.com/JerryYin777/DeepSeek-v2-MoE-MLA-DLB_and_CB-Loss-Implementation">JerryYin777/DeepSeek-v2-MoE-MLA-DLB_and_CB-Loss-Implementation: Self implementation of Device-Level Balance Loss and Communication Balance Loss of DeepSeek v2 Tech Report（Not Given in Official Code）</A>
								<DT><A HREF="https://github.com/laekov/fastmoe">laekov/fastmoe: A fast MoE impl for PyTorch</A>
								<DT><A HREF="https://x.com/DAlistarh/status/1909568131367911843">MoE-Quant, a fast version of GPTQ for MoEs</A>
							</DL><p>
							<DT><H3 FOLDED>Elastic Inference</H3>
							<DL><p>
								<DT><A HREF="https://github.com/UKPLab/sentence-transformers/releases/tag/v2.7.0">Release v2.7.0 - CachedGISTEmbedLoss, easy Matryoshka inference &amp; evaluation, CrossEncoder, Intel Gaudi2 · UKPLab/sentence-transformers</A>
								<DT><A HREF="https://arxiv.org/abs/2310.07707">[2310.07707] MatFormer: Nested Transformer for Elastic Inference</A>
								<DT><A HREF="https://twitter.com/adityakusupati/status/1714001115732427176">(1) Aditya Kusupati en X: "Announcing MatFormer - a nested🪆(Matryoshka) Transformer that offers elasticity across deployment constraints. MatFormer is an architecture that lets us use 100s of accurate smaller models that we never actually trained for! https://t.co/wzR7To7HZu 1/9 https://t.co/mBF0ZOIjlz" / X</A>
								<DT><A HREF="https://github.com/RAIVNLab/MatFormer-OLMo">MatFormer: Nested Transformer for Elastic Inference</A>
							</DL><p>
							<DT><H3 FOLDED>Block Transformer</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2406.02657">Block Transformer: Global-to-Local Language Modeling for Fast Inference</A>
							</DL><p>
							<DT><A HREF="https://poloclub.github.io/transformer-explainer/">Transformer Explainer: LLM Transformer Model Visually Explained</A>
							<DT><A HREF="https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c">kernl</A>
							<DT><A HREF="https://research.google/blog/alternating-updates-for-efficient-transformers/">Alternating updates for efficient transformers</A>
							<DT><A HREF="https://arxiv.org/abs/2301.13310">[2301.13310] Alternating Updates for Efficient Transformers</A>
							<DT><A HREF="https://kexue.fm/archives/9948">(main: whole series) Transformer Upgrade Road: 16. "Review" Length Extrapolation Technology</A>
							<DT><A HREF="https://kexue.fm/archives/9844">VQ the key, and the complexity of Transformer becomes linear</A>
							<DT><A HREF="https://www.youtube.com/watch?v=PnWOeIgl3GA">Language Modeling with Reduced Densities</A>
							<DT><A HREF="https://x.com/giffmana/status/1873869654252544079">DiffTransformer</A>
							<DT><A HREF="https://www.youtube.com/watch?v=zWXxPWcfuc8">Memory Layers at Scale - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-distributed-inference</H3>
						<DL><p>
							<DT><H3 FOLDED>deepspeed-inference</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2207.00032.pdf">DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeedExamples/tree/master/inference/mii">DeepSpeedExamples/inference/mii</A>
							</DL><p>
							<DT><A HREF="https://github.com/bytedance/flux">bytedance/flux: A fast communication-overlapping library for tensor parallelism on GPUs.</A>
							<DT><A HREF="https://github.com/feifeibear/long-context-attention">feifeibear/long-context-attention: USP: Unified (a.k.a. Hybrid, 2D) Sequence Parallel Attention for Long Context Transformers Model Training and Inference</A>
							<DT><A HREF="https://www.youtube.com/watch?v=-uyXE7dY5H0">NIPS: Oral Session 4 - Ilya Sutskever - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-server</H3>
						<DL><p>
							<DT><H3 FOLDED>transformer-inference-prefill</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/siyan_zhao">Prepacking</A>
								<DT><A HREF="https://arxiv.org/abs/2404.09529">[2404.09529] Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models</A>
								<DT><A HREF="https://github.com/siyan-zhao/prepacking">siyan-zhao/prepacking: The source code of our work "Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models"</A>
								<DT><A HREF="https://twitter.com/siyan_zhao/status/1780288750624612850">(1) Siyan Zhao en X: "🚨LLM RESEARCHERS🚨Want a free boost in speed and memory efficiency for your HuggingFace🤗LLM with ZERO degradation in generation quality? Introducing Prepacking, a simple method to obtain up to 6x speedup and 16x memory efficiency gains in prefilling prompts of varying lengths.... https://t.co/O8XsjhOLGZ" / X</A>
							</DL><p>
							<DT><H3 FOLDED>Orca</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2305.05920">Fast Distributed Inference Serving for Large Language Models</A>
								<DT><A HREF="https://www.usenix.org/conference/osdi22/presentation/yu">Orca: A Distributed Serving System for Transformer-Based Generative Models | USENIX</A>
								<DT><A HREF="https://strint.notion.site/TurboTransformer-8776e615b6484f17a82ca3af81bcacfc">TurboTransformer: 变长输入任务的优化</A>
							</DL><p>
							<DT><H3 FOLDED>transformer-inference-server-decode</H3>
							<DL><p>
								<DT><H3 FOLDED>speculative decoding</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2302.01318.pdf">Accelerating Large Language Model Decoding with Speculative Sampling</A>
									<DT><A HREF="https://arxiv.org/abs/2211.17192">[CRITICAL] Fast Inference from Transformers via Speculative Decoding</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM">NVIDIA/TensorRT-LLM: TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/docs/source/conceptual/speculation.md">text-generation-inference/docs/source/conceptual/speculation.md at main · huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/speculative_decoding.md">TensorRT-LLM/docs/source/speculative_decoding.md</A>
									<DT><A HREF="https://github.com/lucidrains/speculative-decoding">lucidrains/speculative-decoding: Explorations into some recent techniques surrounding speculative decoding</A>
									<DT><A HREF="https://github.com/hao-ai-lab/LookaheadDecoding">hao-ai-lab/LookaheadDecoding</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>transformer-inference-server-kv-cache</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/XueFz/status/1768690833988174040">(1) Fuzhao Xue en X: "KV cache may be the most redundant memory usage, but it’s non-trivial to compress it in a lossless way. My Takeaway: 1) Adaptively append/merge the current token to KV cache. This is a simple but smart solution to achieve better trade-off between RNN and Transformer, 2) Designed..." / X</A>
								<DT><A HREF="https://github.com/SqueezeAILab/KVQuant">SqueezeAILab/KVQuant: KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</A>
								<DT><A HREF="https://www.youtube.com/watch?v=r_UBBfTPcF0">Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>transformer-inference-server-continous batching</H3>
							<DL><p>
								<DT><A HREF="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/">Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</A>
							</DL><p>
							<DT><H3 FOLDED>MoonCake</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2407.00079">[2407.00079] Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</A>
								<DT><A HREF="https://github.com/kvcache-ai/Mooncake?tab=readme-ov-file">kvcache-ai/Mooncake: Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI.</A>
								<DT><A HREF="https://zhuanlan.zhihu.com/p/705754254">Mooncake (1): 在月之暗面做月饼，Kimi 以 KVCache 为中心的分离式推理架构 - 知乎</A>
								<DT><A HREF="https://zhuanlan.zhihu.com/p/705910725">关于 Mooncake 的碎碎念 - 知乎</A>
								<DT><A HREF="https://zhuanlan.zhihu.com/p/706204757">Mooncake (2)：Kimi “泼天的流量”怎么接，分离架构下基于预测的调度策略 - 知乎</A>
								<DT><A HREF="https://zhuanlan.zhihu.com/p/707997501">Mooncake (3): 开源数据集！以及一些感谢和碎碎念 - 知乎</A>
							</DL><p>
							<DT><H3 FOLDED>FlexGen</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2303.06865">[2303.06865] High-throughput Generative Inference of Large Language Models with a Single GPU</A>
							</DL><p>
							<DT><H3 FOLDED>MuxServe</H3>
							<DL><p>
								<DT><A HREF="https://x.com/haoailab/status/1805307696297689119">(1) Hao AI Lab en X: "Multiple LLM serving has emerged as a crucial and costly demand. Want to co-serve multiple LLMs with better utilization? Introducing MuxServe - flexible spatial-temporal multiplexing - up to 1.8x higher throughput Blog: https://t.co/Pep94vUFTw Paper: https://t.co/X1Jhov3QOY https://t.co/mXrHMSLPS1" / X</A>
								<DT><A HREF="https://hao-ai-lab.github.io/blogs/muxserve/">MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving | Hao AI Lab @ UCSD</A>
								<DT><A HREF="https://arxiv.org/abs/2404.02015">[2404.02015] MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving</A>
								<DT><A HREF="https://github.com/hao-ai-lab/MuxServe">hao-ai-lab/MuxServe</A>
								<DT><A HREF="https://github.com/EfficientLLMSys/MuxServe-vLLM">EfficientLLMSys/MuxServe-vLLM</A>
							</DL><p>
							<DT><A HREF="https://proceedings.mlsys.org/paper_files/paper/2023/hash/523f87e9d08e6071a3bbd150e6da40fb-Abstract-mlsys2023.html">Efficiently Scaling Transformer Inference</A>
							<DT><A HREF="https://arxiv.org/pdf/2204.02311.pdf">PaLM: Scaling Language Modeling with Pathways</A>
							<DT><A HREF="https://twitter.com/aleks_madry/status/1642972175572545545">AI deployment as suppling chain problem</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-flops</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/karpathy/status/1781047292486914189">Karpathy: Llama 3 model card (flops model training taxonomy)</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-benchmarking</H3>
						<DL><p>
							<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance (2024)</A>
							<DT><A HREF="https://arxiv.org/pdf/2011.02327.pdf">INFERBENCH: UNDERSTANDING DEEP LEARNING INFERENCE SERVING WITH AN AUTOMATIC BENCHMARKING SYSTEM</A>
							<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic | kipply's blog</A>
							<DT><A HREF="https://bytemlperf.ai/guide/introduction.html">Introduction - ByteMLPerf</A>
							<DT><A HREF="https://www.modular.com/blog/how-to-be-confident-in-your-performance-benchmarking">Modular: How to Be Confident in Your Performance Benchmarking</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-manifesto</H3>
						<DL><p>
							<DT><A HREF="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#methods-overview">Large Transformer Model Inference Optimization | Lil'Log</A>
							<DT><A HREF="https://arxiv.org/abs/2211.05102">[2211.05102] Efficiently Scaling Transformer Inference</A>
							<DT><A HREF="https://www.overleaf.com/project/64399db50ab28ba5e3335352">Empirical Analysis of Compute-Optimal Large Language Model Training - Online LaTeX Editor Overleaf</A>
							<DT><A HREF="https://github.com/DataCrunch-io/inference_playbook/blob/main/manifesto.MD">inference_playbook/manifesto.MD at main · DataCrunch-io/inference_playbook · GitHub</A>
							<DT><A HREF="https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c">Hugging Face Transformer Inference Under 1 Millisecond Latency | by Michaël Benesty | Towards Data Science</A>
							<DT><A HREF="https://gist.github.com/lattner/31ed37682ef1576b16bca1432ea9f782#overall-vision">Swift Concurrency Manifesto</A>
							<DT><A HREF="https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&hw=ph&test=db">Round 20 results - TechEmpower Framework Benchmarks</A>
							<DT><A HREF="https://els-rd.github.io/transformer-deploy/compare/">Which tool to choose for your inference? - transformer-deploy by Lefebvre Dalloz</A>
							<DT><A HREF="https://github.com/ELS-RD/kernl">ELS-RD/kernl: Kernl lets you run PyTorch transformer models several times faster on GPU with a single line of code, and is designed to be easily hackable.</A>
							<DT><A HREF="https://github.com/ELS-RD/transformer-deploy">ELS-RD/transformer-deploy: Efficient, scalable and enterprise-grade CPU/GPU inference server for 🤗 Hugging Face transformer models 🚀</A>
						</DL><p>
						<DT><H3 FOLDED>transformer-inference-big-picture</H3>
						<DL><p>
							<DT><A HREF="https://karpathy.ai/stateofgpt.pdf">https://karpathy.ai/stateofgpt.pdf</A>
							<DT><A HREF="https://www.youtube.com/watch?v=bZQun8Y4L2A">State of GPT | BRK216HFS - YouTube</A>
							<DT><A HREF="https://excalidraw.com/#room=93d0feb81016e7e5d6c7,TYDGkAGBNB0A8Fa-5-EhDg">Excalidraw</A>
							<DT><A HREF="https://www.youtube.com/@rutgersefficientaiseminar9909">Rutgers Efficient AI Seminar - YouTube</A>
							<DT><A HREF="https://openreview.net/forum?id=wIPIhHd00i">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time | OpenReview</A>
							<DT><A HREF="https://www.youtube.com/@neubig/videos">Graham Neubig  (CMU)</A>
						</DL><p>
						<DT><H3 FOLDED>QPS</H3>
						<DL><p>
							<DT><A HREF="https://x.com/NoamShazeer/status/1803790708358410380">(1) Noam Shazeer en X: "Character AI is serving 20,000 QPS. Here are the technologies we use to serve hyper-efficiently. [https://t.co/R14Jt9Z5yo ]" / X</A>
							<DT><A HREF="https://research.character.ai/optimizing-inference/">Optimizing AI Inference at Character.AI</A>
							<DT><A HREF="https://x.com/cis_female/status/1803816791677808964">(1) sophia en X: "&amp;gt;20k QPS MQA everywhere local attention, 1/6 layers use global attn KV cache tied between layers, cached on host memory with "95% cache rate" trained in int8 precision" / X</A>
						</DL><p>
						<DT><H3 FOLDED>edge-inference</H3>
						<DL><p>
							<DT><H3 FOLDED>ktransformers</H3>
							<DL><p>
								<DT><A HREF="https://github.com/kvcache-ai/ktransformers">kvcache-ai/ktransformers: A Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations</A>
							</DL><p>
							<DT><H3 FOLDED>Jetson</H3>
							<DL><p>
								<DT><H3 FOLDED>jetson-language-models</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=jSKHeYVcAB8">Gemma3 on Jetson Orin Nano: Live demo running Visual Language Models at 15 Tokens/s (with examples!) - YouTube</A>
								</DL><p>
								<DT><A HREF="https://www.techpowerup.com/gpu-specs/jetson-orin-nano-8-gb.c4082">NVIDIA Jetson Orin Nano 8 GB Specs | TechPowerUp GPU Database</A>
								<DT><A HREF="https://blogs.nvidia.com/blog/jetson-generative-ai-supercomputer/">NVIDIA Unveils Its Most Affordable Generative AI Supercomputer | NVIDIA Blog</A>
							</DL><p>
							<DT><A HREF="https://www.c4isrnet.com/opinion/2024/06/26/how-the-military-is-preparing-for-ai-at-the-edge/">How the military is preparing for AI at the edge</A>
							<DT><A HREF="https://www.youtube.com/watch?v=IjjZcnwkrAg">The BIG 3 Embedded Protocols - I2C, SPI, UART - YouTube</A>
							<DT><A HREF="https://research.facebook.com/publications/machine-learning-at-facebook-understanding-inference-at-the-edge/">Machine Learning at Facebook: Understanding Inference at the Edge - Meta Research</A>
							<DT><A HREF="https://github.com/kvcache-ai/ktransformers">kvcache-ai/ktransformers: A Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations</A>
							<DT><A HREF="https://huggingface.co/collections/facebook/mobilellm-6722be18cb86c20ebe113e95">MobileLLM - a facebook Collection</A>
							<DT><A HREF="https://github.com/OpenBMB/MiniCPM">OpenBMB/MiniCPM: MiniCPM3-4B: An edge-side LLM that surpasses GPT-3.5-Turbo.</A>
						</DL><p>
						<DT><A HREF="https://astralord.github.io/posts/transformer-inference-optimization-toolset/">Transformers Inference Optimization Toolset | AstraBlog</A>
						<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic | kipply's blog</A>
						<DT><A HREF="https://kipp.ly/transformer-param-count/">LLM Parameter Counting | kipply's blog</A>
						<DT><A HREF="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">Large Transformer Model Inference Optimization | Lil'Log</A>
						<DT><A HREF="https://arxiv.org/abs/2009.06732">[2009.06732] Efficient Transformers: A Survey</A>
						<DT><A HREF="https://arxiv.org/abs/2211.05102">[2211.05102] Efficiently Scaling Transformer Inference</A>
						<DT><A HREF="https://www.zhihu.com/people/liang-de-peng">GiantPandaCV (chinese high-proffesional discussions)</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance (2024)</A>
						<DT><A HREF="https://horace.io/brrr_intro.html">Making Deep Learning go Brrrr From First Principles</A>
						<DT><A HREF="https://arxiv.org/pdf/2302.14017.pdf">Full Stack Optimization of Transformer Inference: a Survey</A>
						<DT><A HREF="https://arxiv.org/abs/2303.06865">[2303.06865] High-throughput Generative Inference of Large Language Models with a Single GPU</A>
						<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic</A>
						<DT><A HREF="https://arxiv.org/pdf/2211.05102.pdf">Efficiently Scaling Transformer Inference</A>
						<DT><A HREF="https://huggingface.co/papers/2312.11514">Paper page - LLM in a flash: Efficient Large Language Model Inference with Limited Memory</A>
						<DT><A HREF="https://twitter.com/atiorh/status/1737912777153609918">(Apple) Atila en X: "My takeaways from Apple's “LLM in a flash" (1/n)" / X</A>
						<DT><A HREF="https://www.semianalysis.com/p/inference-race-to-the-bottom-make">Inference Race To The Bottom - Make It Up On Volume?</A>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1773268740806705266">optimal batch size (bytes per param, flops, bytes/token, arithmetic intensity )</A>
						<DT><A HREF="https://developer.nvidia.com/blog/accelerating-hpc-applications-with-nsight-compute-roofline-analysis/">Accelerating HPC Applications with NVIDIA Nsight Compute Roofline Analysis | NVIDIA Technical Blog</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance (2024 main)</A>
						<DT><A HREF="https://arxiv.org/abs/2401.08092">[2401.08092] A Survey of Resource-efficient LLM and Multimodal Foundation Models</A>
						<DT><A HREF="https://research.google/blog/alternating-updates-for-efficient-transformers/">Alternating updates for efficient transformers</A>
						<DT><A HREF="https://github.com/UbiquitousLearning/Efficient_Foundation_Model_Survey?tab=readme-ov-file">UbiquitousLearning/Efficient_Foundation_Model_Survey: Survey Paper List - Efficient LLM and Foundation Models</A>
						<DT><A HREF="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/">Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</A>
						<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/reference/memory.md">TensorRT-LLM/docs/source/reference/memory.md</A>
						<DT><A HREF="https://mlcommons.org/2024/03/mlperf-inference-v4/">New MLPerf Inference Benchmark Results Highlight The Rapid Growth of Generative AI Models - MLCommons</A>
						<DT><A HREF="https://arxiv.org/pdf/2405.00208">A PRIMER ON THE INNER WORKINGS OF TRANSFORMER-BASED LANGUAGE MODELS</A>
						<DT><A HREF="https://github.com/cuda-mode/awesomeMLSys">cuda-mode/awesomeMLSys: An ML Systems Onboarding list</A>
						<DT><A HREF="https://github.com/xai-org/grok-1">xai-org/grok-1: Grok open release</A>
						<DT><A HREF="https://github.com/madsys-dev/deepseekv2-profile/blob/main/workspace/blog/optimizing-mla.md">deepseekv2-profile/workspace/blog/optimizing-mla.md at main · madsys-dev/deepseekv2-profile</A>
						<DT><A HREF="https://kexue.fm/archives/10091">The ultimate struggle between cache and effects: from MHA, MQA, GQA (by Jianlin SU (the GOD creator of ROPE)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=ptGDaGUXInw">Mark Russinovich | Generative AI in the Cloud: Inside Microsoft AI Innovation - YouTube</A>
						<DT><A HREF="https://developer.nvidia.com/blog/demystifying-ai-inference-deployments-for-trillion-parameter-large-language-models/">Demystifying AI Inference Deployments for Trillion Parameter Large Language Models | NVIDIA Technical Blog</A>
						<DT><A HREF="https://github.com/xjdr-alt/mla_blog_translation">DeepSeek-V2 High-performance Inference Optimization Notes: MLA Optimization</A>
						<DT><A HREF="https://github.com/DefTruth/Awesome-LLM-Inference">DefTruth/Awesome-LLM-Inference: 📖A curated list of Awesome LLM Inference Paper with codes, TensorRT-LLM, vLLM, streaming-llm, AWQ, SmoothQuant, WINT8/4, Continuous Batching, FlashAttention, PagedAttention etc.</A>
						<DT><A HREF="https://x.com/reinerpope/status/1591150725320802304">(1) Reiner Pope en X: "1/7 Transformers allow efficient large-scale training, but inference (generating text) can be slow and suffer communication/memory bottlenecks. Our new paper proposes techniques for efficient low-latency inference on PaLM 8B—540B models: https://t.co/4npbzqRBIN https://t.co/d647YA4xxk" / X</A>
						<DT><A HREF="https://www.artfintel.com/p/where-do-llms-spend-their-flops">Where do LLMs spend their FLOPS? - by Finbarr Timbers</A>
						<DT><A HREF="https://www.artfintel.com/p/transformer-inference-tricks">Transformer inference tricks - by Finbarr Timbers</A>
						<DT><A HREF="https://www.artfintel.com/p/efficient-llm-inference">Efficient LLM inference - by Finbarr Timbers</A>
						<DT><A HREF="https://arxiv.org/pdf/2305.05920">Fast Distributed Inference Serving for Large Language Models</A>
						<DT><A HREF="https://zeux.io/2024/03/15/llm-inference-sol/">zeux.io - LLM inference speed of light</A>
						<DT><A HREF="https://x.com/arankomatsuzaki/status/1818828480366219601">Scaling Inference Compute with Repeated Sampling (efficient transformer inference batch axis)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=mYRqvB1_gRk&t=835s">Exploring the Latency/Throughput &amp; Cost Space for LLM Inference // Timothée Lacroix // CTO Mistral - YouTube</A>
						<DT><A HREF="https://www.baseten.co/blog/llm-transformer-inference-guide/">A guide to LLM inference and performance</A>
						<DT><A HREF="https://github.com/drisspg/transformer_nuggets">drisspg/transformer_nuggets: A place to store reusable transformer components of my own creation or found on the interwebs</A>
						<DT><A HREF="https://www.njkumar.com/calculating-gpt2s-inference-speedups/">Calculating GPT-2’s Inference Speedups | njkumar</A>
						<DT><A HREF="https://github.com/feifeibear/LLMRoofline">feifeibear/LLMRoofline: Compare different hardware platforms via the Roofline Model for LLM inference tasks.</A>
						<DT><A HREF="https://jax-ml.github.io/scaling-book/transformers/">All the Transformer Math You Need to Know | How To Scale Your Model</A>
					</DL><p>
					<DT><H3 FOLDED>Language Models</H3>
					<DL><p>
						<DT><A HREF="https://github.com/Tencent/Tencent-Hunyuan-Large">Tencent/Tencent-Hunyuan-Large</A>
						<DT><A HREF="https://github.com/lucidrains/x-transformers">lucidrains/x-transformers: A simple but complete full-attention transformer with a set of promising experimental features from various papers</A>
						<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
						<DT><A HREF="https://arxiv.org/pdf/1910.10683.pdf">(T5): Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</A>
						<DT><A HREF="https://arxiv.org/abs/2001.08361">(Kaplan) Scaling Laws for Neural Language Models</A>
						<DT><A HREF="https://arxiv.org/pdf/2307.09288.pdf">Llama 2: Open Foundation and Fine-Tuned Chat Models</A>
						<DT><A HREF="https://arxiv.org/abs/2312.11805">Gemini: A Family of Highly Capable Multimodal Models</A>
						<DT><A HREF="https://arxiv.org/abs/2310.06825">[2310.06825] Mistral 7B</A>
						<DT><A HREF="https://arxiv.org/abs/2401.04088">[2401.04088] Mixtral of Experts</A>
						<DT><A HREF="https://arxiv.org/abs/2204.02311">[2204.02311] PaLM: Scaling Language Modeling with Pathways</A>
						<DT><A HREF="https://arxiv.org/abs/2205.05131">[Abletative &amp; Pareto-frontier] UL2: Unifying Language Learning Paradigms</A>
						<DT><A HREF="https://openreview.net/pdf?id=gEZrGCozdqR">FLAN: Finetuned Language Models are Zero-Shot Learners</A>
						<DT><A HREF="https://arxiv.org/abs/2205.01068">[2205.01068] OPT: Open Pre-trained Transformer Language Models</A>
						<DT><A HREF="https://arxiv.org/pdf/2203.02155.pdf">(OpenAI, 2022) Training language models to follow instructions</A>
						<DT><A HREF="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space (word2vec)</A>
						<DT><A HREF="https://arxiv.org/abs/2204.07705">Super-NaturalInstructions: Generalization via Instructions (Allen AI)</A>
						<DT><A HREF="https://arxiv.org/pdf/2303.17568.pdf">CodeGeeX</A>
						<DT><A HREF="https://papers.labml.ai/paper/18eb0daa07cb11edb9b9d35608ee6155">Formal Algorithms for Transformers</A>
						<DT><A HREF="https://arxiv.org/abs/2103.00020">CLIP: Learning Transferable Visual Models From Natural Language Supervision</A>
						<DT><A HREF="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf">Supplement GPT-3 few-shot generalizable learners</A>
						<DT><A HREF="https://arxiv.org/abs/2307.09793">[2307.09793] On the Origin of LLMs: An Evolutionary Tree and Graph</A>
						<DT><A HREF="https://arxiv.org/pdf/2307.06435.pdf">2023 Summary</A>
						<DT><A HREF="https://huggingface.co/transformers/v4.8.0/glossary.html">Glossary — transformers 4.7.0 documentation</A>
						<DT><H3 FOLDED>language-models-courses</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=orDKvo8h71o">Stanford CS25: V4 I Hyung Won Chung of OpenAI - YouTube</A>
							<DT><A HREF="https://people.cs.umass.edu/~miyyer/cs685/">Advanced NLP - CS 685, Spring 2024, UMass Amherst</A>
							<DT><A HREF="https://people.cs.umass.edu/~miyyer/cs685/schedule.html">Schedule - CS 685, Spring 2022, UMass Amherst</A>
							<DT><A HREF="https://stanford-cs324.github.io/winter2022/">CS324 - Large Language Models</A>
							<DT><A HREF="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/">COS 597G: Understanding Large Language Models</A>
							<DT><A HREF="https://www.youtube.com/watch?v=AKMuA_TVz3A">Ilya Sutskever (OpenAI): An observation on Generalization</A>
							<DT><A HREF="https://phontron.com/class/anlp2024/assets/slides/anlp-15-tourofllms.pdf">CS11-711 Advanced NLP Tour of Modern LLMs</A>
							<DT><A HREF="https://phontron.com/class/anlp2024/lectures/">Lectures | 11-711 ANLP</A>
							<DT><A HREF="https://www.youtube.com/watch?v=RciT5fcuN1E">High Performance LLMs in Jax 2024 -- Session 2 - YouTube</A>
							<DT><A HREF="https://github.com/karpathy/makemore">karpathy/makemore: An autoregressive character-level language model for making more things</A>
							<DT><A HREF="https://x.com/lileics/status/1860174527994692082">(1) Lei Li en X: "I will teach Large Language Model Systems again in Spring 2025. (11868 for CMU folks) The course syllabus (tentative) is online at https://t.co/6AmKDbdHt6 CMU ppl are welcome to enroll. For others, I will release the materials online. Or apply to CMU LLM/GenAI certificate program" / X</A>
							<DT><A HREF="https://phontron.com/class/anlp-fall2024/">CMU Advanced NLP Fall 2024 | 11-711 ANLP</A>
							<DT><A HREF="https://llmsystem.github.io/llmsystem2025spring/">LLM Systems | Large Language Model Systems</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-people</H3>
						<DL><p>
							<DT><A HREF="https://www.yitay.net/papers">Papers — Yi Tay</A>
							<DT><A HREF="https://scholar.google.com/citations?hl=en&user=wsGvgA8AAAAJ&view_op=list_works&citft=1&email_for_op=antonio.jfdominguez%40gmail.com&gmla=AOV7GLN3W4O6NIDNdCcBkV_iDC21-xJFgE9EtTcF0RmBzdcKrEF46210Wvjs822ZRzG_SBibT3xddH6GJgU3SPO710DKm_SctPk3WkFFHzFwhDdat8yKJ1X7TYY8-WYpbAyAHud1T-e8q4oYIVEKVcszYYDshws0si2Wiyf2DLsvrAwbWUqybXjXFvwMczvMx_ckXdir3sbM9FRUTfFY9wB3nYllfhw3gwJzC_k1AsIXvsdXsn4XioWf4Ik">‪Noam Shazeer‬ - (Transformers author)</A>
							<DT><A HREF="https://scholar.google.com/citations?hl=en&user=oR9sCGYAAAAJ&view_op=list_works&citft=1&email_for_op=antonio.jfdominguez%40gmail.com&gmla=AOV7GLOU01RwzH9ABWVgReuJ4vYIX-R16ZT_WjsIZt7HhK2cY3eb5fbOdZNkOCbIlM91bVmTEhMFcavI9r8QcGTItl1zpC_q0beRNBvJIqorFTZN7iChYoX3jUywwyJ5CwLvpFYruduPtNhAPsDXUg7xpsGLiSh45meJipgi3nKsNJzMieDISepUsp8-3hmuKU7cdrhNGNZp3ztviA_HkpQpZQa3cuL70CtXUOwgBmAIue-nEexRvwjcglc">‪Ashish Vaswani‬ - ‪Google Scholar‬</A>
							<DT><A HREF="https://twitter.com/ThomasScialom">MetaAI: Thomas Scialom (Llama)</A>
						</DL><p>
						<DT><H3 FOLDED>capabilities</H3>
						<DL><p>
							<DT><H3 FOLDED>interpretability</H3>
							<DL><p>
								<DT><H3 FOLDED>mechanistic interpretability</H3>
								<DL><p>
									<DT><H3 FOLDED>mechanistic-interpretability-people</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/NeelNanda5">(1) Neel Nanda (@NeelNanda5) / Twitter</A>
										<DT><A HREF="https://github.com/saprmarks">saprmarks</A>
									</DL><p>
									<DT><H3 FOLDED>mechanistic-interpretability-lectures</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=OI1we2bUseI&list=PL7m7hLIqA0hr4dVOgjNwP2zjQGVHKeB7T">Real-Time Research Walkthrough: Addition in GPT-J - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=KV5gbOmHbjU&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=3">A Walkthrough of A Mathematical Framework for Transformer Circuits - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=yo4QvDn-vsU">Real-Time Research Recording: Can a Transformer Re-Derive Positional Info? - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=pC4zRb_5noQ">CS25 I Stanford Seminar 2022 - Transformer Circuits, Induction Heads, In-Context Learning - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=4JKuyfejWTU&list=LL&index=3">Mor Geva: Transformer Feed Forward Layers are Key-Value Memories, and Build Predictions - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=m8tzXelUTLo">Real-Time Research Walkthrough: Mover Heads Part 1/2 - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=I1ELSZNFeHc">David Bau - Direct Model Editing and Mechanistic Interpretability - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=2Rdp9GvcYOE">Chris Olah - Looking Inside Neural Networks with Mechanistic Interpretability - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=_Ygf0GnlwmY&t=5193s">Mechanistic Interpretability - NEEL NANDA (DeepMind) - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>transformer debugger</H3>
									<DL><p>
										<DT><A HREF="https://github.com/openai/transformer-debugger?tab=readme-ov-file">openai/transformer-debugger</A>
										<DT><A HREF="https://github.com/neelnanda-io/TransformerLens">TransformerLens: A library for mechanistic interpretability of language models</A>
										<DT><A HREF="https://www.loom.com/share/6bd8c6bde84b42a98f9a26a969d4a3ad?sid=4a09ac29-58a2-433e-b55d-762414d9a7fa">4. TDB worked example: name mover heads, part 2</A>
										<DT><A HREF="https://github.com/joennlae/tensorli">joennlae/tensorli: Absolute minimalistic implementation of a GPT-like transformer using only numpy (&lt;650 lines).</A>
										<DT><A HREF="https://explainextended.com/2023/12/31/happy-new-year-15/">Happy New Year: GPT in 500 lines of SQL - EXPLAIN EXTENDED at EXPLAIN EXTENDED</A>
										<DT><A HREF="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html">Language models can explain neurons in language models</A>
									</DL><p>
									<DT><H3 FOLDED>SAE</H3>
									<DL><p>
										<DT><A HREF="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</A>
										<DT><A HREF="https://blog.withmartian.com/post/scaling-ai-interpret">Scaling AI Interpretability</A>
										<DT><A HREF="https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html">An Intuitive Explanation of Sparse Autoencoders for LLM Interpretability | Adam Karvonen</A>
										<DT><A HREF="https://thesephist.com/posts/prism/">Prism: mapping interpretable concepts and features in a latent space of language | thesephist.com</A>
										<DT><A HREF="https://x.com/livgorton/status/1813253322582925821">(1) Liv en X: "Early work on InceptionV1 found that many individual neurons seemed monosemantic. Of course, there were also polysemantic neurons, and in my recent paper, I used SAEs to attack this. But what do SAEs do with all those apparently monosemantic neurons?" / X</A>
										<DT><A HREF="https://distill.pub/2020/circuits/branch-specialization/">Branch Specialization</A>
										<DT><A HREF="https://distill.pub/2020/circuits/visualizing-weights/">Visualizing Weights</A>
										<DT><A HREF="https://www.youtube.com/watch?v=o_cAOa5fMhE">Latent Space Visualisation: PCA, t-SNE, UMAP | Deep Learning Animated - YouTube</A>
										<DT><A HREF="https://x.com/i/bookmarks?post_id=1834251979771564368">The hope is that just optimizing something to be sparse without optimizing it to be interpretable will stumble across that interpretable decomposition</A>
										<DT><A HREF="https://www.cs.utexas.edu/~flame/pubs/GotoTOMS_revision.pdf?ref=broutonlab.com">Anatomy of High-Performance Matrix Multiplication</A>
										<DT><A HREF="https://x.com/i/bookmarks?post_id=1863487582631547027">minSAE</A>
										<DT><A HREF="https://github.com/cloneofsimo/minSAE">cloneofsimo/minSAE</A>
										<DT><A HREF="https://github.com/callummcdougall/sae_vis">callummcdougall/sae_vis: Create feature-centric and prompt-centric visualizations for sparse autoencoders (like those from Anthropic's published research).</A>
										<DT><A HREF="https://x.com/NeelNanda5/status/1904988240542834724?t=SqE4zcoGYd8EiGW-Ice8Pw&s=09">We study if SAEs help probes generalise OOD (they don't 😢). Based on this + parallel negative results on real-world tasks, we're de-prioritising SAE work.</A>
									</DL><p>
									<DT><H3 FOLDED>mechanistic-interpretability-dit</H3>
									<DL><p>
										<DT><A HREF="https://x.com/alec_helbling/status/1894833831724884193">ConceptAttention: An approach to interpreting DiT</A>
										<DT><A HREF="https://github.com/helblazer811/ConceptAttention">helblazer811/ConceptAttention: ConceptAttention: A method for interpreting multi-modal diffusion transformers.</A>
										<DT><A HREF="https://arxiv.org/abs/2502.04320">[2502.04320] ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features</A>
									</DL><p>
									<DT><A HREF="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</A>
									<DT><A HREF="https://twitter.com/NeelNanda5/status/1682827872191348736">GPT-J arithmetic addtion</A>
									<DT><A HREF="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</A>
									<DT><A HREF="https://transformer-circuits.pub/2024/april-update/index.html#scaling-laws">Scaling Laws for Dictionary Learning</A>
									<DT><A HREF="https://github.com/saprmarks/dictionary_learning">saprmarks/dictionary_learning</A>
									<DT><A HREF="https://transformer-circuits.pub/2023/toy-double-descent/index.html">Superposition, Memorization, and Double Descent</A>
									<DT><A HREF="https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating">Toy Models of Superposition</A>
									<DT><A HREF="https://transformer-circuits.pub/2021/framework/index.html#def-privileged-basis">Privileged Bases in the Transformer Residual Stream</A>
									<DT><A HREF="https://github.com/anthropics/toy-models-of-superposition/blob/main/toy_models.ipynb">toy-models-of-superposition/toy_models.ipynb</A>
									<DT><A HREF="https://github.com/alan-cooney/CircuitsVis">CircuitsVis: Mechanistic Interpretability Visualizations using React</A>
									<DT><A HREF="https://arxiv.org/pdf/2307.09458.pdf">Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chincilla (DeepMind)</A>
									<DT><A HREF="https://copy-suppression.streamlit.app/">LLM: copy-supression</A>
									<DT><A HREF="https://github.com/ArthurConmy/Automatic-Circuit-Discovery">ArthurConmy/Automatic-Circuit-Discovery</A>
									<DT><A HREF="https://github.com/neelnanda-io/Exploring-2L-SAE">neelnanda-io/Exploring-2L-SAE</A>
									<DT><A HREF="https://github.com/yizhe-ang/interactive-transformer">interactive-transformer: A visual interface for understanding Transformers</A>
									<DT><A HREF="https://www.youtube.com/watch?v=PnwC74s1nmc">Theoretical and Practical Insights from Linear Transformers</A>
									<DT><A HREF="https://github.com/neelnanda-io/neelutils">neelnanda-io/neelutils: Random utils for personal use</A>
									<DT><A HREF="https://github.com/neelnanda-io/Grokking">Grokking: A Mechanistic Interpretability Analysis of Grokking</A>
									<DT><A HREF="https://github.com/neelnanda-io/TransformerLens">TransformerLens: A library for mechanistic interpretability of language models</A>
									<DT><A HREF="https://github.com/google-deepmind/tracr">(DeepMind) Tracr: TRAnsformer Compiler for RASP</A>
									<DT><A HREF="https://twitter.com/DrJimFan/status/1613918800444899328?lang=en">We train Transformers to encode algorithms in their weights, such as sorting, counting, and balancing parentheses from lots of data. I never thought we may also go in the *reverse* direction: *compile* Transformer weights directly from explicit code</A>
									<DT><A HREF="https://twitter.com/DrJimFan/status/1613966404721729536">Thinking Like Transformers</A>
									<DT><A HREF="https://srush.github.io/raspy/">Thinking like Transformer</A>
									<DT><A HREF="https://www.youtube.com/watch?v=zUCoxhExe0o">Stanford Seminar - Computing with High-Dimensional Vectors</A>
									<DT><A HREF="https://github.com/saprmarks/geometry-of-truth">saprmarks/geometry-of-truth</A>
									<DT><A HREF="https://www.youtube.com/watch?v=XuFWN0xcM_U">[short] Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models - YouTube</A>
									<DT><A HREF="https://arxiv.org/abs//2401.06102">[2401.06102] Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models</A>
									<DT><A HREF="https://arxiv.org/abs/2401.05566">[2401.05566] Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</A>
									<DT><A HREF="https://alan-cooney.github.io/CircuitsVis/?path=/docs/attention-attentionheads--induction-heads-layer">attention / AttentionHeads - Induction Heads Layer ⋅ Storybook</A>
									<DT><A HREF="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens">interpreting GPT: the logit lens — LessWrong</A>
									<DT><A HREF="https://x.com/mlpowered/status/1792948212728524917">dictonary learning working on a fronteir model</A>
									<DT><A HREF="https://www.anthropic.com/news/mapping-mind-language-model">Mapping the Mind of a Large Language Model \ Anthropic</A>
									<DT><A HREF="https://x.com/TransluceAI/status/1849142531906547779">Transluce en X: "Announcing Transluce, a nonprofit research lab building open source, scalable technology for understanding AI systems and steering them in the public interest. Read a letter from the co-founders Jacob Steinhardt and Sarah Schwettmann: https://t.co/IUIhBjpYhS https://t.co/jl91xv7Vsc" / X</A>
									<DT><A HREF="https://www.markov.bio/research/mech-interp-path-to-e2e-biology">Through a Glass Darkly | Markov Bio</A>
									<DT><A HREF="https://www.youtube.com/watch?v=YpFaPKOeNME">NEURAL NETWORKS ARE REALLY WEIRD... - YouTube</A>
									<DT><A HREF="https://github.com/neelnanda-io/Qwen-MLP-Exploration">neelnanda-io/Qwen-MLP-Exploration</A>
									<DT><A HREF="https://github.com/neelnanda-io/neelutils/blob/master/setup.py">neelutils/setup.py at master · neelnanda-io/neelutils</A>
									<DT><A HREF="https://www.goodfire.ai/papers/mapping-latent-spaces-llama/">Mapping the Latent Space of Llama 3.3 70B - Goodfire Papers</A>
									<DT><A HREF="https://x.com/arankomatsuzaki/status/1884452248660717644">Open Problems in Mechanistic Interpretability This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Bj9BD2D3DzA">Tracing the thoughts of a large language model - YouTube</A>
								</DL><p>
								<DT><A HREF="https://thesephist.com/posts/prism/">Prism: mapping interpretable concepts and features in a latent space of language | thesephist.com</A>
								<DT><A HREF="https://research.google/blog/patchscopes-a-unifying-framework-for-inspecting-hidden-representations-of-language-models/">Patchscopes: A unifying framework for inspecting hidden representations of language models</A>
								<DT><A HREF="https://x.com/pascalefung/status/1824665235179049027">(1) Pascale Fung en X: "We always knew that Chomsky was wrong about language models, it’s nice to have a paper showing you just how wrong he was! #ACL2024 best papsr. https://t.co/bpzRX10l8F" / X</A>
								<DT><A HREF="https://blbadger.github.io/llm-invertibility.html">Information between tokens | Form and Formula</A>
							</DL><p>
							<DT><H3 FOLDED>compression</H3>
							<DL><p>
								<DT><H3 FOLDED>compression-people</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/drjwrae">(1) Jack Rae (@drjwrae) / X</A>
									<DT><A HREF="https://www.youtube.com/watch?v=boiW5qhrGH4">(George Hotz) Hutter Prize: Intelligence as Compression</A>
									<DT><A HREF="http://www.hutter1.net/">Homepage of Marcus Hutter</A>
									<DT><A HREF="https://arxiv.org/abs/2308.07037">[Alex Graves] Bayesian Flow Networks</A>
									<DT><A HREF="https://jveness.info/">Homepage of Joel Veness</A>
								</DL><p>
								<DT><H3 FOLDED>solomonoff-induction</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google-deepmind/neural_networks_solomonoff_induction">google-deepmind/neural_networks_solomonoff_induction: Learning Universal Predictors</A>
									<DT><A HREF="https://www.lesswrong.com/posts/Kyc5dFDzBg4WccrbK/an-intuitive-explanation-of-solomonoff-induction">An Intuitive Explanation of Solomonoff Induction — LessWrong</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference">Solomonoff's theory of inductive inference - Wikipedia</A>
									<DT><A HREF="https://www.sciencedirect.com/science/article/pii/S0019995864902232">A formal theory of inductive inference. Part I - ScienceDirect</A>
								</DL><p>
								<DT><H3 FOLDED>compression-lectures</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=AKMuA_TVz3A&t=1851s">Ilya Sutskever: An Observation on Generalization</A>
									<DT><A HREF="https://www.youtube.com/watch?v=dO4TPJkeaaU&t=1234s">Compression for AGI - (Jack Rae)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=OPZxs6IXH00&t=10s">Ilya Sutskever - Opening Remarks: Confronting the Possibility of AGI</A>
								</DL><p>
								<DT><H3 FOLDED>Kolmogorov-compressor</H3>
								<DL><p>
									<DT><A HREF="https://en.wikipedia.org/wiki/Kolmogorov_complexity#Compression">Kolmogorov complexity - Wikipedia</A>
									<DT><A HREF="https://www.youtube.com/watch?v=AKMuA_TVz3A&t=1851s">Ilya Sutskever: An Observation on Generalization</A>
								</DL><p>
								<DT><A HREF="https://deepmind.google/research/publications/39768/">Language Modeling Is Compression - Google DeepMind</A>
								<DT><A HREF="https://github.com/Ma-Lab-Berkeley/CRATE">Ma-Lab-Berkeley/CRATE: Code for CRATE (Coding RAte reduction TransformEr).</A>
								<DT><A HREF="https://twitter.com/YiMaTweets/status/1693302230664020449/photo/1">rate reduction flow equation for maximizing information gain</A>
								<DT><A HREF="https://arxiv.org/abs/2306.01129">[2306.01129] White-Box Transformers via Sparse Rate Reduction</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Sx2PVCAiGhQ">George Hotz | Programming | Context Tree Weighting: compression = AI</A>
								<DT><A HREF="https://github.com/GFNOrg/gfn-lm-tuning">Amortizing Intractable Inference in Large Language Models (uTransfer creator)</A>
								<DT><A HREF="https://bellard.org/">Fabrice Bellard's Home Page</A>
								<DT><A HREF="https://bellard.org/nncp/">NNCP: Lossless Data Compression with Neural Networks</A>
								<DT><A HREF="https://bellard.org/nncp/nncp_v2.1.pdf">NNCP v2: Lossless Data Compression with Transformer</A>
								<DT><A HREF="https://blog.research.google/2022/12/accelerating-text-generation-with.html?m=1">Accelerating text generation with Confident Adaptive Language Modeling (CALM) – Google Research Blog</A>
								<DT><A HREF="https://twitter.com/a_stadt/status/1737849248560066794">Sample-Efficient Pre-training</A>
								<DT><A HREF="https://twitter.com/arankomatsuzaki/status/1780073500536872990">Compression Represents Intelligence Linearly</A>
								<DT><A HREF="https://github.com/hkust-nlp/llm-compression-intelligence">hkust-nlp/llm-compression-intelligence: Official github repo for the paper "Compression Represents Intelligence Linearly"</A>
								<DT><A HREF="https://arxiv.org/abs/2404.09937">[2404.09937] Compression Represents Intelligence Linearly</A>
								<DT><A HREF="https://www.youtube.com/watch?v=xjnp42BuxOo">Compression Represents Intelligence Linearly HKU &amp; Tencent 2024 - YouTube</A>
								<DT><A HREF="https://arxiv.org/abs/2401.14953">[2401.14953] Learning Universal Predictors</A>
								<DT><A HREF="https://github.com/google-deepmind/neural_networks_solomonoff_induction">google-deepmind/neural_networks_solomonoff_induction: Learning Universal Predictors</A>
								<DT><A HREF="https://github.com/KindXiaoming/pykan">KindXiaoming/pykan: Kolmogorov Arnold Networks</A>
								<DT><A HREF="https://github.com/cedrickchee/awesome-ml-model-compression">cedrickchee/awesome-ml-model-compression: Awesome machine learning model compression research papers, tools, and learning material.</A>
								<DT><A HREF="https://www.artfintel.com/p/papers-ive-read-this-week-713">Artificial Fintelligence | Finbarr Timbers | Substack</A>
								<DT><A HREF="https://github.com/google-deepmind/language_modeling_is_compression">google-deepmind/language_modeling_is_compression</A>
								<DT><A HREF="https://github.com/nadavrot/compressor">nadavrot/compressor: An educational implementation of a modern compressor in Rust</A>
								<DT><A HREF="https://github.com/zipnn/zipnn">zipnn/zipnn: A lossless and near-lossless compression method optimized for numbers/tensors in the Foundation Models environment</A>
								<DT><A HREF="https://github.com/facebook/zstd">facebook/zstd: Zstandard - Fast real-time compression algorithm</A>
								<DT><A HREF="https://www.youtube.com/watch?v=WWz8P-yevJk">Either Transformers Leak Entropy Or They Violate Physics: Which One Do You Think Is True? - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>reasoning</H3>
							<DL><p>
								<DT><H3 FOLDED>In-Context Learning</H3>
								<DL><p>
									<DT><H3 FOLDED>chain-of-thought</H3>
									<DL><p>
										<DT><H3 FOLDED>Test-Time Compute</H3>
										<DL><p>
											<DT><A HREF="https://arxiv.org/html/2408.03314v1">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1834689621956886921">Elephant in the room: O1's ‘Reasoning’ might not be CoT itself as many may think of, but rather CoT is used to represent/monitoring the Reasoning result and as a means for RL.
Might not be able to achieve 'strawberry' by trainin on CoT, which is a misconception.</A>
											<DT><A HREF="https://arxiv.org/abs/2305.20050">[2305.20050] Let's Verify Step by Step</A>
											<DT><A HREF="https://arxiv.org/abs/2112.00114">[2112.00114] Show Your Work: Scratchpads for Intermediate Computation with Language Models</A>
											<DT><A HREF="https://www.youtube.com/watch?v=_Bw5o55SRL8">Inference Time Compute - YouTube</A>
											<DT><A HREF="https://github.com/Dereck0602/Awesome_Test_Time_LLMs">Dereck0602/Awesome_Test_Time_LLMs</A>
											<DT><A HREF="https://x.com/heyyalexwang/status/1882128373515952202">(1) Alex Wang en X: "did you know you've been doing test-time learning this whole time? transformers, SSMs, RNNs, are all test-time regressors but with different design choices we present a unifying framework that derives sequence layers (and higher-order attention👀) from a *single* equation 🧵 https://t.co/BEemnpWeeW" / X</A>
										</DL><p>
										<DT><A HREF="https://arxiv.org/abs/2210.09261">[2210.09261] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them</A>
										<DT><A HREF="https://github.com/FranxYao/chain-of-thought-hub">FranxYao/chain-of-thought-hub: Benchmarking large language models' complex reasoning ability with chain-of-thought prompting</A>
										<DT><A HREF="https://arxiv.org/pdf/2310.07923.pdf">THE EXPRESSIVE POWER OF TRANSFORMERS WITH CHAIN OF THOUGHT</A>
										<DT><A HREF="https://arxiv.org/abs/2310.10845">[2310.10845] CoTFormer: More Tokens With Attention Make Up For Less Depth</A>
										<DT><A HREF="https://arxiv.org/pdf/2311.01460.pdf">IMPLICIT CHAIN OF THOUGHT REASONING VIA KNOWLEDGE DISTILLATION (Microsoft)</A>
										<DT><A HREF="https://twitter.com/ZeyuanAllenZhu/status/1706829354888798296">Physics of Language Models: Part 3.2, knowledge manipulation</A>
										<DT><A HREF="https://github.com/neelnanda-io/CoT-Interp">neelnanda-io/CoT-Interp</A>
										<DT><A HREF="https://www.youtube.com/watch?v=9B_YxazMAY0">Fine, I'll talk about the Meta papers y'all keep sending me - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>icl-continuous</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/daniel_m_cer/status/1528655068010106881/photo/1">(Lester, 2022)SPoT:  SOFT PROMPT</A>
										<DT><A HREF="https://ai.googleblog.com/2022/02/guiding-frozen-language-models-with.html">Google AI Blog: Guiding Frozen Language Models with Learned Soft Prompts</A>
										<DT><A HREF="https://arxiv.org/abs/2110.07904">LESTER SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</A>
										<DT><A HREF="https://arxiv.org/abs/2104.08691">(Lester, 2021) The Power of Scale for Parameter-Efficient Prompt Tuning</A>
										<DT><A HREF="https://arxiv.org/abs/2106.07704">HTP Text Generation with Efficient (Soft) Q-Learning</A>
										<DT><A HREF="https://arxiv.org/abs/2104.05240">Soft-prompt: Factual Probing Is [MASK]: Learning vs. Learning to Recall</A>
										<DT><A HREF="https://arxiv.org/abs/2104.06599">Soft Prompts: Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</A>
										<DT><A HREF="https://arxiv.org/abs/2101.00190">HT: (0.1%params) Prefix-Tuning: Optimizing Continuous Prompts</A>
									</DL><p>
									<DT><H3 FOLDED>Tu Vu (Twiiter &amp; Goolge Research)</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/tuvuumass">Tu Vu (@tuvuumass) / Twitter</A>
										<DT><A HREF="https://twitter.com/sangmichaelxie/status/1554553711241805824">In-Context Learning as Bayesian Inference</A>
										<DT><A HREF="https://twitter.com/quocleix/status/1532072473763532802">Least-to-most prompting: Teach How To Breakdown Complex Problems</A>
										<DT><A HREF="https://twitter.com/_jasonwei/status/1529292177414799361">Self-Supervised Chain Of Thought by introducing "Step by step" reasoning</A>
										<DT><A HREF="https://twitter.com/AkariAsai/status/1528996269280022528">Akari Asai en Twitter: "Introducing 𝗔𝗧𝗧𝗘𝗠𝗣𝗧, a new modular, multi-task, and parameter-efficient approach to combine knowledge from multiple tasks to solve a new task using a small trainable of parameters 🔥 while keeping the original LM *frozen* 🧊 [1/9] Paper 📜: https://t.co/IgJvwCX5tU https://t.co/u26WVGCwsc" / Twitter</A>
										<DT><A HREF="https://twitter.com/tsiprasd/status/1555302289824366592">LLMs can do in-context learning, but are they "learning" new tasks</A>
										<DT><A HREF="https://twitter.com/Fortune_VY/status/1550819534940114946">(Q,A,R): what should be learned to achieve the correct answer (VERIFIERS)</A>
									</DL><p>
									<DT><H3 FOLDED>icl-vision</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2203.05557.pdf">Conditional Prompt Learning for Vision-Language Models</A>
										<DT><A HREF="https://arxiv.org/abs/2109.01134">[2109.01134] Learning to Prompt for Vision-Language Models</A>
										<DT><A HREF="https://arxiv.org/abs/2110.08484">A Good Prompt Is Worth Millions of Parameters</A>
									</DL><p>
									<DT><H3 FOLDED>system prompt</H3>
									<DL><p>
										<DT><A HREF="https://x.com/NorthstarBrain/status/1804823489632723057">claude 3.5 Sonnet: system prompt</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2208.01066">[Universal Approximator] What Can Transformers Learn In-Context?</A>
									<DT><A HREF="https://openreview.net/forum?id=8p3fu56lKc&referrer=%5Bthe%20profile%20of%20Tengyu%20Ma%5D(%2Fprofile%3Fid%3D~Tengyu_Ma1)">One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention | OpenReview</A>
									<DT><A HREF="https://arxiv.org/pdf/2212.07677.pdf">Transformers Learn In-Context by Gradient Descent</A>
									<DT><A HREF="https://arxiv.org/abs/2305.08298">Symbol tuning improves in-context learning in language models</A>
									<DT><A HREF="https://arxiv.org/pdf/2206.07682.pdf">(Wei, 2022) Emergent Abilities of Large Language Models</A>
									<DT><A HREF="https://arxiv.org/pdf/2201.11903.pdf">(Wei, 2022) CHAIN OF THOUGHT</A>
									<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
									<DT><A HREF="https://rylanschaeffer.github.io/blog_posts/2022-01-20-google-brain-flan.html">FLAN: Finetuned Language Models are Zero-Shot Learners</A>
									<DT><A HREF="https://arxiv.org/abs/2401.14953">[2401.14953] Learning Universal Predictors</A>
									<DT><A HREF="https://github.com/google-deepmind/neural_networks_solomonoff_induction">google-deepmind/neural_networks_solomonoff_induction</A>
									<DT><A HREF="https://arxiv.org/html/2311.07772v4#bib.bib13">In-context Learning and Gradient Descent Revisited</A>
									<DT><A HREF="https://paperswithcode.com/paper/finetuned-language-models-are-zero-shot">Finetuned Language Models Are Zero-Shot Learners</A>
									<DT><A HREF="https://openreview.net/pdf?id=NiEtU7blzN">LARGE LANGUAGE MODELS CAN SELF-IMPROVE</A>
									<DT><A HREF="https://www.lesswrong.com/posts/qwqowdhnMreKQvxLv/paper-large-language-models-can-self-improve-linkpost">Large Language Models Can Self-improve (Semi-Supervised Learning)</A>
									<DT><A HREF="https://arxiv.org/pdf/2102.07350.pdf">Prompt Programming for Large Language Models</A>
									<DT><A HREF="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">(OpenAI GPT2, 2019) Language Models are Unsupervised Multitask Learners</A>
									<DT><A HREF="https://arxiv.org/pdf/2107.13586.pdf">MAIN (Liu, 2021) Pre-train, Prompt, and Predict: Systematic Survey</A>
									<DT><A HREF="https://arxiv.org/pdf/2012.15723.pdf">(Gao, 2021) Making Pre-LLM Better Few-shot Learners</A>
									<DT><A HREF="http://pretrain.nlpedia.ai/">PRETRAIN LANGUAGE MODELS</A>
									<DT><A HREF="http://pretrain.nlpedia.ai/data/pdf/basics.pdf">Formalization of Prompting</A>
									<DT><A HREF="https://arxiv.org/abs/2112.00114">(Nye, 2021) SCRATCHPADS for Intermediate Computation</A>
									<DT><A HREF="https://arxiv.org/abs/2110.08207">(Prompt Training, 2021) Multitask Prompted Training Enables Zero-Shot Task Generalization</A>
									<DT><A HREF="https://aclanthology.org/2022.acl-long.60.pdf">(THEORY, 2022) An Information-theoretic Approach</A>
									<DT><A HREF="https://arxiv.org/abs/2103.10385">(Liu, 2021) P-Tuning: GPT Understands, Too</A>
									<DT><A HREF="https://arxiv.org/abs/2104.08786">Overcoming Few-Shot Prompt ORDER SENSITIVITY</A>
									<DT><A HREF="https://arxiv.org/abs/2205.12548">[2205.12548] RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning</A>
									<DT><A HREF="https://www.youtube.com/watch?v=8HwHGGb1zpQ&t=2474s">Prompt-based learning</A>
									<DT><A HREF="https://arxiv.org/abs/2203.06566">PromptChainer: Visual Programming</A>
									<DT><A HREF="https://arxiv.org/abs/2103.08493">HT How Many Data Points is a Prompt Worth?</A>
									<DT><A HREF="https://arxiv.org/abs/2101.06804">(Liu, 2021) What Makes Good In-Context Examples for GPT-3?</A>
									<DT><A HREF="https://github.com/microsoft/semantic-kernel/blob/main/python/README.md">semantic-kernel/README.md at main</A>
									<DT><A HREF="https://arxiv.org/pdf/2303.03846.pdf">LARGER LANGUAGE MODELS DO IN-CONTEXT LEARNING DIFFERENTLY</A>
									<DT><A HREF="https://arxiv.org/abs/2210.09261">Challenging BIG-Bench Tasks and Whether CoT Can Solve Them</A>
									<DT><A HREF="https://twitter.com/_akhaliq/status/1736581357705314731">ReST meets ReAct: Self-Improvement for Multi-Step Reasoning</A>
									<DT><A HREF="https://docs.google.com/document/d/1x3TRnRXz8PCHAWAgb7pzNfgAfBuKPxsyvBVZKXKeBFc/edit#">Experimentation Prompts: Infographics - Google Docs</A>
									<DT><A HREF="https://x.com/davidsamuelcz/status/1800213563124097476">(1) David Samuel en X: "BERTs are not dead!🧟 But just misunderstood and overshadowed by their GPT siblings. In this paper, we travel back into 2020 and speculate on an alternative history where DeBERTa is the first model to show in-context learning abilities. Paper: https://t.co/vTPxC3UtqU 1/6 https://t.co/ipVu3bAQvx" / X</A>
									<DT><A HREF="https://arxiv.org/abs/2406.04823">[2406.04823] BERTs are Generative In-Context Learners</A>
									<DT><A HREF="https://x.com/EricElmoznino/status/1848416471795614076">Lens of Occam's Razon, giving a normative account of next token prediction objectives</A>
									<DT><A HREF="https://www.youtube.com/watch?v=qR56cyMdDXg">Every attention head explained - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>reasoning-latent-space</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2412.06769">Training Large Language Models to Reason in a Continuous Latent Space</A>
									<DT><A HREF="https://x.com/nrehiew_/status/1866488228607582503">(1) wh en X: "This paper from Meta proposes a method to not have the model reason in token space but directly model its reasoning using its hidden state. The authors also do a lot of cool interpretability work in this paper. Aesthetically, I like it alot and its simple to implement https://t.co/QafTwY542x" / X</A>
									<DT><A HREF="https://x.com/nrehiew_/status/1866846473138421930">(1) wh en X: "So my idea is that &amp;gt; We don't want to just naively concat the last hidden state &amp;gt; The latent that we concat back should be dynamic based on the current state &amp;gt; its literally what attention is used for https://t.co/aT5M0Y9HHe" / X</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2311.11045">[2311.11045] Orca 2: Teaching Small Language Models How to Reason</A>
								<DT><A HREF="https://www.youtube.com/watch?v=pA-azVdihQc&t=1890s">What is Machine Learning Good For? - Alex Davies (DeepMind)</A>
								<DT><A HREF="https://huggingface.co/microsoft/Orca-2-13b">microsoft/Orca-2-13b · Hugging Face</A>
								<DT><A HREF="https://www.youtube.com/watch?v=QDkm_BU9Deo">[short] Generative AI for Math: Part I MATHPILE: A Billion-Token-Scale Pretraining Corpus for Math - YouTube</A>
								<DT><A HREF="https://x.com/deepseek_ai/status/1859200141355536422">DeepSeek-R1-Lite-Preview</A>
								<DT><A HREF="https://github.com/ethansmith2000/neurallambda">ethansmith2000/neurallambda: Reasoning Computers. Lambda Calculus, Fully Differentiable. Also Neural Stacks, Queues, Arrays, Lists, Trees, and Latches.</A>
								<DT><A HREF="https://x.com/nrehiew_/status/1866488228607582503">(1) wh en X: "This paper from Meta proposes a method to not have the model reason in token space but directly model its reasoning using its hidden state. The authors also do a lot of cool interpretability work in this paper. Aesthetically, I like it alot and its simple to implement https://t.co/QafTwY542x" / X</A>
								<DT><A HREF="https://www.youtube.com/watch?v=_Bw5o55SRL8">Inference Time Compute - YouTube</A>
								<DT><A HREF="https://docs.google.com/document/d/16SXbrv9-9L5ANnXxQJlFeXy6AQHYSymvmSDJxB4yudc/edit?tab=t.0#heading=h.goep0zm9mhkk">26dec24 - spot check of reasoning systems - Google Docs</A>
								<DT><A HREF="https://www.youtube.com/watch?v=7Gfn4eK8myg">REFT Reasoning with REinforced Fine TuningByteDance 2024 - YouTube</A>
								<DT><A HREF="https://x.com/_xjdr/status/1875295492906828005">(1) xjdr en X: "if you will forgive a bit of anthropomorphizing, to the extent models think, they do not think in tokens. tokens are merely our samplers' best interpretation of the probability distributions that emerge from the actual 'thinking' process. they're an incredibly low bandwidth" / X</A>
								<DT><A HREF="https://x.com/hxiao/status/1886250705415229627">OpenAI's Deep Research</A>
							</DL><p>
							<DT><H3 FOLDED>aligment</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/@AlignmentWorkshop">Alignment Workshop - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-8mKJd2SwI4">Owain Evans - Out-of-context Reasoning in LLMs - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>NEXT-TOKEN PREDICTION</H3>
							<DL><p>
								<DT><H3 FOLDED>multi-token-prediction</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/facebook/multi-token-prediction">facebook/multi-token-prediction · Hugging Face</A>
									<DT><A HREF="https://x.com/syhw/status/1803112949566799902">facebook/multi-token-prediction</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2403.06963.pdf">THE PITFALLS OF NEXT-TOKEN PREDICTION</A>
								<DT><A HREF="https://arxiv.org/abs/2411.00660">[2411.00660] Physics in Next-token Prediction</A>
								<DT><A HREF="https://github.com/gregorbachmann/Next-Token-Failures">gregorbachmann/Next-Token-Failures</A>
								<DT><A HREF="https://www.youtube.com/watch?v=OF8A_b-3q84">The mean and variance of inputs through the layers of a deep narrow neural network - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MFOb9sh9geY">Multi-token Prediction and RemoteCLIP - YouTube</A>
								<DT><A HREF="https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know/">The Many Ways that Digital Minds Can Know – Ryan Moulton's Articles</A>
								<DT><A HREF="https://x.com/alesstolfo/status/1805976764705038708">LLMs don’t just output the next token, they also output confidence. How is this computed?</A>
							</DL><p>
							<DT><H3 FOLDED>emergence</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://en.wikipedia.org/wiki/Kolmogorov_complexity#Compression">Kolmogorov complexity - Wikipedia</A>
							<DT><A HREF="https://arxiv.org/pdf/1911.12543.pdf">How Can We Know What Language Models Know?</A>
							<DT><A HREF="https://openai.com/research/unsupervised-sentiment-neuron">Unsupervised sentiment neuron</A>
							<DT><A HREF="https://www.youtube.com/watch?v=KV5gbOmHbjU">A Mathematical Framework for Transformer Circuits</A>
							<DT><A HREF="https://towardsdatascience.com/visualizing-word-embedding-with-pca-and-t-sne-961a692509f5">Visualizing Word Embedding with PCA and t-SNE</A>
							<DT><A HREF="https://arxiv.org/pdf/2105.12202.pdf">LM Transformer based: Context Sensitive Models (Attention Mechanism)</A>
							<DT><A HREF="https://twitter.com/wesg52/status/1653750337373880322">What features are neurons in LLMs actualy extracting?</A>
							<DT><A HREF="https://www.youtube.com/watch?v=t5LjgczaS80&t=741s">Gail Weiss: Thinking Like Transformers</A>
							<DT><A HREF="https://arxiv.org/pdf/2211.03495.pdf">How Much Does Attention Actually Attend?</A>
							<DT><A HREF="https://arxiv.org/abs/2310.10348">[2310.10348] Attribution Patching Outperforms Automated Circuit Discovery</A>
							<DT><A HREF="https://twitter.com/aaquib_syed1/status/1714386165237776653">ttribution Patching Outperforms Automated Circuit Discovery (Thread)</A>
							<DT><A HREF="https://papers.labml.ai/paper/7719e8e6c2cf11edb95839eec3084ddd">Eliciting Latent Predictions from Transformers with the Tuned Lens</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Ohl5AGUOLXk">Quantifying and Understanding Memorization in DNN</A>
							<DT><A HREF="https://twitter.com/val_kharvd/status/1655397456652140545">Transformerlens</A>
							<DT><A HREF="https://www.youtube.com/watch?v=VY7SCl_DFho">Interpretable vs Explainable Machine Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2301.04589?utm_source=substack&utm_medium=email">Memory Augmented Large Language Models are Computationally Universal</A>
							<DT><A HREF="https://arxiv.org/abs/2212.08073">[2212.08073] Constitutional AI: Harmlessness from AI Feedback (Antrophic)</A>
							<DT><A HREF="https://bbycroft.net/llm">LLM Visualization</A>
							<DT><A HREF="https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling">Pythia: Analyzing Large Language Models Across Training and Scaling</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M74WKIh0ciI&list=LL&index=24">Three weight matrices in a neural network growing connections as two parallel networks are merged. - YouTube</A>
							<DT><A HREF="https://www.youtube.com/@AlignmentWorkshop">Alignment Workshop - YouTube</A>
							<DT><A HREF="https://twitter.com/IasonGabriel/status/1781262674711466483">The Ethics of LLMS</A>
							<DT><A HREF="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/ethics-of-advanced-ai-assistants/the-ethics-of-advanced-ai-assistants-2024-i.pdf">The Ethics of Advanced AI Assistants</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Z1bXBinTtnQ">LLM - Reasoning SOLVED (new research) - YouTube</A>
							<DT><A HREF="https://x.com/franklyn_wang/status/1858527547484434515">(1) Franklyn Wang en X: "Doubling o1-preview performance on ARC-AGI with one simple trick 🚀 tldr: by providing human-like representations to o1, we are able to substantially increase performance on @arcprize. https://t.co/VIRMoBENjA" / X</A>
							<DT><A HREF="https://arxiv.org/abs/2412.05117">[2412.05117] Transformers Can Navigate Mazes With Multi-Step Prediction</A>
						</DL><p>
						<DT><H3 FOLDED>Physics of Language Models</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2305.13673">[2305.13673] Physics of Language Models: Part 1, Learning Hierarchical Language Structures</A>
							<DT><A HREF="https://physics.allen-zhu.com/part-2-grade-school-math/part-2-1">Physics of Language Models - Part 2.1, Hidden Reasoning Process</A>
							<DT><A HREF="https://physics.allen-zhu.com/part-3-knowledge/part-3-3">Physics of Language Models - Part 3.3: Scaling Laws</A>
							<DT><A HREF="https://www.youtube.com/watch?v=bpp6Dz8N2zY">Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process - YouTube</A>
							<DT><A HREF="https://x.com/i/bookmarks?post_id=1818493152711569551">(1/7) Physics of LM, Part 2.1 with 8 results for LLM reasoning is out: http://arxiv.org/abs/2407.20311. Probing reveals that LLMs secretly develop some "level-2" reasoning skill beyond Humans.</A>
							<DT><A HREF="https://x.com/ZeyuanAllenZhu/status/1887807438570877330">(1) Zeyuan Allen-Zhu, Sc.D. en X: "(1/8) We classify L1~L5 intelligence, and observe only Gemini-2-FT, DeepSeek-R1, OpenAI-o1 can reach L2; most are only L1 (o3-mini). Yet, one can still use L1-level AIs to arbitrate disputes and ensure fair review of L4-level scientific papers. (older tweet reached 70K views)⬇️ https://t.co/TWIs8NeUEU" / X</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-training</H3>
						<DL><p>
							<DT><H3 FOLDED>language-models-training-data</H3>
							<DL><p>
								<DT><H3 FOLDED>data</H3>
								<DL><p>
									<DT><A HREF="https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only">There Are No New Ideas in AI... Only New Datasets</A>
								</DL><p>
								<DT><H3 FOLDED>data-curation</H3>
								<DL><p>
									<DT><H3 FOLDED>FineWeb-2</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/datasets/HuggingFaceFW/fineweb-2">HuggingFaceFW/fineweb-2 · Datasets at Hugging Face</A>
									</DL><p>
									<DT><H3 FOLDED>data-cleaning</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/lilac_ai">Lilac: Analyze, structure, and clean unstructured data with AI</A>
										<DT><A HREF="https://lilacml.com/blog/introducing-lilac.html">Introducing Lilac - 🌸 Lilac</A>
										<DT><A HREF="https://github.com/ChenghaoMou/text-dedup">ChenghaoMou/text-dedup: All-in-one text de-duplication</A>
										<DT><A HREF="https://publish.obsidian.md/chenghao/posts/20230220150602">Large-scale Near-deduplication Behind BigCode</A>
										<DT><A HREF="https://lmsys.org/blog/2023-11-14-llm-decontaminator/">LLM-based decontaminator</A>
									</DL><p>
									<DT><H3 FOLDED>text-dedup</H3>
									<DL><p>
										<DT><H3 FOLDED>bloom-filter</H3>
										<DL><p>
											<DT><A HREF="https://github.com/allenai/bff">allenai/bff</A>
										</DL><p>
										<DT><A HREF="https://github.com/ChenghaoMou/text-dedup">ChenghaoMou/text-dedup: All-in-one text de-duplication</A>
									</DL><p>
									<DT><H3 FOLDED>dataset-distillation</H3>
									<DL><p>
										<DT><A HREF="https://snats.xyz/pages/articles/breaking_some_laws.html">snats website</A>
										<DT><A HREF="https://snats.xyz/pages/articles/breaking_some_laws.html">I want to break some laws too</A>
										<DT><A HREF="https://github.com/Distill-Generalization-Group/Distill-Generalization">Distill-Generalization-Group/Distill-Generalization</A>
									</DL><p>
									<DT><H3 FOLDED>JEST</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2406.17711">[2406.17711] Data curation via joint example selection further accelerates multimodal learning</A>
										<DT><A HREF="https://arxiv.org/html/2405.15613v2">Automatic Data Curation for Self-Supervised Learning: A Clustering-Based Approach</A>
									</DL><p>
									<DT><A HREF="https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf">GPT-3:Details of Common Crawl Filtering</A>
									<DT><A HREF="https://github.com/cloneofsimo/imgdataset_process">cloneofsimo/imgdataset_process</A>
									<DT><A HREF="https://developer.nvidia.com/blog/gpu-accelerated-json-data-processing-with-rapids/?ncid=so-link-395145-vt27#cid=an01_so-link_en-us">GPU-Accelerated JSON Data Processing with RAPIDS</A>
									<DT><A HREF="https://arxiv.org/pdf/2310.20707.pdf">WHAT’S IN MY BIG DATA? (Allen Institute for AI)</A>
									<DT><A HREF="https://lilacml.com/blog/introducing-lilac.html">Introducing Lilac - 🌸 Lilac</A>
									<DT><A HREF="https://www.ibm.com/downloads/cas/X9W4O6BM">Granite Foundation Models</A>
									<DT><A HREF="https://beam.apache.org/">Apache Beam®: Batch and streaming data processing</A>
									<DT><A HREF="https://github.com/google/seqio">google/seqio: Task-based datasets, preprocessing, and evaluation for sequence models.</A>
									<DT><A HREF="https://github.com/nikitakit/sabertooth/blob/80ab40a6bd50f4e012e0e5489a5d9b8315bc6758/rust/create_pretraining_data/src/main.rs#L4">sabertooth/rust/create_pretraining_data/src/main.rs at 80ab40a6bd50f4e012e0e5489a5d9b8315bc6758 · nikitakit/sabertooth</A>
									<DT><A HREF="https://voltrondata.com/benchmarks/theseus">Benchmarking Report: Theseus Engine</A>
									<DT><A HREF="https://github.com/NVIDIA/spark-rapids">NVIDIA/spark-rapids: Spark RAPIDS plugin - accelerate Apache Spark with GPUs</A>
									<DT><A HREF="https://www.youtube.com/watch?v=crRbIgNqetg">Lightning Talk: Tensor Query Processing - Matteo Interlandi, Microsoft - YouTube</A>
									<DT><A HREF="https://laion.ai/blog/laion-pop/">LAION POP: 600,000 high-resolution images with detailed descriptions | LAION</A>
									<DT><A HREF="https://openreview.net/forum?id=UmvSlP-PyV">Beyond neural scaling laws: beating power law scaling via data pruning | OpenReview</A>
									<DT><A HREF="https://github.com/microsoft/markitdown">microsoft/markitdown: Python tool for converting files and office documents to Markdown.</A>
									<DT><A HREF="https://www.youtube.com/watch?v=dzB0HMtMIfI">Master Real-time Data Pipelines with Kafka and Flink - 3 hr Course - DataExpert.io Free Boot Camp - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=nt38LZhk7jQ">Maintain Data Pipelines Like Netflix and Airbnb - DataExpert.io Free Bootcamp - Week 6 - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=ggQQXGXDmS8">Data Structure Is All You Need: Outperform Deepseek For $30 - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>Datasets</H3>
								<DL><p>
									<DT><H3 FOLDED>datasets-google-research</H3>
									<DL><p>
										<DT><A HREF="https://github.com/google-research-datasets">Google Research Datasets</A>
										<DT><A HREF="https://github.com/google-research-datasets/natural-questions">google-research-datasets/natural-questions: Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems.</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-common-crawls</H3>
									<DL><p>
										<DT><H3 FOLDED>FineWeb</H3>
										<DL><p>
											<DT><A HREF="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb: decanting the web for the finest text data at scale - a Hugging Face Space by HuggingFaceFW</A>
											<DT><A HREF="https://huggingface.co/datasets/HuggingFaceFW/fineweb">HuggingFaceFW/fineweb · Datasets at Hugging Face</A>
											<DT><A HREF="https://x.com/LoubnaBenAllal1/status/1797175938972606975">(1) Loubna Ben Allal en X: "🍷 FineWeb technical report is out and so is 📚 FineWeb-Edu, a 1.3 trillion tokens dataset that outperforms all other open web datasets, with remarkable improvements on educational benchmarks such as MMLU, ARC, and OpenBookQA. Technical report: https://t.co/lfOZYYJKxq Dataset: https://t.co/urC5qjmx3v" / X</A>
											<DT><A HREF="https://x.com/drjwrae/status/1797286202825232497">(1) Jack Rae en X: "Some really nice data research + artifacts coming out from HuggingFace these days. Creating the definitive open source dataset has probably more value than most open-sourced models at this stage." / X</A>
											<DT><A HREF="https://x.com/gui_penedo/status/1797173053123916036">(1) Guilherme Penedo en X: "We are (finally) releasing the 🍷 FineWeb technical report! In it, we detail and explain every processing decision we took, and we also introduce our newest dataset: 📚 FineWeb-Edu, a (web only) subset of FW filtered for high educational content. Link: https://t.co/MRsc8Q5K9q https://t.co/HVfFnKbeso" / X</A>
											<DT><A HREF="https://x.com/jd_pressman/status/1797396190210015716/photo/1">The 4 types of synthetic data</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1796901742715498604">The only thing we know is "make it as diverse as possible"..</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1797320347999670368">CommonCrawl over time</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1797313173449764933">FineWeb-Edu llm.c</A>
											<DT><A HREF="https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py">datatrove/examples/fineweb.py</A>
										</DL><p>
										<DT><H3 FOLDED>DataComp</H3>
										<DL><p>
											<DT><A HREF="https://www.datacomp.ai/dclm/">DataComp</A>
											<DT><A HREF="https://arxiv.org/abs/2406.11794">[2406.11794] DataComp-LM: In search of the next generation of training sets for language models</A>
											<DT><A HREF="https://x.com/TsingYoga/status/1804728355239199181">It's Time to Scale Down the Data</A>
										</DL><p>
										<DT><A HREF="https://commoncrawl.org/">Common Crawl - Open Repository of Web Crawl Data</A>
										<DT><A HREF="https://huggingface.co/datasets/HuggingFaceFW/fineweb">HuggingFaceFW/fineweb · Datasets at Hugging Face</A>
										<DT><A HREF="https://huggingface.co/datasets/THUDM/LongBench">THUDM/LongBench</A>
										<DT><A HREF="https://huggingface.co/datasets/c4">c4</A>
										<DT><A HREF="https://arxiv.org/abs/2309.04662">[2309.04662] MADLAD-400: A Multilingual And Document-Level Large Audited Dataset</A>
										<DT><A HREF="https://huggingface.co/datasets/tiiuae/falcon-refinedweb">tiiuae/falcon-refinedweb</A>
										<DT><A HREF="https://twitter.com/_philschmid/status/1714280568853303720">IBM's Granite LLMs</A>
										<DT><A HREF="https://commoncrawl.org/get-started">Common Crawl - Accessing the Data: AWS S3 buckets</A>
										<DT><A HREF="https://github.com/sophiawisdom/data">sophiawisdom/data</A>
									</DL><p>
									<DT><H3 FOLDED>LAION</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/laion_ai/status/1714883417383346493">LAION: 608 B chess moves, 236 Rubik's Cube moves, 39 B A* moves in ASCII Mazes</A>
										<DT><A HREF="https://laion.ai/blog/laion-pop/">LAION POP: 600,000 high-resolution images with detailed descriptions | LAION</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-math</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=QDkm_BU9Deo">Generative AI for Math: Part I MATHPILE: A Billion-Token-Scale Pretraining Corpus for Math</A>
										<DT><A HREF="https://gair-nlp.github.io/MathPile/">Generative AI for Math: Part I MATHPILE: A Billion-Token-Scale Pretraining Corpus for Math</A>
										<DT><A HREF="https://arxiv.org/abs/2310.06786">OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text</A>
										<DT><A HREF="https://huggingface.co/datasets/open-web-math/open-web-math">open-web-math/open-web-math</A>
										<DT><A HREF="https://huggingface.co/datasets/math-ai/StackMathQA">math-ai/StackMathQA</A>
										<DT><A HREF="https://huggingface.co/datasets/math-eval/TAL-SCQ5K">math-eval/TAL-SCQ5K</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-dolma</H3>
									<DL><p>
										<DT><A HREF="https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64">AI2 Dolma: 3 Trillion Token Open Corpus for LLMs | AI2 Blog</A>
										<DT><A HREF="https://drive.google.com/file/d/12gOf5I5RytsD159nSP7iim_5zN31FCXq/view">dolma-datasheet</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-madlad-400</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2309.04662">[2309.04662] MADLAD-400: A Multilingual And Document-Level Large Audited Dataset</A>
										<DT><A HREF="https://huggingface.co/datasets/allenai/MADLAD-400">allenai/MADLAD-400</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-orca</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/datasets/Open-Orca/OpenOrca">Open-Orca/OpenOrca</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-the-stack</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/datasets/bigcode/the-stack">bigcode/the-stack</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-the-pile</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/datasets/JeanKaddour/minipile">JeanKaddour/minipile</A>
									</DL><p>
									<DT><H3 FOLDED>datasets-videos</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/papers/2402.19479">Paper page - Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</A>
									</DL><p>
									<DT><A HREF="https://jingfengyang.github.io/gpt">Why did all of the public reproduction of GPT-3 fail?</A>
									<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1693951301942759884">mixture dataset</A>
									<DT><A HREF="https://annas-blog.org/duxiu-exclusive.html">Anna's Blog: Exclusive access for LLM companies to largest Chinese non-fiction book collection in the world - Anna’s Blog</A>
									<DT><A HREF="http://www.gharchive.org/">GH Archive</A>
									<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1745900747810619436">Datasets that I really like</A>
									<DT><A HREF="https://huggingface.co/datasets/ropes">ropes (reasoning)</A>
									<DT><A HREF="https://huggingface.co/datasets/euirim/goodwiki">euirim/goodwiki</A>
									<DT><A HREF="https://huggingface.co/datasets/deepmind/code_contests">deepmind/code_contests</A>
									<DT><A HREF="https://allenai.org/data/entailmentbank">EntailmentBank Dataset — Allen Institute for AI</A>
									<DT><A HREF="https://huggingface.co/datasets/gsm8k">gsm8k · Datasets at Hugging Face</A>
									<DT><A HREF="https://www.dataprovenance.org/data-provenance-explorer/dataset-explorer">Data Provenance Initiative</A>
									<DT><A HREF="https://datasetsearch.research.google.com/">Dataset Search</A>
								</DL><p>
								<DT><H3 FOLDED>Crawler</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=leCYxw0Qv1E">Scalable Extraction of Training Data from (Production) LLMs</A>
									<DT><A HREF="https://arxiv.org/abs/2309.04662">[2309.04662] MADLAD-400: A Multilingual And Document-Level Large Audited Dataset</A>
									<DT><A HREF="https://github.com/flairNLP/fundus">flairNLP/fundus: A very simple news crawler with a funny name</A>
								</DL><p>
								<DT><H3 FOLDED>sample-quality</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2309.05463.pdf">Microsoft: Textbooks Are All You Need II: phi-1.5 technical report (phi-2)</A>
									<DT><A HREF="https://twitter.com/jerome_massot/status/1558122719698309126?s=20&t=ya4vHLuGuS3E88MMZhewsw">Dataset selection value</A>
									<DT><A HREF="https://twitter.com/ShayneRedford/status/1660670374206652419">When and where does pretraining (PT) data matter?</A>
									<DT><A HREF="https://twitter.com/jerome_massot/status/1558122719698309126?s=20&t=ya4vHLuGuS3E88MMZhewsw">Data-Centric AI</A>
									<DT><A HREF="https://github.com/shayne-longpre/a-pretrainers-guide/blob/main/A%20Pretrainer's%20Guide%20To%20Training%20Data.pdf">A Pretrainer's Guide To Training Data</A>
									<DT><A HREF="https://twitter.com/sbmaruf/status/1664965734831738881">Systematic Study of ChatGPT on Benchmarks Datasets</A>
								</DL><p>
								<DT><H3 FOLDED>training-data-document-processing</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/UDOP">microsoft/UDOP: Unifying Vision, Text, and Layout for Universal Document Processing</A>
									<DT><A HREF="https://twitter.com/_akhaliq/status/1742369195034099731">A layout-aware generative language model for multimodal document understanding (JPMorgan)</A>
									<DT><A HREF="https://twitter.com/mezaoptimizer/status/1725389571817378142">GPT-4V OCR problem of math -&gt; LaTeX</A>
								</DL><p>
								<DT><H3 FOLDED>training-data-synthetic-data</H3>
								<DL><p>
									<DT><H3 FOLDED>combine human and AI to annotate at super-human level!</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://twitter.com/home">Synthetic Data from Diffusion Models Improves ImageNet</A>
									<DT><A HREF="https://arxiv.org/pdf/2312.06585.pdf">Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</A>
									<DT><A HREF="https://arxiv.org/abs/2302.04761">[2302.04761] Toolformer: Language Models Can Teach Themselves to Use Tools</A>
									<DT><A HREF="https://twitter.com/andrew_n_carr/status/1709754581184643089">Meta SAM: Progressive labeling with weak-&gt;strong models effectiveness</A>
									<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1709760111156093201">Computer vision papers data process</A>
									<DT><A HREF="https://arxiv.org/abs/2304.02643">[2304.02643] Segment Anything (SAM)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=zpqBiNV9XWE">A Tale of Tails: Model Collapse as a Change of Scaling Laws - YouTube</A>
									<DT><A HREF="https://arxiv.org/abs//2402.07043">[2402.07043] A Tale of Tails: Model Collapse as a Change of Scaling Laws</A>
									<DT><A HREF="https://huggingface.co/blog/cosmopedia">Cosmopedia: how to create large-scale synthetic data for pre-training Large Language Models</A>
									<DT><A HREF="https://www.youtube.com/watch?v=kgqDtfC_pRY">Synthetic data from scratch - Clip from livestream 7/May/2024 - Claude 3 Opus, phi-1, phi-3, ChatGPT - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=HJb_GsX1Bhs">Synthetic data that finally helps instead of hurts!! - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>datasets-stack</H3>
								<DL><p>
									<DT><H3 FOLDED>hf-datasets</H3>
									<DL><p>
										<DT><A HREF="https://x.com/vwxyzjn/status/1818706123375231463">(1) Costa Huang en X: "Releasing `costa_utils` to help better visualize HF datasets. No more squinting eyes at narrowly formatted texts! ``` pip install costa_utils python -m costa_utils.hf_viz \ --sft AI-MO/NuminaMath-TIR \ --split train \ --sft_messages_column_name messages \ https://t.co/ySJ0xTV8AH" / X</A>
									</DL><p>
									<DT><A HREF="https://github.com/allenai/unified-io-2">allenai/unified-io-2</A>
									<DT><A HREF="https://til.simonwillison.net/duckdb/remote-parquet">Summing columns in remote Parquet files using DuckDB</A>
									<DT><A HREF="https://medium.com/yandex/ytsaurus-exabyte-scale-storage-and-processing-system-is-now-open-source-42e7f5fa5fc6">YTsaurus: Exabyte-Scale Storage and Processing System</A>
									<DT><A HREF="https://github.com/google/seqio">google/seqio: Task-based datasets, preprocessing, and evaluation for sequence models.</A>
									<DT><A HREF="https://lilacml.com/blog/introducing-lilac.html">Introducing Lilac - 🌸 Lilac</A>
								</DL><p>
								<DT><H3 FOLDED>compressor</H3>
								<DL><p>
									<DT><A HREF="https://github.com/nadavrot/compressor">nadavrot/compressor: An educational implementation of a modern compressor in Rust</A>
									<DT><A HREF="https://github.com/cloneofsimo/imagenet.int8">cloneofsimo/imagenet.int8</A>
								</DL><p>
								<DT><H3 FOLDED>datasets-labeless</H3>
								<DL><p>
									<DT><A HREF="https://www.databricks.com/blog/tao-using-test-time-compute-train-efficient-llms-without-labeled-data">TAO: Using test-time compute to train efficient LLMs without labeled data | Databricks Blog</A>
								</DL><p>
								<DT><A HREF="https://docs.ffcv.io/writing_datasets.html">Writing a dataset to FFCV format — FFCV documentation</A>
								<DT><A HREF="https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1">FineWeb: decanting the web for the finest text data at scale - a Hugging Face Space by HuggingFaceFW</A>
								<DT><A HREF="https://arxiv.org/abs/2406.11794v1">[2406.11794v1] DataComp-LM: In search of the next generation of training sets for language models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=HYvhj9W2qHQ">Cloud Native Data Loaders for Machine Learning Using Zarr and Xarray - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=W73Sp7XKuVE">Luca Soldaini - Curating Pretrain Data (AI2 / Dolma) - YouTube</A>
								<DT><A HREF="https://x.com/ZackAnkner/status/1797595682439901565">"New paper where we explore using a small LM’s perplexity to prune the pretraining data for larger LMs. We find that small LMs can prune data for up to 30x larger LMs, data pruning works in the overtrained and data-constrained regimes, and more! https://t.co/XYbI0Ijois" / X</A>
								<DT><A HREF="https://arxiv.org/abs/2405.20541">[2405.20541] Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models</A>
								<DT><A HREF="https://x.com/robertnishihara/status/1798759330868760680">(Robert Nishihara) Ray Server: Dataset preparation</A>
								<DT><A HREF="https://x.com/iamtrask/status/1797528618429468966">(1) Andrew Trask en X: "fantastic work by @huggingface! this stuff really moves the field forward. in 2015 I was listening to Thomas Mikolov talk about word2vec... and he didn't explain it as an algorithmic innovation... he described it as something like ~"i wanted to simplify the architecture so i" / X</A>
								<DT><A HREF="https://x.com/Vaishaal/status/1803198064364232918">(1) Vaishaal Shankar en X: "I am really excited to introduce DataComp for Language Models (DCLM), our new testbed for controlled dataset experiments aimed at improving language models. 1/x https://t.co/uNe5mUJJxb" / X</A>
								<DT><A HREF="https://www.dataprovenance.org/">Data Provenance Initiative</A>
								<DT><A HREF="https://x.com/Dorialexander/status/1864692907506323606">(1) Alexander Doria en X: "“They said it could not be done”. We’re releasing Pleias 1.0, the first suite of models trained on open data (either permissibly licensed or uncopyrighted): Pleias-3b, Pleias-1b and Pleias-350m, all based on the two trillion tokens set from Common Corpus. https://t.co/wWN8LRccsC" / X</A>
								<DT><A HREF="https://x.com/iScienceLuvr/status/1879743492450037889">Towards Best Practices for Open Datasets for LLM Training</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-evaluation</H3>
							<DL><p>
								<DT><H3 FOLDED>TEST SET CONTAMINATION</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2310.17623.pdf">PROVING TEST SET CONTAMINATION IN BLACK BOX LANGUAGE MODELS</A>
									<DT><A HREF="https://twitter.com/suchenzang/status/1701615026648605095">Susan Zhang: I think Phi-1.5 trained on the benchmarks. Particularly, GSM8K.</A>
								</DL><p>
								<DT><H3 FOLDED>lm-evaluation-harness</H3>
								<DL><p>
									<DT><A HREF="https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file">EleutherAI/lm-evaluation-harness: A framework for few-shot evaluation of language models.</A>
									<DT><A HREF="https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md">lm-evaluation-harness/docs/interface.md at main · EleutherAI/lm-evaluation-harness</A>
									<DT><A HREF="https://imbue.com/research/70b-evals/">Ensuring accurate model evaluations: open-sourced, cleaned datasets for models that reason and code - imbue</A>
									<DT><A HREF="https://x.com/Thom_Wolf/status/1805985020168769988">(1) Thomas Wolf en X: "Very excited to release the new version of the Open LLM Leaderboard, v2 – it's much harder than the previous version as you can see on some of the v1 &amp;lt;&amp;gt; v2 scores comparison I'm posting below Updated: As open models keeps getting better and saturating some of the evaluations it https://t.co/zv6dSQCnhJ" / X</A>
									<DT><A HREF="https://www.databricks.com/blog/calibrating-mosaic-evaluation-gauntlet">Calibrating the Mosaic Evaluation Gauntlet | Databricks Blog</A>
									<DT><A HREF="https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#tensor--data-parallel-and-fast-offline-batching-inference-with-sglang">EleutherAI/lm-evaluation-harness: A framework for few-shot evaluation of language models.</A>
								</DL><p>
								<DT><H3 FOLDED>intrinsic evaluation</H3>
								<DL><p>
									<DT><H3 FOLDED>perplexity</H3>
									<DL><p>
										<DT><A HREF="https://cs.stanford.edu/people/karpathy/tsnejs/csvdemo.html">Perplexity</A>
										<DT><A HREF="https://sjmielke.com/comparing-perplexities.htm">Can you compare perplexity across different segmentations?</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>language-models-evals</H3>
								<DL><p>
									<DT><A HREF="https://www.jasonwei.net/blog/evals">Successful language model evals — Jason Wei</A>
									<DT><A HREF="https://arxiv.org/pdf/2311.12022.pdf">GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark (Anthropic)</A>
									<DT><A HREF="https://github.com/Sanger2000/human-eval/blob/master/run.py">HumanEval: pass@K</A>
									<DT><A HREF="https://arxiv.org/abs/2311.07911">[2311.07911] Instruction-Following Evaluation for Large Language Models</A>
									<DT><A HREF="https://github.com/OpenBMB/UltraEval">OpenBMB/UltraEval: An open source framework for evaluating foundation models.</A>
									<DT><A HREF="https://arxiv.org/abs/2311.12983">[2311.12983] GAIA: a benchmark for General AI Assistants</A>
								</DL><p>
								<DT><A HREF="https://github.com/EleutherAI/lm-evaluation-harness">EleutherAI/lm-evaluation-harness: A framework for few-shot evaluation of language models.</A>
								<DT><A HREF="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">Evaluation Metrics for Language Modeling</A>
								<DT><A HREF="https://www.jasonwei.net/blog/evals">Successful language model evals — Jason Wei</A>
								<DT><A HREF="https://x.com/haileysch__/status/1793957143910953185">formalizing what is done evaluating models with loglikelihood multiple choice and perplexity evals</A>
								<DT><A HREF="https://twitter.com/AIatMeta/status/1715041427283902793">Meta: GenBench (Generalizatio testing)</A>
								<DT><A HREF="https://arxiv.org/abs/2311.07689?utm_source=twitter&utm_medium=organic_social&utm_campaign=research&utm_content=image">[2311.07689] MART: Improving LLM Safety with Multi-round Automatic Red-Teaming</A>
								<DT><A HREF="https://twitter.com/huihan_li/status/1724334480339710287">LINK: framework for systematically generating data in the long-tail distribution</A>
								<DT><A HREF="https://cdn.openai.com/openai-preparedness-framework-beta.pdf">OpenAI: Preparedness Framework (Beta)</A>
								<DT><A HREF="https://www.lesswrong.com/posts/hQPfLsDKWtdvMwyyr/on-openai-s-preparedness-framework">On OpenAI’s Preparedness Framework — LessWrong</A>
								<DT><A HREF="https://cdn.openai.com/papers/weak-to-strong-generalization.pdf">WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION</A>
								<DT><A HREF="https://arxiv.org/pdf/2312.11444.pdf">An In-depth Look at Gemini's Language Abilities</A>
								<DT><A HREF="https://crfm.stanford.edu/helm/v0.2.2/?group=core_scenarios">Holistic Evaluation of Language Models (HELM)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=2CIIQ5KZWUM">Evaluating LLM-based Applications - YouTube</A>
								<DT><A HREF="https://arxiv.org/abs/2311.15930">[2311.15930] WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models</A>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1727111453117997185">Claude: Decline to answer</A>
								<DT><A HREF="https://github.com/meta-llama/llama3/blob/main/eval_details.md">llama3/eval_details.md at main · meta-llama/llama3</A>
								<DT><A HREF="https://github.com/openai/simple-evals">openai/simple-evals</A>
								<DT><A HREF="https://twitter.com/gallabytes/status/1784694561861947823">ways of evaluating a langauge encoder</A>
								<DT><A HREF="https://twitter.com/srush_nlp/status/1785293229165818298">LMSys: Chatbot Arena</A>
								<DT><A HREF="https://arxiv.org/abs/2405.14782">[2405.14782] Lessons from the Trenches on Reproducible Evaluation of Language Models</A>
								<DT><A HREF="https://huggingface.co/spaces/open-llm-leaderboard/blog">Open-LLM performances are plateauing, let’s make the leaderboard steep again - a Hugging Face Space by open-llm-leaderboard</A>
								<DT><A HREF="https://www.youtube.com/watch?v=3gb-ZkVRemQ&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=27">Stanford CS25: V4 I Jason Wei &amp; Hyung Won Chung of OpenAI - YouTube</A>
								<DT><A HREF="https://docs.google.com/presentation/d/1JKpqsbkr5Fg-bj1iElPaC-ToTVpRmRLKZmN89krwl04/edit?resourcekey=0-VPgp_Yc4krPPW3Mxv6UjgQ#slide=id.g25dfdd1f5b2_0_8">2024 stanford cs25 guest lecture jason wei - Google Slides</A>
								<DT><A HREF="https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/EVAL_GAUNTLET.md">llm-foundry/scripts/eval/local_data/EVAL_GAUNTLET.md (main)</A>
								<DT><A HREF="https://www.anthropic.com/research/statistical-approach-to-model-evals">A statistical approach to model evaluations \ Anthropic</A>
								<DT><A HREF="https://x.com/_jasonwei/status/1876701085601341662">(1) Jason Wei en X: "Nice paper from Deepmind takes a fresh angle on factuality: https://t.co/3MDQlmNU9W While most existing factuality datasets focus on public world knowledge, this paper evaluates whether responses are consistent with a provided document as context. This is an elegant and" / X</A>
							</DL><p>
							<DT><H3 FOLDED>tokenizer</H3>
							<DL><p>
								<DT><H3 FOLDED>BPE</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/docs/transformers/tokenizer_summary">Byte-Pair Encoding (BPE)</A>
									<DT><A HREF="https://github.com/youkaichao/fast_bpe_tokenizer">youkaichao/fast_bpe_tokenizer: fast bpe tokenizer, simple to understand, easy to use</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Byte_pair_encoding">Byte pair encoding - Wikipedia</A>
								</DL><p>
								<DT><H3 FOLDED>tiktokenizer</H3>
								<DL><p>
									<DT><A HREF="https://tiktokenizer.vercel.app/">Tiktokenizer</A>
									<DT><A HREF="https://github.com/openai/tiktoken">openai/tiktoken: tiktoken is a fast BPE tokeniser for use with OpenAI's models.</A>
								</DL><p>
								<DT><H3 FOLDED>sentencepiece</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=2QO3vzwHXhg&t=3551s">sentencepiece_model pb ParseFromString deserialize tokenizer vocab dict</A>
									<DT><A HREF="https://github.com/google/sentencepiece">google/sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation.</A>
									<DT><A HREF="https://github.com/google/sentencepiece/blob/master/python/add_new_vocab.ipynb">sentencepiece/python/add_new_vocab.ipynb</A>
								</DL><p>
								<DT><H3 FOLDED>tokenizers-rust</H3>
								<DL><p>
									<DT><A HREF="https://github.com/guillaume-be/rust-tokenizers">guillaume-be/rust-tokenizers: Rust-tokenizer offers high-performance tokenizers for modern language models, including WordPiece, Byte-Pair Encoding (BPE) and Unigram (SentencePiece) models</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=zduSFxRajkE">Let's build the GPT Tokenizer</A>
								<DT><A HREF="https://arxiv.org/abs/2310.05737">Language Model Beats Diffusion -- Tokenizer is Key to Visual Gen</A>
								<DT><A HREF="https://gist.github.com/Winston-503/39f7bc5fe1d93dcfa38361b119fbe2d7">json_yaml_tokens.py</A>
								<DT><A HREF="https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html">Exploring BERT's Vocabulary</A>
								<DT><A HREF="https://www.youtube.com/watch?v=nu9lBnyRjos">SpaceByte: Deleting Tokenization from Large Language Modeling - YouTube</A>
								<DT><A HREF="https://www.reedbeta.com/blog/programmers-intro-to-unicode/">A Programmer’s Introduction to Unicode – Nathan Reed’s coding blog</A>
							</DL><p>
							<DT><H3 FOLDED>context-window</H3>
							<DL><p>
								<DT><H3 FOLDED>Linear Transformers</H3>
								<DL><p>
									<DT><A HREF="https://openai.com/research/requests-for-research-2">Transformers with Linear Attention</A>
								</DL><p>
								<DT><H3 FOLDED>KV cache</H3>
								<DL><p>
									<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8">LLM Inference Series: 4. KV caching, a deeper look | by Pierre Lienhart (main)</A>
									<DT><A HREF="https://arxiv.org/pdf/2305.05920">total number of bytes to store the key-value cache</A>
									<DT><A HREF="https://arxiv.org/abs/2403.05527">[2403.05527] GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM</A>
									<DT><A HREF="https://openreview.net/pdf?id=e9D2STGwLJ">Adaptive KV Cache Compression for LLMs</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen">DeepSpeed/blogs/deepspeed-fastgen at master · microsoft/DeepSpeed</A>
									<DT><A HREF="https://twitter.com/bio_bootloader/status/1790539323600949542">context caching for Google Gemini</A>
									<DT><A HREF="https://github.com/madsys-dev/deepseekv2-profile/blob/main/workspace/blog/optimizing-mla.md">deepseekv2-profile/workspace/blog/optimizing-mla.md at main · madsys-dev/deepseekv2-profile</A>
									<DT><A HREF="https://x.com/haileysch__/status/1787583052039802887">MLA TL:DR</A>
									<DT><A HREF="https://x.com/teortaxesTex/status/1790407782325420277/photo/4">Teortaxes en X: "he blog on MLA by Jianlin SU (the GOD creator of ROPE) is also a good reference</A>
									<DT><A HREF="https://x.com/teortaxesTex/status/1793180779943543246">most KV cache is redundant, actually</A>
									<DT><A HREF="https://github.com/google/maxtext/blob/f7ee8c636fd500995e76c227b351d48680ab7890/MaxText/layers/attentions.py#L435">maxtext/MaxText/layers/attentions.py</A>
									<DT><A HREF="https://github.com/kvcache-ai/Mooncake?tab=readme-ov-file">kvcache-ai/Mooncake: Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI.</A>
									<DT><A HREF="https://arxiv.org/abs/2407.00079">[2407.00079] Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</A>
									<DT><A HREF="https://arxiv.org/html/2405.04434v2">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</A>
									<DT><A HREF="https://x.com/tri_dao/status/1712904220519944411">Flash-Decoding</A>
									<DT><A HREF="https://x.com/GriffinAdams92/status/1819072387469516884">(1) Griffin Adams en X: "Announcing Cold Compress 1.0 with @answerdotai A hackable toolkit for using and creating KV cache compression methods. Built on top of @cHHillee and Team’s GPT-Fast for torch.compilable, light-weight performance. Develop novel methods in as little as 1 line of new code. https://t.co/HRgd54hG3T" / X</A>
									<DT><A HREF="https://github.com/AnswerDotAI/cold-compress/blob/main/cache.py">cold-compress/cache.py at main · AnswerDotAI/cold-compress</A>
									<DT><A HREF="https://github.com/LMCache/LMCache">LMCache/LMCache</A>
								</DL><p>
								<DT><H3 FOLDED>long-context</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/MInference">microsoft/MInference: To speed up Long-context LLMs' inference, approximate and dynamic sparse calculate the attention, which reduces inference latency by up to 10x for pre-filling on an A100 while maintaining accuracy.</A>
									<DT><A HREF="https://export.arxiv.org/pdf/2407.02490">MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2302.10866.pdf">Hyena Hierarchy</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2023-03-07-hyena">Hyena Hierarchy: Towards Larger Convolutional Language Models · Hazy Research</A>
								<DT><A HREF="https://github.com/HazyResearch/safari">HazyResearch/safari: Convolutions for Sequence Modeling</A>
								<DT><A HREF="https://arxiv.org/abs/2305.01625">[2305.01625] Unlimiformer: Long-Range Transformers with Unlimited Length Input</A>
								<DT><A HREF="https://twitter.com/haoliuhl/status/1664396377252667393">Blockwise Parallel Transformer</A>
								<DT><A HREF="https://arxiv.org/pdf/2305.19370.pdf">Blockwise Parallel Transformer for Large Context Models</A>
								<DT><A HREF="https://www.cerebras.net/blog/variable-sequence-length-training-for-long-context-large-language-models/?utm_content=258053990&utm_medium=social&utm_source=linkedin&hss_channel=lcp-10858000">Variable Sequence Length Training for Long-Context Large Language Models - Cerebras</A>
								<DT><A HREF="https://twitter.com/arankomatsuzaki/status/1637612922934382593">CoLT5</A>
								<DT><A HREF="https://arxiv.org/abs/2307.03172">[2307.03172] Lost in the Middle: How Language Models Use Long Contexts</A>
								<DT><A HREF="https://arxiv.org/pdf/2108.12409.pdf">ALiBi</A>
								<DT><A HREF="https://twitter.com/theemozilla/status/1726650665718685821">Yarn-LLAMA-2-70b-32k</A>
								<DT><A HREF="https://www.youtube.com/watch?v=_u47Esw4d8o">[short] On the Long Range Abilities of Transformers - YouTube</A>
								<DT><A HREF="https://arxiv.org/abs/2404.07143">[2404.07143] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</A>
								<DT><A HREF="https://arxiv.org/abs/2402.13753">[2402.13753] LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens</A>
								<DT><A HREF="https://arxiv.org/abs/2310.01889">[2310.01889] Ring Attention with Blockwise Transformers for Near-Infinite Context</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MRTTGMlKgb8">Leave No Context Behind</A>
								<DT><A HREF="https://arxiv.org/abs/2404.08801">[2404.08801] Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length</A>
								<DT><A HREF="https://www.youtube.com/watch?v=r_UBBfTPcF0">Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=T5haJyhVPCY">Leave No Context Behind Efficient Infinite Context Transformers with Infini attention Google 2024 - YouTube</A>
								<DT><A HREF="https://huggingface.co/crusoeai">crusoeai (Crusoe AI)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=RCSvpYb90qE">LongRoPE &amp; Theta Scaling to 1 Mio Token</A>
								<DT><A HREF="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#gemini-model-updates</A>
								<DT><A HREF="https://kexue.fm/archives/10122">Transformer Upgrade Road: 18. RoPE Base Design Principles</A>
								<DT><A HREF="https://arxiv.org/abs/2109.00301">[2109.00301] $\infty$-former: Infinite Memory Transformer</A>
								<DT><A HREF="https://arxiv.org/abs/2405.07719">[2405.07719] USP: A Unified Sequence Parallelism Approach for Long Context Generative AI</A>
								<DT><A HREF="https://arxiv.org/abs/2403.00071">[2403.00071] Resonance RoPE: Improving Context Length Generalization of Large Language Models</A>
								<DT><A HREF="https://arxiv.org/abs/2309.00071">[2309.00071] YaRN: Efficient Context Window Extension of Large Language Models</A>
								<DT><A HREF="https://github.com/RulinShao/LightSeq">RulinShao/LightSeq: Official repository for LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers</A>
								<DT><A HREF="https://arxiv.org/pdf/2310.03294">DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training</A>
								<DT><A HREF="https://github.com/jzhang38/EasyContext">jzhang38/EasyContext: Memory optimization and training recipes to extrapolate language models' context length to 1 million tokens, with minimal hardware.</A>
								<DT><A HREF="https://qwenlm.github.io/blog/qwen2.5-turbo/">Extending the Context Length to 1M Tokens! | Qwen</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-architecture</H3>
							<DL><p>
								<DT><H3 FOLDED>seq2seq</H3>
								<DL><p>
									<DT><H3 FOLDED>Transformer</H3>
									<DL><p>
										<DT><H3 FOLDED>transformer-flops</H3>
										<DL><p>
											<DT><H3 FOLDED>transformer-arithmetic</H3>
											<DL><p>
												<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic | kipply's blog</A>
												<DT><A HREF="https://jax-ml.github.io/scaling-book/transformers/">All the Transformer Math You Need to Know | How To Scale Your Model</A>
												<DT><A HREF="https://github.com/GHGmc2/deepseek-projection/blob/master/doc/transformer_math.md">deepseek-projection/doc/transformer_math.md at master · GHGmc2/deepseek-projection</A>
											</DL><p>
											<DT><A HREF="https://twitter.com/karpathy/status/1781047292486914189">Karpathy: Llama 3 model card (flops model training taxonomy)</A>
											<DT><A HREF="https://arxiv.org/abs/2104.04473">[2104.04473] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM (slide 46 pt2)</A>
											<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic | kipply's blog</A>
											<DT><A HREF="https://www.usenix.org/system/files/osdi22-yu.pdf">Orca: A Distributed Serving System for Transformer-Based Generative Models</A>
											<DT><A HREF="https://arxiv.org/pdf/2305.05920">Fast Distributed Inference Serving for Large Language Models</A>
											<DT><A HREF="https://www.baseten.co/blog/llm-transformer-inference-guide/">A guide to LLM inference and performance</A>
											<DT><A HREF="https://zeux.io/2024/03/15/llm-inference-sol/">zeux.io - LLM inference speed of light</A>
											<DT><A HREF="https://www.artfintel.com/p/where-do-llms-spend-their-flops">Where do LLMs spend their FLOPS? - by Finbarr Timbers</A>
											<DT><A HREF="https://arxiv.org/pdf/2204.02311">PaLM 1: MFU compute usage</A>
											<DT><A HREF="https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4">The FLOPs Calculus of Language Model Training | by Dzmitry Bahdanau | Medium</A>
											<DT><A HREF="https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb">nanoGPT/transformer_sizing.ipynb at master · karpathy/nanoGPT</A>
											<DT><A HREF="https://github.com/EleutherAI/cookbook/blob/main/benchmarks/sizing/transformer_flops.py#L139">cookbook/benchmarks/sizing/transformer_flops.py at main · EleutherAI/cookbook</A>
											<DT><A HREF="https://kipp.ly/transformer-param-count/">LLM Parameter Counting | kipply's blog</A>
											<DT><A HREF="https://arxiv.org/abs/2203.15556">(F. FLOPs computation) Training Compute-Optimal Large Language Models</A>
											<DT><A HREF="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</A>
											<DT><A HREF="https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters">How does GPT-3 spend its 175B parameters? — LessWrong</A>
											<DT><A HREF="https://github.com/huggingface/nanotron/blob/03d67f2103d5be0dc15ea6022a6cf16d6a633064/src/nanotron/models/starcoder2.py#L1587">nanotron/src/nanotron/models/starcoder2.py: get_flops</A>
											<DT><A HREF="https://arxiv.org/pdf/2205.05198">Reducing Activation Recomputation in Large Transformer Models 6.3</A>
											<DT><A HREF="https://www.adamcasson.com/posts/transformer-flops">Transformer FLOPs | Adam Casson</A>
										</DL><p>
										<DT><H3 FOLDED>transformer-params</H3>
										<DL><p>
											<DT><A HREF="https://kipp.ly/transformer-param-count/">LLM Parameter Counting | kipply's blog</A>
											<DT><A HREF="https://arxiv.org/abs/2203.15556">(F. FLOPs computation) Training Compute-Optimal Large Language Models</A>
											<DT><A HREF="https://github.com/huggingface/transformers/blob/b7ea171403d53a2aa9bce422f1fad8fb1150844b/examples/research_projects/movement-pruning/counts_parameters.py#L4">transformers/examples/research_projects/movement-pruning/counts_parameters.py at b7ea171403d53a2aa9bce422f1fad8fb1150844b · huggingface/transformers</A>
											<DT><A HREF="https://www.lesswrong.com/posts/3duR8CrvcHywrnhLo/how-does-gpt-3-spend-its-175b-parameters">How does GPT-3 spend its 175B parameters? — LessWrong</A>
											<DT><A HREF="https://wandb.ai/wandb_fc/tips/reports/How-To-Calculate-Number-of-Model-Parameters-for-PyTorch-and-TensorFlow-Models--VmlldzoyMDYyNzIx">How To Calculate Number of Model Parameters for PyTorch and TensorFlow Models | tips – Weights &amp; Biases</A>
										</DL><p>
										<DT><H3 FOLDED>transformer-memory</H3>
										<DL><p>
											<DT><A HREF="https://arxiv.org/pdf/2205.05198">Reducing Activation Recomputation in Large Transformer Models</A>
											<DT><A HREF="https://arxiv.org/abs/1910.02054">[1910.02054] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</A>
										</DL><p>
										<DT><H3 FOLDED>transformer-forward-pass</H3>
										<DL><p>
											<DT><A HREF="https://drive.google.com/file/d/1z9Orx5Gvxh7OG5BTcHCF8I8qX_ui1Ppj/view">mlkv-Transformer Steps 400.png - Google Drive</A>
										</DL><p>
										<DT><H3 FOLDED>Transformer++</H3>
										<DL><p>
											<DT><A HREF="https://arxiv.org/abs/2312.00752">[2312.00752] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</A>
											<DT><A HREF="https://twitter.com/rasbt/status/1729123424990068743">Simple Transformer Block</A>
											<DT><A HREF="https://github.com/RobertRiachi/nanoPALM">RobertRiachi/nanoPALM</A>
										</DL><p>
										<DT><H3 FOLDED>transformer-lectures</H3>
										<DL><p>
											<DT><A HREF="https://www.youtube.com/watch?v=dbo3kNKPaUA">Large Language Models (in 2023) Hyung Won Chung</A>
											<DT><A HREF="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g2885e521b53_0_0">Language Language Models (in 2023) - Google Slides</A>
											<DT><A HREF="https://www.youtube.com/watch?v=orDKvo8h71o">Stanford CS25: V4 I Hyung Won Chung of OpenAI - YouTube</A>
											<DT><A HREF="https://docs.google.com/presentation/d/1u05yQQaw4QXLVYGLI6o3YoFHv6eC3YN8GvWD8JMumpE/edit#slide=id.g2885e521b53_0_0">Hyung Won (OpenAI): Shaping the future of AI from the history of Transformer</A>
											<DT><A HREF="https://www.youtube.com/watch?v=iqmjzecbJHE">Orignal transformer paper "Attention is all you need" introduced by a layman | Shawn's ML Notes - YouTube</A>
										</DL><p>
										<DT><H3 FOLDED>positional encoding</H3>
										<DL><p>
											<DT><A HREF="https://kexue.fm/archives/9105">A Theoretical Flaw and Countermeasure of Relative Position Encoding Transformer</A>
											<DT><A HREF="https://www.youtube.com/watch?v=T3OT8kqoqjc&list=LL&index=2&t=40s">How positional encoding in transformers works? - YouTube</A>
											<DT><A HREF="https://mp.weixin.qq.com/s/0peSNWN0ypMopPR0Q_pujQ">一文看懂 LLaMA 中的旋转式位置编码（Rotary Position Embedding）</A>
										</DL><p>
										<DT><H3 FOLDED>residual stream</H3>
										<DL><p>
											<DT><A HREF="https://kexue.fm/archives/8994">Why do we need residuals? A perspective from DeepNet</A>
											<DT><A HREF="https://lucasb.eyer.be/snips/residuals.html">History of Residuals and a Word of Caution</A>
											<DT><A HREF="https://github.com/microsoft/ResiDual">microsoft/ResiDual: ResiDual: Transformer with Dual Residual Connections, https://arxiv.org/abs/2304.14802</A>
										</DL><p>
										<DT><A HREF="https://poloclub.github.io/transformer-explainer/">Transformer Explainer: LLM Transformer Model Visually Explained</A>
										<DT><A HREF="https://web.stanford.edu/~jurafsky/slp3/10.pdf">Chapter 10: Transformers and Large Language Models</A>
										<DT><A HREF="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer (new version)</A>
										<DT><A HREF="https://gist.github.com/sophiawisdom/ccdff5b7ebcd782393dbc5be3f0866f9">shittytransformer.py</A>
										<DT><A HREF="https://github.com/xjdr-alt/simple_transformer">xjdr-alt/simple_transformer: Simple Transformer in Jax</A>
										<DT><A HREF="https://github.com/joschu/jax-exp/blob/master/jax_transformer.py#L96">John Schulman: jax-exp/jax_transformer.py at master</A>
										<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes — Good Coding Style | by Noam Shazeer | Medium</A>
										<DT><A HREF="https://github.com/openai/finetune-transformer-lm/">openai/finetune-transformer-lm: Code and model for the paper "Improving Language Understanding by Generative Pre-Training"</A>
										<DT><A HREF="https://docs.google.com/presentation/d/1u05yQQaw4QXLVYGLI6o3YoFHv6eC3YN8GvWD8JMumpE/edit#slide=id.g2885e521b53_0_0">Hyung Won (OpenAI): Shaping the future of AI from the history of Transformer</A>
										<DT><A HREF="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g2885e521b53_0_0">Language Language Models (in 2023) - Google Slides</A>
										<DT><A HREF="https://x.com/hwchung27/status/1800676312916656592">Shaping the future of AI from the history of Transformer</A>
										<DT><A HREF="https://github.com/hyunwoongko/transformer">transformer step-by-step impl</A>
										<DT><A HREF="https://www.youtube.com/watch?v=ZAO6y_oJtFA">Street Fighting Transformers - YouTube</A>
										<DT><A HREF="https://kexue.fm/archives/4765">A brief reading of "Attention is All You Need" (introduction + code)</A>
										<DT><A HREF="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits</A>
										<DT><A HREF="https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising">What happened to BERT &amp; T5? On Transformer Encoders, PrefixLM and Denoising Objectives — Yi Tay</A>
										<DT><A HREF="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</A>
										<DT><A HREF="https://atcold.github.io/NYU-DLSP20/en/week12/12-3/">Attention and the Transformer · Deep Learning</A>
										<DT><A HREF="https://github.com/madsys-dev/deepseekv2-profile/blob/main/mla/modeling_deepseek.py#L1181">deepseekv2-profile/mla/modeling_deepseek.py at main · madsys-dev/deepseekv2-profile</A>
										<DT><A HREF="https://www.youtube.com/watch?v=S9eKuRVigjY">Building a ML Transformer in a Spreadsheet - YouTube</A>
										<DT><A HREF="https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/transformer_sizing.ipynb">Transformer Theoretical Model</A>
										<DT><A HREF="https://github.com/Aleph-Alpha/scaling/blob/main/src/scaling/transformer/model/layers/layer.py#L268">scaling/src/scaling/transformer/model/layers/layer.py at main · Aleph-Alpha/scaling</A>
										<DT><A HREF="https://github.com/lucidrains/x-transformers">lucidrains/x-transformers: A simple but complete full-attention transformer with a set of promising experimental features from various papers</A>
										<DT><A HREF="https://github.com/drisspg/transformer_nuggets">drisspg/transformer_nuggets: A place to store reusable transformer components of my own creation or found on the interwebs</A>
										<DT><A HREF="https://leimao.github.io/blog/Transformer-Explained/">Transformer Explained In One Single Page - Lei Mao's Log Book</A>
										<DT><A HREF="https://github.com/mikaylagawarecki/temp/blob/main/mha.py">temp/mha.py at main · mikaylagawarecki/temp</A>
										<DT><A HREF="https://x.com/Sumanth_077/status/1851263873749565851">Transformer visually explained</A>
										<DT><A HREF="https://towardsdatascience.com/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e">Drawing the Transformer Network from Scratch (Part 1) | by Thomas Kurbiel | Towards Data Science</A>
										<DT><A HREF="https://sites.google.com/view/gbrainprinceton/projects/spectral-transformers">Google DeepMind Princeton - Spectral Transformers</A>
										<DT><A HREF="https://www.youtube.com/watch?v=KJtZARuO3JY">Visualizing transformers and attention | Talk for TNG Big Tech Day '24 - YouTube</A>
										<DT><A HREF="https://github.com/feifeibear/LLMRoofline">feifeibear/LLMRoofline: Compare different hardware platforms via the Roofline Model for LLM inference tasks.</A>
										<DT><A HREF="https://arxiv.org/pdf/2303.06318">what are all the ways the ops in a transformer can be parallelized across GPUs + pretty diagrams?</A>
									</DL><p>
									<DT><H3 FOLDED>GPT</H3>
									<DL><p>
										<DT><H3 FOLDED>GPT-2</H3>
										<DL><p>
											<DT><H3 FOLDED>cloneofsimo-min-max-gpt</H3>
											<DL><p>
												<DT><H3 FOLDED>gpt-2-mup</H3>
												<DL><p>
													<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt/blob/7b017b8a0680e8eec6328c7ea3edfca7592107e0/tweakablegpt.py#L53">tweakablegpt.py#L53</A>
												</DL><p>
												<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt">cloneofsimo/min-max-gpt: Minimal (400 LOC) implementation Maximum (multi-node, FSDP) GPT training</A>
												<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
											</DL><p>
											<DT><H3 FOLDED>gpt-2-fp8</H3>
											<DL><p>
												<DT><A HREF="https://github.com/cchan/nanoGPT-fp8">cchan/nanoGPT-fp8</A>
												<DT><A HREF="https://x.com/itsclivetime/status/1655515089506820097">(1) Clive Chan en X: "WIP FP8 training on consumer graphics cards - 🧵/4 I hacked nanoGPT to use TransformerEngine on RTX 4090 and ran a few iterations of GPT-2 training: - nanoGPT Block (+flashattn) =&amp;gt; TE TransformerLayer (both BF16): 15% faster - BF16 =&amp;gt; FP8: additional +18% https://t.co/cJNWehoGeu" / X</A>
											</DL><p>
											<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt">cloneofsimo/min-max-gpt: Minimal (400 LOC) implementation Maximum (multi-node, FSDP) GPT training</A>
											<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
											<DT><A HREF="https://kexue.fm/archives/9529">Why are current LLMs all decoder-only architectures?</A>
											<DT><A HREF="https://github.com/openai/gpt-2">openai/gpt-2: Code for the paper "Language Models are Unsupervised Multitask Learners" (Tensorflow impl)</A>
											<DT><A HREF="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py">huggingface/transformers PyTorch implementation</A>
											<DT><A HREF="https://www.youtube.com/watch?v=l8pRSuU81PU">Let's reproduce GPT-2 (124M) - YouTube</A>
											<DT><A HREF="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</A>
											<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
											<DT><A HREF="https://x.com/kellerjordan0/status/1803566078985117903">Sophia optimizer</A>
											<DT><A HREF="https://github.com/KellerJordan/modded-nanogpt/tree/sophia">KellerJordan/modded-nanogpt at sophia</A>
											<DT><A HREF="https://github.com/pytorch-labs/gpt-fast">pytorch-labs/gpt-fast: Simple and efficient pytorch-native transformer text generation in &lt;1000 LOC of python.</A>
											<DT><A HREF="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/quickstart.html">Getting Started — Transformer Engine 1.7.0 documentation</A>
											<DT><A HREF="https://openai.com/index/image-gpt/">Image GPT | OpenAI</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1805335482999750773">crossentropy loss is the main memory bottleneck (memory-bound)</A>
											<DT><A HREF="https://github.com/Lightning-AI/litgpt">Lightning-AI/litgpt: Load, pretrain, finetune, deploy 20+ LLMs on your own data. Uses state-of-the-art techniques: flash attention, FSDP, 4-bit, LoRA, and more.</A>
											<DT><A HREF="https://github.com/mgmalek/efficient_cross_entropy">mgmalek/efficient_cross_entropy</A>
											<DT><A HREF="https://mp.weixin.qq.com/s/8F3eAHDBjQkHHBmrAEoOfw">"Illustrated Large Model Training: Data Parallel Part 2 (ZeRO, Zero Redundancy Optimization)"</A>
											<DT><A HREF="https://zhuanlan.zhihu.com/p/624740065">Analyze the parameters, computation, intermediate activation, and KV cache of the transformer model</A>
											<DT><A HREF="https://huggingface.co/spaces/BBuf/megatron-lm-parallel-group-playground">Megatron Lm Parallel Group Playground - a Hugging Face Space by BBuf</A>
											<DT><A HREF="https://tspeterkim.github.io/posts/mixed-precision-from-scratch">Mixed Precision Training from Scratch | Taeksang Peter Kim</A>
											<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes — Good Coding Style | by Noam Shazeer | Medium</A>
											<DT><A HREF="https://kidger.site/thoughts/jaxtyping/">No more shape errors! Type annotations for the shape+dtype of tensors/arrays. · Patrick Kidger</A>
											<DT><A HREF="http://giantpandacv.com/project/PyTorch/AI%20Infra%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8A%E5%9C%A8LLM%E8%AE%AD%E7%BB%83%E4%B8%AD%E5%87%8F%E5%B0%91%E6%BF%80%E6%B4%BB%E5%80%BC%E5%86%85%E5%AD%98%E3%80%8B/">AI Infra paper reading: "Reducing activation value memory in LLM training"</A>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1805564665428164915">sparsity pattern 2:4 PyTorch training</A>
											<DT><A HREF="https://pytorch.org/blog/accelerating-neural-network-training/">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity | PyTorch</A>
											<DT><A HREF="https://imbue.com/research/70b-infrastructure/">From bare metal to a 70B model: infrastructure set-up and scripts - imbue</A>
											<DT><A HREF="https://imbue.com/research/70b-carbs/">Open-sourcing CARBS: a cost-effective hyperparameter optimizer that helps scale small experiments to large language models - imbue</A>
											<DT><A HREF="https://imbue.com/research/70b-intro/">Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings - imbue</A>
											<DT><A HREF="https://github.com/imbue-ai/cluster-health/tree/master">imbue-ai/cluster-health</A>
											<DT><A HREF="https://github.com/imbue-ai/carbs">imbue-ai/carbs: Cost aware hyperparameter tuning algorithm</A>
											<DT><A HREF="https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e">GPT from Scratch with MLX. Define and train GPT-2 on your MacBook | by Pranav Jadhav | Jun, 2024 | Towards Data Science</A>
											<DT><A HREF="https://github.com/pytorch/torchtune">pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning</A>
											<DT><A HREF="https://github.com/pytorch/ao">pytorch/ao: Create and integrate custom data types, layouts and kernels with up to 2x speedups with 65% less VRAM for inference and training</A>
											<DT><A HREF="https://docs.google.com/presentation/d/1JKpqsbkr5Fg-bj1iElPaC-ToTVpRmRLKZmN89krwl04/edit?resourcekey=0-VPgp_Yc4krPPW3Mxv6UjgQ#slide=id.g25dfdd1f5b2_0_8">2024 stanford cs25 guest lecture jason wei - Google Slides</A>
											<DT><A HREF="https://www.youtube.com/watch?v=6jTQ61tBeoQ">GPT (nanoGPT) from a beginner's perspective (Part 1) - YouTube</A>
											<DT><A HREF="https://github.com/google/maxtext/blob/53167aa550b49bfc867236790bc8065545f0d300/MaxText/layers/gpt3.py#L151">maxtext/MaxText/layers/gpt3.py at 53167aa550b49bfc867236790bc8065545f0d300 · google/maxtext</A>
											<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/blob/main/examples/attn_causal/h100_train.cu">ThunderKittens/examples/attn_causal/h100_train.cu</A>
											<DT><A HREF="https://github.com/HazyResearch/nanoGPT-TK">HazyResearch/nanoGPT-TK: The simplest, fastest repository for training/finetuning medium-sized GPTs. Now, with kittens!</A>
											<DT><A HREF="https://github.com/google-deepmind/nanodo/tree/main">google-deepmind/nanodo</A>
											<DT><A HREF="https://x.com/karpathy/status/1814038096218083497">(1) Andrej Karpathy en X: "LLM model size competition is intensifying... backwards! My bet is that we'll see models that "think" very well and reliably that are very very small. There is most likely a setting even of GPT-2 parameters for which most people will consider GPT-2 "smart". The reason current" / X</A>
											<DT><A HREF="https://x.com/Yuchenj_UW/status/1814159545280971115">(1) Yuchen Jin en X: "Let's go even deeper! Training GPT-2 (7.3B) using @karpathy's llm.c with 32 H100 GPUs. *GPUs go brrr at night* 🔥 - Setup: 4 H100 nodes connected with 400Gb/s InfiniBand - Training speed: 327K tokens/s, MFU: 46.7%. Pretty linear scaling! - Due to some parameters in the 7.3B https://t.co/7Z6dEqXeYL" / X</A>
											<DT><A HREF="https://x.com/Yuchenj_UW/status/1815417740452397187">(1) Yuchen Jin en X: "GPU-hours vs. performance for different sizes of GPT-2 (from 124M to 2.7B) trained with @karpathy's llm.c + FineWeb-Edu 👾 @eliebakouch and @diegoasua were interested in seeing the plots between compute vs. loss and eval in the last post. Here it is (y-axis capped at 4 in the https://t.co/YsCPQS19TJ" / X</A>
											<DT><A HREF="https://github.com/graphcore-research/out-of-the-box-fp8-training/blob/main/out_of_the_box_fp8_training.ipynb">out-of-the-box-fp8-training/out_of_the_box_fp8_training.ipynb at main · graphcore-research/out-of-the-box-fp8-training</A>
											<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/e9024c691f27a41fabd94617d39d75813b649f26/examples/gpt2.py#L115">tinygrad/examples/gpt2.py at e9024c691f27a41fabd94617d39d75813b649f26 · tinygrad/tinygrad</A>
											<DT><A HREF="https://github.com/joennlae/tensorli">joennlae/tensorli: Absolute minimalistic implementation of a GPT-like transformer using only numpy (&lt;650 lines).</A>
											<DT><A HREF="https://github.com/huggingface/nanotron/blob/03d67f2103d5be0dc15ea6022a6cf16d6a633064/src/nanotron/models/starcoder2.py">GPT with Multi-Query Attention, RoPe, SWA and GQA</A>
											<DT><A HREF="https://x.com/karpathy/status/1859305141385691508">nanoGPT: GPT-2 (124M) pre-training 5 min</A>
											<DT><A HREF="https://github.com/KellerJordan/modded-nanogpt/tree/master">KellerJordan/modded-nanogpt: NanoGPT (124M) in 5 minutes</A>
											<DT><A HREF="https://x.com/Yuchenj_UW/status/1861477701821047287">(1) Yuchen Jin en X: "GPT-2 (1.6B) NanoGPT speedrun finished 🏃 Conclusion: - Achieved 2X token efficiency compared to the llm.c baseline. - Per single-step (~1M tokens), llm.c remains 26.9% faster on 8xH100s, I was wrong in my previous post. - Wall-clock time: GPT-2 (1.6B) speedrun completed in 16.3 https://t.co/FumoCfydwz" / X</A>
											<DT><A HREF="https://www.youtube.com/watch?v=d1LNUvkRMEg&t=13076s">GPT-2 from Scratch in C (Day 1/2) - YouTube</A>
										</DL><p>
										<DT><H3 FOLDED>gpt-layernorm</H3>
										<DL><p>
											<DT><A HREF="https://kexue.fm/archives/9009">Why is Pre Norm not as effective as Post Norm?</A>
										</DL><p>
										<DT><A HREF="https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Supplemental.pdf">Supplement GPT-3 few-shot generalizable learners</A>
										<DT><A HREF="https://github.com/siliconflow/nexfort-gpt/blob/main/model.py">nexfort-gpt/model.py at main · siliconflow/nexfort-gpt</A>
										<DT><A HREF="https://strint.notion.site/GPT-84ab3b065bdc4fb883710e52c58a2210">GPT-2 formal description and impl (if flash)</A>
										<DT><A HREF="https://github.com/facebookresearch/fairseq/blob/920a548ca770fb1a951f7f4289b4d3a0c1bc226f/fairseq/models/transformer_lm.py#L539">fairseq/fairseq/models/transformer_lm.py at 920a548ca770fb1a951f7f4289b4d3a0c1bc226f · facebookresearch/fairseq</A>
										<DT><A HREF="https://x.com/johannes_hage/status/1861527140279431396/photo/1">new 1.6B nanoGPT speedrun dashboard (PrimeIntellect)</A>
										<DT><A HREF="https://www.njkumar.com/calculating-gpt2s-inference-speedups/">Calculating GPT-2’s Inference Speedups | njkumar</A>
									</DL><p>
									<DT><H3 FOLDED>T5</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/pdf/1910.10683.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</A>
										<DT><A HREF="https://twitter.com/YiTayML/status/1651932898512211970">~2 x the parameters for the same compute cost. Basically free model sparsity (sparse w.r.t to enc/dec blocks).</A>
										<DT><A HREF="https://twitter.com/YiTayML/status/1668302949276356609">(Yi Tay): Some thoughts/observations (T5 models at 30B &amp; 65B)</A>
										<DT><A HREF="https://arxiv.org/abs/2306.04757">InstructEval: Table 2 (Flan vs Alpaca) EncDec vs Dec param claim</A>
										<DT><A HREF="https://arxiv.org/abs/2207.10551">[2207.10551] Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?</A>
										<DT><A HREF="https://kexue.fm/archives/6933">explaning masking attention</A>
										<DT><A HREF="https://x.com/Birchlabs/status/1899106303596712239">(1) Birchlabs en X: "be careful using HF tokenizers. fast and slow tokenizers give different results, and neither necessarily matches the sentencepiece reference implementation. https://t.co/D4jD9VKBra" / X</A>
									</DL><p>
									<DT><H3 FOLDED>EncDec</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=ORzGEnHTSfk">What BERT Can’t Do: The Transformer's Decoder</A>
									</DL><p>
									<DT><H3 FOLDED>Llama</H3>
									<DL><p>
										<DT><H3 FOLDED>Llama 3</H3>
										<DL><p>
											<DT><H3 FOLDED>LLaMA 3.1</H3>
											<DL><p>
												<DT><A HREF="https://x.com/_xjdr/status/1815631563628986487">Llama3.1 405B base is officially good based on my private evals. I am excited.</A>
												<DT><A HREF="https://scontent.fsvq5-1.fna.fbcdn.net/v/t39.2365-6/452387774_1036916434819166_4173978747091533306_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=DTS7hDTcxZoQ7kNvgF6h1Ib&_nc_ht=scontent.fsvq5-1.fna&oh=00_AYBQ4-CcPRnxbpROau5tQYCMi55zbAoi_PoSQoqp6cMvAw&oe=66ADF38D">The Llama 3 Herd of Models</A>
												<DT><A HREF="https://github.com/karpathy/nano-llama31">karpathy/nano-llama31: nanoGPT style version of Llama 3.1</A>
												<DT><A HREF="https://arxiv.org/abs/2407.21783">[2407.21783] The Llama 3 Herd of Models</A>
												<DT><A HREF="https://arxiv.org/pdf/2407.21783">The Llama 3 Herd of Models</A>
												<DT><A HREF="https://github.com/thecharlieblake/lovely-llama">thecharlieblake/lovely-llama: An implementation of the Llama architecture, to instruct and delight</A>
												<DT><A HREF="https://github.com/LambdaLabsML/Llama-3.2-vision-study/tree/main/throughput">Llama-3.2-vision-study/throughput at main · LambdaLabsML/Llama-3.2-vision-study</A>
												<DT><A HREF="https://zhuanlan.zhihu.com/p/710665670">Llama 3.1-405B训练推理技术 - 知乎</A>
											</DL><p>
											<DT><H3 FOLDED>llama-3-model</H3>
											<DL><p>
												<DT><A HREF="https://github.com/naklecha/llama3-from-scratch">naklecha/llama3-from-scratch: llama3 implementation one matrix multiplication at a time</A>
												<DT><A HREF="https://github.com/meta-llama/llama3">meta-llama/llama3: the main Llama 3 GitHub site - will be moved under Meta-Llama</A>
												<DT><A HREF="https://github.com/meta-llama/llama3/blob/main/llama/model.py">llama3/llama/model.py</A>
												<DT><A HREF="https://github.com/ai-compiler-study/kernels/blob/main/models/llama/llama/model.py">kernels/models/llama/llama/model.py at main · ai-compiler-study/kernels</A>
												<DT><A HREF="https://x.com/samsja19/status/1891994496503542103">(1) samsja en X: "What is the the gold standard https://t.co/wyGjSmUiOA implementation in pytorch ? I want: * fa3 / flex attention support * compatible with HF hub checkpoint * not bloated * Properly maintained and tested I think that it do not exist 😅" / X</A>
											</DL><p>
											<DT><A HREF="https://ai.meta.com/blog/meta-llama-3/">Introducing Meta Llama 3: The most capable openly available LLM to date</A>
											<DT><A HREF="https://twitter.com/astonzhangAZ/status/1780990210576441844">pre-training, human data, scaling, long context, post-training &amp; eval</A>
											<DT><A HREF="https://github.com/meta-llama/llama3/blob/main/eval_details.md">llama3/eval_details.md at main · meta-llama/llama3</A>
											<DT><A HREF="https://twitter.com/karpathy/status/1781047292486914189">(1) Andrej Karpathy en X: "The model card has some more interesting info too: https://t.co/EzNGMu57am Note that Llama 3 8B is actually somewhere in the territory of Llama 2 70B, depending on where you look. This might seem confusing at first but note that the former was trained for 15T tokens, while the..." / X</A>
											<DT><A HREF="https://twitter.com/karpathy/status/1781028605709234613">(1) Andrej Karpathy en X: "Congrats to @AIatMeta on Llama 3 release!! 🎉 https://t.co/fSw615zE8S Notes: Releasing 8B and 70B (both base and finetuned) models, strong-performing in their model class (but we'll see when the rankings come in @ @lmsysorg :)) 400B is still training, but already encroaching..." / X</A>
											<DT><A HREF="https://x.com/danielhanchen/status/1792553901524472311">Llama 3 arch on 1 mathematics page</A>
											<DT><A HREF="https://huggingface.co/imbue/llama3-question-quality">imbue/llama3-question-quality · Hugging Face</A>
											<DT><A HREF="https://github.com/meta-llama/llama/commit/12b676b909368581d39cebafae57226688d5676a">Update download.sh · meta-llama/llama@12b676b</A>
											<DT><A HREF="https://github.com/likejazz/llama3.np">likejazz/llama3.np: llama3.np is a pure NumPy implementation for Llama 3 model.</A>
											<DT><A HREF="https://www.llama.com/">Llama</A>
											<DT><A HREF="https://github.com/abhisheknair10/Llama3.cu">abhisheknair10/Llama3.cu: Lightweight Llama 3-8B Inference Engine in CUDA C</A>
											<DT><A HREF="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct">meta-llama/Llama-3.3-70B-Instruct · Hugging Face</A>
											<DT><A HREF="https://arxiv.org/abs/2204.05149">[2204.05149] The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink</A>
										</DL><p>
										<DT><H3 FOLDED>Llama 4</H3>
										<DL><p>
											<DT><A HREF="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</A>
											<DT><A HREF="https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md">llama-models/models/llama4/MODEL_CARD.md at main · meta-llama/llama-models</A>
											<DT><A HREF="https://github.com/meta-llama/llama-models/tree/main/models/llama4">llama-models/models/llama4 at main · meta-llama/llama-models</A>
											<DT><A HREF="https://github.com/meta-llama/llama-models/blob/main/models/llama4/model.py">llama-models/models/llama4/model.py at main · meta-llama/llama-models</A>
										</DL><p>
										<DT><H3 FOLDED>OPT</H3>
										<DL><p>
											<DT><A HREF="https://github.com/zphang/minimal-opt">zphang/minimal-opt</A>
										</DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2307.09288.pdf">Llama 2: Open Foundation and Fine-Tuned Chat Models</A>
										<DT><A HREF="https://arxiv.org/abs/2302.13971">[2302.13971] LLaMA: Open and Efficient Foundation Language Models</A>
										<DT><A HREF="https://github.com/alibaba/Megatron-LLaMA">alibaba/Megatron-LLaMA: Best practice for training LLaMA models in Megatron-LM</A>
										<DT><A HREF="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">Building Meta’s GenAI Infrastructure - Engineering at Meta</A>
										<DT><A HREF="https://github.com/linkedin/Liger-Kernel/issues/119">[fun] llama.triton · Issue #119 · linkedin/Liger-Kernel</A>
										<DT><A HREF="https://github.com/caiwanxianhust/FasterLLaMA">FasterLLaMA</A>
									</DL><p>
									<DT><H3 FOLDED>RNN</H3>
									<DL><p>
										<DT><H3 FOLDED>LSTM</H3>
										<DL><p>
											<DT><H3 FOLDED>xLSTM</H3>
											<DL><p>
												<DT><A HREF="https://x.com/antferdom/status/1799474799661691014">xLSTM won't replace the Transformer. Two bitter lessons</A>
											</DL><p>
										</DL><p>
										<DT><A HREF="https://kexue.fm/archives/10017">Chapter on Space and Time: Viewing Attention as a Quadratic RNN</A>
										<DT><A HREF="https://www.youtube.com/watch?v=FRy7eLuosic">Transformers are Multi-State RNNs</A>
										<DT><A HREF="https://web.stanford.edu/~jurafsky/slp3/9.pdf">RNN and LSTM</A>
									</DL><p>
									<DT><H3 FOLDED>Gemma</H3>
									<DL><p>
										<DT><H3 FOLDED>Gemma 2</H3>
										<DL><p>
											<DT><A HREF="https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf">Gemma 2: Improving Open Language Models at a Practical Size</A>
											<DT><A HREF="https://x.com/danielhanchen/status/1806372357684220308">Gemma 2 Analysis</A>
											<DT><A HREF="https://x.com/mvpatel2000/status/1806344519333323258">tech report dicussion (Mihir Patel)</A>
											<DT><A HREF="https://huggingface.co/blog/gemma2">Welcome Gemma 2 - Google’s new open LLM</A>
										</DL><p>
										<DT><A HREF="https://ai.google.dev/gemma">Gemma - a family of lightweight, state-of-the art open models from Google  |  Google for Developers</A>
										<DT><A HREF="https://github.com/google/gemma.cpp">google/gemma.cpp: lightweight, standalone C++ inference engine for Google's Gemma models.</A>
										<DT><A HREF="https://arxiv.org/abs/2403.08295">[2403.08295] Gemma: Open Models Based on Gemini Research and Technology</A>
									</DL><p>
									<DT><H3 FOLDED>Mistral</H3>
									<DL><p>
										<DT><A HREF="https://x.com/danielhanchen/status/1814317286389666094">Mistral NeMo 12B</A>
										<DT><A HREF="https://github.com/yixiaoer/mistral-v0.2-jax">yixiaoer/mistral-v0.2-jax: JAX implementation of the Mistral 7b v0.2 model</A>
									</DL><p>
									<DT><H3 FOLDED>Grok</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://www.yitay.net/blog/model-architecture-blogpost-encoders-prefixlm-denoising">What happened to BERT &amp; T5? On Transformer Encoders, PrefixLM and Denoising Objectives — Yi Tay</A>
									<DT><A HREF="https://x.com/YiTayML/status/1813262126162845772">(1) Yi Tay en X: "Decided to start a new blog series about model architectures in the era of LLMs. 😀 Here's part 1 on broader architectures like Transformer Encoders/Encoder-Decoders, PrefixLM and denoising objectives. 😄 A frequently asked question: "The people who worked on language and NLP https://t.co/ndY1Oye75q" / X</A>
									<DT><A HREF="https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf">Sequence to Sequence Learning with Neural Networks</A>
								</DL><p>
								<DT><H3 FOLDED>architecture-efficient</H3>
								<DL><p>
									<DT><H3 FOLDED>Parallel Layers</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ai-compiler-study/kernels/blob/main/models/llama/llama/model.py">kernels/models/llama/llama/model.py at main · ai-compiler-study/kernels</A>
									</DL><p>
									<DT><A HREF="https://github.com/JeanKaddour/NoTrainNoGain">Revisiting Efficient Training Algorithms For Transformer-based Language Models</A>
									<DT><A HREF="https://github.com/Lightning-AI/lit-gpt">Lightning-AI/lit-gpt</A>
									<DT><A HREF="https://twitter.com/ZihangDai/status/1281349893873897478/photo/1">(1) Zihang Dai (@ZihangDai) xAI: Funnel-Transformer</A>
									<DT><A HREF="https://github.com/laiguokun/Funnel-Transformer">laiguokun/Funnel-Transformer</A>
									<DT><A HREF="https://arxiv.org/pdf/2009.06732.pdf">Efficient Transformers: A Survery</A>
									<DT><A HREF="https://arxiv.org/abs/2110.12894">[2110.12894] The Efficiency Misnomer</A>
									<DT><A HREF="https://arxiv.org/abs/2009.06732">[2009.06732] Efficient Transformers: A Survey</A>
								</DL><p>
								<DT><H3 FOLDED>Attention</H3>
								<DL><p>
									<DT><H3 FOLDED>Multi-Query Attention</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2310.06825">[2310.06825] Mistral 7B</A>
									</DL><p>
									<DT><H3 FOLDED>GQA</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html">Rethinking Attention with Performers – Google Research Blog</A>
									<DT><A HREF="https://kipp.ly/transformer-inference-arithmetic/">Transformer Inference Arithmetic</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention">HazyResearch/flash-attention: Fast and memory-efficient exact attention</A>
									<DT><A HREF="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html">Self-Attention</A>
									<DT><A HREF="https://arxiv.org/pdf/2205.14135v2.pdf">Flash Attention</A>
									<DT><A HREF="https://theaisummer.com/self-attention/">Why multi-head self attention works</A>
								</DL><p>
								<DT><H3 FOLDED>Mixture of Expert Models</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-MoE</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/blog/training-moes/?utm_content=298456196&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Training MoEs at Scale with PyTorch | PyTorch</A>
										<DT><A HREF="https://x.com/mvpatel2000/status/1806391255297147218">Training MoEs at Scale with PyTorch (DBRX)</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</A>
									<DT><A HREF="https://arxiv.org/abs/2201.05596">[DeepSpeed-MoE: Advancing MoE Inference and Training</A>
									<DT><A HREF="https://github.com/stanford-futuredata/megablocks">stanford-futuredata/megablocks</A>
									<DT><A HREF="https://mistral.ai/news/mixtral-of-experts/">Mixtral of experts | Mistral AI | Open source models</A>
									<DT><A HREF="https://twitter.com/amanrsanger/status/1690072802454568960">WHY</A>
									<DT><A HREF="https://github.com/lucidrains/st-moe-pytorch">lucidrains/st-moe-pytorch: Implementation of ST-Moe</A>
									<DT><A HREF="https://arxiv.org/abs/2309.05444">[2309.05444] Pushing Mixture of Experts to the Limit</A>
									<DT><A HREF="https://www.youtube.com/watch?v=wLjJ34ygZVc">HuggingGPT</A>
									<DT><A HREF="https://twitter.com/arankomatsuzaki/status/1757229323126243620">Scaling Laws for Fine-Grained Mixture of Experts</A>
									<DT><A HREF="https://twitter.com/Tgale96/status/1773342371993751830">databricks/megablocks</A>
									<DT><A HREF="https://twitter.com/tedzadouri/status/1701579765973713285">Pushing MoE to the Limit: Extremely Parameter Efficient MoE</A>
									<DT><A HREF="https://x.com/cloneofsimo/status/1800228889341599963">Why are MoEs effective?</A>
									<DT><A HREF="https://arxiv.org/pdf/2305.14705">Mixture-of-Experts Meets Instruction Tuning</A>
									<DT><A HREF="http://incompleteideas.net/papers/sutton-86.pdf">Two Problems With Backpropagation and other steepest-descent learning procedures for networks</A>
									<DT><A HREF="https://x.com/teortaxesTex/status/1805066323409707336">(1) Teortaxes▶️ en X: "On why serious labs (CAI, DS, probably all of the Western frontier) work on reducing KV cache You need MoEs with higher sparsity to save compute. But more experts = cache up = batch size down = MFU down = you lose to basic stuff like Mixtral, or even dense https://t.co/VUtsNg8A8I https://t.co/uWDxjtpevX" / X</A>
									<DT><A HREF="https://arxiv.org/abs/2404.02852">[2404.02852] Toward Inference-optimal Mixture-of-Expert Large Language Models</A>
									<DT><A HREF="https://pytorch.org/blog/training-moes/?utm_content=298456196&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Training MoEs at Scale with PyTorch | PyTorch</A>
									<DT><A HREF="https://x.com/mvpatel2000/status/1806391255297147218">Training MoEs at Scale with PyTorch (DBRX)</A>
									<DT><A HREF="https://arxiv.org/html/2404.02258v1">Mixture-of-Depths: Dynamically allocating compute in transformer-based language models</A>
									<DT><A HREF="https://www.youtube.com/watch?v=XczxSi2kHfo">MoE-Level Performance Without The Added Computation - YouTube</A>
									<DT><A HREF="https://www.databricks.com/blog/training-moes-scale-pytorch-and-databricks">Training MoEs at Scale with PyTorch and Databricks | Databricks Blog</A>
									<DT><A HREF="https://x.com/_xjdr/status/1819075319149838490">(1) xjdr en X: "MoD + Modality specific MoE + early fusion?! I would be so happy if L4 (or at least a large model variant) ends up adopting this architecture (i know, i know, one thing at a time, etc, but we could still stand to get a bit more creative in the attention layer ... ) https://t.co/di0mSa3jfV" / X</A>
									<DT><A HREF="https://www.youtube.com/watch?v=F70KlwO4wP0&list=LL&index=19&t=658s">1 Million Tiny Experts in an AI? Fine-Grained MoE Explained - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=8wLG3TIcCXk">Multi-Head Mixture-of-Experts - YouTube</A>
									<DT><A HREF="https://x.com/arankomatsuzaki/status/1819010194887586276">(1) Aran Komatsuzaki en X: "Meta presents MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts - Dividesy expert modules into modality-specific groups - Achieves better performance than the baseline MoE abs: https://t.co/aXWVr0plp9 alphaxiv: https://t.co/4fbBt9CPmY https://t.co/l2dzDdAUzW" / X</A>
									<DT><A HREF="https://arxiv.org/abs/2407.21770">[2407.21770] MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts</A>
									<DT><A HREF="https://github.com/microsoft/Tutel">microsoft/Tutel: Tutel MoE: An Optimized Mixture-of-Experts Implementation</A>
									<DT><A HREF="https://arxiv.org/abs/2411.04996">[2411.04996] Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models</A>
									<DT><A HREF="https://x.com/_xjdr/status/1872415714516316637">First 3 layers of DSv3 are dense</A>
									<DT><A HREF="https://github.com/laekov/fastmoe">laekov/fastmoe: A fast MoE impl for PyTorch</A>
								</DL><p>
								<DT><H3 FOLDED>State Space Model</H3>
								<DL><p>
									<DT><H3 FOLDED>SSM-Samba</H3>
									<DL><p>
										<DT><A HREF="https://github.com/microsoft/Samba">microsoft/Samba: Official implementation of "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling"</A>
									</DL><p>
									<DT><H3 FOLDED>SSM-simba</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2403.15360">[2403.15360] SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2312.00752">[2312.00752] Mamba: Linear-Time Sequence Modeling with Selective State Spaces</A>
									<DT><A HREF="https://www.youtube.com/watch?v=hN0XvyuWlsM">Math Reading Group - State Space Models - (04/03/2024) - YouTube</A>
									<DT><A HREF="https://github.com/sophiawisdom/ssms">sophiawisdom/ssms: GPU kernels for state space models</A>
									<DT><A HREF="https://kexue.fm/archives/10114">Revisiting SSM (I): Linear Systems and HiPPO Matrix</A>
									<DT><A HREF="https://kexue.fm/archives/10180">重温SSM（四）：有理生成函数的新视角 - 科学空间|Scientific Spaces</A>
									<DT><A HREF="https://github.com/NVlabs/hymba">NVlabs/hymba</A>
									<DT><A HREF="https://x.com/QuentinAnthon15/status/1861505524598280260">Zamba2 &amp; Zyda2</A>
									<DT><A HREF="https://x.com/PavloMolchanov/status/1861484218087584217">Hymba</A>
									<DT><A HREF="https://arxiv.org/pdf/2404.08819">The Illusion of State in State-Space Models</A>
								</DL><p>
								<DT><H3 FOLDED>Concept Models</H3>
								<DL><p>
									<DT><A HREF="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/">Large Concept Models: Language Modeling in a Sentence Representation Space | Research - AI at Meta</A>
									<DT><A HREF="https://www.youtube.com/watch?v=2ZLd0uZvwbU">Experimenting With LCM Models (Meta's Alternative To LLM Models) - YouTube</A>
									<DT><A HREF="https://colab.research.google.com/drive/1BxPSbAzp724qaeNducQ_tgWUasH7yBip?usp=sharing">LCM Experiments - Colab</A>
								</DL><p>
								<DT><H3 FOLDED>Activation Function</H3>
								<DL><p>
									<DT><H3 FOLDED>SwiGLU</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2002.05202">(SwiGLU) GLU Variants Improve Transformer</A>
									</DL><p>
									<DT><H3 FOLDED>GELU</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/1606.08415">[1606.08415] Gaussian Error Linear Units (GELUs)</A>
										<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.GELU.html">GELU — PyTorch 2.3 documentation</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/39853">[proposal] Add approx variant option to F.gelu · Issue #39853 · pytorch/pytorch (approximate=tanh)</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=OmudSvOQhCg">Why do we need activation functions? - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>Inductive Bias</H3>
								<DL><p>
									<DT><H3 FOLDED>No bias</H3>
									<DL><p>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2207.10551">[2207.10551] Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?</A>
								<DT><A HREF="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer (new version)</A>
								<DT><A HREF="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</A>
								<DT><A HREF="https://github.com/Mooler0410/LLMsPracticalGuide">Mooler0410/LLMsPracticalGuide</A>
								<DT><A HREF="https://twitter.com/m__dehghani/status/1524796275052498945">architecture of the model is not actually that important (T5 UL2)</A>
								<DT><A HREF="https://kipp.ly/blog/">Transformers Taxonomy</A>
								<DT><A HREF="https://bbycroft.net/llm">LLM Visualization</A>
								<DT><A HREF="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuits</A>
								<DT><A HREF="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</A>
								<DT><A HREF="https://arxiv.org/abs/2305.19370">[2305.19370] Blockwise Parallel Transformer for Long Context Large Models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=1aXOXHA7Jcw&t=3032s">Greg Yang | Large N Limits: Random Matrices &amp; Neural Networks</A>
								<DT><A HREF="https://www.youtube.com/watch?v=KV5gbOmHbjU">A Mathematical Framework for Transformer Circuits</A>
								<DT><A HREF="https://www.youtube.com/watch?v=P2LTAUO1TdA">Change of basis | Chapter 13, Essence of linear algebra</A>
								<DT><A HREF="https://www.youtube.com/watch?v=PnWOeIgl3GA">Language Modeling with Reduced Densities</A>
								<DT><A HREF="https://atcold.github.io/NYU-DLSP21/en/week10/10-3/">Transformer Encoder-predictor-decoder architecture (NYU)</A>
								<DT><A HREF="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer – Jay Alammar</A>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1651927473884655616">(Yi Tay): Clarifying misconceptions about architectures (training obj)</A>
								<DT><A HREF="https://gist.github.com/sbmaruf/4832bd9a7f66df3c8bab8ab913a7a31a">Architecture config mapping vs T5 Table 2</A>
								<DT><A HREF="https://x.com/andrewgwils/status/1800532164418867245">Andrew Gordon Wilson en X: "Another major barrier is hypers -- initializations, LR, etc. You could easily try a new structure and not realize why it fails. The naive hypers just don’t work. But adapting the great work of @TheGregYang on muP to structure-aware initialization, we achieve exciting results! 4/8 https://t.co/NucLouwFIF" / X</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-training-objectives</H3>
							<DL><p>
								<DT><H3 FOLDED>Causal</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/haileysch__/status/1691483230761857024">Causal &gt;&gt; UL2</A>
								</DL><p>
								<DT><H3 FOLDED>Non-Causal</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>UL2</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/Megatron-LM/pull/268">Megatron-LM: Add UL2 data sampling and pretraining</A>
								</DL><p>
								<DT><H3 FOLDED>Self-Play</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2312.06585.pdf">Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</A>
									<DT><A HREF="https://github.com/lucidrains/ReST-EM-pytorch">lucidrains/ReST-EM-pytorch: Implementations and explorations into the ReST𝐸𝑀 algorithm in the new deepmind paper "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"</A>
								</DL><p>
								<DT><H3 FOLDED>Self-Training</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2312.06585.pdf">Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models</A>
								</DL><p>
								<DT><H3 FOLDED>Generative Information Retrieval</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/presentation/d/19lAeVzPkh20Ly855tKDkz1uv-1pHV_9GxfntiTJPUug/edit#slide=id.g22efd0a58a4_2_0">Google DeepMind (SIGIR 2023 keynote)</A>
								</DL><p>
								<DT><H3 FOLDED>Fill In The Middle</H3>
								<DL><p>
									<DT><A HREF="https://openai.com/blog/gpt-3-edit-insert/">New GPT-3 Capabilities: Edit &amp; Insert</A>
									<DT><A HREF="https://beta.openai.com/playground/p/default-translate-code?mode=edit&model=code-davinci-edit-001">Playground - OpenAI API</A>
									<DT><A HREF="https://community.openai.com/t/introducing-insert-and-edits-capabilities/15993">Introducing Insert and Edits Capabilities - Announcements - OpenAI API Community Forum</A>
									<DT><A HREF="https://github.com/cloneofsimo/fim-llama-deepspeed">cloneofsimo/fim-llama-deepspeed</A>
								</DL><p>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1651927473884655616">(Yi Tay): Clarifying misconceptions about architectures (training obj)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=YEUclZdj_Sc">Why next-token prediction is enough for AGI - Ilya Sutskever</A>
								<DT><A HREF="https://kexue.fm/archives/9797">EMO: Classification loss function designed based on optimal transmission</A>
								<DT><A HREF="https://twitter.com/haileysch__/status/1691483230761857024">Causal &gt;&gt; UL2</A>
								<DT><A HREF="https://github.com/NVIDIA/Megatron-LM/pull/268">Megatron-LM: Add UL2 data sampling and pretraining</A>
								<DT><A HREF="https://github.com/lucidrains/ReST-EM-pytorch">lucidrains/ReST-EM-pytorch: Implementations and explorations into the ReST𝐸𝑀 algorithm in the new deepmind paper "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models"</A>
								<DT><A HREF="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5">OLMo-7B | OLMo-7B – Weights &amp; Biases</A>
								<DT><A HREF="https://x.com/cloneofsimo/status/1792376397278953493/photo/1">3 days in, I see gradient norm sloooowly increasing (ref Olmo)</A>
								<DT><A HREF="https://arxiv.org/abs/2309.14322">[2309.14322] Small-scale proxies for large-scale Transformer training instabilities</A>
							</DL><p>
							<DT><H3 FOLDED>training-details</H3>
							<DL><p>
								<DT><H3 FOLDED>Vocabulary</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2204.14268.pdf">SetencePiece 256K Vocabulary</A>
									<DT><A HREF="https://x.com/karpathy/status/1621578354024677377?lang=en">Andrej Karpathy: increase vocab size from 50257 to 50304 (nearest multiple of 64) Careful with your powers of 2</A>
									<DT><A HREF="https://arxiv.org/abs/1909.08053">[1909.08053] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (Section 5.1)</A>
									<DT><A HREF="https://x.com/abhi_venigalla/status/1621710130051190784">MosaicML: GPT-1.3B MFU went from 49% -&gt; 53%</A>
									<DT><A HREF="https://x.com/cHHillee/status/1630274804795445248">Horace He en X: "Recently, Karpathy tweeted that *increasing* the size of his matmul made it run faster. But... why? Many people seem content to leave this as black magic. But luckily, this *can* be understood! Here's a plot of FLOPs achieved for square matmuls. Let's explain each curve! 1/19 https://t.co/9xLVzswVmv" / X</A>
									<DT><A HREF="https://gist.github.com/Chillee/f86675147366a7a0c6e244eaa78660f7#file-4-matmul-bench-py">PT 2.0 Benchmarks</A>
									<DT><A HREF="https://www.thonking.ai/p/answer-key-what-shapes-do-matrix">Solutions: What Shapes Do Matrix Multiplications Like?</A>
									<DT><A HREF="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications">What Shapes Do Matrix Multiplications Like? [medium]</A>
								</DL><p>
								<DT><H3 FOLDED>Bitwise determinism</H3>
								<DL><p>
									<DT><A HREF="https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html">Stanford CRFM</A>
									<DT><A HREF="https://arxiv.org/pdf/2204.02311.pdf">PaLM: Section 5 "Training Details"</A>
								</DL><p>
								<DT><H3 FOLDED>Weight initialization</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/cloneofsimo/status/1741381460274331916">effective weight init scheme</A>
									<DT><A HREF="https://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling with Pathways (fain-in variance scaling)</A>
									<DT><A HREF="https://thegregyang.com/">Greg Yang | Professional page</A>
									<DT><A HREF="https://cs231n.github.io/neural-networks-2/">CS231n Convolutional Neural Networks for Visual Recognition</A>
								</DL><p>
								<DT><H3 FOLDED>Sequence length</H3>
								<DL><p>
									<DT><H3 FOLDED>RoPE</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2403.00071">[2403.00071] Resonance RoPE: Improving Context Length Generalization of Large Language Models</A>
										<DT><A HREF="https://gist.github.com/cloneofsimo/1adaec43d08f4af936b02ca151b8e1a1">rope visualization</A>
									</DL><p>
									<DT><H3 FOLDED>YaRN</H3>
									<DL><p>
										<DT><A HREF="https://www.modular.com/ai-resources/yarn#:~:text=Perplexity%20Performance%3A%20YaRN%20achieves%20lower,up%20to%20128k%20context%20lengths.">Yarn key concepts</A>
										<DT><A HREF="https://arxiv.org/abs/2309.00071">[2309.00071] YaRN: Efficient Context Window Extension of Large Language Models</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>Batch size</H3>
								<DL><p>
									<DT><A HREF="https://openai.com/index/how-ai-training-scales/">How AI training scales | OpenAI</A>
									<DT><A HREF="https://arxiv.org/pdf/2204.02311.pdf">PALM: progresive larger batch size (GPT-3 style)</A>
									<DT><A HREF="https://arxiv.org/abs/1907.04164">[1907.04164] Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model</A>
									<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1790024970824740949">maxing batchsize and performance</A>
									<DT><A HREF="https://twitter.com/cloneofsimo/status/1789680163133018593">larger batch size leads to more compute-efficient optimization</A>
									<DT><A HREF="https://arxiv.org/abs/1711.00489">[1711.00489] Don't Decay the Learning Rate, Increase the Batch Size</A>
									<DT><A HREF="https://arxiv.org/abs/1812.06162">[1812.06162] An Empirical Model of Large-Batch Training</A>
									<DT><A HREF="https://x.com/cloneofsimo/status/1854498464861933948">An Empirical Model of Large-Batch Training</A>
									<DT><A HREF="https://x.com/SeunghyunSEO7/status/1877188952920125836?t=m1mu7IO4gpotxZrp3hz0AQ&s=31">(1) Seunghyun Seo en X: "The concept of critical batch size is quite simple. Let’s assume we have a training dataset with 1M tokens. If we use a batch size of 10, we can update model param 100,000 times. On the other hand, if we increase the batch size to 100, the step size decreases to 10,000 (1/n)." / X</A>
								</DL><p>
								<DT><H3 FOLDED>Regularization</H3>
								<DL><p>
									<DT><H3 FOLDED>RMS</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/1910.07467">[1910.07467] Root Mean Square Layer Normalization</A>
										<DT><A HREF="https://imbue.com/research/70b-intro/">Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings - imbue</A>
									</DL><p>
									<DT><A HREF="https://kexue.fm/archives/7681">L2 regularization is not as good as expected? It may be caused by "weight scale shift"</A>
								</DL><p>
								<DT><H3 FOLDED>Dropout</H3>
								<DL><p>
									<DT><A HREF="https://kexue.fm/archives/8770">MLM and MAE from the perspective of Dropout: Some new inspirations</A>
									<DT><A HREF="https://arxiv.org/pdf/1912.10095">Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks</A>
									<DT><A HREF="https://arxiv.org/pdf/2303.01500">Dropout Reduces Underfitting</A>
									<DT><A HREF="https://developers.google.com/machine-learning/crash-course/training-neural-networks/best-practices">Training Neural Networks: Best Practices  |  Machine Learning  |  Google for Developers</A>
									<DT><A HREF="https://ramnathkumar181.github.io/Dropout-A-Simple-Way-to-Prevent-Neural-Networks-from-Overfitting/">Dropout- A Simple Way to Prevent Neural Networks from Overfitting – Ramnath Kumar – Predoctoral Researcher, Google DeepMind</A>
									<DT><A HREF="https://eitca.org/artificial-intelligence/eitc-ai-dltf-deep-learning-with-tensorflow/training-a-neural-network-to-play-a-game-with-tensorflow-and-open-ai/training-model/examination-review-training-model/what-is-the-purpose-of-the-dropout-process-in-the-fully-connected-layers-of-a-neural-network/">What is the purpose of the dropout process in the fully connected layers of a neural network? - EITCA Academy</A>
								</DL><p>
								<DT><H3 FOLDED>Training Instability</H3>
								<DL><p>
									<DT><A HREF="https://github.com/apple/ml-sigma-reparam">apple/ml-sigma-reparam</A>
									<DT><A HREF="https://twitter.com/Birchlabs/status/1735709582096302243">Stabilizing Transformer Training by Preventing Attention Entropy Collapse</A>
								</DL><p>
								<DT><H3 FOLDED>Gradient accumulation</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2212.14034">[2212.14034] Cramming: Training a Language Model on a Single GPU in One Day</A>
								</DL><p>
								<DT><H3 FOLDED>MFU</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2204.02311">PaLM: B Compute Usage and Environmental Impact (achieved tokens / peak tokens)</A>
									<DT><A HREF="https://arxiv.org/pdf/2304.01433.pdf">[TPUv4] Do peak FLOPS/second predict real performance?</A>
									<DT><A HREF="https://vram.asmirnov.xyz/">VRAM Calculator</A>
									<DT><A HREF="https://arxiv.org/abs/2205.05198">[2205.05198] Reducing Activation Recomputation in Large Transformer Models</A>
									<DT><A HREF="https://arxiv.org/pdf/2204.02311.pdf">PALM: 4.1 Training Efficiency</A>
									<DT><A HREF="https://pytorch.org/blog/maximizing-training-throughput/?utm_content=293931524&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Maximizing Training Throughput Using PyTorch FSDP and Torch.compile | PyTorch</A>
									<DT><A HREF="https://x.com/HamelHusain/status/1800315287574847701">managing and debugging GPU HBM</A>
									<DT><A HREF="https://parlance-labs.com/">Parlance Labs: A consultancy focused on LLMs</A>
									<DT><A HREF="https://arxiv.org/pdf/2205.05198">Reducing Activation Recomputation in Large Transformer Models sec 6.3</A>
									<DT><A HREF="https://github.com/cchan/nanoGPT-fp8/blob/master/bench.py#L115">mfu = model.estmiate_mfu(batch_size * 1 * num_steps, et)</A>
									<DT><A HREF="https://github.com/cchan/nanoGPT-fp8/blob/3ab2c33a371d684e0fc2c9ff24c0b55ce9ca0389/model.py#L363">nanoGPT: model.py#L363</A>
									<DT><A HREF="https://github.com/Aleph-Alpha/scaling/blob/main/src/scaling/transformer/utils/get_tflops.py">scaling/src/scaling/transformer/utils/get_tflops.py at main · Aleph-Alpha/scaling</A>
								</DL><p>
								<DT><H3 FOLDED>epoch</H3>
								<DL><p>
									<DT><A HREF="https://x.com/Yuchenj_UW/status/1820912632028918094">Is one epoch all you need?</A>
								</DL><p>
								<DT><A HREF="https://kexue.fm/archives/8620">A brief discussion on Transformer initialization, parameterization and sta</A>
								<DT><A HREF="https://openai.com/research/techniques-for-training-large-neural-networks">Techniques for training large neural networks</A>
								<DT><A HREF="https://www.youtube.com/watch?v=k_HMgpJKBso">LLaMA 2 w/ Thomas Scialom (LLaMA 2 lead) - YouTube</A>
								<DT><A HREF="https://github.com/stas00/ml-engineering/blob/master/insights/ai-battlefield.md#ml-engineers-heaven-and-hell">stas00: The AI Battlefield Engineering - What You Need To Know</A>
								<DT><A HREF="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf">metaseq/projects/OPT/chronicles/OPT175B_Logbook</A>
								<DT><A HREF="https://blog.replit.com/llm-training">Replit - How to train your own Large Language Models</A>
								<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/README.md">DeepLearningExamples/README.md at master</A>
								<DT><A HREF="https://jingfengyang.github.io/gpt">Why did all of the public reproduction of GPT-3 fail?</A>
								<DT><A HREF="https://ai.google/static/documents/palm2techreport.pdf">PaLM 2 Technical Report</A>
								<DT><A HREF="http://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks (Andrew Karphathy)</A>
								<DT><A HREF="https://scontent.fsvq5-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=sv_RQgqDkdAAX_BgWRs&_nc_ht=scontent.fsvq5-1.fna&oh=00_AfA6QXt9YOU7MOECaRK3gKak7CECYE6GJZ6SQn6y_aji5Q&oe=65A5C63F">Llama 2 (Table 2): training duration (hours)</A>
								<DT><A HREF="https://news.ycombinator.com/item?id=38222277">Google Cloud TPU Multislice Training | Hacker News</A>
								<DT><A HREF="http://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks (Andrew Karphathy) DATA</A>
								<DT><A HREF="https://console.cloud.google.com/storage/browser/_details/madlad-400-checkpoints/checkpoints/10b-mt/10b-mt.gin;tab=live_object">madlad-400...0b-mt.gin (training and model config file)</A>
								<DT><A HREF="https://github.com/axboe/fio">axboe/fio: Flexible I/O Tester</A>
								<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/storage">Filesystems and IO</A>
								<DT><A HREF="https://github.com/zhengzangw/awesome-huge-models">A collection of details things about LLMs</A>
								<DT><A HREF="https://arxiv.org/abs/1907.04164">[1907.04164] Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model</A>
								<DT><A HREF="https://blog.eleuther.ai/nyt-yi-34b-response/">Yi-34B, Llama 2, and common practices in LLM training: a fact check of the New York Times | EleutherAI Blog</A>
								<DT><A HREF="https://x.com/leilavclark/status/1805700631199642094">From bare metal to a 70B model: infrastructure set-up and scripts</A>
								<DT><A HREF="https://www.lesswrong.com/posts/2JJtxitp6nqu6ffak/basic-facts-about-language-models-during-training-1">Basic facts about language models during training — LessWrong</A>
								<DT><A HREF="https://arxiv.org/abs/2407.02783">[2407.02783] 52B to 1T: Lessons Learned via Tele-FLM Series</A>
								<DT><A HREF="https://docs.google.com/spreadsheets/d/14vbBbuRMEHoqeuMHkTfw3uiZVmyXNuoSp8s-aHvfvZk/edit?gid=0#gid=0">Common LLM Settings - Google Sheets</A>
							</DL><p>
							<DT><H3 FOLDED>training-scaling</H3>
							<DL><p>
								<DT><H3 FOLDED>scaling-blueprint</H3>
								<DL><p>
									<DT><A HREF="https://github.com/cloneofsimo/scaling-guide">cloneofsimo/scaling-guide</A>
									<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
									<DT><A HREF="https://github.com/jxbz/modula">jxbz/modula: Scalable neural net training via automatic normalization in the modular norm.</A>
									<DT><A HREF="https://jeremybernste.in/modula/bad-scaling/">Bad scaling - Modula documentation</A>
									<DT><A HREF="https://jeremybernste.in/modula/golden-rules/">Golden rules for scaling - Modula documentation</A>
									<DT><A HREF="https://aleph-alpha.com/open-source-codebase-scaling-for-non-commercial-research/">Open-sourcing Codebase Scaling for Non-commercial Research - ALEPH ALPHA - AI for Enterprises and Governments</A>
									<DT><A HREF="https://aleph-alpha.com/de/introducing-pharia-1-llm-transparent-and-compliant/">Introducing Pharia-1-LLM: transparent and compliant</A>
									<DT><A HREF="https://github.com/Aleph-Alpha/scaling">Aleph-Alpha/scaling: Scaling is a distributed training library and installable dependency designed to scale up neural networks, with a dedicated module for training large language models.</A>
									<DT><A HREF="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training">Prime Intellect</A>
									<DT><A HREF="https://arxiv.org/abs/2311.05610">[2311.05610] Efficient Parallelization Layouts for Large-Scale Distributed Model Training</A>
									<DT><A HREF="https://x.com/cloneofsimo/status/1831875356116316193">Simo Ryu en X: "This is good research question and I think there are really promising three alternative axis of scaling (other than number of parameters and data) one is objective: one can scale the number of task objectives. For example, it seems to be the case that predicting multiple token" / X</A>
									<DT><A HREF="https://x.com/GCResearchTeam/status/1864322846777897103">super weights", context-parallelism, scaling laws for precision and critical batch sizes.</A>
									<DT><A HREF="https://apollo-lmms.github.io/">Apollo: An Exploration of Video Understanding in Large Multimodal Models</A>
									<DT><A HREF="https://x.com/SeunghyunSEO7/status/1877188952920125836?t=m1mu7IO4gpotxZrp3hz0AQ&s=31">(1) Seunghyun Seo en X: "The concept of critical batch size is quite simple. Let’s assume we have a training dataset with 1M tokens. If we use a batch size of 10, we can update model param 100,000 times. On the other hand, if we increase the batch size to 100, the step size decreases to 10,000 (1/n)." / X</A>
									<DT><A HREF="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook - a Hugging Face Space by nanotron</A>
									<DT><A HREF="https://github.com/GHGmc2/scaling-book">GHGmc2/scaling-book: Home for "How To Scale Your Model", a short blog-style textbook about scaling LLMs on TPUs</A>
									<DT><A HREF="https://jax-ml.github.io/scaling-book/">How To Scale Your Model</A>
								</DL><p>
								<DT><H3 FOLDED>Scaling laws</H3>
								<DL><p>
									<DT><H3 FOLDED>Chinchilla</H3>
									<DL><p>
										<DT><H3 FOLDED>chinchilla-trap</H3>
										<DL><p>
											<DT><A HREF="https://ai.meta.com/blog/meta-llama-3/">model performance continues to improve even after the model is trained on two orders of magnitude more data (Scaling up pretraining section)</A>
											<DT><A HREF="https://www.databricks.com/blog/how-long-should-you-train-your-language-model">How Long Should You Train Your Language Model? | Databricks Blog</A>
										</DL><p>
										<DT><A HREF="https://arxiv.org/abs/2404.10102">[2404.10102] Chinchilla Scaling: A replication attempt</A>
										<DT><A HREF="https://twitter.com/akbirthko/status/1782286554985107631">how good a text compressor we can make for a fixed compute budget</A>
										<DT><A HREF="https://github.com/kyo-takano/chinchilla">kyo-takano/chinchilla: A toolkit for scaling law research ⚖</A>
										<DT><A HREF="https://www.cerebras.net/model-lab/">Model Lab - Cerebras</A>
									</DL><p>
									<DT><A HREF="https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training">An empirical analysis of compute-optimal large language model training</A>
									<DT><A HREF="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval">Gopher, ethical considerations, and retrieval</A>
									<DT><A HREF="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</A>
									<DT><A HREF="https://twitter.com/ZeyuanAllenZhu/status/1777513016592040248">Physics of Language Models: Knowledge Capacity Scaling Laws (Meta AI)</A>
									<DT><A HREF="https://arxiv.org/abs/2404.05405">[2404.05405] Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws</A>
									<DT><A HREF="https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications">Chinchilla's Scaling Laws</A>
									<DT><A HREF="https://irhum.github.io/blog/chinchilla/">Thoughts on Chinchilla (irhum's blog)</A>
									<DT><A HREF="https://thegregyang.com/">Tensor Programs</A>
									<DT><A HREF="https://arxiv.org/abs/2310.02244">Tensor Programs VI: Feature Learning in Infinite-Depth NN</A>
									<DT><A HREF="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws For Neural Languag Models</A>
									<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
									<DT><A HREF="https://arxiv.org/abs/1712.00409">[1712.00409] Deep Learning Scaling is Predictable, Empirically</A>
									<DT><A HREF="https://arxiv.org/abs/2010.14701">[2010.14701] Scaling Laws for Autoregressive Generative Modeling</A>
									<DT><A HREF="https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html">Scaling vision transformers to 22 billion parameters (Google)</A>
									<DT><A HREF="https://arxiv.org/pdf/2301.03728.pdf">Scaling Laws For Generative Mixed-Modal Language Models</A>
									<DT><A HREF="https://www.youtube.com/watch?v=0D23NeBjCeQ">A Theory for Emergence of Complex Skills in Language Models</A>
									<DT><A HREF="https://www.youtube.com/watch?v=dbo3kNKPaUA">Large Language Models (in 2023) - YouTube</A>
									<DT><A HREF="https://arxiv.org/pdf/2203.17189.pdf">Scaling Up Models and Data with t5x and seqio (James Bradbury)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=K-cXYoqHxBc">More Is Different for AI - Scaling Up, Emergence</A>
									<DT><A HREF="https://www.youtube.com/watch?v=J4LzMGn6FdM">TinyLlama: An Open-Source Small Language Model (model saturation)</A>
									<DT><A HREF="https://twitter.com/khoomeik/status/1769333863401107602">language model scaling laws appear to be sensitive to data complexity</A>
									<DT><A HREF="https://twitter.com/khoomeik/status/1773248562555486588">scaling laws: gzip compression ratio and model size</A>
									<DT><A HREF="https://twitter.com/ChombaBupe/status/1777352725858029727/photo/2">multimodal scaling laws: sample efficiency</A>
									<DT><A HREF="https://github.com/bethgelab/frequency_determines_performance">Pretraining Concept Frequency Determines Multimodal Model Performance</A>
									<DT><A HREF="https://ai.meta.com/blog/meta-llama-3/">Introducing Meta Llama 3: The most capable openly available LLM to date</A>
									<DT><A HREF="https://arxiv.org/abs/2405.10938">[2405.10938] Observational Scaling Laws and the Predictability of Language Model Performance</A>
									<DT><A HREF="https://www.pnas.org/doi/10.1073/pnas.2311878121">Explaining neural scaling laws | PNAS (2024)</A>
									<DT><A HREF="https://arxiv.org/abs/2207.10551">[2207.10551] Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?</A>
									<DT><A HREF="https://x.com/yaroslavvb/status/1815206242438246709">How surprising are neural scaling laws? simple experiment</A>
								</DL><p>
								<DT><H3 FOLDED>Pretraining Without Attention</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2212.10544">[2212.10544] Pretraining Without Attention</A>
									<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1784696357892063565">(transformers) same parameter count, a wildly different architecture</A>
								</DL><p>
								<DT><A HREF="https://x.com/hwchung27/status/1712209280529727705">A counterintuitive implication of scale: trying to solve a more general version of the problem is an easier way to solve the original problem than directly tackling it</A>
								<DT><A HREF="https://irhum.github.io/blog/chinchilla/">Thoughts on Chinchilla (irhum's blog)</A>
								<DT><A HREF="https://github.com/gregorbachmann/scaling_mlps">gregorbachmann/scaling_mlps</A>
								<DT><A HREF="https://arxiv.org/pdf/2001.08361.pdf">Scaling Laws For Neural Languag Models (Pre-Chinchilla)</A>
								<DT><A HREF="https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training">An empirical analysis of compute-optimal large language model training</A>
								<DT><A HREF="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</A>
								<DT><A HREF="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval">Gopher, ethical considerations, and retrieval</A>
								<DT><A HREF="https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications">Chinchilla's Scaling Laws</A>
								<DT><A HREF="https://thegregyang.com/">Tensor Programs</A>
								<DT><A HREF="https://arxiv.org/abs/2310.02244">[2310.02244] Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks</A>
								<DT><A HREF="https://arxiv.org/pdf/2005.14165.pdf">(Brown, 2020) Language Models are Few-Shot Learners</A>
								<DT><A HREF="https://arxiv.org/abs/1712.00409">[1712.00409] Deep Learning Scaling is Predictable, Empirically</A>
								<DT><A HREF="https://arxiv.org/abs/2010.14701">[2010.14701] Scaling Laws for Autoregressive Generative Modeling</A>
								<DT><A HREF="https://ai.googleblog.com/2023/03/scaling-vision-transformers-to-22.html">Scaling vision transformers to 22 billion parameters (Google)</A>
								<DT><A HREF="https://arxiv.org/pdf/2301.03728.pdf">Scaling Laws For Generative Mixed-Modal Language Models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=0D23NeBjCeQ">A Theory for Emergence of Complex Skills in Language Models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=dbo3kNKPaUA">Large Language Models (in 2023) - YouTube</A>
								<DT><A HREF="https://arxiv.org/pdf/2203.17189.pdf">Scaling Up Models and Data with t5x and seqio (James Bradbury)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=K-cXYoqHxBc">More Is Different for AI - Scaling Up, Emergence</A>
								<DT><A HREF="https://www.youtube.com/watch?v=J4LzMGn6FdM">TinyLlama: An Open-Source Small Language Model (model saturation)</A>
								<DT><A HREF="https://twitter.com/khoomeik/status/1769333863401107602">language model scaling laws appear to be sensitive to data complexity</A>
								<DT><A HREF="https://twitter.com/khoomeik/status/1773248562555486588">scaling laws: gzip compression ratio and model size</A>
								<DT><A HREF="https://transformer-circuits.pub/2024/april-update/index.html#scaling-laws">Scaling Laws for Dictionary Learning</A>
								<DT><A HREF="https://arxiv.org/abs/2404.10102">[2404.10102] Chinchilla Scaling: A replication attempt</A>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1780639257389904013">Chinchilla Scaling: A replication attempt (twitter thread)</A>
								<DT><A HREF="https://github.com/hkust-nlp/llm-compression-intelligence">hkust-nlp/llm-compression-intelligence: Official github repo for the paper "Compression Represents Intelligence Linearly"</A>
							</DL><p>
							<DT><H3 FOLDED>training-efficient</H3>
							<DL><p>
								<DT><H3 FOLDED>Activation Recomputation</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2205.05198">[2205.05198] Reducing Activation Recomputation in Large Transformer Models</A>
								</DL><p>
								<DT><H3 FOLDED>training-checkpointing</H3>
								<DL><p>
									<DT><H3 FOLDED>checkpointing-io</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2101.08734">[2101.08734] Clairvoyant Prefetching for Distributed Machine Learning I/O</A>
										<DT><A HREF="https://github.com/NVIDIA/aistore/blob/master/docs/overview.md">aistore/docs/overview.md at master · NVIDIA/aistore</A>
										<DT><A HREF="https://arxiv.org/abs/2001.01858">[2001.01858] High Performance I/O For Large Scale Deep Learning</A>
										<DT><A HREF="https://www.youtube.com/watch?v=1CdduHa-KgA&t=2186s">File I/O for Game Developers: Past, Present, and Future with C++ - Guy Davidson - CppCon 2023 - YouTube</A>
										<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/storage">Filesystems and IO</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=bfexfASu9h4">Scalable and Efficient AI: From Supercomputers to Smartphones (Microsoft)</A>
									<DT><A HREF="https://pytorch.org/docs/stable/distributed.checkpoint.html">Distributed Checkpoint - torch.distributed.checkpoint — PyTorch 2.1</A>
									<DT><A HREF="https://blog.research.google/2022/09/tensorstore-for-high-performance.html?m=1">TensorStore for High-Performance, Scalable Array Storage</A>
									<DT><A HREF="https://learn.microsoft.com/en-us/azure/machine-learning/reference-checkpoint-performance-for-large-models?view=azureml-api-2&tabs=PYTORCH">Optimize Checkpoint Performance for Large Models - Azure Machine Learning | Microsoft Learn</A>
									<DT><A HREF="https://github.com/libffcv/ffcv">libffcv/ffcv: FFCV: Fast Forward Computer Vision (and other ML workloads!)</A>
									<DT><A HREF="https://github.com/google/orbax">google/orbax: Orbax provides common utility libraries for JAX users.</A>
									<DT><A HREF="https://github.com/ByteDance-Seed/ByteCheckpoint">ByteDance-Seed/ByteCheckpoint: ByteCheckpoint: An Unified Checkpointing Library for LFMs</A>
								</DL><p>
								<DT><A HREF="https://github.com/JeanKaddour/NoTrainNoGain">NoTrainNoGain: Revisiting Efficient Training Algorithms For Transformer-based Language Models</A>
								<DT><A HREF="https://github.com/Lightning-AI/lit-gpt">Lightning-AI/lit-gpt</A>
								<DT><A HREF="https://twitter.com/ZihangDai/status/1281349893873897478/photo/1">Funnel-Transformer</A>
								<DT><A HREF="https://twitter.com/edwardjhu/status/1714283903908020386">μTransfer</A>
								<DT><A HREF="https://twitter.com/itsclivetime/status/1650254229620260864">LLM rules of thumb (Clive Shan)</A>
								<DT><A HREF="https://bettergpt.chat////">Better GPT</A>
							</DL><p>
							<DT><H3 FOLDED>distributed-training</H3>
							<DL><p>
								<DT><H3 FOLDED>model-parallelism-lectures</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=4h7YBUXiCZE">Abstractions for Expressive, Efficient Parallel and Distributed Comp</A>
									<DT><A HREF="https://www.youtube.com/watch?v=xtxxLWZznBI">Demystifying Parallel and Distributed Deep Learning (ETH) (Microsoft)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=0qoUqE695X0">Lecture 12.4 Scaling up (Mixed precision, Data-parallelism, FSDP) - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>layers</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/layers">text-generation-inference/server/text_generation_server/layers at main · huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/layers/mlp.py">text-generation-inference/server/text_generation_server/layers/mlp.py at main · huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/meta-llama/llama3/blob/main/llama/model.py">llama3/llama/model.py at main · meta-llama/llama3</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/tree/main/python/sglang/srt/layers">sglang/python/sglang/srt/layers at main · sgl-project/sglang</A>
								</DL><p>
								<DT><H3 FOLDED>data-parallelism</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2006.15704">[2006.15704] PyTorch Distributed: Experiences on Accelerating Data Parallel Training</A>
									<DT><A HREF="https://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf">GeePS: Scalable deep learning on distributed GPUs with a GPU-specialized parameter server</A>
									<DT><A HREF="https://www.mishalaskin.com/posts/data_parallel">Training Deep Networks with Data Parallelism in Jax</A>
									<DT><A HREF="https://twitter.com/Muennighoff/status/1661895337248686081">How to keep scaling LLMs when data runs out?</A>
									<DT><A HREF="https://github.com/huggingface/datablations">huggingface/datablations: Scaling Data-Constrained Language Models</A>
									<DT><A HREF="https://twitter.com/MishaLaskin/status/1628880374255296512">Training Deep Networks with Data Parallelism in Jax (Twitter thread)</A>
									<DT><A HREF="https://arxiv.org/pdf/1106.5730">Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</A>
									<DT><A HREF="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/scaling/JAX/data_parallel_intro.ipynb#scrollTo=IUTeFZleFojG">data_parallel_intro.ipynb - Colab</A>
								</DL><p>
								<DT><H3 FOLDED>pipeline-parallelism</H3>
								<DL><p>
									<DT><H3 FOLDED>pipeline-parallelism-torch</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=_rC49PeqrWc">PiPPy: Automated Pipeline Parallelism for PyTorch - YouTube</A>
										<DT><A HREF="https://github.com/pytorch/PiPPy">pytorch/PiPPy: Pipeline Parallelism for PyTorch</A>
									</DL><p>
									<DT><H3 FOLDED>GPipe</H3>
									<DL><p>
										<DT><A HREF="https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html">GPipe</A>
										<DT><A HREF="https://www.youtube.com/watch?v=9s2cum25Kkc">GPipe</A>
										<DT><A HREF="https://arxiv.org/abs/1811.06965">[1811.06965] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</A>
									</DL><p>
									<DT><H3 FOLDED>pp-visualization</H3>
									<DL><p>
										<DT><A HREF="https://github.com/Victarry/PP-Schedule-Visualization">Victarry/PP-Schedule-Visualization</A>
									</DL><p>
									<DT><H3 FOLDED>zero-bubble-parallelism</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sail-sg/zero-bubble-pipeline-parallelism">sail-sg/zero-bubble-pipeline-parallelism: Zero Bubble Pipeline Parallelism</A>
										<DT><A HREF="https://arxiv.org/abs/2408.03505">[2408.03505] Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation</A>
									</DL><p>
									<DT><A HREF="https://siboehm.com/articles/22/pipeline-parallel-training">Pipeline-Parallelism: Distributed Training via Model Partitioning</A>
									<DT><A HREF="https://github.com/siboehm/shallowspeed">siboehm/ShallowSpeed: Small scale distributed training of sequential deep learning models, built on Numpy and MPI.</A>
									<DT><A HREF="https://x.com/fly51fly/status/1821526433682063654">[CL] Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation W Feng, Y Chen, S Wang, Y Peng... [Bytedance &amp;amp; Harvard University] (2024) https://t.co/zaxd5Oc2ax - This paper proposes Optimus, a distributed training system to accelerate large-scale</A>
								</DL><p>
								<DT><H3 FOLDED>tensor-parallelism</H3>
								<DL><p>
									<DT><A HREF="https://www.mishalaskin.com/posts/tensor_parallel">Sharding Large Models with Tensor Parallelism</A>
									<DT><A HREF="https://irhum.github.io/blog/pjit/">irhum.github.io - Tensor Parallelism with jax.pjit</A>
									<DT><A HREF="https://twitter.com/__tinygrad__/status/1742365883048284421">Tinygrad's multiGPU tensor sharding</A>
									<DT><A HREF="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html">SPMD: Parallel Evaluation in JAX</A>
									<DT><A HREF="https://blog.eleuther.ai/transformer-math/">Transformer Math 101 | EleutherAI Blog</A>
									<DT><A HREF="https://github.com/pytorch-labs/gpt-fast/blob/main/tp.py#L124">gpt-fast/tp.py at main · pytorch-labs/gpt-fast</A>
									<DT><A HREF="https://github.com/bytedance/flux">bytedance/flux: A fast communication-overlapping library for tensor parallelism on GPUs.</A>
									<DT><A HREF="https://www.determined.ai/blog/tp">Tensor Parallelism in Three Levels of Difficulty | Determined AI</A>
								</DL><p>
								<DT><H3 FOLDED>sequence-parallelism</H3>
								<DL><p>
									<DT><H3 FOLDED>SP-Ulysses</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2309.14509">[2309.14509] DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models</A>
										<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md">DeepSpeed/blogs/deepspeed-ulysses/README.md at master · microsoft/DeepSpeed</A>
										<DT><A HREF="https://www.youtube.com/watch?v=TXoG47b-cSI">DeepSpeed Ulysses: System Optimizations for Enabling Training of Long Sequence Transformer Models - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>SP-Ring</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/experimental/_attention.py">pytorch/torch/distributed/tensor/experimental/_attention.py: _templated_ring_attention</A>
										<DT><A HREF="https://coconut-mode.com/posts/ring-attention/">Ring Attention Explained | Coconut Mode</A>
										<DT><A HREF="https://arxiv.org/abs/2310.01889">[2310.01889] Ring Attention with Blockwise Transformers for Near-Infinite Context</A>
										<DT><A HREF="https://arxiv.org/abs/2410.08368">[2410.08368] ElasticTok: Adaptive Tokenization for Image and Video</A>
										<DT><A HREF="https://arxiv.org/pdf/2402.08268">WORLD MODEL ON MILLION-LENGTH VIDEO AND LANGUAGE WITH BLOCKWISE RINGATTENTION</A>
										<DT><A HREF="https://github.com/haoliuhl/ringattention">haoliuhl/ringattention: Large Context Attention</A>
										<DT><A HREF="https://github.com/lucidrains/ring-attention-pytorch">lucidrains/ring-attention-pytorch: Implementation of 💍 Ring Attention, from Liu et al. at Berkeley AI, in Pytorch</A>
										<DT><A HREF="https://github.com/LargeWorldModel/LWM">LargeWorldModel/LWM: Large World Model -- Modeling Text and Video with Millions Context</A>
										<DT><A HREF="https://github.com/zhuzilin/ring-flash-attention">zhuzilin/ring-flash-attention: Ring attention implementation with flash attention</A>
										<DT><A HREF="https://github.com/gpu-mode/ring-attention">gpu-mode/ring-attention: ring-attention experiments</A>
										<DT><A HREF="https://arxiv.org/pdf/2311.09431">STRIPED ATTENTION: Faster Ring Attention for Causal Transformers</A>
										<DT><A HREF="https://chat.deepseek.com/a/chat/s/b249b54e-af39-45fe-b6ae-40d0186f806d">Sequence Level Parallelism in Transformers</A>
									</DL><p>
									<DT><H3 FOLDED>Dynamic Sequence Parallelism (DSP)</H3>
									<DL><p>
										<DT><A HREF="https://github.com/NUS-HPC-AI-Lab/VideoSys?tab=readme-ov-file">NUS-HPC-AI-Lab/VideoSys: VideoSys: An easy and efficient system for video generation</A>
										<DT><A HREF="https://arxiv.org/abs/2403.10266">[2403.10266] DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers</A>
										<DT><A HREF="https://github.com/NUS-HPC-AI-Lab/VideoSys/blob/master/docs/dsp.md">VideoSys/docs/dsp.md at master · NUS-HPC-AI-Lab/VideoSys</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2405.07719">[2405.07719] USP: A Unified Sequence Parallelism Approach for Long Context Generative AI</A>
									<DT><A HREF="https://github.com/RulinShao/LightSeq">RulinShao/LightSeq: Official repository for LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers</A>
									<DT><A HREF="https://arxiv.org/abs/2411.01783v2">[2411.01783v2] Context Parallelism for Scalable Million-Token Inference</A>
									<DT><A HREF="https://www.harmdevries.com/post/context-length/">In the long (context) run | Harm de Vries</A>
								</DL><p>
								<DT><H3 FOLDED>Megatron-LM</H3>
								<DL><p>
									<DT><H3 FOLDED>megatron-lm-rocm</H3>
									<DL><p>
										<DT><A HREF="https://github.com/TurkuNLP/Megatron-DeepSpeed">TurkuNLP/Megatron-DeepSpeed: Ongoing research training transformer language models at scale, including: BERT &amp; GPT-2</A>
										<DT><A HREF="https://github.com/vosen/ZLUDA">vosen/ZLUDA: CUDA on AMD GPUs</A>
									</DL><p>
									<DT><H3 FOLDED>NeMO</H3>
									<DL><p>
										<DT><A HREF="https://github.com/NVIDIA/NeMo">NVIDIA/NeMo: NeMo: a toolkit for conversational AI</A>
										<DT><A HREF="https://docs.nvidia.com/nemo-framework/user-guide/latest/multimodalmodels/text2image/imagen/index.html">Imagen — NVIDIA NeMo Framework User Guide latest documentation</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/1909.08053">[1909.08053] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</A>
									<DT><A HREF="https://arxiv.org/abs/2104.04473">[2104.04473] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</A>
									<DT><A HREF="https://github.com/alibaba/Megatron-LLaMA">alibaba/Megatron-LLaMA: Best practice for training LLaMA models in Megatron-LM</A>
									<DT><A HREF="https://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/353/files#diff-df547fcaa58ea4cb2f31099e838dd94f10c92fbb14a5111ed4ffced4ee5f0c4e">Pre-train UL2</A>
									<DT><A HREF="https://twitter.com/YangYou1991/status/1770334665678970882">Dynamic Sequence Parallelism: parallel axis</A>
									<DT><A HREF="https://huggingface.co/spaces/BBuf/megatron-lm-parallel-group-playground">Megatron Lm Parallel Group Playground - a Hugging Face Space by BBuf</A>
									<DT><A HREF="http://giantpandacv.com/project/PyTorch/AI%20Infra%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8A%E5%9C%A8LLM%E8%AE%AD%E7%BB%83%E4%B8%AD%E5%87%8F%E5%B0%91%E6%BF%80%E6%B4%BB%E5%80%BC%E5%86%85%E5%AD%98%E3%80%8B/">AI Infra paper reading: "Reducing activation value memory in LLM training"</A>
									<DT><A HREF="https://github.com/EleutherAI/cookbook/blob/main/benchmarks/sizing/megatron_wrapper.py#L102">cookbook/benchmarks/sizing/megatron_wrapper.py at main · EleutherAI/cookbook</A>
									<DT><A HREF="https://strint.notion.site/Megatron-LM-86381cfe51184b9c888be10ee82f3812">Megatron-LM 的分布式执行调研：原理和最佳实践</A>
									<DT><A HREF="https://www.youtube.com/watch?v=ImKyR1tsPPE">ML Scalability &amp; Performance Reading Group Session 8: Megatron-LM - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>DeepSpeed</H3>
								<DL><p>
									<DT><H3 FOLDED>ZeRO-stage-1</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>ZeRO-stage-2</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>ZeRO-stage-3</H3>
									<DL><p>
										<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/training/model-parallelism">ml-engineering/training/model-parallelism at master · stas00/ml-engineering</A>
									</DL><p>
									<DT><H3 FOLDED>ZeRO-infinity</H3>
									<DL><p>
										<DT><H3 FOLDED>Videos</H3>
										<DL><p>
											<DT><A HREF="https://www.youtube.com/watch?v=smDC_mOGQ5U">[REFAI Seminar 03/30/23] Efficient Trillion Parameter Scale Training</A>
											<DT><A HREF="https://www.youtube.com/watch?v=YEdtLCjSZFY">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale</A>
											<DT><A HREF="https://www.youtube.com/watch?v=7IposV4_LY4">Unleashing the Power of BLOOM 176B with AWS</A>
										</DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2104.07857.pdf">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</A>
										<DT><A HREF="https://dl.acm.org/doi/10.1145/3458817.3476205">ZeRO-infinity | Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</A>
										<DT><A HREF="https://arxiv.org/pdf/2306.09782.pdf">LOMO: Heterogeneous Training System (ZeRO-Infinity)</A>
										<DT><A HREF="https://sliu583.gitbook.io/blog/specific-work/shivarams-group/group-papers/lists/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training">ZeRO-Infinity and DeepSpeed: Unlocking unprecedented model</A>
										<DT><A HREF="https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html">TRAIN 1 TRILLION+ PARAMETER MODELS</A>
										<DT><A HREF="https://www.amazon.science/blog/scaling-to-trillion-parameter-model-training-on-aws">Scaling to trillion-parameter model training on AWS</A>
									</DL><p>
									<DT><H3 FOLDED>DeepSpeed-domino</H3>
									<DL><p>
										<DT><A HREF="https://x.com/MSFTDeepSpeed/status/1861171902506549532">DeepSpeed en X: "Introducing Domino: a novel zero-cost communication tensor parallelism (TP) training engine for both single node and multi-node settings. - Near-complete communication hiding - Novel multi-node scalable TP solution Blog: https://t.co/08bPanyr9M https://t.co/KudHdIs3aY" / X</A>
										<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-domino/README.md">DeepSpeed/blogs/deepspeed-domino/README.md at master · microsoft/DeepSpeed</A>
										<DT><A HREF="https://arxiv.org/abs/2409.15241">[2409.15241] Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping</A>
									</DL><p>
									<DT><H3 FOLDED>deepspeed-config</H3>
									<DL><p>
										<DT><A HREF="https://www.deepspeed.ai/docs/config-json/">DeepSpeed Configuration JSON - DeepSpeed</A>
									</DL><p>
									<DT><H3 FOLDED>deepspeed-profiler</H3>
									<DL><p>
										<DT><A HREF="https://github.com/google/paxml/issues/65">[Question] Very low MFU(30%~35%) when train bf16 Llama2 and GPT model with single SXM4 A100 machine. · Issue #65 · google/paxml</A>
									</DL><p>
									<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt">cloneofsimo/min-max-gpt: minGPT that scales</A>
									<DT><A HREF="https://arxiv.org/abs/1910.02054">[1910.02054] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</A>
									<DT><A HREF="https://github.com/microsoft/Megatron-DeepSpeed">microsoft/Megatron-DeepSpeed</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/main/main_classes/deepspeed#deepspeed-trainer-integration">DeepSpeed Integration</A>
									<DT><A HREF="https://www.youtube.com/watch?v=wbG2ZEDPIyw">Microsoft DeepSpeed introduction at KAUST</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/824f2ffd5124d24935811bc738ed903796ab13ac/benchmark/deepspeed/benchmark_gpt2.py#L86">alpa/benchmark/deepspeed/benchmark_gpt2.py</A>
									<DT><A HREF="https://github.com/cloneofsimo/reverse_eng_deepspeed_study">cloneofsimo/reverse_eng_deepspeed_study: DeepSpeed Study</A>
									<DT><A HREF="https://mp.weixin.qq.com/s/8F3eAHDBjQkHHBmrAEoOfw">Illustrated explanation of large model training: Data parallelism, Part 2 (ZeRO, zero redundancy optimization)</A>
									<DT><A HREF="https://nn.labml.ai/scaling/zero3/index.html">Zero-DP Memory Optimization</A>
									<DT><A HREF="https://github.com/cloneofsimo/min-fsdp/tree/main/journey/understanding_zero">min-fsdp/journey/understanding_zero at main · cloneofsimo/min-fsdp</A>
									<DT><A HREF="https://github.com/intelligent-machine-learning/dlrover">intelligent-machine-learning/dlrover: DLRover: An Automatic Distributed Deep Learning System</A>
								</DL><p>
								<DT><H3 FOLDED>GSPMD</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2105.04663.pdf">General and Scalable Parallellization for ML Computation Graphs</A>
								</DL><p>
								<DT><H3 FOLDED>Collective operation</H3>
								<DL><p>
									<DT><A HREF="https://en.wikipedia.org/wiki/Collective_operation">Collective operation - Wikipedia</A>
									<DT><A HREF="https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/">MPI Scatter, Gather, and Allgather · MPI Tutorial</A>
								</DL><p>
								<DT><H3 FOLDED>automate distributed training</H3>
								<DL><p>
									<DT><H3 FOLDED>Alpa</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/GoogleAI/status/1521938108568154112">Google AI en Twitter: "Alpa is a framework that uses just one line of code to easily automate the complex model parallelism process for large #DeepLearning models. Learn more and check out the code. https://t.co/xFfW5tml9v https://t.co/qYIhHnzwSG" / Twitter</A>
										<DT><A HREF="https://ai.googleblog.com/2022/05/alpa-automated-model-parallel-deep.html">Alpa: Automated Model-Parallel Deep Learning – Google AI Blog</A>
										<DT><A HREF="https://github.com/alpa-projects/alpa">alpa-projects/alpa: Training and serving large-scale neural networks</A>
										<DT><A HREF="https://www.youtube.com/watch?v=y1NXHjcl6V0">Alpa: Automated Model-Parallel Deep Learning - Zhuohan Li | Stanford MLSys #59 - YouTube</A>
										<DT><A HREF="https://github.com/alpa-projects/mms">alpa-projects/mms: Multi model serving</A>
										<DT><A HREF="https://www.youtube.com/watch?v=oVC3SB3GqrI">OSDI '22 - Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>redco</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tanyuqian/redco">tanyuqian/redco: NAACL '24 (Best Demo Paper RunnerUp) / MlSys @ NeurIPS '23 - RedCoast: A Lightweight Tool to Automate Distributed Training and Inference</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>activation-recomputation</H3>
								<DL><p>
									<DT><A HREF="https://lilianweng.github.io/posts/2021-09-25-train-large/#mixture-of-experts-moe">How to Train Really Large Models on Many GPUs? | Lil'Log</A>
									<DT><A HREF="https://sumanthrh.com/post/distributed-and-efficient-finetuning/">Everything about Distributed Training and Efficient Finetuning | Sumanth's Personal Website</A>
								</DL><p>
								<DT><H3 FOLDED>t5x + seqio</H3>
								<DL><p>
									<DT><A HREF="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=GprA5UsAAAAJ&sortby=pubdate&citation_for_view=GprA5UsAAAAJ:roLk4NBRz8UC">Scaling Up Models and Data with t5x and seqio</A>
									<DT><A HREF="https://arxiv.org/pdf/2203.17189.pdf">Scaling Up Models and Data with t5x and seqio</A>
									<DT><A HREF="https://twitter.com/rasbt/status/1638538494887821313">(1) Sebastian Raschka en X: ""Meet in the Middle: A New Pre-training Paradigm" for large language models (LLM). In this paper, the authors propose to develop a bidirectional LLM using the full sequence information during pretraining and using context from both sides during inference. https://t.co/FHY4Vof90I... https://t.co/QFxtKO3iJb" / X</A>
									<DT><A HREF="https://twitter.com/rasbt/status/1638538494887821313">Meet in the Middle: A New Pre-training Paradigm</A>
									<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x</A>
								</DL><p>
								<DT><H3 FOLDED>training-distributed-parallelism-heterogenous</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2206.01288v1">Decentralized Training of Foundation Models in Heterogeneous Environments</A>
								</DL><p>
								<DT><H3 FOLDED>DisTrO</H3>
								<DL><p>
									<DT><A HREF="https://x.com/NousResearch/status/1863622813317464157">Nous Research en X: "Nous Research announces the pre-training of a 15B parameter language model over the internet, using Nous DisTrO and heterogeneous hardware contributed by our partners at @Oracle, @LambdaAPI, @NorthernDataGrp, @CrusoeCloud, and the Andromeda Cluster. This run presents a loss https://t.co/BAcyoSScGS" / X</A>
									<DT><A HREF="https://arxiv.org/abs/2411.19870">[2411.19870] DeMo: Decoupled Momentum Optimization</A>
									<DT><A HREF="https://github.com/bloc97/DeMo">bloc97/DeMo: DeMo: Decoupled Momentum Optimization</A>
								</DL><p>
								<DT><H3 FOLDED>DiLoCo</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2311.08105">[2311.08105] DiLoCo: Distributed Low-Communication Training of Language Models</A>
									<DT><A HREF="https://arxiv.org/pdf/2407.07852">https://arxiv.org/pdf/2407.07852</A>
									<DT><A HREF="https://arxiv.org/abs/2407.07852">[2407.07852] OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training</A>
									<DT><A HREF="https://github.com/PrimeIntellect-ai/OpenDiloco?tab=readme-ov-file">PrimeIntellect-ai/OpenDiloco: OpenDiLoCo: An Open-Source Framework for Globally Distributed Low-Communication Training</A>
									<DT><A HREF="https://github.com/PrimeIntellect-ai/diloco_simple">PrimeIntellect-ai/diloco_simple: torch implementation of diloco</A>
									<DT><A HREF="https://www.primeintellect.ai/blog/our-approach-to-decentralized-training">Prime Intellect</A>
									<DT><A HREF="https://x.com/NousResearch/status/1828121648383566270">(1) Nous Research en X: "What if you could use all the computing power in the world to train a shared, open source AI model? Preliminary report: https://t.co/b1XgJylsnV Nous Research is proud to release a preliminary report on DisTrO (Distributed Training Over-the-Internet) a family of https://t.co/h2gQJ4m7lB" / X</A>
									<DT><A HREF="https://x.com/Ar_Douillard/status/1836706696086847779">(1) Arthur Douillard en X: "So DisTrO is a kind of a ElasticSGD (https://t.co/Y7pbei6zIf) / PAPA (https://t.co/JndN7kIyl7)?" / X</A>
									<DT><A HREF="https://x.com/nearcyan/status/1836598231415054705">DisTrO mathemathical expression</A>
									<DT><A HREF="https://arxiv.org/abs/2405.17517">[2405.17517] WASH: Train your Ensemble with Communication-Efficient Weight Shuffling, then Average</A>
									<DT><A HREF="https://x.com/mlia_isir">(1) MLIA (@mlia_isir) / X</A>
									<DT><A HREF="https://github.com/NousResearch/DisTrO">NousResearch/DisTrO: Distributed Training Over-The-Internet</A>
								</DL><p>
								<DT><A HREF="https://lilianweng.github.io/posts/2021-09-25-train-large/">How to Train Really Large Models on Many GPUs? | Lil'Log</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-uyXE7dY5H0">NIPS: Oral Session 4 - Ilya Sutskever - YouTube</A>
								<DT><A HREF="https://imbue.com/research/70b-infrastructure/">From bare metal to a 70B model: infrastructure set-up and scripts - imbue</A>
								<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/training/model-parallelism">ml-engineering/training/model-parallelism at master · stas00/ml-engineering</A>
								<DT><A HREF="https://arxiv.org/abs/2311.08105">[2311.08105] DiLoCo: Distributed Low-Communication Training of Language Models</A>
								<DT><A HREF="https://sumanthrh.com/post/distributed-and-efficient-finetuning/">Everything about Distributed Training and Efficient Finetuning</A>
								<DT><A HREF="https://www.databricks.com/blog/mosaic-ai-training-capabilities">Building DBRX-class Custom LLMs with Mosaic AI Training | Databricks Blog</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1803941734629388590">HSDP, GPU sharding and node recover (Mihir Patel)</A>
								<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
								<DT><A HREF="https://github.com/siboehm/shallowspeed">siboehm/ShallowSpeed: Small scale distributed training of sequential deep learning models, built on Numpy and MPI.</A>
								<DT><A HREF="https://github.com/FZJ-JSC/tutorial-multi-gpu">FZJ-JSC/tutorial-multi-gpu: Efficient Distributed GPU Programming for Exascale, an SC/ISC Tutorial</A>
								<DT><A HREF="https://arxiv.org/abs/2205.01068">[2205.01068] OPT: Open Pre-trained Transformer Language Models</A>
								<DT><A HREF="https://arxiv.org/abs/2203.12533">[2203.12533] Pathways: Asynchronous Distributed Dataflow for ML</A>
								<DT><A HREF="https://pytorch.org/docs/stable/distributed.html#distributed-key-value-store">Distributed communication package - torch.distributed</A>
								<DT><A HREF="https://github.com/stanford-crfm/levanter">stanford-crfm/levanter</A>
								<DT><A HREF="https://arxiv.org/abs/2204.06745">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</A>
								<DT><A HREF="https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e">the world’s largest distributed LLM training job on TPU v5e</A>
								<DT><A HREF="https://arxiv.org/abs/2303.06318">[2303.06318] A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training</A>
								<DT><A HREF="http://giantpandacv.com/project/PyTorch/AI%20Infra%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E4%B9%8B%E3%80%8A%E5%9C%A8LLM%E8%AE%AD%E7%BB%83%E4%B8%AD%E5%87%8F%E5%B0%91%E6%BF%80%E6%B4%BB%E5%80%BC%E5%86%85%E5%AD%98%E3%80%8B/">AI Infra paper reading: "Reducing activation value memory in LLM training"</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1805629542914211951">Imbue blogs</A>
								<DT><A HREF="https://github.com/imbue-ai/cluster-health">imbue-ai/cluster-health</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1803446449796989021">Nemotron-340B was trained on 768x8 H100: visualization</A>
								<DT><A HREF="https://twitter.com/dlwh/status/1677923909763747840">JAX/Haliax/Levanter</A>
								<DT><A HREF="https://colab.research.google.com/drive/18_BrtDpe1lu89M4T6fKzda8DdSLtFJhi">Tensor Parallelism in Haliax - Colaboratory</A>
								<DT><A HREF="https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf">TensorFlow: A System for Large-Scale Machine Learning</A>
								<DT><A HREF="https://arxiv.org/abs/2204.06514">[2204.06514] Scalable Training of Language Models using JAX pjit and TPUv4</A>
								<DT><A HREF="https://arxiv.org/abs/1904.00962">[1904.00962] Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</A>
								<DT><A HREF="https://github.com/stanford-crfm/levanter">stanford-crfm/levanter: Legibile, Scalable, Reproducible Foundation Models with Named Tensors and Jax</A>
								<DT><A HREF="https://github.com/froystig">froystig (Roy Frostig)</A>
								<DT><A HREF="https://x.com/awnihannun/status/1815516199876452393">How to shard your LLM</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Hr2FWHBuNXs">L12b Parallelization -- Instructor: Wilson Yan - YouTube</A>
								<DT><A HREF="https://drive.google.com/file/d/1GFL3XMB96EZs5Yg0tbQal8ZEyqHQ58hp/view">L12 Multi-Modal Models (SP24).pdf - Google Drive</A>
								<DT><A HREF="https://arxiv.org/abs/2311.05610">[2311.05610] Efficient Parallelization Layouts for Large-Scale Distributed Model Training</A>
								<DT><A HREF="https://github.com/intelligent-machine-learning/dlrover/blob/master/atorch/README.md">dlrover/atorch/README.md at master · intelligent-machine-learning/dlrover</A>
								<DT><A HREF="https://github.com/AlibabaPAI/torchacc">AlibabaPAI/torchacc: PyTorch distributed training acceleration framework</A>
								<DT><A HREF="https://irrationalanalysis.substack.com/p/a-background-proof-guide-on-communication">A Background-Proof Guide on Communication Systems</A>
								<DT><A HREF="https://main-horse.github.io/posts/visualizing-6d/">Visualizing 6D Mesh Parallelism · main</A>
								<DT><A HREF="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook - a Hugging Face Space by nanotron</A>
								<DT><A HREF="https://arxiv.org/pdf/2303.06318">what are all the ways the ops in a transformer can be parallelized across GPUs + pretty diagrams?</A>
							</DL><p>
							<DT><H3 FOLDED>training-optimizer</H3>
							<DL><p>
								<DT><H3 FOLDED>optimization-hyperparameters</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437/photo/1">Aggressive learning rate</A>
									<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437">The right moment to decay is always as late as possible</A>
									<DT><A HREF="https://twitter.com/borisdayma/status/1489292077313703939">Impact of learning rate</A>
									<DT><A HREF="https://arxiv.org/pdf/1803.02021.pdf">UNDERSTANDING SHORT-HORIZON BIAS IN STOCHASTIC META-OPTIMIZATION (aggresive)</A>
								</DL><p>
								<DT><H3 FOLDED>bf16-optimizer</H3>
								<DL><p>
									<DT><A HREF="https://github.com/imoneoi/bf16_fused_adam">imoneoi/bf16_fused_adam: BFloat16 Fused Adam Operator for PyTorch</A>
									<DT><A HREF="https://github.com/AmericanPresidentJimmyCarter/test-torch-bfloat16-vit-training">AmericanPresidentJimmyCarter/test-torch-bfloat16-vit-training</A>
								</DL><p>
								<DT><H3 FOLDED>Adam</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2304.09871">theory on Adam Instability in Large-Scale Machine Learning</A>
									<DT><A HREF="https://x.com/thecharlieblake/status/1800875303486861735">Adam Per Parameter Memory Usage, Excluding Activations</A>
								</DL><p>
								<DT><H3 FOLDED>Shampo</H3>
								<DL><p>
									<DT><H3 FOLDED>shampo-impl</H3>
									<DL><p>
										<DT><A HREF="https://github.com/google-research/google-research/tree/master/scalable_shampoo">Distributed Shampoo Implementation</A>
										<DT><A HREF="https://github.com/cloneofsimo/zeroshampoo">cloneofsimo/zeroshampoo</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2002.09018.pdf">Scalable Second Order Optimization for Deep Learning (Shampo)</A>
									<DT><A HREF="https://twitter.com/_arohan_/status/1713343013563576540">Rohan Anil en X: "From ICLR, μP 🤝 Shampoo</A>
									<DT><A HREF="https://x.com/jxbz/status/1821610272454537459">Jeremy Bernstein: Okay, executive summary on Shampoo, zeroth matrix powers, stochastic spectral descent and Modula.</A>
									<DT><A HREF="https://x.com/_arohan_/status/1819102492468396315">(1) rohan anil en X: "Distributed Shampoo has dethroned Nesterov Adam marking a new era for deep learning optimization. 👑 🤘 Non-diagonal preconditioning is here! This is the AlexNet moment for optimization for deep learning. I am extremely happy. An email from 2021. https://t.co/iARx990Ji1" / X</A>
									<DT><A HREF="https://rosanneliu.com/dlctfs/dlct_210312.pdf">Scalable Second Order Optimization for Deep Learning</A>
									<DT><A HREF="https://mlcommons.org/2024/08/mlc-algoperf-benchmark-competition/">Announcing the results of the inaugural AlgoPerf: Training Algorithms benchmark competition - MLCommons</A>
									<DT><A HREF="https://arxiv.org/abs/2405.18144">[2405.18144] 4-bit Shampoo for Memory-Efficient Network Training</A>
									<DT><A HREF="https://arxiv.org/abs/2309.06497">[2309.06497] A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale</A>
									<DT><A HREF="https://proceedings.mlr.press/v80/gupta18a">Shampoo: Preconditioned Stochastic Tensor Optimization</A>
									<DT><A HREF="https://arxiv.org/abs/1910.02054">[1910.02054] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</A>
									<DT><A HREF="https://arxiv.org/abs/1909.08053">[1909.08053] Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</A>
									<DT><A HREF="https://x.com/ShamKakade6/status/1836476197968187778">ShampoO with Adam in Preconditioner</A>
								</DL><p>
								<DT><H3 FOLDED>Lion</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/automl/tree/master/lion#language-modeling">Google AutoML: Lion Optimizer over Adam</A>
								</DL><p>
								<DT><H3 FOLDED>Sophia</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/tengyuma/status/1714708089205912004">Sophia</A>
									<DT><A HREF="https://github.com/Liuhong99/Sophia#tuning-the-hyperparameter-rho">Sophia: The official implementation of “Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training”</A>
									<DT><A HREF="https://arxiv.org/abs/2305.14342">[2305.14342] Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</A>
								</DL><p>
								<DT><H3 FOLDED>ADOPT</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2411.02853">[2411.02853] ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal Rate</A>
									<DT><A HREF="https://github.com/iShohei220/adopt">iShohei220/adopt: Official Implementation of "ADOPT: Modified Adam Can Converge with Any β2 with the Optimal Rate"</A>
									<DT><A HREF="https://x.com/ishohei220/status/1859785311808876998">(1) Shohei Taniguchi en X: "**Update on the ADOPT optimizer** To address several reports that ADOPT sometimes gets unstable, a minor modification has been made to the algorithm. We observe that this modification greatly improves stability in many cases. https://t.co/dHuo4Z2GMz" / X</A>
									<DT><A HREF="https://x.com/torchcompiled/status/1865556474501714193">Adam, LaProp &amp; ADOPT updates differences made simple</A>
								</DL><p>
								<DT><H3 FOLDED>MARS</H3>
								<DL><p>
									<DT><A HREF="https://x.com/QuanquanGu/status/1858602326434738621">(2) Quanquan Gu en X: "Today’s the day to launch! Introducing MARS (Make vAriance Reduction Shine): the ultimate LLM optimizer. Let’s unite, innovate, and take our shot at MARS! 🚀🚀🚀 Paper: https://t.co/cjTGJV5yCR Code: https://t.co/azBai8UnSd https://t.co/XrJ9ucjMe1" / X</A>
									<DT><A HREF="https://arxiv.org/pdf/2411.10438">MARS: Unleashing the Power of Variance Reduction for Training Large Models</A>
									<DT><A HREF="https://github.com/AGI-Arena/MARS">AGI-Arena/MARS: The official implementation of MARS: Unleashing the Power of Variance Reduction for Training Large Models</A>
								</DL><p>
								<DT><H3 FOLDED>MUON</H3>
								<DL><p>
									<DT><A HREF="https://x.com/torchcompiled/status/1861177127317708964">(1) Ethan 🇦🇺 en X: "FSDP2 support for MUON has just landed as well! https://t.co/ZnKfxQnbJZ" / X</A>
									<DT><A HREF="https://github.com/ethansmith2000/fsdp_optimizers">ethansmith2000/fsdp_optimizers: supporting pytorch FSDP for optimizers</A>
									<DT><A HREF="https://x.com/rami_mmo/status/1862349005600301444">rami en X: "I ran 80000 simulations and muon (with adam embed/unembed) seems to outperform adam (tuned) when training a DiT model. https://t.co/HYpwmTUONB" / X</A>
									<DT><A HREF="https://kellerjordan.github.io/posts/muon/">Muon: An optimizer for hidden layers in neural networks | Keller Jordan blog</A>
									<DT><A HREF="https://x.com/cloneofsimo/status/1907731069878825400">Adam vs Shampoo vs Muon on MNIST. All follow the lr approx sqrt(BS) law</A>
								</DL><p>
								<DT><H3 FOLDED>SOAP</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ethansmith2000/fsdp_optimizers/blob/main/soap.py">fsdp_optimizers/soap.py at main · ethansmith2000/fsdp_optimizers</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=5exL8UYxpsI&t=63s">IFML SEMINAR: 1/26/24 - Meta Optimization (Google DeepMind)</A>
								<DT><A HREF="https://arxiv.org/pdf/1510.01799.pdf">efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters</A>
								<DT><A HREF="https://arxiv.org/abs/2308.01814">[2308.01814] Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit</A>
								<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437/photo/1">Aggressive learning rate</A>
								<DT><A HREF="https://twitter.com/sytelus/status/1713462676838588565">loss scaling FP16 training</A>
								<DT><A HREF="https://twitter.com/zacharynado/status/1729582361115848789">training algorithm* that is faster** than Adam**</A>
								<DT><A HREF="https://arxiv.org/abs/2310.18313">[2310.18313] FP8-LM: Training FP8 Large Language Models</A>
								<DT><A HREF="https://nn.labml.ai/optimizers/index.html">Optimizers</A>
								<DT><A HREF="https://github.com/SonicCodes/hyperada">SonicCodes/hyperada: Linear hypernetwork ada</A>
								<DT><A HREF="https://github.com/apple/ml-ademamix">apple/ml-ademamix</A>
							</DL><p>
							<DT><H3 FOLDED>training-hyperparameters</H3>
							<DL><p>
								<DT><H3 FOLDED>Maximal Update Parametrization (μP) and Hyperparameter Transfer</H3>
								<DL><p>
									<DT><H3 FOLDED>ezmup</H3>
									<DL><p>
										<DT><A HREF="https://github.com/cloneofsimo/ezmup">cloneofsimo/ezmup: Simple implementation of muP, based on Spectral Condition for Feature Learning</A>
									</DL><p>
									<DT><H3 FOLDED>mup-examples</H3>
									<DL><p>
										<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt/blob/7b017b8a0680e8eec6328c7ea3edfca7592107e0/tweakablegpt.py#L53">tweakablegpt.py#L53 (Simo)</A>
										<DT><A HREF="https://github.com/huggingface/nanotron/blob/03d67f2103d5be0dc15ea6022a6cf16d6a633064/examples/mup/README.md">nanotron/examples/mup/README.md at 03d67f2103d5be0dc15ea6022a6cf16d6a633064 · huggingface/nanotron</A>
										<DT><A HREF="https://wandb.ai/neuralink/exp14_mup_grid_search/reports/-Spectral-Transfer-MLP-s-Experiment-Results--Vmlldzo3NDQ0NTQw?accessToken=xe0mkunx3y8t0xzbzxu9caqcre57or5la58d9o209hinanlmzoaj7es24m4elvdj">[Spectral µTransfer] MLP's Experiment Results | exp14_mup_grid_search – Weights &amp; Biases</A>
										<DT><A HREF="https://graphcore-research.github.io/posts/how-to-scale/">Scale-preserving nonlinearities for u-μP - Graphcore Research Blog</A>
									</DL><p>
									<DT><A HREF="https://twitter.com/cloneofsimo/status/1741381460274331916">muP is effective weight init scheme everyone should use. With ezmup 3 LOC is all you need+ it's model agnostic (Simo)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=1aXOXHA7Jcw&t=3032s">Greg Yang | Large N Limits: Random Matrices &amp; Neural Networks</A>
									<DT><A HREF="https://www.microsoft.com/en-us/research/blog/%C2%B5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/">µtransfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks</A>
									<DT><A HREF="https://arxiv.org/abs/2203.03466">[2203.03466] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer</A>
									<DT><A HREF="https://arxiv.org/abs/2305.19268">[2305.19268] Intriguing Properties of Quantization at Scale (Emergent LLM properties &gt; 6B)</A>
									<DT><A HREF="https://github.com/microsoft/mup">mup</A>
									<DT><A HREF="https://decentdescent.org/tp5.html">Tuning GPT-3 on a Single GPU</A>
									<DT><A HREF="https://github.com/cloneofsimo/ezmup">cloneofsimo/ezmup: Simple implementation of muP, based on Spectral Condition for Feature Learning</A>
									<DT><A HREF="https://www.youtube.com/watch?v=z8-C42mAwBc">μTransfer</A>
									<DT><A HREF="https://blog.speechmatics.com/mup">Reduce Model Tuning Costs with MuP</A>
									<DT><A HREF="https://x.com/CerebrasSystems/status/1796578819400442033">(1) Cerebras en X: "(1/n) Paper drop: https://t.co/fcr3Jr2ckD TLDR: We introduce the sparse maximal update parameterization (SμPar), which ensures optimal HPs remain the same for any width or sparsity level. This dramatically reduces HP tuning costs, allowing SμPar to achieve superior losses. 🧵 👇 https://t.co/K0sY4kOKVT" / X</A>
									<DT><A HREF="https://arxiv.org/abs/2405.15743">[2405.15743] Sparse maximal update parameterization: A holistic approach to sparse training dynamics</A>
									<DT><A HREF="https://x.com/andrewgwils/status/1800532164418867245">Andrew Gordon Wilson en X: "Another major barrier is hypers -- initializations, LR, etc. You could easily try a new structure and not realize why it fails. The naive hypers just don’t work. But adapting the great work of @TheGregYang on muP to structure-aware initialization, we achieve exciting results! 4/8 https://t.co/NucLouwFIF" / X</A>
									<DT><A HREF="https://github.com/imbue-ai/carbs">imbue-ai/carbs: Cost aware hyperparameter tuning algorithm</A>
									<DT><A HREF="https://www.youtube.com/watch?v=XpU3mDKJOak">Greg Yang - "Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer" - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=DuBQCBWcq4M&t=340s">Greg Yang — Feature Learning in Infinite-Width Neural Networks - YouTube</A>
									<DT><A HREF="https://jeremybernste.in/modula/bad-scaling/">Bad scaling - Modula documentation</A>
									<DT><A HREF="https://jeremybernste.in/modula/golden-rules/">Golden rules for scaling - Modula documentation</A>
									<DT><A HREF="https://graphcore-research.github.io/posts/how-to-scale/">Scale-preserving nonlinearities for u-μP - Graphcore Research Blog</A>
									<DT><A HREF="https://arxiv.org/abs/2407.17465">[2407.17465] u-$μ$P: The Unit-Scaled Maximal Update Parametrization</A>
									<DT><A HREF="https://blog.eleuther.ai/mutransfer/">The Practitioner's Guide to the Maximal Update Parameterization | EleutherAI Blog</A>
									<DT><A HREF="https://x.com/edwardjhu">(1) Edward Hu (@edwardjhu) / X</A>
								</DL><p>
								<DT><H3 FOLDED>learning rate</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/Guodzh/status/1489371872777191437">The right moment to decay is always as late as possible</A>
									<DT><A HREF="https://twitter.com/borisdayma/status/1489292077313703939">Impact of learning rate</A>
									<DT><A HREF="https://optax.readthedocs.io/en/latest/api/optimizer_schedules.html#optax.linear_schedule">Optimizer Schedules — Optax documentation</A>
									<DT><A HREF="https://gist.github.com/fabianp/4d44c21fca10cd4dc4bc0d27774fd48e">linear_schedule.ipynb</A>
								</DL><p>
								<DT><H3 FOLDED>clip gradient norm</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>CARBS</H3>
								<DL><p>
									<DT><A HREF="https://imbue.com/research/70b-intro/">Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings - imbue</A>
								</DL><p>
								<DT><H3 FOLDED>Modula</H3>
								<DL><p>
									<DT><A HREF="https://jeremybernste.in/modula/">Modula is a deep learning framework designed for graceful scaling. Neural networks written in Modula automatically transfer learning rate across scale</A>
									<DT><A HREF="https://x.com/jxbz/status/1851328119539429487">The General Theory of Modular Duality</A>
								</DL><p>
								<DT><A HREF="https://twitter.com/cloneofsimo/status/1741381460274331916">muP is effective weight init scheme everyone should use. With ezmup 3 LOC is all you need+ it's model agnostic (Simo)</A>
								<DT><A HREF="https://arxiv.org/abs/2304.05187">[2304.05187] Automatic Gradient Descent: Deep Learning without Hyperparameters</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-uyXE7dY5H0">NIPS: Oral Session 4 - Ilya Sutskever - YouTube: batch_size, learning rate, init scale, norm of gradient is clipped to</A>
							</DL><p>
							<DT><H3 FOLDED>training-scheduler</H3>
							<DL><p>
								<DT><A HREF="https://x.com/_clashluke/status/1813911399049089437/photo/2">Accelerated FusedSMA RMSProp</A>
								<DT><A HREF="https://github.com/ClashLuke/schedule_free">ClashLuke/schedule_free: Schedule-Free Optimization in PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-training-profiling</H3>
							<DL><p>
								<DT><A HREF="https://github.com/bigcode-project/bigcode-analysis/blob/main/multi_query_experiments/attention_types_imp.py">BigCode: Experiment analysis and profiling. Attention Types</A>
								<DT><A HREF="https://mlcommons.org/en/">MLCommons</A>
								<DT><A HREF="https://gist.github.com/jboner/2841832">Latency Numbers Every Programmer Should Know</A>
							</DL><p>
							<DT><H3 FOLDED>training-multilingual</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2205.06266.pdf">Lifting the Curse of Multilinguality by Pre-training Modular Transformers</A>
								<DT><A HREF="https://twitter.com/PfeiffJo/status/1525009949478330368">Lifting the Curse of Miltilinguality by Pre-training Modular Transformers</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ghL31wWlpFY">Massively Multilingual Shallow Fusion with Large Language Model</A>
								<DT><A HREF="https://arxiv.org/abs/2304.09151">[2304.09151] UniMax (mT5)</A>
								<DT><A HREF="https://arxiv.org/abs/2010.11934">[2010.11934] mT5: A massively multilingual pre-trained text-to-text transformer</A>
								<DT><A HREF="https://twitter.com/LucasBandarkar/status/1697650503726125294">Bebele: multilingual capabilities of LLMs</A>
								<DT><A HREF="https://unbabel.com/announcing-tower-an-open-multilingual-llm-for-translation-related-tasks/?utm_campaign=Tower%20Announcement&utm_content=278163588&utm_medium=social&utm_source=linkedin&hss_channel=lcp-3327165">Tower : An Open Multilingual LLM for Translation-Related Tasks</A>
								<DT><A HREF="https://www.microsoft.com/en-us/research/project/llmlingua/">LLMLingua - Microsoft Research</A>
								<DT><A HREF="https://twitter.com/arankomatsuzaki/status/1742369432091976085">Llama Beyond English: An Empirical Study on Language Capability</A>
								<DT><A HREF="https://www.youtube.com/watch?v=f1ZayIAz210">NeurIPS 2023 Oral 2A Efficient Learning - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-training-lectures</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=k_HMgpJKBso">LLaMA 2 w/ Thomas Scialom (LLaMA 2 lead) - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5RUOrXl3nag&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb">OPT-175B: Open Pretrained Transformer</A>
								<DT><A HREF="https://www.youtube.com/watch?v=g0gREwiDbis">Neural Networks Efficient Training Fundamentals</A>
								<DT><A HREF="https://www.youtube.com/watch?v=DK-QXsiycpk&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb&index=2">GPT-NeoX-20B | BigScience BLOOM</A>
								<DT><A HREF="https://www.youtube.com/watch?v=hc0u4avAkuM&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb&index=3">Megatron-LM | ZeRO | DeepSpeed | Mixed Precision</A>
								<DT><A HREF="https://www.youtube.com/watch?v=pTChDs5uD8I&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb&index=4">BigScience BLOOM | 3D Parallelism</A>
								<DT><A HREF="https://www.youtube.com/watch?v=iJ0IVZgGjTM&list=PLBoQnSflObcltw2wSTV1UYMkSgyrRcjcb&index=5">T0: Multitask Prompted Training Enables</A>
								<DT><A HREF="https://www.youtube.com/watch?v=CAbHbm9769Q">Hugging Face: Accelerating Transformers in Production</A>
								<DT><A HREF="https://www.youtube.com/watch?v=jyOqtw4ry2w">8-bit Methods for Efficient Deep Learning with Tim Dettmers</A>
								<DT><A HREF="https://www.youtube.com/watch?v=B3Az2EONCHE">Large Language Models - Michael Douglas</A>
								<DT><A HREF="https://www.youtube.com/watch?v=R_o6nUC1Nzo&list=PLgKuh-lKre11GbZWneln-VZDLHyejO7YD&index=14">Tutorial on Deep Learning II - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=b1c1yh6I594">Math Reading Group - Neural Tangent Kernels (07/05/23)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=XfpMkf4rD6E">Transformers United 2023: Introduction to Transformers</A>
								<DT><A HREF="https://www.youtube.com/watch?v=tfWPCeyh77k">Lessons from scale for large language models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=bZQun8Y4L2A&t=723s">State of GPT | BRK216HFS - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=nUSvsLLlz9U">Tesla CVPR2023 Workshop - YouTube (Data Engine)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=DHjwbleAgPQ">Why do Neural Networks use Linear Algebra?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=tndPB8z98Zg">Machine Learning and Theory Calculations</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ytbYRIN0N4g">Softmax Function Explained In Depth with 3D Visuals</A>
								<DT><A HREF="https://www.youtube.com/watch?v=DZoICV92VGE">The Magic Behind QLORA: Efficient Finetuning of Quantized LLMs</A>
								<DT><A HREF="https://www.youtube.com/watch?v=0QczhVg5HaI&t=558s">Why Neural Networks can learn (almost) anything</A>
								<DT><A HREF="https://www.youtube.com/watch?v=j4yz1k3Ueso">The Power of Symmetry III</A>
								<DT><A HREF="https://www.youtube.com/watch?v=SGInyKjzF7A">Optimizing Large Language Models with Reinforcement Learning</A>
								<DT><A HREF="https://www.youtube.com/watch?v=2Zi6wFQQl3E">Generalization bounds for Neural Network Based Decoders</A>
								<DT><A HREF="https://www.youtube.com/watch?v=g0gREwiDbis">SLXCA 2021 featuring Dr. Geoffrey Hinton &amp; Jennifer Smith</A>
							</DL><p>
							<DT><H3 FOLDED>mixed-precision-training</H3>
							<DL><p>
								<DT><H3 FOLDED>amp-dtypes</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2209.05433">[2209.05433] FP8 Formats for Deep Learning</A>
									<DT><A HREF="https://github.com/P3109/Public/blob/main/Value%20Tables/make-value-tables.ipynb">P3109 - Arithmetic Formats for Machine Learning</A>
									<DT><A HREF="https://github.com/P3109/Public/blob/main/Shared%20Reports/P3109%20WG%20Interim%20Report.pdf">8-bit formats</A>
									<DT><A HREF="https://axil.github.io/a-comprehensive-guide-to-numpy-data-types.html">A Comprehensive Guide to NumPy Data Types</A>
									<DT><A HREF="https://github.com/google/jaxtyping">google/jaxtyping: Type annotations and runtime checking for shape and dtype of JAX/NumPy/PyTorch/etc. arrays.</A>
								</DL><p>
								<DT><A HREF="https://github.com/tspeterkim/mixed-precision-from-scratch">tspeterkim/mixed-precision-from-scratch: Mixed precision training from scratch with Tensors and CUDA</A>
								<DT><A HREF="https://arxiv.org/pdf/1710.03740.pdf">MIXED PRECISION TRAINING</A>
								<DT><A HREF="https://tspeterkim.github.io/posts/mixed-precision-from-scratch">Mixed Precision Training from Scratch | Taeksang Peter Kim</A>
								<DT><A HREF="https://arxiv.org/abs/2310.10537">[2310.10537] Microscaling Data Formats for Deep Learning</A>
								<DT><A HREF="https://azure.github.io/MS-AMP/">MS-AMP Documentation | MS-AMP</A>
								<DT><A HREF="https://arxiv.org/abs/2309.17224">(Graphcore) Training and inference of LLMs using FP8</A>
								<DT><A HREF="https://twitter.com/sytelus/status/1713462676838588565">loss scaling FP16 training (training stability)</A>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">NVIDIA: Train With Mixed Precision</A>
								<DT><A HREF="https://arxiv.org/abs/2310.10537">language models at sub-8-bit weights, activations, and gradients</A>
								<DT><A HREF="https://arxiv.org/abs/1905.12322">[1905.12322] A Study of BFLOAT16 for Deep Learning Training</A>
								<DT><A HREF="https://github.com/P3109/Public/blob/main/Briefs/Discusson%20on%20Rounding.pdf">Public/Briefs/Discusson on Rounding.pdf</A>
								<DT><A HREF="https://arxiv.org/abs/2310.18313">[2310.18313] FP8-LM: Training FP8 Large Language Models</A>
								<DT><A HREF="https://www.youtube.com/watch?v=xnBDg0lYQdU">Weekly AI paper overview- 6/18/24 - YouTube</A>
								<DT><A HREF="https://github.com/Azure/MS-AMP">Azure/MS-AMP: Microsoft Automatic Mixed Precision Library</A>
								<DT><A HREF="https://github.com/graphcore-research/out-of-the-box-fp8-training/blob/main/out_of_the_box_fp8_training.ipynb">out-of-the-box-fp8-training/out_of_the_box_fp8_training.ipynb at main · graphcore-research/out-of-the-box-fp8-training</A>
								<DT><A HREF="https://github.com/pytorch/ao/tree/v0.5.0/torchao/prototype/quantized_training">torchao: Quantized training</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1834221330390290807">NVIDIA doesn't want you to know this one trick: train ML models with INT8 Tensor Cores🤯
Up to 70% end2end speedup on 1x 4090 and 40% speedup on 1x A100 with 4 lines of code. No noticeable accuracy loss.</A>
								<DT><A HREF="https://x.com/papers_anon/status/1836953617602887910?s=12">Scaling FP8 training to trillion-token LLMs</A>
								<DT><A HREF="https://github.com/NVlabs/COAT">NVlabs/COAT: Compressing Optimizer states and Activations for Memory-Efficient FP8 training</A>
								<DT><A HREF="https://arxiv.org/abs/2410.19313">[2410.19313] COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training</A>
							</DL><p>
							<DT><H3 FOLDED>sparse-training</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2310.06927">Sparse Fine-tuning for Inference Acceleration of Large Language Models</A>
								<DT><A HREF="https://huggingface.co/collections/neuralmagic/sparse-finetuning-mpt-65241d875b29204d6d42697d">Sparse Finetuning MPT - a neuralmagic Collection</A>
							</DL><p>
							<DT><H3 FOLDED>mid-training</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>post-training</H3>
							<DL><p>
								<DT><H3 FOLDED>language-models-rl</H3>
								<DL><p>
									<DT><H3 FOLDED>veRL</H3>
									<DL><p>
										<DT><H3 FOLDED>sglang-verl</H3>
										<DL><p>
											<DT><A HREF="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/issues/74">veRL-SGLang Roadmap · Issue #74 · zhaochenyang20/Awesome-ML-SYS-Tutorial</A>
											<DT><A HREF="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html">SGLang Backend — verl documentation</A>
										</DL><p>
										<DT><A HREF="https://github.com/volcengine/verl">volcengine/verl: verl: Volcano Engine Reinforcement Learning for LLMs</A>
									</DL><p>
									<DT><H3 FOLDED>PPO</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/abs/2307.04964">[2307.04964] Secrets of RLHF in Large Language Models Part I: PPO</A>
										<DT><A HREF="https://x.com/_apoorvnandan/status/1866331177067725136">numpy proximal policy optimization implemenation</A>
										<DT><A HREF="https://yugeten.github.io/posts/2025/01/ppogrpo/">A vision researcher’s guide to some RL stuff: PPO &amp; GRPO - Yuge (Jimmy) Shi</A>
									</DL><p>
									<DT><H3 FOLDED>DPO</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/pdf/2305.18290.pdf">Direct Preference Optimization (DPO): Your Language Model is Secretly a Reward Model</A>
									</DL><p>
									<DT><H3 FOLDED>RLHF</H3>
									<DL><p>
										<DT><A HREF="https://www.lesswrong.com/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research">Thoughts on the impact of RLHF research</A>
										<DT><A HREF="https://www.youtube.com/watch?v=SGInyKjzF7A">Optimizing Large Language Models with Reinforcement Learning</A>
										<DT><A HREF="https://twitter.com/vwxyzjn/status/1716818343133659598">RLHF codebase</A>
										<DT><A HREF="https://www.youtube.com/watch?v=fqC3D-zNJUM">AI Safety, RLHF, and Self-Supervision - Jared Kaplan</A>
										<DT><A HREF="https://twitter.com/_robertkirk/status/1712083230965280784">understanding the effects of RLHF on LLM generalisation</A>
										<DT><A HREF="https://huggingface.co/blog/rlhf">Illustrating Reinforcement Learning from Human Feedback (RLHF)</A>
										<DT><A HREF="https://twitter.com/rm_rafailov/status/1781145338759533016">Language models are not a reward function, but Q func</A>
										<DT><A HREF="https://x.com/natolambert/status/1859643351441535345">Tulu: (1) Nathan Lambert en X: "I've spent the last two years scouring all available resources on RLHF specifically and post training broadly. Today, with the help of a totally cracked team, we bring you the fruits of that labor — Tülu 3, an entirely open frontier model post training recipe. We beat Llama 3.1 https://t.co/bcJwYk0HVV" / X</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=SGInyKjzF7A">Optimizing Large Language Models with Reinforcement Learning</A>
									<DT><A HREF="https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81">Reinforcement Learning for Language Models</A>
									<DT><A HREF="https://www.youtube.com/watch?v=dbo3kNKPaUA">Large Language Models (in 2023)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=zjrM-MW-0y0&t=5s">Instruction finetuning and RLHF lecture (NYU CSCI 2590)</A>
									<DT><A HREF="https://github.com/huggingface/trl">huggingface/trl: Train transformer language models with reinforcement learning.</A>
									<DT><A HREF="https://github.com/OpenRLHF/OpenRLHF">OpenRLHF/OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework (70B+ PPO Full Tuning &amp; Iterative DPO &amp; LoRA &amp; RingAttention &amp; RFT)</A>
									<DT><A HREF="https://www.youtube.com/playlist?list=PL_vLws1T4Nx1LSx7bXag7mc2IZIgcY6We">RL for LLM from Scratch with 1 GPU - YouTube</A>
								</DL><p>
								<DT><A HREF="https://github.com/pytorch/torchtune">pytorch/torchtune: PyTorch native finetuning library</A>
								<DT><A HREF="https://x.com/hsu_byron/status/1866577403918917655">(1) Byron Hsu en X: "Introducing the first open-source optimized post-training losses in Liger Kernel with ~80% memory reduction, featuring DPO, CPO, ORPO, SimPO, JSD, and more, achieving up to 70% end-to-end speedup through larger batch size. Use it as any PyTorch module - Available today in Liger https://t.co/dqKSzNd0Me" / X</A>
								<DT><A HREF="https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/chunked_loss/orpo_loss.py#L12">Liger-Kernel/src/liger_kernel/chunked_loss/orpo_loss.py at main · linkedin/Liger-Kernel</A>
							</DL><p>
							<DT><A HREF="https://openai.com/research/techniques-for-training-large-neural-networks">Techniques for training large neural networks</A>
							<DT><A HREF="https://www.youtube.com/watch?v=fKMB5UlVY1E&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=27">Stanford CS25: V4 I Overview of Transformers - YouTube</A>
							<DT><A HREF="https://github.com/JonasGeiping/cramming">JonasGeiping/cramming: Cramming the training of a (BERT-type) language model into limited compute.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=2Zi6wFQQl3E">Generalization bounds for Neural Network Based Decoders</A>
							<DT><A HREF="https://www.youtube.com/watch?v=0D23NeBjCeQ">A Theory for Emergence of Complex Skills in Language Models</A>
							<DT><A HREF="https://www.youtube.com/watch?v=g0gREwiDbis">Dr. Geoffrey Hinton: A brief study of neural networks</A>
							<DT><A HREF="https://www.lesswrong.com/posts/diutNaWF669WgEt3v/the-scaling-inconsistency-openai-s-new-insight">One Epoch: the scaling “inconsistency”</A>
							<DT><A HREF="https://twitter.com/Yampeleg/status/1672600308365623296">Cheatsheet: Full Training vs. LoRA Adapters VS. VS VS. Smaller Models</A>
							<DT><A HREF="https://vram.asmirnov.xyz/">VRAM Calculator</A>
							<DT><A HREF="https://x.com/typedfemale/status/1848216197080260712">(2) typedfemale en X: "i've talked to multiple very experienced people about how they would organize their ideal training codebase - all of them said "i'd put everything in a single file"" / X</A>
							<DT><A HREF="https://x.com/PytorchToAtoms/status/1848230941635776752">(2) Pytorch To Atoms en X: "Top 5 bloatware: 1. Hugging face transformer 2. Megatron-lm 3. NeMo 4. Mosaicml Composer/LLM Foundry 5. Hugging face diffuser" / X</A>
						</DL><p>
						<DT><H3 FOLDED>DeepSeek</H3>
						<DL><p>
							<DT><H3 FOLDED>deepseek-people</H3>
							<DL><p>
								<DT><A HREF="https://scholar.google.com/citations?user=1s79Z5cAAAAJ&hl=zh-CN">‪Fuli Luo（罗福莉）‬ - ‪Google 学术搜索‬</A>
							</DL><p>
							<DT><H3 FOLDED>deepseek-model</H3>
							<DL><p>
								<DT><H3 FOLDED>deepseek-model-v3</H3>
								<DL><p>
									<DT><A HREF="https://mp.weixin.qq.com/s/WFJxnTF9fGIIXPA7GQ5V2w">DeepSeek V3 MoE Expert Parallelism</A>
									<DT><A HREF="https://github.com/GHGmc2/deepseek-projection">GHGmc2/deepseek-projection: DeepSeek-V2/V3 projection of num_params, FLOPs and MFU.</A>
								</DL><p>
								<DT><H3 FOLDED>deepseek-model-v2</H3>
								<DL><p>
									<DT><A HREF="https://x.com/_xjdr/status/1838248197531279640">DeepSeek reference implemetation</A>
									<DT><A HREF="https://github.com/madsys-dev/deepseekv2-profile/blob/main/mla/modeling_deepseek.py">deepseekv2-profile/mla/modeling_deepseek.py at main · madsys-dev/deepseekv2-profile</A>
									<DT><A HREF="https://zhuanlan.zhihu.com/p/700214123?utm_psn=1779287628619632640">DeepSeek-V2 高性能推理 (1)：通过矩阵吸收十倍提速 MLA 算子 - 知乎</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>DeepSeek-v3</H3>
							<DL><p>
								<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-V3">deepseek-ai/DeepSeek-V3</A>
								<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">DeepSeek-V3/DeepSeek_V3.pdf at main · deepseek-ai/DeepSeek-V3</A>
								<DT><A HREF="https://x.com/_xjdr/status/1873074545818812911">xjdr en X: "i think the optimal way to run DSv3 at scale is have the experts on an 8xH100 machine and then the rest of the layers on their own H100. You could then theoretically have 2 DGX machines with 8 instances of the model running and then load balancing to the expert machines over IB." / X</A>
								<DT><A HREF="https://x.com/srush_nlp/status/1876640795765379531">LLM Infrastructure, pages 12-18</A>
								<DT><A HREF="https://www.youtube.com/watch?v=8v2l6SJECW4">DeepSeek-V3 - YouTube</A>
								<DT><A HREF="https://github.com/feifeibear/DPSKV3MFU/blob/main/dpskv3_flops.py">DPSKV3MFU/dpskv3_flops.py at main · feifeibear/DPSKV3MFU</A>
								<DT><A HREF="https://mp.weixin.qq.com/s/WFJxnTF9fGIIXPA7GQ5V2w">DeepSeek V3 MoE Expert Parallelism</A>
								<DT><A HREF="https://www.youtube.com/watch?v=4YC8LsuFe7s">Understand DeepSeek V3 From Scratch | Full Course - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>deepseek-r1</H3>
							<DL><p>
								<DT><H3 FOLDED>deepseek-rl</H3>
								<DL><p>
									<DT><A HREF="https://yugeten.github.io/posts/2025/01/ppogrpo/">A vision researcher’s guide to some RL stuff: PPO &amp; GRPO - Yuge (Jimmy) Shi</A>
								</DL><p>
								<DT><H3 FOLDED>GRPO</H3>
								<DL><p>
									<DT><A HREF="https://www.k-a.in/grpo.html">GRPO</A>
									<DT><A HREF="https://yugeten.github.io/posts/2025/01/ppogrpo/">A vision researcher’s guide to some RL stuff: PPO &amp; GRPO - Yuge (Jimmy) Shi</A>
								</DL><p>
								<DT><H3 FOLDED>alibaba-QwQ</H3>
								<DL><p>
									<DT><A HREF="https://qwenlm.github.io/blog/qwq-32b-preview/">QwQ: Reflect Deeply on the Boundaries of the Unknown | Qwen</A>
								</DL><p>
								<DT><H3 FOLDED>alibaba-Marco-o1</H3>
								<DL><p>
									<DT><A HREF="https://github.com/AIDC-AI/Marco-o1">AIDC-AI/Marco-o1: An Open Large Reasoning Model for Real-World Solutions</A>
									<DT><A HREF="https://arxiv.org/abs/2411.14405">[2411.14405] Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2501.12948">[2501.12948] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</A>
								<DT><A HREF="https://x.com/zizhpan/status/1859196598871277856">deepseek reassoning</A>
								<DT><A HREF="https://x.com/_xjdr/status/1859272181844422813">(1) xjdr en X: "whalebros cooked here. Not only does it seem to replicate the o1-preview results, it seems to pretty effectively replicate (at least parts of) the process. My guess is it uses something very similar to the lets verify step-by-step ORMs / PRMs to train and reward the the CoT in" / X</A>
								<DT><A HREF="https://x.com/deepseek_ai/status/1859200141355536422">DeepSeek-R1-Lite-Preview</A>
								<DT><A HREF="https://x.com/cloneofsimo/status/1859874409894511092">(1) Simo Ryu en X: "Well deepseek hype is real They both got roughly right but deepseek was better https://t.co/qhHc4KeYjm" / X</A>
								<DT><A HREF="https://x.com/natolambert/status/1859643355698786549">(1) Nathan Lambert en X: "Right to the fun stuff. To finish our models, we use a new technique called Reinforcement Learning with Verifiable Rewards, where we train on math problems or prompts with constraints, and only reward the algorithm if the generation is correct. We find this improves performance https://t.co/iViDGmgRBB" / X</A>
								<DT><A HREF="https://chat.deepseek.com/">DeepSeek - Into the Unknown</A>
								<DT><A HREF="https://x.com/cloneofsimo/status/1861153771159724457">(1) Simo Ryu en X: "Wait o1 mightve been this work all along? https://t.co/7JFjtXnJo8 https://t.co/PRAi2b8Yi9" / X</A>
								<DT><A HREF="https://x.com/edwardjhu">(1) Edward Hu (@edwardjhu) / X</A>
								<DT><A HREF="https://cdn.openai.com/o1-system-card-20241205.pdf">https://cdn.openai.com/o1-system-card-20241205.pdf</A>
								<DT><A HREF="https://x.com/zhyncs42/status/1881246950663901454">DeepSeek-R1 released (21/01/25)</A>
								<DT><A HREF="https://huggingface.co/deepseek-ai/DeepSeek-R1/tree/main">deepseek-ai/DeepSeek-R1 at main</A>
								<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-R1">deepseek-ai/DeepSeek-R1</A>
								<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">DeepSeek-R1/DeepSeek_R1.pdf at main · deepseek-ai/DeepSeek-R1</A>
								<DT><A HREF="https://github.com/facebookresearch/coconut">facebookresearch/coconut: Training Large Language Model to Reason in a Continuous Latent Space</A>
								<DT><A HREF="https://github.com/feifeibear/DPSKV3MFU/blob/main/dpskv3_flops.py">DPSKV3MFU/dpskv3_flops.py at main · feifeibear/DPSKV3MFU</A>
								<DT><A HREF="https://github.com/ganler/code-r1">ganler/code-r1: Reproducing R1 for Code with Reliable Rewards</A>
							</DL><p>
							<DT><H3 FOLDED>deepseek-infra</H3>
							<DL><p>
								<DT><H3 FOLDED>deepseek-inference-engine</H3>
								<DL><p>
									<DT><H3 FOLDED>deepseek-vllm</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://github.com/deepseek-ai/open-infra-index/blob/main/OpenSourcing_DeepSeek_Inference_Engine/README.md">open-infra-index/OpenSourcing_DeepSeek_Inference_Engine/README.md at main · deepseek-ai/open-infra-index</A>
								</DL><p>
								<DT><H3 FOLDED>expert-parallelism</H3>
								<DL><p>
									<DT><A HREF="https://github.com/deepseek-ai/DeepEP">deepseek-ai/DeepEP: DeepEP: an efficient expert-parallel communication library</A>
									<DT><A HREF="https://mp.weixin.qq.com/s/WFJxnTF9fGIIXPA7GQ5V2w">DeepSeek V3 MoE Expert Parallelism</A>
									<DT><A HREF="https://chatgpt.com/c/67af31b2-c114-800c-b939-3ba7a9477731">MoE EP320 Efficiency in Torch</A>
									<DT><A HREF="https://nvidia.github.io/TensorRT-LLM/advanced/expert-parallelism.html">Expert Parallelism in TensorRT-LLM — tensorrt_llm documentation</A>
									<DT><A HREF="https://www.youtube.com/watch?v=upZf-BWF1i0">George Hotz | mixture of experts (like deepseek) on tinygrad sovereign AMD stack | AMD YOLO - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>deepseek-cost-model</H3>
								<DL><p>
									<DT><A HREF="https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md">open-infra-index/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md at main · deepseek-ai/open-infra-index</A>
								</DL><p>
								<DT><A HREF="https://github.com/deepseek-ai/open-infra-index?tab=readme-ov-file">deepseek-ai/open-infra-index</A>
								<DT><A HREF="https://github.com/deepseek-ai/smallpond">deepseek-ai/smallpond: A lightweight data processing framework built on DuckDB and 3FS.</A>
								<DT><A HREF="http://sortbenchmark.org/2014_06_CloudSort_v_0_4.pdf">CloudSort: A TCO Sort Benchmark Microsoft</A>
							</DL><p>
							<DT><H3 FOLDED>deepseek-moe-fine-tuning</H3>
							<DL><p>
								<DT><A HREF="https://github.com/deepseek-ai/ESFT">deepseek-ai/ESFT: Expert Specialized Fine-Tuning</A>
							</DL><p>
							<DT><A HREF="https://github.com/deepseek-ai/open-infra-index/blob/main/OpenSourcing_DeepSeek_Inference_Engine/README.md">open-infra-index/OpenSourcing_DeepSeek_Inference_Engine/README.md at main · deepseek-ai/open-infra-index</A>
							<DT><A HREF="https://arxiv.org/abs/2401.02954">[2401.02954] DeepSeek LLM: Scaling Open-Source Language Models with Longtermism ( LLM (hyperparams, dataset basics))</A>
							<DT><A HREF="https://arxiv.org/abs/2401.14196">[2401.14196] DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence (Coder (data engineering, continued pretraining)</A>
							<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf">DeepSeek-Coder-V2/paper.pdf at main · deepseek-ai/DeepSeek-Coder-V2</A>
							<DT><A HREF="https://arxiv.org/abs/2401.06066">[2401.06066] DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models (Fine-grained MoE (architecture design)</A>
							<DT><A HREF="https://arxiv.org/abs/2402.03300">[2402.03300] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (Math (better data, RL)</A>
							<DT><A HREF="https://arxiv.org/abs/2403.05525">[2403.05525] DeepSeek-VL: Towards Real-World Vision-Language Understanding (VL (multimodality)</A>
							<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-VL">deepseek-ai/DeepSeek-VL: DeepSeek-VL: Towards Real-World Vision-Language Understanding</A>
							<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/deepseek-v2-tech-report.pdf">DeepSeek-V2/deepseek-v2-tech-report.pdf at main · deepseek-ai/DeepSeek-V2 (MLA)</A>
							<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-V2">deepseek-ai/DeepSeek-V2: DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</A>
							<DT><A HREF="https://x.com/teortaxesTex/status/1787866166242763217">DeepSeek mid 2024 status</A>
							<DT><A HREF="https://arxiv.org/html/2405.04434v2">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</A>
							<DT><A HREF="https://arxiv.org/abs/1911.02150">[1911.02150] Fast Transformer Decoding: One Write-Head is All You Need</A>
							<DT><A HREF="https://github.com/deepseek-ai">DeepSeek</A>
							<DT><A HREF="https://x.com/AdeptAILabs/status/1699835884097651145">AdeptAI Persimmon--8B</A>
							<DT><A HREF="https://x.com/teortaxesTex/status/1805055350011232352/photo/1">DeepSeek timeline</A>
							<DT><A HREF="https://github.com/THUDM/CodeGeeX4">THUDM/CodeGeeX4: CodeGeeX4-ALL-9B, a versatile model for all AI software development scenarios, including code completion, code interpreter, web search, function calling, repository-level Q&amp;A and much more.</A>
							<DT><A HREF="https://x.com/deepseek_ai/status/1829137969719984345">Auxiliary-Loss-Free Load Balancing Strategy for MoE</A>
							<DT><A HREF="https://arxiv.org/abs/2408.15664">[2408.15664] Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts</A>
							<DT><A HREF="https://x.com/deepseek_ai/status/1832026579180163260">(1) DeepSeek en X: "🚀 Exciting news! We’ve officially launched DeepSeek-V2.5 – a powerful combination of DeepSeek-V2-0628 and DeepSeek-Coder-V2-0724! Now, with enhanced writing, instruction-following, and human preference alignment, it’s available on Web and API. Enjoy seamless Function Calling, https://t.co/dXF2hFvnKK" / X</A>
							<DT><A HREF="https://drive.google.com/file/d/1DW5ohZWxoCEOdrUQjokKreuArHqJdtKb/view">Unveiling_DeepSeek.pdf - Google Drive</A>
							<DT><A HREF="https://x.com/deepseek_ai/status/1866459751053488344">DeepSeek-V2.5-1210: Internet Search</A>
							<DT><A HREF="https://x.com/Guodaya">(1) Daya Guo (@Guodaya) / X</A>
							<DT><A HREF="https://github.com/datawhalechina/unlock-deepseek">datawhalechina/unlock-deepseek: DeepSeek 系列工作解读、扩展和复现。</A>
							<DT><A HREF="https://github.com/hkust-nlp/CodeIO">hkust-nlp/CodeIO: CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction</A>
							<DT><A HREF="https://www.youtube.com/watch?v=CNA1sb2fJS4">Inference Time Scaling for Generalist Reward Modeling - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>ByteDance-Seed</H3>
						<DL><p>
							<DT><H3 FOLDED>Seed-Thinking-v1.5</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5">ByteDance-Seed/Seed-Thinking-v1.5</A>
							</DL><p>
							<DT><A HREF="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5">ByteDance-Seed/Seed-Thinking-v1.5</A>
							<DT><A HREF="https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf">Seed-Thinking-v1.5/seed-thinking-v1.5.pdf at main · ByteDance-Seed/Seed-Thinking-v1.5</A>
						</DL><p>
						<DT><H3 FOLDED>Qwen</H3>
						<DL><p>
							<DT><H3 FOLDED>Qwen2.5-Max</H3>
							<DL><p>
								<DT><A HREF="https://qwenlm.github.io/blog/qwen2.5-max/">Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen</A>
								<DT><A HREF="https://huggingface.co/collections/Qwen/qwen25-1m-679325716327ec07860530ba">Qwen2.5-1M - a Qwen Collection</A>
							</DL><p>
							<DT><A HREF="https://qwenlm.github.io/blog/qwen2.5-turbo/">Extending the Context Length to 1M Tokens! | Qwen</A>
							<DT><A HREF="https://github.com/QwenLM/Qwen2.5-Coder">QwenLM/Qwen2.5-Coder: Qwen2.5-Coder is the code version of Qwen2.5, the large language model series developed by Qwen team, Alibaba Cloud.</A>
							<DT><A HREF="https://github.com/QwenLM/Qwen">QwenLM/Qwen: The official repo of Qwen (通义千问) chat &amp; pretrained large language model proposed by Alibaba Cloud.</A>
							<DT><A HREF="https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M">Qwen/Qwen2.5-14B-Instruct-1M · Hugging Face</A>
						</DL><p>
						<DT><H3 FOLDED>multimodal</H3>
						<DL><p>
							<DT><H3 FOLDED>VLM</H3>
							<DL><p>
								<DT><H3 FOLDED>cogVLM</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>InternVL</H3>
								<DL><p>
									<DT><H3 FOLDED>InternVL-3</H3>
									<DL><p>
										<DT><A HREF="https://internvl.github.io/blog/2025-04-11-InternVL-3.0/">InternVL3</A>
									</DL><p>
									<DT><A HREF="https://github.com/InternLM/InternLM-XComposer">InternLM/InternLM-XComposer: InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output</A>
									<DT><A HREF="https://github.com/OpenGVLab/InternVL">OpenGVLab/InternVL: [CVPR 2024 Oral] InternVL Family: A Pioneering Open-Source Alternative to GPT-4o. 接近GPT-4o表现的可商用开源多模态对话模型</A>
									<DT><A HREF="https://github.com/InternLM/lmdeploy">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.</A>
									<DT><A HREF="https://internvl.github.io/blog/2025-04-11-InternVL-3.0/">InternVL3</A>
								</DL><p>
								<DT><H3 FOLDED>Florence</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>grok-2</H3>
								<DL><p>
									<DT><A HREF="https://x.ai/blog/grok-2">Grok-2 Beta Release</A>
								</DL><p>
								<DT><A HREF="https://www.microsoft.com/en-us/research/publication/florence-2-advancing-a-unified-representation-for-a-variety-of-vision-tasks/">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks - Microsoft Research</A>
								<DT><A HREF="https://www.youtube.com/watch?v=446QYqELoIs">Vision Language Models: PaLI-3 and COMM - YouTube</A>
								<DT><A HREF="https://x.com/huybery/status/1829187788153204776">Binyuan Hui en X: "Try our new Qwen2-VL: https://t.co/uyIeCOtRUa ⚠️ Three Secrets of Success for Qwen2-VL ⚠️ 1️⃣ A key architectural improvement in Qwen2-VL is the implementation of Naive Dynamic Resolution support. Unlike its predecessor, Qwen2-VL can handle arbitrary image resolutions, mapping https://t.co/QtokQgJcqT" / X</A>
								<DT><A HREF="https://x.com/A_K_Nain/status/1850719695164649929">(1) Aakash Kumar Nain en X: "With the Ferret-v2 paper, Apple demonstrated the power of building multimodal models for mobile devices. Now, Microsoft has done the same, but for the desktop UI. Presenting Omniparser, the latest advancement from Microsoft for vision-based multimodal workflows. Here is a summary https://t.co/ag6MCIk2Yd" / X</A>
								<DT><A HREF="https://github.com/PKU-YuanGroup/LLaVA-o1">PKU-YuanGroup/LLaVA-o1</A>
								<DT><A HREF="https://github.com/jingyaogong/minimind-v">jingyaogong/minimind-v: 🚀 「大模型」1小时从0训练26M参数的视觉多模态VLM！🌏 Train a 26M-parameter VLM from scratch in just 1 hours!</A>
								<DT><A HREF="https://github.com/jingyaogong/minimind">jingyaogong/minimind: 🚀🚀 「大模型」2小时完全从0训练26M的小参数GPT！🌏 Train a 26M-parameter GPT from scratch in just 2h!</A>
							</DL><p>
							<DT><H3 FOLDED>omniGen</H3>
							<DL><p>
								<DT><A HREF="https://github.com/VectorSpaceLab/OmniGen">VectorSpaceLab/OmniGen: OmniGen: Unified Image Generation. https://arxiv.org/pdf/2409.11340</A>
								<DT><A HREF="https://x.com/mtschannen/status/1863622784376586499">JetFormer</A>
								<DT><A HREF="https://huggingface.co/papers/2411.16318">Paper page - One Diffusion to Generate Them All</A>
							</DL><p>
							<DT><H3 FOLDED>adept labs</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/adept/fuyu-8b">adept/fuyu-8b · Hugging Face</A>
								<DT><A HREF="https://www.adept.ai/">Adept: Useful General Intelligence</A>
								<DT><A HREF="https://www.adept.ai/blog/fuyu-8b">Fuyu-8B: A Multimodal Architecture for AI Agents</A>
								<DT><A HREF="https://www.adept.ai/blog/adept-fuyu-heavy">Adept Fuyu-Heavy: A new multimodal model</A>
								<DT><A HREF="https://www.adept.ai/blog/persimmon-8b">Releasing Persimmon-8B</A>
								<DT><A HREF="https://github.com/grahamannett/finetune-fuyu/blob/main/train-simple.py">finetune-fuyu/multimodal-train-simple.py</A>
							</DL><p>
							<DT><H3 FOLDED>LLaVA</H3>
							<DL><p>
								<DT><A HREF="https://github.com/haotian-liu/LLaVA#install">haotian-liu/LLaVA: [NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond.</A>
							</DL><p>
							<DT><H3 FOLDED>multimodal-bytedance</H3>
							<DL><p>
								<DT><A HREF="https://github.com/bytedance/MoMA">bytedance/MoMA: MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</A>
							</DL><p>
							<DT><A HREF="https://huggingface.co/papers/2311.00571">Paper page - LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing</A>
							<DT><A HREF="https://www.youtube.com/watch?v=DTpKUnbkT_k">Multimodal Reasoning: PaLM-E &amp; Gemini - Aakanksha Chowdhery | Stanford MLSys #90 - YouTube</A>
							<DT><A HREF="https://arxiv.org/abs//2404.04125">[2404.04125] No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</A>
							<DT><A HREF="https://github.com/xinyu1205/recognize-anything">xinyu1205/recognize-anything: Open-source and strong foundation image recognition models.</A>
							<DT><A HREF="https://github.com/mlfoundations/clip_quality_not_quantity">mlfoundations/clip_quality_not_quantity</A>
							<DT><A HREF="https://publications.reka.ai/reka-core-tech-report.pdf">Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models</A>
							<DT><A HREF="https://x.com/DrJimFan/status/1793318771932995793">(1) Jim Fan en X: "What makes up the abstract concept of an apple? We read the word "apple" as a string, see 2D pictures online, 3D shape in real life, and moving apples in videos. We touch the apple, feel its geometry in our palms and texture through the rich tactile sensation on our fingers. Do https://t.co/2LzxYa4f3N" / X</A>
							<DT><A HREF="https://x.com/Ethan_smith_20/status/1792024324464857197">(1) Ethan en X: "if you like this, you'll really like this https://t.co/NHgBACYxGT pretraining on text is merely an adequately challenging modality to learn computational primitives for universal transfer. there are other modalities which we can do causal modeling on and develop similar https://t.co/A8X0NRZpuP" / X</A>
							<DT><A HREF="https://arxiv.org/abs/2103.05247">[2103.05247] Pretrained Transformers as Universal Computation Engines</A>
							<DT><A HREF="https://www.youtube.com/watch?v=IwYiETZEGY0">What does AI have to do with Plato's Allegory of the Cave? - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=IwYiETZEGY0&list=LL&index=11&t=25s">What does AI have to do with Plato's Allegory of the Cave? - YouTube</A>
							<DT><A HREF="https://arxiv.org/abs/2405.07987">[2405.07987] The Platonic Representation Hypothesis</A>
							<DT><A HREF="https://github.com/facebookresearch/ImageBind">facebookresearch/ImageBind: ImageBind One Embedding Space to Bind Them All</A>
							<DT><A HREF="https://x.com/olivierhenaff/status/1805995802352910557">multimodal dataset creation: joint example selection (JEST)</A>
							<DT><A HREF="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">BradyFU/Awesome-Multimodal-Large-Language-Models: :sparkles::sparkles:Latest Advances on Multimodal Large Language Models</A>
							<DT><A HREF="https://www.youtube.com/watch?v=vAmKB7iPkWw&list=LL&index=10&t=15s">Coding a Multimodal (Vision) Language Model from scratch in PyTorch with full explanation - YouTube</A>
							<DT><A HREF="https://x.com/_weiping/status/1836226447863877837">NVLM</A>
							<DT><A HREF="https://x.com/songhan_mit/status/1849109966898008133">VILA-U: multi-modal token in, multi-modal token out, single autoregresive</A>
							<DT><A HREF="https://github.com/PKU-YuanGroup/LLaVA-o1">PKU-YuanGroup/LLaVA-o1</A>
						</DL><p>
						<DT><H3 FOLDED>agents</H3>
						<DL><p>
							<DT><H3 FOLDED>language-models-tools</H3>
							<DL><p>
								<DT><H3 FOLDED>function calling</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>model-context-protocol</H3>
								<DL><p>
									<DT><A HREF="https://x.com/alexalbert__/status/1861079762506252723">(1) Alex Albert en X: "Introducing the Model Context Protocol (MCP) An open standard we've been working on at Anthropic that solves a core challenge with LLM apps - connecting them to your data. No more building custom integrations for every data source. MCP provides one protocol to connect them all: https://t.co/kYsivQyPDq" / X</A>
									<DT><A HREF="https://github.com/modelcontextprotocol">Model Context Protocol</A>
									<DT><A HREF="https://github.com/modelcontextprotocol/create-python-server">modelcontextprotocol/create-python-server: Create a Python MCP server</A>
								</DL><p>
								<DT><H3 FOLDED>agent2agent</H3>
								<DL><p>
									<DT><A HREF="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">Announcing the Agent2Agent Protocol (A2A) - Google Developers Blog</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2302.04761">[2302.04761] Toolformer: Language Models Can Teach Themselves to Use Tools</A>
							</DL><p>
							<DT><H3 FOLDED>test-time-compute (sampling)</H3>
							<DL><p>
								<DT><H3 FOLDED>parallel-generation</H3>
								<DL><p>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1835352391648158189">This is my current mental model for how o1 works (replace batch for parallel generation in the last picture)</A>
									<DT><H3 FOLDED>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/blog/cuda-free-inference-for-llms/?utm_content=306418723&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024">CUDA-Free Inference for LLMs | PyTorch</A>
										<DT><A HREF="https://github.com/facebookresearch/xformers/blob/c909f0d6a3f991c547bebe24b312286632a73473/xformers/ops/fmha/flash3.py#L56">xformers/xformers/ops/fmha/flash3.py at c909f0d6a3f991c547bebe24b312286632a73473 · facebookresearch/xformers</A>
										<DT><A HREF="https://gist.github.com/Chillee/2e270fc5413dbbce58c779f8c4eac66c">flex_attention_tutorial.py</A>
										<DT><A HREF="https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit">The Custom Operators Manual - Google Docs</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/blob/a7c47e0f028c2a9e67cbc99ab67692ec765d3dd0/python/sglang/srt/layers/prefill_attention.py#L147">sglang/python/sglang/srt/layers/prefill_attention.py at a7c47e0f028c2a9e67cbc99ab67692ec765d3dd0 · sgl-project/sglang</A>
										<DT><A HREF="https://github.com/drisspg/transformer_nuggets/blob/83f754eb670b73aa789a924b6b9fab67784ca28f/transformer_nuggets/flash/flash_attention.py#L340">transformer_nuggets/transformer_nuggets/flash/flash_attention.py at 83f754eb670b73aa789a924b6b9fab67784ca28f · drisspg/transformer_nuggets</A>
										<DT><A HREF="https://discuss.pytorch.org/t/getting-triton-to-generate-all-kernels/189173/6">Getting Triton to generate all kernels - torch.compile / torch._inductor - PyTorch Forums</A>
										<DT><A HREF="https://gist.github.com/gradjitta/550f6a7666228c8539ca11fc78f4ec95">custom op for flash attention 3 (compatible with torch.compile)</A>
										<DT><A HREF="https://gist.github.com/antferdom/f7874ab68f4c1183d2b8196d2ace3ffc">FlashAttention v3 within torch.compile compatible</A>
										<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/e2182cc21d5be2a1d71c8ca7eb1bc425563041f1/hopper/benchmark_attn.py#L236">flash-attention/hopper/benchmark_attn.py at e2182cc21d5be2a1d71c8ca7eb1bc425563041f1 · Dao-AILab/flash-attention</A>
										<DT><A HREF="https://github.com/search?q=repo%3Apytorch%2Fpytorch%20scaled_dot_product_attention&type=code">Code search results</A>
										<DT><A HREF="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-894/developer-guide/index.html">Developer Guide :: NVIDIA cuDNN Documentation</A>
										<DT><A HREF="https://github.com/NVIDIA/cudnn-frontend/issues/52">What's the difference of flash attention implement between cudnn and Dao-AILab? · Issue #52 · NVIDIA/cudnn-frontend</A>
										<DT><A HREF="https://github.com/NVIDIA/cudnn-frontend/blob/1.0/release/samples/python/test_mhas.py#L431">cudnn-frontend/samples/python/test_mhas.py at 1.0/release · NVIDIA/cudnn-frontend</A>
										<DT><A HREF="https://drive.google.com/drive/u/3/home">Home - Google Drive</A>
										<DT><A HREF="https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#testing-an-operator">Custom C++ and CUDA Operators — PyTorch Tutorials 2.4.0+cu121 documentation</A>
										<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">torch.nn.functional.scaled_dot_product_attention — PyTorch 2.4 documentation</A>
										<DT><A HREF="https://www.youtube.com/watch?v=-adYfpAenvo&t=117s">Pearl Harbor - Japanese Empire - Theme Suite - YouTube</A>
										<DT><A HREF="https://x.com/i/bookmarks?post_id=1835352391648158189">(1) Guardados / X</A>
										<DT><A HREF="https://arxiv.org/pdf/2403.09629">https://arxiv.org/pdf/2403.09629</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>Contrastive Search</H3>
								<DL><p>
									<DT><A HREF="https://github.com/yxuansu/SimCTG">yxuansu/SimCTG: [NeurIPS'22] A Contrastive Framework for Neural Text Generation</A>
									<DT><A HREF="https://arxiv.org/abs/2210.14140">[2210.14140] Contrastive Search Is What You Need For Neural Text Generation</A>
									<DT><A HREF="https://twitter.com/joao_gante/status/1590293010385760256">Contastive Search</A>
									<DT><A HREF="https://huggingface.co/blog/introducing-csearch">Generating Human-level Text with Contrastive Search in Transformers 🤗</A>
								</DL><p>
								<DT><H3 FOLDED>deterministic</H3>
								<DL><p>
									<DT><H3 FOLDED>Greedy Search</H3>
									<DL><p>
										<DT><A HREF="https://en.wikipedia.org/wiki/Beam_search">Beam search - Wikipedia</A>
										<DT><A HREF="https://twitter.com/cwolferesearch/status/1659608476455256078">Greedy decoding steps and theory</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>stochastic</H3>
								<DL><p>
									<DT><H3 FOLDED>top-k</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>nNucleus-top-p</H3>
									<DL><p>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>grammar-based</H3>
								<DL><p>
									<DT><H3 FOLDED>xgrammar</H3>
									<DL><p>
										<DT><A HREF="https://github.com/mlc-ai/xgrammar">mlc-ai/xgrammar</A>
										<DT><A HREF="https://arxiv.org/abs/2411.15100">[2411.15100] XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</A>
									</DL><p>
									<DT><A HREF="https://github.com/sgl-project/sgl-learning-materials/blob/main/slides/lmsys_1st_meetup_xgrammar.pdf">sgl-learning-materials/slides/lmsys_1st_meetup_xgrammar.pdf at main · sgl-project/sgl-learning-materials</A>
									<DT><A HREF="https://github.com/outlines-dev/outlines">outlines-dev/outlines: Structured Text Generation</A>
									<DT><A HREF="https://github.com/outlines-dev/outlines/blob/main/outlines/grammars/arithmetic.lark">outlines/outlines/grammars/arithmetic.lark at main · outlines-dev/outlines</A>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/issues/4218">llama : speed-up grammar sampling · Issue #4218 · ggerganov/llama.cpp</A>
									<DT><A HREF="https://lmsys.org/blog/2024-02-05-compressed-fsm/">Fast JSON Decoding for Local LLMs with Compressed Finite State Machine | LMSYS Org</A>
									<DT><A HREF="https://huggingface.co/docs/text-generation-inference/en/conceptual/guidance">Guidance</A>
									<DT><A HREF="https://github.com/rhohndorf/pydantic-gbnf-grammar-generator">rhohndorf/pydantic-gbnf-grammar-generator: Generates llama.cpp compatible grammars from pydantic objects</A>
									<DT><A HREF="https://saibo-creator.github.io/uploads/NexThink_talk_2024_06_18.pdf">Grammar-Constrained Decoding for Large Language Models</A>
									<DT><A HREF="https://arxiv.org/html/2312.07104v2">SGLang: Efficient Execution of Structured Language Model Programs</A>
									<DT><A HREF="https://www.aidancooper.co.uk/constrained-decoding/">A Guide to Structured Outputs Using Constrained Decoding</A>
									<DT><A HREF="https://x.com/mrsiipa/status/1860642562517913742">(1) maharshi en X: "what an amazing read: converting json to regex then regex to finite state machines, and then optimising it is brilliant! https://t.co/K0232ncNe8" / X</A>
								</DL><p>
								<DT><H3 FOLDED>prompting</H3>
								<DL><p>
									<DT><A HREF="https://research.character.ai/prompt-design-at-character-ai/">Prompt Design at Character.AI</A>
									<DT><A HREF="https://github.com/character-ai/prompt-poet?ref=research.character.ai">character-ai/prompt-poet at research.character.ai</A>
								</DL><p>
								<DT><H3 FOLDED>entropy-based-decoding</H3>
								<DL><p>
									<DT><H3 FOLDED>entropix</H3>
									<DL><p>
										<DT><A HREF="https://github.com/Pleias/Quest-Best-Tokens/blob/main/New%20physics%20of%20LLM.pdf">Quest-Best-Tokens/New physics of LLM.pdf at main · Pleias/Quest-Best-Tokens</A>
										<DT><A HREF="https://x.com/Dorialexander/status/1868284566055821672">(1) Alexander Doria en X: "I'm releasing my ongoing experiments of using attention score to enhance the reliability of retrieval-augmented generation. https://t.co/9bHuRmhKJd Similarly to the entropix/softmax is not enough reproduction, code provides a slow dive into LLM internals using pytorch directly. https://t.co/6iCiM40CMs" / X</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/math/0211159">[math/0211159] The entropy formula for the Ricci flow and its geometric applications</A>
									<DT><A HREF="https://x.com/_xjdr/status/1859658343587119463">(1) xjdr en X: "large scale synthetic data generation post training without humans in the loop test time steering with external verifiers this is the way to the frontier" / X</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=AvHLJqtmQkE">Typical Decoding for Natural Language Generation</A>
								<DT><A HREF="https://huggingface.co/blog/how-to-generate">Basics</A>
								<DT><A HREF="https://arxiv.org/abs/2307.15337">[2307.15337] Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding</A>
								<DT><A HREF="https://arxiv.org/pdf/1905.00174.pdf">Unsupervised Temperature Scaling</A>
								<DT><A HREF="https://blog.research.google/2024/01/introducing-aspire-for-selective.html">Introducing ASPIRE for selective prediction in LLMs – Google Research Blog</A>
								<DT><A HREF="https://arxiv.org/abs/2406.16838">[2406.16838] From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models</A>
								<DT><A HREF="https://x.com/wellecks/status/1806326626546036802">(1) Sean Welleck en X: "What do nucleus sampling, tree-of-thought, and PagedAttention have in common? They're all part of our new survey: "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models" https://t.co/faHwQcxLUX https://t.co/eY9cNstBbd" / X</A>
								<DT><A HREF="https://arxiv.org/abs/2408.04220">[2408.04220] Diffusion Guided Language Modeling</A>
								<DT><A HREF="https://github.com/spcl/graph-of-thoughts">spcl/graph-of-thoughts: Official Implementation of "Graph of Thoughts: Solving Elaborate Problems with Large Language Models"</A>
								<DT><A HREF="https://github.com/codelion/optillm">codelion/optillm: Optimizing inference proxy for LLMs</A>
								<DT><A HREF="https://blog.dottxt.co/coalescence.html">Coalescence: making LLM inference 5x faster</A>
							</DL><p>
							<DT><H3 FOLDED>agents-swe</H3>
							<DL><p>
								<DT><H3 FOLDED>morph-labs</H3>
								<DL><p>
									<DT><A HREF="https://x.com/i/bookmarks">autonomous swe agents with foundational integration into infra</A>
								</DL><p>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1841729482220306603">superhuman quality synthetic data for evaluating and post-training AI SWEs.</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-formal-proving</H3>
							<DL><p>
								<DT><H3 FOLDED>LeanDojo</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=u-pkmdkQoMU">Alex Gu | LeanDojo: Theorem Proving with Retrieval-Augmented Language Models - YouTube</A>
									<DT><A HREF="https://leandojo.org/">LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</A>
								</DL><p>
								<DT><A HREF="https://jesse-michael-han.github.io/blog/imo-gc-geo/">Building geometry solvers for the IMO Grand Challenge (Jesse Michael Han)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=dOplrIJEYBo">Alpha Everywhere: AlphaGeometry, AlphaCodium and the Future of LLMs - YouTube</A>
								<DT><A HREF="https://github.com/google-deepmind/alphageometry">google-deepmind/alphageometry</A>
								<DT><A HREF="https://www.youtube.com/watch?v=AayZuuDDKP0">Terence Tao, "Machine Assisted Proof" - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=eZbYSOpga2U">Trieu H. Trinh | Solving olympiad geometry without human demonstrations - YouTube</A>
							</DL><p>
							<DT><A HREF="https://github.com/mem0ai/mem0">mem0ai/mem0: The Memory layer for your AI apps</A>
							<DT><A HREF="https://www.youtube.com/watch?v=6K83UXV_oK0">Building Dynamic AI Memory Systems: Sam Whitmore's Approach to Personalization - YouTube</A>
							<DT><A HREF="https://x.com/_akhaliq/status/1858533342993408129">(1) AK en X: "The Dawn of GUI Agent A Preliminary Case Study with Claude 3.5 Computer Use Game (Honkai: Star Rail) Claude 3.5 Computer Use can help complete Honkai: Star Rail daily tasks, accurately locating and interacting with in-game elements. https://t.co/ste95A560K" / X</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-code-generation</H3>
						<DL><p>
							<DT><H3 FOLDED>language-models-code-generation-evaluation</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/amanrsanger/status/1588585377946021888?s=12&amp;t=WP151JoXlub3KMUA8iqo0g">Model pass@k evaluation comparison</A>
								<DT><A HREF="https://huggingface.co/spaces/evaluate-metric/code_eval">Hugging Face Evaluation: pass@k</A>
								<DT><A HREF="https://arxiv.org/pdf/2107.03374.pdf">Evaluating Large Language Models Trained on Code</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-code-generation-benchmarks</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/CodeXGLUE">microsoft/CodeXGLUE: CodeXGLUE</A>
								<DT><A HREF="https://paperswithcode.com/dataset/humaneval">HumanEval Dataset | Papers With Code</A>
								<DT><A HREF="https://openai.com/blog/grade-school-math/">Solving Math Word Problems</A>
								<DT><A HREF="https://arxiv.org/pdf/2210.14868.pdf">Multi-Lingual Code Generation (Execution Evaluation)</A>
								<DT><A HREF="https://huggingface.co/datasets/THUDM/humaneval-x">THUDM/humaneval-x · Datasets at Hugging Face</A>
								<DT><A HREF="https://arxiv.org/abs/2211.11501">[2211.11501] DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-code-generation-multilingual</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/TransCoder">facebookresearch/TransCoder: Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf</A>
								<DT><A HREF="https://github.com/facebookresearch/TransCoder">facebookresearch/TransCoder</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-code-generation-architectures</H3>
							<DL><p>
								<DT><H3 FOLDED>DeepSeek-Coder</H3>
								<DL><p>
									<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-Coder-V2?tab=readme-ov-file">deepseek-ai/DeepSeek-Coder-V2</A>
									<DT><A HREF="https://x.com/deepseek_ai/status/1813921111694053644">DeepSeek-V2-0628</A>
								</DL><p>
								<DT><H3 FOLDED>Code Llama (Llama 3 70b)</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>Codestral</H3>
								<DL><p>
									<DT><A HREF="https://mistral.ai/news/codestral/">Codestral: Hello, World! | Mistral AI | Frontier AI in your hands</A>
									<DT><A HREF="https://x.com/GuillaumeLample/status/1813231491154899012">Guillaume Lample @ ICLR 2024 en X: "Today we are releasing two small models: Mathstral 7B and Codestral Mamba 7B. On the MATH benchmark, Mathstral 7B obtains 56.6% pass@1, outperforming Minerva 540B by more than 20%. Mathstral scores 68.4% on MATH with majority voting@64, and 74.6% using a reward model. Codestral https://t.co/k125XPnur4" / X</A>
								</DL><p>
								<DT><H3 FOLDED>StarCoder2</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>Codex</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2207.14255">Efficient Training of Language Models to Fill in the Middle</A>
									<DT><A HREF="https://arxiv.org/abs/2006.03511">Unsupervised Translation of Programming Languages</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Wc7dcwF7QaA&t=4846s">Codex: Evaluating Large Language Models Trained on Code - YouTube</A>
									<DT><A HREF="https://arxiv.org/pdf/2107.03374.pdf">Codex: Evaluating Large Language Models Trained on Code</A>
									<DT><H3 FOLDED>codex-training-data</H3>
									<DL><p>
										<DT><A HREF="https://help.openai.com/en/articles/5480054-understanding-codex-training-data-and-outputs">Understanding Codex training data and outputs | OpenAI Help Center</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>InCoder</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2204.05999.pdf">InCoder</A>
									<DT><A HREF="https://arxiv.org/abs/2204.05999">InCoder: A Generative Model for Code Infilling and Synthesis</A>
									<DT><A HREF="https://sites.google.com/view/incoder-code-models">InCoder</A>
								</DL><p>
								<DT><H3 FOLDED>CodeGen</H3>
								<DL><p>
									<DT><A HREF="https://github.com/salesforce/CodeGen/tree/main/codegen25">CodeGen/codegen25 at main · salesforce/CodeGen</A>
									<DT><A HREF="https://github.com/facebookresearch/CodeGen">facebookresearch/CodeGen: Reference implementation of code generation projects from Facebook AI Research. General toolkit to apply machine learning to code, from dataset creation to model training and evaluation. Comes with pretrained models.</A>
									<DT><A HREF="https://arxiv.org/pdf/2203.13474.pdf">CodeGen (JAX): Conversational</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://arxiv.org/pdf/2107.03374.pdf">(Chen, 2021) Codex: Evaluating Large Language Models Trained on Code</A>
							<DT><A HREF="https://arxiv.org/pdf/2203.07814.pdf">(Deepmind, 2022) Competition-Level Code Generationwith AlphaCode</A>
							<DT><A HREF="https://arxiv.org/pdf/2207.14502.pdf">(HOT) Languages Models Can Teach Themselves (Kaplan Theorem)</A>
							<DT><A HREF="https://arxiv.org/abs/2207.10397">CodeT: Code Generation with Generated Tests</A>
							<DT><A HREF="https://github.com/microsoft/CodeT/tree/main/DIVERSE">CodeT/DIVERSE at main · microsoft/CodeT</A>
							<DT><A HREF="https://arxiv.org/pdf/2208.05950.pdf">Interactive Code Generation via Test-Driven User-Intent Formalization</A>
							<DT><A HREF="https://arxiv.org/pdf/2102.07350.pdf">Prompt Programming: choice of prompts that determine the quality of output</A>
							<DT><A HREF="https://twitter.com/davisblalock/status/1558347542101839873">"Language Models Can Teach Themselves to Program Better"</A>
							<DT><A HREF="https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/?utm_source=linkedin&utm_medium=organic_social&utm_content=image&utm_campaign=fair">Meta Large Language Model Compiler: Foundation Models of Compiler Optimization | Research - AI at Meta</A>
							<DT><A HREF="https://x.com/BigCodeProject/status/1813618988246790452">BigCodeBench-Hard</A>
							<DT><A HREF="https://x.com/MinyangTian1/status/1813182904593199553">SciCode</A>
							<DT><A HREF="https://x.com/lmarena_ai/status/1856444009323082093">Which language model is best for coding? Copilot Arena leaderboard</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-embeddings</H3>
						<DL><p>
							<DT><H3 FOLDED>embeddings-semantic-search</H3>
							<DL><p>
								<DT><A HREF="https://www.patterns.app/blog/2023/02/19/ask-hn-gpt-embeddings-question-answering/">AskHN - The collective GPT-embodied wisdom of Hacker News | Patterns</A>
								<DT><A HREF="https://www.youtube.com/watch?v=HAseTSX6FT8">Fast, Accurate and Robust Multilingual Syntactic Analysis – Slav Petrov (Google) - 2012 - YouTube</A>
								<DT><A HREF="https://urimerhav.substack.com/p/vector-similarity-search-is-hopeless?utm_campaign=post&triedRedirect=true">Vector Similarity Search is Hopeless - by Uri Merhav</A>
								<DT><A HREF="https://huggingface.co/papers/2411.12644">Paper page - CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=RkYuH_K7Fx4">What is?</A>
							<DT><A HREF="https://twitter.com/_akhaliq/status/1742034428619096327">Improving Text Embeddings with Large Language Models (MS)</A>
							<DT><A HREF="https://twitter.com/din0s_/status/1742235150530851120">trained on synthetic retrieval data</A>
							<DT><A HREF="https://arxiv.org/abs/1911.02116">[1911.02116] Unsupervised Cross-lingual Representation Learning at Scale (Meta AI)</A>
							<DT><A HREF="https://ai.meta.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/">XLM-R: State-of-the-art cross-lingual understanding through self-supervision</A>
							<DT><A HREF="https://arxiv.org/abs/2401.00368">[ Improving Text Embeddings with Large Language Models (MSFT)</A>
							<DT><A HREF="https://github.com/RAIVNLab/MRL">RAIVNLab/MRL: Code repository for the paper - "Matryoshka Representation Learning"</A>
							<DT><A HREF="https://twitter.com/adityakusupati/status/1541554858829815809">(1) Aditya Kusupati 🪆 en X: "Introducting🪆Matryoshka Representations for Adaptive Deployment🪆 TL;DR: up to 14× lower real-world classification &amp;amp; retrival costs at web-scale at no loss in accuracy &amp;amp; w/o any overhead across setups. Paper: https://t.co/JMP9ED72L2 Code: https://t.co/SEccseeDxz [1/11] https://t.co/dXl03V1CUc" / X</A>
							<DT><A HREF="https://huggingface.co/blog/embedding-quantization">Binary and Scalar Embedding Quantization for Significantly Faster &amp; Cheaper Retrieval</A>
							<DT><A HREF="https://github.com/tensorchord/pgvecto.rs">tensorchord/pgvecto.rs: Scalable, Low-latency and Hybrid-enabled Vector Search in Postgres. Revolutionize Vector Search, not Database.</A>
							<DT><A HREF="https://github.com/UKPLab/sentence-transformers/releases/tag/v2.7.0">Release v2.7.0 - CachedGISTEmbedLoss, easy Matryoshka inference &amp; evaluation, CrossEncoder, Intel Gaudi2 · UKPLab/sentence-transformers</A>
							<DT><A HREF="https://x.com/victorialslocum/status/1830955879803318650">(1) Victoria Slocum en X: "There’s this super scary-looking equation in the Matryoshka Representation Learning paper (https://t.co/5tn1VOIK7D), but once you break it down, it’s actually not that bad. So, let’s go through it, inside to out: 🟡 Yellow: This is the data label corresponding to the input (x). https://t.co/PyemCI8u7K" / X</A>
							<DT><A HREF="https://arxiv.org/pdf/2205.13147">Matryoshka Representation Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2410.02525">[2410.02525] Contextual Document Embeddings</A>
							<DT><A HREF="https://x.com/XingyouSong/status/1861190895623667754">(1) Richard Song en X: "LLM embeddings are surprisingly great for high dimensional regression. Why? Because they preserve Lipschitz continuity better than traditional methods! But bigger models aren’t always better, due to confounding factors like RL-HF. Tweeting on behalf of @erictang000 whom I had a https://t.co/AefM4dXaT7" / X</A>
							<DT><A HREF="https://x-tabdeveloping.github.io/turftopic/clustering/">Clustering Models - Turftopic</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-fine-tuning</H3>
						<DL><p>
							<DT><H3 FOLDED>Instruction Tuning</H3>
							<DL><p>
								<DT><H3 FOLDED>Datasets</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/yuntiandeng/status/1724503257601458575">WildChat: 650K user-ChatGPT interactions in the wild</A>
								</DL><p>
								<DT><H3 FOLDED>FLAN</H3>
								<DL><p>
									<DT><H3 FOLDED>flan-prompt-template</H3>
									<DL><p>
										<DT><A HREF="https://jrodthoughts.medium.com/this-google-model-combines-reasoning-and-acting-in-a-single-language-model-935317c3b111">Combine Reasoning and Acting in a Single Language Model</A>
									</DL><p>
									<DT><A HREF="https://jrodthoughts.medium.com/this-google-model-combines-reasoning-and-acting-in-a-single-language-model-935317c3b111">Combine Reasoning and Acting in a Single Language Model</A>
									<DT><A HREF="https://github.com/google-research/FLAN/blob/main/flan/baseline_templates.py">Templates for baseline guide language models (GLM) prompts</A>
									<DT><A HREF="https://arxiv.org/abs/2109.01652">[2109.01652] Finetuned Language Models Are Zero-Shot Learners</A>
									<DT><A HREF="https://arxiv.org/abs/2301.13688">[2301.13688] The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/model_doc/flan-t5">FLAN-T5</A>
									<DT><A HREF="https://arxiv.org/abs/2210.11416">Flan-PaLM: Scaling Instruction-Finetuned Language Models</A>
									<DT><A HREF="https://chat.openai.com/c/e261dbd6-8206-4e2d-8fdc-17cb3aafd611">Textual graph structures representations</A>
								</DL><p>
								<DT><A HREF="https://openai.com/blog/instruction-following/">InstructGPT</A>
								<DT><A HREF="https://twitter.com/ShayneRedford/status/1683927467436937219">The FLAN Collection</A>
								<DT><A HREF="https://twitter.com/burkov/status/1715541145881624617">chatting capability</A>
								<DT><A HREF="https://twitter.com/karpathy/status/1728143712059056467">(Andrej Karpathy) special tokens &lt;|BROWSE|&gt;  tooling usage</A>
								<DT><A HREF="https://twitter.com/Muennighoff/status/1717212559584149674">complexity of instruction data</A>
								<DT><A HREF="https://www.youtube.com/watch?v=mcep6W8oB1I&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=22">Stanford CS25: V3 I Recipe for Training Helpful Chatbots - YouTube</A>
								<DT><A HREF="https://github.com/character-ai/prompt-poet?ref=research.character.ai">character-ai/prompt-poet at research.character.ai</A>
								<DT><A HREF="https://github.com/alibaba/ChatLearn">alibaba/ChatLearn: A flexible and efficient training framework for large-scale alignment tasks</A>
							</DL><p>
							<DT><H3 FOLDED>Supervised-Fine-Tuning (SFT)</H3>
							<DL><p>
								<DT><A HREF="https://liyuan24.github.io/writings/supervised_fine_tuning.html">Supervised Fine Tuning From Scratch | Liyuan’s Log</A>
							</DL><p>
							<DT><H3 FOLDED>Parameter-Efficient Fine-Tuning-(PEFT)</H3>
							<DL><p>
								<DT><H3 FOLDED>LoRA</H3>
								<DL><p>
									<DT><H3 FOLDED>QLoRA</H3>
									<DL><p>
										<DT><A HREF="https://x.com/laurensweitkamp/status/1899853378877817052">Laurens: QLoRA dequantization kernel using Triton, Llama models</A>
									</DL><p>
									<DT><H3 FOLDED>lora-serving</H3>
									<DL><p>
										<DT><A HREF="https://github.com/punica-ai/punica">punica-ai/punica: Serving multiple LoRA finetuned LLM as one</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=NDV65M-2T3g">Flat minima generalize for low-rank matrix recovery - YouTube</A>
									<DT><A HREF="https://gist.github.com/Chillee/a8d2070b1b7b3f97d8c87bac3c366f8e">lora_example.py</A>
									<DT><A HREF="https://irhum.github.io/blog/lorawd/">irhum.github.io - LoRA and Weight Decay</A>
									<DT><A HREF="https://x.com/datavistics/status/1805929136390656137">(1) Derek Thomas en X: "Check out this animation on LoRA Inference! https://t.co/0ixOHmUcQR" / X</A>
								</DL><p>
								<DT><H3 FOLDED>Prompt-Tuning</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2102.07350.pdf">Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</A>
									<DT><A HREF="https://arxiv.org/pdf/2202.13169.pdf">A SYSTEMATIC EVALUATION OF LARGE LANGUAGE MODELS OF CODE</A>
									<DT><A HREF="https://arxiv.org/pdf/2208.05950.pdf">Interactive Code Generation via Test-Driven User-Intent Formalization</A>
									<DT><A HREF="https://openreview.net/pdf?id=NiEtU7blzN">LARGE LANGUAGE MODELS CAN SELF-IMPROVE</A>
									<DT><A HREF="https://arxiv.org/pdf/2107.03374.pdf">Evaluating Large Language Models Trained on Code</A>
									<DT><A HREF="https://www.youtube.com/watch?v=80jwVkYOu0w">The Power of Scale for Parameter-Efficient Prompt Tuning - YouTube</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2403.14608">[2403.14608] Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</A>
								<DT><A HREF="https://arxiv.org/pdf/2401.01286.pdf">A Comprehensive Study of Knowledge Editing for Large Language Models</A>
								<DT><A HREF="https://arxiv.org/pdf/2205.05638.pdf">T-FEW: Few-Shot Parameter-Efficient Fine-Tuning vs In-Context Learning</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MQwryfkydc0">Unsloth.ai: Easily finetune &amp; train LLMs - YouTube</A>
								<DT><A HREF="https://x.com/danielhanchen/status/1866548183729410423">(1) Daniel Han en X: "Unsloth can now do 89K context finetuning on a 80GB GPU for Llama 3.3 70B in 4bit - 13x longer than HF+FA2! 1. We worked with the Cut Cross Entropy authors to make it work in @UnslothAI. CCE is like FA2, but for cross entropy. Via on the fly matrix mults, one does not have to https://t.co/kP34vHWJCF" / X</A>
							</DL><p>
							<DT><H3 FOLDED>language-models-merge</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1725513067293724897">CombLM</A>
								<DT><A HREF="https://arxiv.org/abs/2312.15166">[2312.15166] SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling</A>
								<DT><A HREF="https://huggingface.co/papers/2312.15166">Paper page - SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling</A>
								<DT><A HREF="https://arxiv.org/abs/2305.15296">[2305.15296] MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation</A>
								<DT><A HREF="https://x.com/_akhaliq/status/1794938544336568380">Stacking Your Transformers A Closer Look at Model Growth for Efficient LLM Pre-Training LLMs are computationally expensive to pre-train</A>
								<DT><A HREF="https://x.com/nikdimitriadis/status/1797638468610466044">(1) Dimitriadis Nikos en X: "Wouldn't it be great if we could merge the knowledge of 20 specialist models into a single one without losing performance? 💪🏻 Introducing our new ICML paper "Localizing Task Information for Improved Model Merging and Compression". 🎉 📜: https://t.co/JC4mdujKkd 🧵1/9 https://t.co/JrCE0DBYlT" / X</A>
								<DT><A HREF="https://x.com/gabriel_ilharco/status/1603415656699162624">(1) Gabriel Ilharco en X: "Introducing task vectors! A new way to steer models by doing arithmetic with model weights. Subtract to make models forget, add to make them learn 📜: https://t.co/YNQvdYtdSN 🖥️: https://t.co/CVFM68u322 https://t.co/FBgdpByhUB" / X</A>
								<DT><A HREF="https://github.com/mlfoundations/task_vectors">mlfoundations/task_vectors: Editing Models with Task Arithmetic</A>
								<DT><A HREF="https://arxiv.org/pdf/2405.07813">Localizing Task Information for Improved Model Merging and Compression</A>
								<DT><A HREF="https://x.com/francoisfleuret/status/1797652614366339545">(1) François Fleuret en X: "TL;DR: You fine-tune a model on T tasks, and store a bit per task / parameter that indicates to use the base or the multi-task value. So only 2 x 16 + T bits per parameter, and performance are virtually as good as with T dedicated models." / X</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2405.05904#:~:text=We%20demonstrate%20that%20large%20language,consistent%20with%20the%20model's%20knowledge.">[2405.05904] Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</A>
							<DT><A HREF="https://blog.scottlogic.com/2023/11/24/llm-mem.html">LLM finetuning memory requirements</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mcep6W8oB1I">Stanford CS25: V3 I Recipe for Training Helpful Chatbots</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Ckz8XA2hW84&list=LL&index=14&t=1s">Ilya Sutskever (OpenAI) and Jensen Huang (NVIDIA CEO) : AI Today and Vision of the Future (3/2023) - YouTube</A>
							<DT><A HREF="https://github.com/InternLM/xtuner">InternLM/xtuner: An efficient, flexible and full-featured toolkit for fine-tuning LLM (InternLM2, Llama3, Phi3, Qwen, Mistral, ...)</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-retrieval</H3>
						<DL><p>
							<DT><H3 FOLDED>Semantic Search</H3>
							<DL><p>
								<DT><H3 FOLDED>Embedding</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=RkYuH_K7Fx4">What is?</A>
									<DT><A HREF="https://twitter.com/_akhaliq/status/1742034428619096327">Improving Text Embeddings with Large Language Models (MS)</A>
									<DT><A HREF="https://twitter.com/din0s_/status/1742235150530851120">trained on synthetic retrieval data</A>
									<DT><A HREF="https://arxiv.org/abs/1911.02116">[1911.02116] Unsupervised Cross-lingual Representation Learning at Scale (Meta AI)</A>
									<DT><A HREF="https://ai.meta.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/">XLM-R: State-of-the-art cross-lingual understanding through self-supervision</A>
									<DT><A HREF="https://arxiv.org/abs/2401.00368">[ Improving Text Embeddings with Large Language Models (MSFT)</A>
									<DT><A HREF="https://github.com/RAIVNLab/MRL">RAIVNLab/MRL: Code repository for the paper - "Matryoshka Representation Learning"</A>
									<DT><A HREF="https://twitter.com/adityakusupati/status/1541554858829815809">(1) Aditya Kusupati 🪆 en X: "Introducting🪆Matryoshka Representations for Adaptive Deployment🪆 TL;DR: up to 14× lower real-world classification &amp;amp; retrival costs at web-scale at no loss in accuracy &amp;amp; w/o any overhead across setups. Paper: https://t.co/JMP9ED72L2 Code: https://t.co/SEccseeDxz [1/11] https://t.co/dXl03V1CUc" / X</A>
									<DT><A HREF="https://huggingface.co/blog/embedding-quantization">Binary and Scalar Embedding Quantization for Significantly Faster &amp; Cheaper Retrieval</A>
									<DT><A HREF="https://github.com/tensorchord/pgvecto.rs">tensorchord/pgvecto.rs: Scalable, Low-latency and Hybrid-enabled Vector Search in Postgres. Revolutionize Vector Search, not Database.</A>
									<DT><A HREF="https://github.com/UKPLab/sentence-transformers/releases/tag/v2.7.0">Release v2.7.0 - CachedGISTEmbedLoss, easy Matryoshka inference &amp; evaluation, CrossEncoder, Intel Gaudi2 · UKPLab/sentence-transformers</A>
								</DL><p>
								<DT><A HREF="https://www.patterns.app/blog/2023/02/19/ask-hn-gpt-embeddings-question-answering/">AskHN - The collective GPT-embodied wisdom of Hacker News | Patterns</A>
								<DT><A HREF="https://www.youtube.com/watch?v=HAseTSX6FT8">Fast, Accurate and Robust Multilingual Syntactic Analysis – Slav Petrov (Google) - 2012 - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>RETRO</H3>
							<DL><p>
								<DT><A HREF="https://deepmind.google/discover/blog/improving-language-models-by-retrieving-from-trillions-of-tokens/">Improving language models by retrieving from trillions of tokens - Google DeepMind</A>
								<DT><A HREF="https://arxiv.org/abs/2112.04426">[2112.04426] Improving language models by retrieving from trillions of tokens</A>
							</DL><p>
							<DT><H3 FOLDED>RAGO</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2503.14649">[2503.14649] RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/pdf/2407.12854">Scaling Retrieval-Based Language Models with a Trillion-Token Datastore</A>
							<DT><A HREF="https://media.licdn.com/dms/image/D4E22AQENEUGr4PY3Tg/feedshare-shrink_800/0/1699343585425?e=1702512000&v=beta&t=V7uFCIOwllfoE0Is1EpAWSqidGHcZJ2VBGEOpkgmzu8">OpenAI: RAG Success Story</A>
							<DT><A HREF="https://docs.google.com/presentation/d/19lAeVzPkh20Ly855tKDkz1uv-1pHV_9GxfntiTJPUug/edit#slide=id.g2584b5dafc1_0_2025">SIGIR 2023 keynote - Google DeepMind</A>
							<DT><A HREF="https://www.youtube.com/watch?v=g-VvYLhYhOg">Jerry Liu–LlamaIndex – Practical Data Considerations for building Production-Ready LLM Applications - YouTube</A>
							<DT><A HREF="https://github.com/tantaraio/voy">tantaraio/voy: 🕸️🦀 A WASM vector similarity search written in Rust</A>
							<DT><A HREF="https://twitter.com/_akhaliq/status/1742369195034099731">DocLLM: A layout-aware generative language model for multimodal document understanding</A>
							<DT><A HREF="https://arxiv.org/pdf/2209.11755.pdf">Promptagator: Few-shot Dense Retrieval From 8 Examples</A>
							<DT><A HREF="https://arxiv.org/abs/2401.00368">[2401.00368] Improving Text Embeddings with Large Language Models</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mE7IDf2SmJg">Stanford CS25: V3 I Retrieval Augmented Language Models - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=SgVZBn5QNHk">UMass CS685 S24 (Advanced NLP) #21: Detecting LLM-generated text / LLM security - YouTube</A>
							<DT><A HREF="https://x.com/headinthebox/status/1799642956657508585">(1) Erik Meijer en X: "The world's ugliest poster for https://t.co/duXtHCj3mF Yet it captures everything I want to say. https://t.co/ieQGpOdm4q" / X</A>
							<DT><A HREF="https://x.com/din0s_/status/1801271625309937771">(1) dinos en X: "📚 Awesome Information Retrieval 🔍 I’ve compiled a list of some of my favorite IR papers from the past few years. If you’re new to the field and want to understand how Transformer-based retrieval models work before building your RAG application, this should serve as a great https://t.co/SiMm6WKTZR" / X</A>
							<DT><A HREF="https://x.com/RulinShao/status/1813563054459875594">(1) Rulin Shao en X: "🔥We release the first open-source 1.4T-token RAG datastore and present a scaling study for RAG on perplexity and downstream tasks! We show LM+RAG scales better than LM alone, with better performance for the same training compute (pretraining+indexing) https://t.co/5QChH9Jn6u 🧵 https://t.co/OKXsgDV1kv" / X</A>
							<DT><A HREF="https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb">together-cookbook/Open_Contextual_RAG.ipynb at main · togethercomputer/together-cookbook</A>
							<DT><A HREF="https://arxiv.org/abs/2503.14649">[2503.14649] RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-safety</H3>
						<DL><p>
							<DT><H3 FOLDED>Red Teaming</H3>
							<DL><p>
								<DT><A HREF="https://openai.com/blog/red-teaming-network">OpenAI Red Teaming Network</A>
								<DT><A HREF="https://twitter.com/rharang/status/1725161975976497627">NVIDIA NeMo-Guardrails</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback (Antrophic)</A>
							<DT><A HREF="https://ai.meta.com/llama/purple-llama/">Purple Llama - AI at Meta</A>
							<DT><A HREF="https://developer.nvidia.com/blog/best-practices-for-securing-llm-enabled-applications/">Best Practices for Securing LLM-Enabled Applications</A>
							<DT><A HREF="https://github.com/NVIDIA/NeMo-Guardrails">NVIDIA/NeMo-Guardrails: NeMo Guardrails</A>
							<DT><A HREF="https://arxiv.org/abs/2302.10149">[2302.10149] Poisoning Web-Scale Training Datasets is Practical</A>
							<DT><A HREF="https://arxiv.org/pdf/2404.13208">The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions</A>
							<DT><A HREF="https://openai.com/index/openai-safety-update/">OpenAI safety practices | OpenAI</A>
						</DL><p>
						<DT><H3 FOLDED>ASR</H3>
						<DL><p>
							<DT><H3 FOLDED>whisper</H3>
							<DL><p>
								<DT><A HREF="https://github.com/openai/whisper">openai/whisper: Robust Speech Recognition via Large-Scale Weak Supervision</A>
								<DT><A HREF="https://github.com/ggerganov/whisper.cpp/pull/1500">whisper : quantize encoder only by ggerganov · Pull Request #1500 · ggerganov/whisper.cpp</A>
								<DT><A HREF="https://huggingface.co/ylacombe/whisper-large-v3-turbo">ylacombe/whisper-large-v3-turbo · Hugging Face</A>
								<DT><A HREF="https://x.com/FireworksAI_HQ/status/1866218532738109891">(1) Fireworks AI en X: "We made Whisper 20x faster than OpenAI*! Today, we’re beta launching the fastest and most feature-complete audio APIs - transcribe ONE HOUR of audio in as little as 4 seconds. (900:1 transcription speed!) We’re offering it FREE for 2 weeks to celebrate launch - try it https://t.co/Q0PpolyJOZ" / X</A>
								<DT><A HREF="https://x.com/phonic_co">(1) Phonic (@phonic_co) / X</A>
							</DL><p>
							<DT><A HREF="https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices">Navigating the Challenges and Opportunities of Synthetic Voices</A>
							<DT><A HREF="https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0">mozilla-foundation/common_voice_17_0 · Datasets at Hugging Face</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-biology</H3>
						<DL><p>
							<DT><A HREF="https://www.abzu.ai/">Bring drugs to market faster with Abzu®</A>
							<DT><A HREF="https://github.com/epfLLM/meditron">epfLLM/meditron: Meditron is a suite of open-source medical Large Language Models (LLMs).</A>
							<DT><H3 FOLDED>Protein</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/abs/2305.16634">[2305.16634] Machine Learning for Protein Engineering</A>
								<DT><A HREF="https://310.ai/">End 2 End AI Molecule Design – We generate diverse de novo protein sequences from just a text description of the desired properties by Mol.E , a state-of-the-art ML model</A>
								<DT><A HREF="https://www.exscientia.ai/">Exscientia | AI Drug Discovery | Pharmatech</A>
							</DL><p>
							<DT><A HREF="https://atelfo.github.io/2023/12/23/biopharma-from-janssen-to-today.html">The pharma industry from Paul Janssen to today: why drugs got harder to develop and what we can do about it | Alex’s blog</A>
							<DT><A HREF="https://github.com/google-deepmind/alphafold3">google-deepmind/alphafold3: AlphaFold 3 inference pipeline.</A>
							<DT><A HREF="https://github.com/bytedance/Protenix">bytedance/Protenix: A trainable PyTorch reproduction of AlphaFold 3.</A>
							<DT><A HREF="https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/">BiomedParse: A foundation model for smarter, all-in-one biomedical image analysis - Microsoft Research</A>
							<DT><A HREF="https://www.markov.bio/research/mech-interp-path-to-e2e-biology">Through a Glass Darkly | Markov Bio</A>
							<DT><A HREF="https://x.com/shae_mcl/status/1859817929614950431">(1) Shae Mclaughlin en X: "Visualizing transformer model attention in the UCSC genome browser (🧵). I've been exploring how DNA sequence might influence genome organization in the nucleus using transformer models. Started by pretraining a model on reference genomes from multiple species 1/7 https://t.co/hWQqnMSbju" / X</A>
							<DT><A HREF="https://x.com/miangoar/status/1857135910774514101">(1) GAMA Miguel Angel en X: "MMseqs2-GPU has been introduced, showing a 177x speed improvement. Also, a GPU back-end for FoldSeek is coming! 🔥 The future of bioinfo will be rewritten in CUDA. I'm really grateful for the work of amazing scientist like @milot_mirdita and the lab of @thesteinegger https://t.co/9hjtylIrcM" / X</A>
							<DT><A HREF="https://x.com/alexrives/status/1864345082713002437">(1) Alex Rives en X: "Introducing ESM Cambrian. Unsupervised learning can invert biology at scale to reveal the hidden structure of the natural world. We’ve scaled up compute and data to train a new generation of protein language models. ESM C defines a new state of the art for protein https://t.co/GS1ShMvadv" / X</A>
						</DL><p>
						<DT><H3 FOLDED>language-models-byte-models</H3>
						<DL><p>
							<DT><H3 FOLDED>byte-latent-transformer</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/blt">facebookresearch/blt: Code for BLT research paper</A>
								<DT><A HREF="https://dl.fbaipublicfiles.com/blt/BLT__Patches_Scale_Better_Than_Tokens.pdf">https://dl.fbaipublicfiles.com/blt/BLT__Patches_Scale_Better_Than_Tokens.pdf</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=9U1L5qEvly0">[short] Beyond Language Models: Byte Models are Digital World Simulators - YouTube</A>
							<DT><A HREF="https://arxiv.org/abs//2402.19155">[2402.19155] Beyond Language Models: Byte Models are Digital World Simulators</A>
							<DT><A HREF="https://arxiv.org/abs/2401.13660">[2401.13660] MambaByte: Token-free Selective State Space Model</A>
							<DT><A HREF="https://x.com/ArtidoroPagnoni/status/1867601413741981804">Artidoro Pagnoni en X: "🚀 Introducing the Byte Latent Transformer (BLT) – An LLM architecture that scales better than Llama 3 using byte-patches instead of tokens 🤯 Paper 📄 https://t.co/5QGrlJdK0y Code 🛠️ https://t.co/jCdDI5BXwe https://t.co/7XyZdcXWoR" / X</A>
							<DT><A HREF="https://dl.fbaipublicfiles.com/blt/BLT__Patches_Scale_Better_Than_Tokens.pdf">Byte Latent Transformer: Patches Scale Better Than Tokens</A>
						</DL><p>
						<DT><H3 FOLDED>Gemini</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2312.11805">[2312.11805] Gemini: A Family of Highly Capable Multimodal Models</A>
							<DT><A HREF="https://arxiv.org/abs/2403.05530">[2403.05530] Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</A>
							<DT><A HREF="https://arxiv.org/abs/2404.18416">[2404.18416] Capabilities of Gemini Models in Medicine</A>
							<DT><A HREF="https://arxiv.org/abs/2312.11444">[2312.11444] An In-depth Look at Gemini's Language Abilities</A>
							<DT><A HREF="https://aistudio.google.com/app/prompts/new_chat">Untitled prompt | Google AI Studio</A>
						</DL><p>
						<DT><H3 FOLDED>OLMo</H3>
						<DL><p>
							<DT><H3 FOLDED>Tulu</H3>
							<DL><p>
								<DT><A HREF="https://x.com/allen_ai/status/1859643404847808935">(1) Ai2 en X: "Meet Tülu 3 -- a set of state-of-the-art instruct models with fully open data, eval code, and training algorithms. We invented new methods for fine-tuning language models with RL and built upon best practices in the community to scale synthetic instruction and preference data. https://t.co/HFJtfmoqZH" / X</A>
								<DT><A HREF="https://allenai.org/tulu">Tulu | Ai2</A>
								<DT><A HREF="https://allenai.org/papers/tulu-3-report.pdf">TÜLU 3: Pushing Frontiers in Open Language Model Post-Training</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=qMTzor0j418">Hannaneh Hajishirzi - OLMo: Accelerating the Science of Language Modeling (COLM) - YouTube</A>
							<DT><A HREF="https://github.com/allenai/open-instruct">allenai/open-instruct</A>
						</DL><p>
						<DT><H3 FOLDED>large-languages-models-intuitions</H3>
						<DL><p>
							<DT><A HREF="https://x.com/_jasonwei/status/1729585618311950445">(1) Jason Wei en X: "It was an honor to give a guest lecture yesterday at Stanford’s CS330 class, "Deep Multi-Task and Meta-Learning"! I discussed a few very simple intuitions for how I personally think about large language models. Slides: https://t.co/NmusNTUVXb Here are the six intuitions: (1) https://t.co/qjmy7FGWWv" / X</A>
							<DT><A HREF="https://docs.google.com/presentation/d/1hQUd3pF8_2Gr2Obc89LKjmHL0DlH-uof9M0yFVd3FA4/edit#slide=id.g16197112905_0_0">jason wei cs330 talk - Google Slides</A>
							<DT><A HREF="https://x.com/hwchung27/status/1710003293223821658">(1) Hyung Won Chung en X: "I gave a talk at Seoul National University. I titled the talk “Large Language Models (in 2023)”. This was an ambitious attempt to summarize our exploding field. Video: https://t.co/91GKf7kLQy Slides: https://t.co/cigAj0M4PD Trying to summarize the field forced me to think https://t.co/PjzWf5H8vU" / X</A>
						</DL><p>
						<DT><H3 FOLDED>Apple Intelligence</H3>
						<DL><p>
							<DT><H3 FOLDED>hybrid-inference</H3>
							<DL><p>
								<DT><H3 FOLDED>minions</H3>
								<DL><p>
									<DT><A HREF="https://github.com/HazyResearch/minions">HazyResearch/minions: Big &amp; Small LLMs working together</A>
									<DT><A HREF="https://www.youtube.com/watch?v=70Kot0_DFNs">Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models - YouTube</A>
									<DT><A HREF="https://arxiv.org/pdf/2502.15964">Minions: Cost-efficient Collaboration Between On-device and Cloud
Language Models</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models">Apple Intelligence Foundation Language Models - Apple Machine Learning Research</A>
							<DT><A HREF="https://x.com/papers_anon/status/1859807437081149928">Multimodal Autoregressive Pre-training of Large Vision Encoders From Apple. V2 of the AIM generalist vision encoders (L/H/1B/3B). Pairs the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Outperforms CLIP, SigLIP</A>
							<DT><A HREF="https://x.com/TimDarcet/status/1859964751419507066">(1) TimDarcet en X: "AIMv2 looks great! When SSL and text-supervised training both work so well, it was inevitable that combining both would be a great idea Big congrats to @DonkeyShot21 @alaa_nouby @MustafaShukor1 and team!" / X</A>
						</DL><p>
						<DT><A HREF="https://github.com/deepseek-ai/DeepSeek-V2">deepseek-ai/DeepSeek-V2: DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</A>
						<DT><A HREF="https://x.com/teortaxesTex/status/1787866166242763217">papers. Deepseek Core Readings, Volume 1: 1 LLM (hyperparams, dataset basics)</A>
						<DT><A HREF="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2: Language Models are Unsupervised Multitask Learners</A>
						<DT><A HREF="https://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling with Pathways (Arch &amp; Training Setup)</A>
						<DT><A HREF="https://arxiv.org/abs/2305.10403">[2305.10403] PaLM 2 Technical Report</A>
						<DT><A HREF="https://openai.com/research/better-language-models">GPT-2: Better language models and their implications</A>
						<DT><A HREF="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</A>
						<DT><A HREF="https://research.nvidia.com/publication/2024-06_nemotron-4-340b">Nemotron-4 340B | Research</A>
						<DT><A HREF="https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T.pdf">Nemotron-4 340B Technical Report</A>
						<DT><A HREF="https://arxiv.org/abs/2210.11416">FLAN: Scaling Instruction-Finetuned Language Models</A>
						<DT><A HREF="https://arxiv.org/abs/1706.03762">[1706.03762] Attention Is All You Need</A>
						<DT><A HREF="https://openai.com/research/ai-and-compute#appendix-methods">AI and compute</A>
						<DT><A HREF="https://github.com/lucidrains/self-rewarding-lm-pytorch">lucidrains/self-rewarding-lm-pytorch: Implementation of the training framework proposed in Self-Rewarding Language Model, from MetaAI</A>
						<DT><A HREF="https://www.youtube.com/watch?v=KCXDr-UOb9A">Large Language Models in Five Formulas</A>
						<DT><A HREF="https://arxiv.org/abs/2403.15360">[2403.15360] SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series</A>
						<DT><A HREF="https://twitter.com/_akhaliq/status/1772833121609679216">InternLM2 Technical Report</A>
						<DT><A HREF="https://github.com/badripatro/Simba">badripatro/simba: Simba</A>
						<DT><A HREF="https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/">Snowflake Arctic - LLM for Enterprise AI</A>
						<DT><A HREF="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm">Introducing DBRX: A New State-of-the-Art Open LLM | Databricks Blog</A>
						<DT><A HREF="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330">Granite Code Models - a ibm-granite Collection</A>
						<DT><A HREF="https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8">Yi-1.5 (2024/05) - a 01-ai Collection</A>
						<DT><A HREF="https://cohere.com/blog/command-r">Command R: RAG at Production Scale</A>
						<DT><A HREF="https://github.com/xai-org/grok-1">xai-org/grok-1: Grok open release</A>
						<DT><A HREF="https://www.youtube.com/watch?v=zJKGhiZp1uU">Math Reading Group - Kolmogorov Arnold Networks (EvelynM) - (08/06/2024) - YouTube</A>
						<DT><A HREF="https://github.com/kyegomez/Reka-Torch">kyegomez/Reka-Torch: Implementation of the model: "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models" in PyTorch</A>
						<DT><A HREF="https://www.evolutionaryscale.ai/">EvolutionaryScale</A>
						<DT><A HREF="https://cloneofsimo.notion.site/What-to-do-to-scale-up-09e469d7c3444d6a90305397c38a46f5">What to do to scale up?</A>
						<DT><A HREF="https://imbue.com/research/70b-intro/">Training a 70B model from scratch: open-source tools, evaluation datasets, and learnings - imbue</A>
						<DT><A HREF="https://github.com/imbue-ai/cluster-health">imbue-ai/cluster-health</A>
						<DT><A HREF="https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/?utm_source=twitter&utm_medium=organic_social&utm_content=link&utm_campaign=fair">Meta Large Language Model Compiler: Foundation Models of Compiler Optimization | Research - AI at Meta</A>
						<DT><A HREF="https://proceedings.mlr.press/v202/geiping23a.html">Cramming: Training a Language Model on a single GPU in one day.</A>
						<DT><A HREF="https://www.youtube.com/watch?v=orDKvo8h71o">Stanford CS25: V4 I Hyung Won Chung of OpenAI - YouTube</A>
						<DT><A HREF="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g2885e521b53_0_0">Language Language Models (in 2023) - Google Slides</A>
						<DT><A HREF="https://arxiv.org/abs/2207.09238">[2207.09238] Formal Algorithms for Transformers</A>
						<DT><A HREF="https://publications.reka.ai/reka-core-tech-report.pdf">Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models</A>
						<DT><A HREF="https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/">Mistral AI and NVIDIA Unveil Mistral NeMo 12B, a Cutting-Edge Enterprise AI Model | NVIDIA Blog</A>
						<DT><A HREF="https://github.com/microsoft/unilm">microsoft/unilm: Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities</A>
						<DT><A HREF="https://arxiv.org/abs/2407.02783">[2407.02783] 52B to 1T: Lessons Learned via Tele-FLM Series</A>
						<DT><A HREF="https://01-ai.github.io/blog.html?post=en/2024-03-18-Dive-Deeper-into-Yi-9B.md">Dive Deeper into Yi-9B - 01.AI Blog</A>
						<DT><A HREF="https://arxiv.org/pdf/2205.05198">Reducing Activation Recomputation in Large Transformer Models</A>
						<DT><A HREF="https://arxiv.org/abs/2410.14940">[2410.14940] Nova: A Practical and Advanced Alignment</A>
						<DT><A HREF="https://mistral.ai/news/pixtral-large/">Pixtral Large | Mistral AI | Frontier AI in your hands</A>
						<DT><A HREF="https://github.com/NVlabs/hymba">NVlabs/hymba</A>
						<DT><A HREF="https://arxiv.org/abs/2501.08313">[2501.08313] MiniMax-01: Scaling Foundation Models with Lightning Attention</A>
					</DL><p>
					<DT><H3 FOLDED>Hardware</H3>
					<DL><p>
						<DT><H3 FOLDED>GPU</H3>
						<DL><p>
							<DT><H3 FOLDED>gpu-architecture</H3>
							<DL><p>
								<DT><H3 FOLDED>Instinct</H3>
								<DL><p>
									<DT><H3 FOLDED>MI300X</H3>
									<DL><p>
										<DT><A HREF="https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/data-sheets/amd-instinct-mi300x-platform-data-sheet.pdf">DATA SHEET</A>
										<DT><A HREF="https://www.colfax-intl.com/Servers/CX8850s-EI9">Colfax CX8850s-EI9 8U Rackmount Server with 8x AMD Instinct™ MI300X Accelerators</A>
										<DT><A HREF="https://www.reddit.com/r/AMD_Technology_Bets/comments/15lqkiz/no_its_not_comparable_to_the_mi300x_nvidia/">No it's not comparable to the MI300X! - "Nvidia Unveils DGX GH200 Superchip as Response to AMD’s MI300X" - chiplets of the MI300 is unique offers custom nothing nVidia's comes close! : r/AMD_Technology_Bets</A>
										<DT><A HREF="https://www.servethehome.com/amd-instinct-mi300x-gpu-and-mi300a-apus-launched-for-ai-era/amd-instinct-mi300a-vs-gh200-perf-per-watt/">AMD Instinct MI300A Vs GH200 Perf Per Watt - ServeTheHome</A>
									</DL><p>
									<DT><A HREF="https://repo.radeon.com/.hidden/cfa27af7066b8ebd5c73d75110183a62/docs/Change%20Summary_6.0.3_Known_Issues%20%281%29.pdf">AMD GPU kernel driver change summary</A>
									<DT><A HREF="https://chipsandcheese.com/2024/06/25/testing-amds-giant-mi300x/">Testing AMD’s Giant MI300X – Chips and Cheese</A>
									<DT><A HREF="https://x.com/ZyphraAI/status/1866562911663165565">(1) Zyphra en X: "We’ve been hard at work with @AMD to optimize training for AMD GPUs. Today, we’re sharing a critical milestone towards this goal: FlashAttention-2 (FA2) and Mamba-2 backward kernels on AMD MI300X that surpass NVIDIA H100. We @ZyphraAI are the first to achieve this. https://t.co/hefQdQM568" / X</A>
								</DL><p>
								<DT><A HREF="https://x.com/RajaXg/status/1812721241985610147">GPU Architecture Impact</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/66ef1df492f7bc9c8eeb01d7e14db01838e3f0bd/tensorrt_llm/auto_parallel/cluster_info.py">TensorRT-LLM/tensorrt_llm/auto_parallel/cluster_info.py</A>
								<DT><A HREF="https://news.futunn.com/en/post/42190604/nvidia-s-next-generation-gpu-revealed-integrating-eight-hbm-4?level=1&data_ticket=1715948857342483">Nvidia's next-generation GPU revealed: integrating eight HBM 4, TSMC N3 process</A>
								<DT><A HREF="https://github.com/kuterd/nv_isa_solver?tab=readme-ov-file">kuterd/nv_isa_solver: Nvidia Instruction Set Specification Generator</A>
								<DT><A HREF="https://github.com/Jokeren/Awesome-GPU">Jokeren/Awesome-GPU: Awesome resources for GPUs</A>
								<DT><A HREF="https://www.youtube.com/watch?v=h9Z4oGN89MU">How do Graphics Cards Work? Exploring GPU Architecture - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-networking</H3>
							<DL><p>
								<DT><A HREF="https://pytorchtoatoms.substack.com/p/h100b100-analyze-of-5-different-network">H100/B100: Analyze of 5 Different Network Fabrics Types. Debate Between Infiniband and Ethernet</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-micro-benchmarking</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/1903.07486.pdf">Dissecting the NVidia Turing T4 GPU via Microbenchmarking</A>
								<DT><A HREF="https://arxiv.org/pdf/1912.03413.pdf">IPU</A>
								<DT><A HREF="https://arxiv.org/pdf/1804.06826.pdf%5B/url%5D">Volta</A>
								<DT><A HREF="https://trainy.ai/blog/gpu-utilization-misleading">GPU Utilization is a Misleading Metric</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-benchmarking</H3>
							<DL><p>
								<DT><A HREF="https://github.com/FlagOpen/FlagPerf?tab=readme-ov-file">FlagOpen/FlagPerf: FlagPerf is an open-source software platform for benchmarking AI chips.</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-intel-gaudi</H3>
							<DL><p>
								<DT><A HREF="https://habana.ai/products/gaudi2/">Intel Gaudi 2 Neural Network Deep Learning Inference Processor</A>
								<DT><A HREF="https://twitter.com/EMostaque/status/1767199048337932719">(1) Emad acc/acc en X: "The @intel Gaudi2 chips are awesome &amp;amp; run the multimodal diffusion transformer arch that powers #SD3 faster than H100s (!) in scaled training pre fp8 Way cheaper TCO &amp;amp; Gaudi3 set to be 4x faster.. We also saw 673 tok/s inference on our upcoming StableBeluga 2.5 70b model (!)" / X</A>
								<DT><A HREF="https://www.intel.com/content/www/us/en/docs/graphics-for-linux/developer-reference/1-0/alchemist-arctic-sound-m.html">2023 Intel® Processors - Alchemist/Arctic Sound-M Platform</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-amd</H3>
							<DL><p>
								<DT><A HREF="https://github.com/geohot/7900xtx">geohot/7900xtx</A>
							</DL><p>
							<DT><H3 FOLDED>gpu-power-limited</H3>
							<DL><p>
								<DT><A HREF="https://www.thonking.ai/p/strangely-matrix-multiplications">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</A>
								<DT><A HREF="https://twitter.com/dwarkesh_sp/status/1780990840179187715">(1) Dwarkesh Patel en X: "Zuck on: energy-bounded AI progress"</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=jys92_j627A">Digital Design &amp; Computer Architecture -  L23: Memory Hierarchy and Caches (Spring 2022)</A>
							<DT><A HREF="https://github.com/Jokeren/Awesome-GPU">Jokeren/Awesome-GPU: Awesome resources for GPUs</A>
							<DT><A HREF="https://github.com/NVIDIA/open-gpu-doc">NVIDIA/open-gpu-doc: Documentation of NVIDIA chip/hardware interfaces</A>
							<DT><A HREF="https://nvidia.github.io/open-gpu-doc/">open-gpu-doc</A>
							<DT><A HREF="https://www.colfax-intl.com/Servers/CX8850s-EI9">Colfax CX8850s-EI9 8U Rackmount Server with 8x AMD Instinct™ MI300X Accelerators</A>
							<DT><A HREF="https://twitter.com/TechPowerUp/status/1776242252974559508">RISC-V CPU, GPU, and NPU</A>
							<DT><A HREF="https://www.youtube.com/watch?v=gofI47kfD28">Bill Dally | Directions in Deep Learning Hardware - YouTube</A>
							<DT><A HREF="https://techcommunity.microsoft.com/t5/azure-high-performance-computing/annual-roundup-of-ai-infrastructure-breakthroughs-for-2023/ba-p/4097737">Annual Roundup on AI Infrastructure Breakthroughs for 2023</A>
							<DT><A HREF="https://github.com/adam-maj/tiny-gpu">adam-maj/tiny-gpu: A minimal GPU design in Verilog to learn how GPUs work from the ground up</A>
							<DT><A HREF="https://x.com/jimkxa/status/1806105468575846509">AI on RISC-V: 10 years of mistakes</A>
							<DT><A HREF="https://github.com/moderngpu/moderngpu/wiki">Home · moderngpu/moderngpu Wiki</A>
							<DT><A HREF="https://www.youtube.com/watch?v=gofI47kfD28&t=2178s">Bill Dally | Directions in Deep Learning Hardware - YouTube</A>
							<DT><A HREF="https://blog.lepton.ai/the-missing-guide-to-the-h100-gpu-market-91ebfed34516">The Missing Guide to the H100 GPU Market | by Lepton AI | Medium</A>
							<DT><A HREF="https://gist.github.com/sophiawisdom/4962d844ee870d7d9d233cfdac98e903">some things i want to think about</A>
							<DT><A HREF="https://github.com/KohakuBlueleaf/HakuTPU">KohakuBlueleaf/HakuTPU: An AI accelerator implementation with Xilinx FPGA</A>
							<DT><A HREF="https://arxiv.org/html/2502.16631v1">CRIUgpu: Transparent Checkpointing of GPU-Accelerated Workloads</A>
						</DL><p>
						<DT><H3 FOLDED>hw-CPU</H3>
						<DL><p>
							<DT><A HREF="https://cpu.land/">Intro | Putting the "You" in CPU</A>
							<DT><A HREF="https://github.com/hackclub/putting-the-you-in-cpu">hackclub/putting-the-you-in-cpu: A technical explainer by @kognise of how your computer runs programs, from start to finish.</A>
							<DT><A HREF="https://www.anandtech.com/show/15578/cloud-clash-amazon-graviton2-arm-against-intel-and-amd">Amazon's Arm-based Graviton2 Against AMD and Intel: Comparing Cloud Compute</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GA4ONupSl8Y">Two Decades of Hardware Optimizations Down The Drain - YouTube</A>
							<DT><A HREF="https://www.modular.com/blog/understanding-simd-infinite-complexity-of-trivial-problems">Modular: Understanding SIMD: Infinite Complexity of Trivial Problems</A>
						</DL><p>
						<DT><H3 FOLDED>hw-TPU</H3>
						<DL><p>
							<DT><H3 FOLDED>tpu-v7-Ironwood</H3>
							<DL><p>
								<DT><A HREF="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/">Ironwood: The first Google TPU for the age of inference</A>
								<DT><A HREF="https://x.com/itsclivetime/status/1910026068746289286">tpu-v7-ironwood vs NVIDIA GB200</A>
							</DL><p>
							<DT><A HREF="https://github.com/ayaka14732/tpu-starter#72-parallelism">ayaka14732/tpu-starter: Everything you want to know about Google Cloud TPU</A>
							<DT><A HREF="https://github.com/ayaka14732/llama-2-jax">ayaka14732/llama-2-jax: JAX implementation of the Llama 2 model</A>
							<DT><A HREF="https://github.com/stanford-crfm/haliax">stanford-crfm/haliax: Named Tensors for Legible Deep Learning in JAX</A>
							<DT><A HREF="https://github.com/stanford-crfm/levanter">stanford-crfm/levanter: Legibile, Scalable, Reproducible Foundation Models with Named Tensors and Jax</A>
							<DT><A HREF="https://dl.acm.org/doi/pdf/10.1145/3360307">Domain Specific Hardware for training Deep Neural Networks</A>
							<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/design.html">Pallas Design — JAX documentation</A>
							<DT><A HREF="https://cloud.google.com/tpu/docs/multislice-introduction">Cloud TPU Multislice Overview [Public Preview]  |  Google Cloud</A>
							<DT><A HREF="https://github.com/google/maxtext">google/maxtext: A simple, performant and scalable Jax LLM!</A>
							<DT><A HREF="https://github.com/google/paxml">google/paxml: Pax is a Jax-based machine learning framework for training large scale models. Pax allows for advanced and fully configurable experimentation and parallelization, and has demonstrated industry leading model flop utilization rates.</A>
							<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
							<DT><A HREF="https://www.youtube.com/watch?v=NFKubflDb1A">Memory Model</A>
							<DT><A HREF="https://github.com/jinhachung/tptpu-sim">jinhachung/tptpu-sim: A Toy-Purpose TPU Simulator</A>
							<DT><A HREF="https://github.com/ayaka14732/tpu-starter#72-parallelism">ayaka14732/tpu-starter</A>
							<DT><A HREF="https://cloud.google.com/tpu/docs/multislice-introduction">Cloud TPU Multislice Overview [Public Preview]</A>
							<DT><A HREF="https://www.youtube.com/watch?v=rArv2NUXGU8&t=588s">George Hotz | Programming | tinygrad: on the Google Coral Edge TPU | reverse engineering part2 - YouTube</A>
							<DT><A HREF="https://cloud.google.com/kubernetes-engine/docs/concepts/tpus#configuration">Mapping of TPU configuration</A>
							<DT><A HREF="https://www.youtube.com/watch?v=oSCRZkSQ1CE&t=1529s">Jeff Dean (Google): Exciting Trends in Machine Learning</A>
							<DT><A HREF="https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus?hl=en">Introducing Trillium, sixth-generation TPUs | Google Cloud Blog</A>
							<DT><A HREF="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">System architecture  |  Cloud TPU  |  Google Cloud</A>
							<DT><A HREF="https://github.com/cameronshinn/tiny-tpu">cameronshinn/tiny-tpu: Small-scale Tensor Processing Unit built on an FPGA</A>
						</DL><p>
						<DT><H3 FOLDED>hw-Microsoft</H3>
						<DL><p>
							<DT><A HREF="https://news.microsoft.com/source/features/ai/in-house-chips-silicon-to-service-to-meet-ai-demand/">Maia 100</A>
							<DT><A HREF="https://twitter.com/highyieldYT/status/1724908672265150906">Maia 100 (AI accelerator) specs</A>
						</DL><p>
						<DT><H3 FOLDED>hw-IPU</H3>
						<DL><p>
							<DT><H3 FOLDED>Graphcore</H3>
							<DL><p>
								<DT><A HREF="https://www.zdnet.com/article/ai-computer-maker-graphcore-unveils-3-d-chip-promises-500-trillion-parameter-ultra-intelligence-machine/">AI computer maker Graphcore unveils 3-D chip, promises 500-trillion-parameter</A>
								<DT><A HREF="https://www.graphcore.ai/apply-for-our-machine-intelligence-academy">Apply for our Machine Intelligence Academy</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>hw-FPGA</H3>
						<DL><p>
							<DT><A HREF="http://www.clifford.at/icestorm/">Project IceStorm</A>
							<DT><A HREF="https://www.ebay.es/b/Laser-cnc-router/92150/bn_7005611421">Laser cnc router</A>
							<DT><A HREF="https://www.intel.com/content/www/us/en/products/programmable.html">Intel® FPGAs and Programmable Devices - Intel® FPGA</A>
							<DT><A HREF="https://www.xilinx.com/">Xilinx - Adaptable. Intelligent.</A>
							<DT><A HREF="https://link--springer--com.us.debiblio.com/book/10.1007/978-1-4302-6248-0">Beginning FPGA: Programming Metal</A>
							<DT><A HREF="https://link--springer--com.us.debiblio.com/book/10.1007/978-1-4757-3393-8">Functional Decomposition to FPGA Synthesis</A>
							<DT><A HREF="https://www.maxeler.com/">Maxeler Technologies</A>
							<DT><A HREF="https://taalas.com/">Taalas | The model is The Computer</A>
							<DT><A HREF="https://github.com/KohakuBlueleaf/HakuTPU">KohakuBlueleaf/HakuTPU: An AI accelerator implementation with Xilinx FPGA</A>
						</DL><p>
						<DT><H3 FOLDED>hw-Analog</H3>
						<DL><p>
							<DT><A HREF="https://escholarship.org/uc/item/5kb812qd">Stochastic Analog Computation for Machine Learning</A>
							<DT><A HREF="https://semiengineering.com/using-analog-for-ai/">Using Analog For AI</A>
						</DL><p>
						<DT><H3 FOLDED>hw-reverse-engineering</H3>
						<DL><p>
							<DT><A HREF="https://ffri.github.io/ProjectChampollion/part1/">Reverse-engineering Rosetta 2 part1: Analyzing AOT files and Rosetta 2 runtime - Project Champollion</A>
							<DT><A HREF="https://www.infoq.com/news/2020/11/rosetta-2-translation/">How x86 to arm64 Translation Works in Rosetta 2</A>
							<DT><A HREF="https://semiwiki.com/semiconductor-manufacturers/307494-the-semiconductor-ecosystem-explained/">The Semiconductor Ecosystem Explained - SemiWiki</A>
							<DT><H3 FOLDED>Ghidra</H3>
							<DL><p>
								<DT><A HREF="https://ghidra-sre.org/">Ghidra</A>
								<DT><A HREF="https://github.com/NationalSecurityAgency/ghidra">NationalSecurityAgency/ghidra: Ghidra is a software reverse engineering (SRE) framework</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Z04xTlLdZnc">George Hotz | Reverse engineering | same thing we do every weekend documenting the AMD 7900XTX Part2 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Cb2KwcnDKrk">George Hotz | Programming | tinygrad: 2.8k stars! CIFAR, ANE, speed, memory management | Part7 - YouTube</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>ASIC</H3>
						<DL><p>
							<DT><H3 FOLDED>Etched</H3>
							<DL><p>
								<DT><H3 FOLDED>sohu</H3>
								<DL><p>
									<DT><A HREF="https://www.etched.com/">Etched | The World's First Transformer ASIC</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>MatX</H3>
							<DL><p>
								<DT><A HREF="https://matx.com/">MatX | MatX: high throughput chips for LLMs</A>
							</DL><p>
							<DT><H3 FOLDED>MTIA</H3>
							<DL><p>
								<DT><A HREF="https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/?utm_source=linkedin">Our next generation Meta Training and Inference Accelerator</A>
							</DL><p>
							<DT><H3 FOLDED>tesla-DOJO</H3>
							<DL><p>
								<DT><A HREF="https://perspectives.mvdirona.com/2021/08/tesla-project-dojo-overview/">Tesla Project Dojo Overview – Perspectives</A>
							</DL><p>
							<DT><H3 FOLDED>hw-asic-aws</H3>
							<DL><p>
								<DT><A HREF="https://perspectives.mvdirona.com/2018/11/aws-inferentia-machine-learning-processor/">AWS Inferentia Machine Learning Processor – Perspectives</A>
								<DT><A HREF="https://perspectives.mvdirona.com/2022/05/graviton3-ec2-c7g-general-availability/">Graviton3 &amp; EC2 C7g General Availability – Perspectives</A>
								<DT><A HREF="https://perspectives.mvdirona.com/2020/03/anandtech-on-aws-graviton2/">Anandtech on AWS Graviton2 – Perspectives</A>
							</DL><p>
							<DT><H3 FOLDED>Tenstorrent</H3>
							<DL><p>
								<DT><A HREF="https://tenstorrent.com/category/research/">Category: Research - Tenstorrent</A>
								<DT><A HREF="https://github.com/geohot/tt-twitch">geohot/tt-twitch: tenstorrent kernel from twitch</A>
							</DL><p>
							<DT><A HREF="https://perspectives.mvdirona.com/2017/04/tensor-processing-unit/">Tensor Processing Unit – Perspectives</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M_BZOQLw6KU">Digital Design and Comp. Arch. - L13: MIPS Assembly II &amp; Memories (Spring 2024) - YouTube</A>
							<DT><A HREF="https://bytemlperf.ai/">ByteMLPerf</A>
							<DT><A HREF="https://x.com/CerebrasSystems/status/1822009569402613945">(1) Cerebras en X: "🚀 Cerebras Outperforms GPUs by 95x and CPUs by 570x in Stencil Computations Researchers at Rice University and TotalEnergies have developed the StencilPy framework, which optimizes stencil computations across various hardware architectures. On the Cerebras Wafer-Scale Engine https://t.co/eyKh6usihf" / X</A>
						</DL><p>
						<DT><H3 FOLDED>hw-neuromorphic</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>hw-memory</H3>
						<DL><p>
							<DT><H3 FOLDED>HBM</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=CPqdZZooS2g">HBM vs. GDDR6</A>
								<DT><A HREF="https://www.youtube.com/watch?v=k0ROr9_WNCI">SK hynix HBM3E Video - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=yAw63F1W_Us&t=658s">(Asiannometry) The Special Memory Powering the AI Revolution</A>
								<DT><A HREF="https://www.youtube.com/watch?v=JyhN_9MfPNA">HBM3 In The Data Center</A>
								<DT><A HREF="https://www.youtube.com/watch?v=oKyKAiEpOHU">Tackle Memory Bottlenecks with the Versal HBM Series - YouTube</A>
								<DT><A HREF="https://www.yolegroup.com/product/report/generative-ai-2024---deep-impacts-on-processors-memory-advanced-packaging-and-substrates/">Yole Group - Follow the latest trend news in the Semiconductor Industry</A>
								<DT><A HREF="https://investors.micron.com/news-releases/news-release-details/micron-delivers-industrys-fastest-highest-capacity-hbm-advance">Micron Delivers Industry’s Fastest, Highest-Capacity HBM to Advance Generative AI Innovation | Micron Technology</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=nZNd5FjSquk">CppCon 2017: John Lakos “Local ('Arena') Memory Allocators (part 1 of 2)” - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=IRZKCBp9Q7U">Seminar in Computer Architecture - L5: Memory-Centric Computing (Spring 2024) - YouTube</A>
							<DT><A HREF="https://gwern.net/doc/ai/scaling/hardware/2021-jouppi.pdf">bandwidth-capacity pareto front of all digital memory technologies</A>
							<DT><A HREF="https://www.youtube.com/watch?v=aLttVmgYRGw">Modern Solid-State Drives (SSDs) - Lecture 2: Read/Write Operations in Modern SSDs (Spring 2024) - YouTube</A>
							<DT><A HREF="https://www.linkedin.com/pulse/tearing-down-memory-wall-sharada-yeluri/?trackingId=dn9NrSPARD2R%2BUQ9KN6CXQ%3D%3D">(1) Tearing Down the Memory Wall | LinkedIn</A>
						</DL><p>
						<DT><H3 FOLDED>hw-people</H3>
						<DL><p>
							<DT><A HREF="https://substack.com/@tanjb">@tanjb (Clive Chan reading)</A>
							<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1772472408559206798">Clive Chan</A>
						</DL><p>
						<DT><H3 FOLDED>Computational Lithography</H3>
						<DL><p>
							<DT><A HREF="https://developer.nvidia.com/culitho">cuLitho - Accelerate Computational Lithography | NVIDIA Developer</A>
							<DT><A HREF="https://www.youtube.com/watch?v=HxyM2Chu9Vc">Nvidia's Computational Lithography Breakthrough - YouTube</A>
						</DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=gofI47kfD28">Bill Dally | Directions in Deep Learning Hardware - YouTube</A>
						<DT><A HREF="https://cs217.stanford.edu/">Hardware Accelerators for Machine Learning (CS 217) by cs217</A>
						<DT><A HREF="https://rain.ai/">Rain AI</A>
						<DT><A HREF="https://datacrunchltd-my.sharepoint.com/personal/ruben_datacrunch_io/_layouts/15/onedrive.aspx?FolderCTID=0x0120003C33ADC3F352284DAB5968CA9FA1A319&id=%2Fpersonal%2Fruben%5Fdatacrunch%5Fio%2FDocuments%2FDataCrunch%5FShared%2FMisc%2FSemianalysis">Semianalysis - OneDrive</A>
						<DT><A HREF="https://eliyan.com/eliyan-news/eliyan-closes-60-million-series-b-funding-round/">Chiplet Interconnect Pioneer Eliyan Closes $60 Million Series B Funding Round, Co-led by Samsung Catalyst Fund</A>
						<DT><A HREF="https://www.youtube.com/watch?v=tQhn6Fpw5HU">Future Computing Platforms - Talk at Stanford University SystemX Seminar - 08.02.2024 - YouTube</A>
						<DT><A HREF="https://x.com/i/bookmarks?post_id=1803893996093345983">tinybox-fpga</A>
						<DT><A HREF="https://x.com/__tinygrad__/status/1803893996093345983">(1) the tiny corp en X: "We are going to. There's still a ways to go on the software before we know exactly what we are building, but it gets closer every day. We need: * Fully parameterized tensor cores to be able to search over that space. Then we can try a ton of different options in emulation and" / X</A>
						<DT><A HREF="https://github.com/SeoLabCornell/torch2chip">SeoLabCornell/torch2chip: Torch2Chip (MLSys, 2024)</A>
						<DT><A HREF="https://x.com/ogawa_tter/status/1846354116450374142">(1) OGAWA, Tadashi en X: "=&amp;gt; "NVIDIA Contributes NVIDIA GB200 NVL72 Designs to Open Compute Project", Oct 15, 2024 https://t.co/3XJVAwGDC7 Press https://t.co/liza3NL4Mq MGX Accelerated Computing Rack &amp;amp; Tray Spec, OCP Server Project Call, Sep 25, https://t.co/mQnMTbPPqU HGX Form Factor Spec, OCP, May 2022 https://t.co/M4Gh5NWJDe" / X</A>
						<DT><A HREF="https://developer.nvidia.com/blog/nvidia-contributes-nvidia-gb200-nvl72-designs-to-open-compute-project/">NVIDIA Contributes NVIDIA GB200 NVL72 Designs to Open Compute Project | NVIDIA Technical Blog</A>
						<DT><A HREF="https://github.com/fengbintu/Neural-Networks-on-Silicon">fengbintu/Neural-Networks-on-Silicon: This is originally a collection of papers on neural network accelerators. Now it's more like my selection of research on deep learning and computer architecture.</A>
						<DT><A HREF="https://irrationalanalysis.substack.com/p/hot-chips-2024-irrational-recap">Hot Chips 2024: Irrational Recap - Irrational Analysis</A>
					</DL><p>
					<DT><H3 FOLDED>efficient-ai</H3>
					<DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=bfexfASu9h4">Scalable and Efficient AI: From Supercomputers to Smartphones (Microsoft)</A>
						<DT><H3 FOLDED>efficient-ai-i/o</H3>
						<DL><p>
							<DT><H3 FOLDED>NVMe</H3>
							<DL><p>
								<DT><H3 FOLDED>DeepNVMe</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-gds/README.md">DeepSpeed/blogs/deepspeed-gds/README.md at master · microsoft/DeepSpeed</A>
									<DT><A HREF="https://x.com/MSFTDeepSpeed/status/1820433545220993035">(1) DeepSpeed en X: "Introducing DeepNVMe, a suite of optimizations for fast and efficient I/O operations in DL applications. - POSIX-style APIs - Direct HBM/NVMe xfers via NVIDIA GDS - Cheap Inference scaling via NVMe-Offload Blog: https://t.co/W3SrkQQnFQ @Azure @NVIDIADC #FMS24 #GPUDirect https://t.co/c9PgXJlUD8" / X</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/releases/tag/v0.15.0">Release DeepSpeed v0.15.0 · microsoft/DeepSpeed</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/pull/5852">DeepNVMe GDS by jomayeri · Pull Request #5852 · microsoft/DeepSpeed</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/compare/main...mikaylagawarecki:pytorch:gds">Comparing pytorch:main...mikaylagawarecki:gds · pytorch/pytorch</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/pull/5852/files#diff-c0ff3e0c2ca77226e3e4621ad72e642cb2716692ccac2171972a4afcdb9fd557">DeepNVMe GDS by jomayeri · Pull Request #5852 · microsoft/DeepSpeed</A>
								</DL><p>
								<DT><A HREF="https://github.com/hpcaitech/TensorNVMe">hpcaitech/TensorNVMe: A Python library transfers PyTorch tensors between CPU and NVMe</A>
							</DL><p>
							<DT><H3 FOLDED>s3</H3>
							<DL><p>
								<DT><A HREF="https://blog.glennklockwood.com/2025/02/llm-training-without-parallel-file.html">LLM training without a parallel file system</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2101.08734">[2101.08734] Clairvoyant Prefetching for Distributed Machine Learning I/O</A>
							<DT><A HREF="https://github.com/NVIDIA/aistore/blob/master/docs/overview.md">aistore/docs/overview.md at master · NVIDIA/aistore</A>
							<DT><A HREF="https://arxiv.org/abs/2001.01858">[2001.01858] High Performance I/O For Large Scale Deep Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2101.08734">Prefetching for Distributed Machine Learning I/O (bitwise determinisn)</A>
							<DT><A HREF="https://github.com/NVIDIA/aistore/blob/master/docs/overview.md">NVIDIA/aistore</A>
							<DT><A HREF="https://arxiv.org/abs/2203.17189">Scaling Up Models and Data with t5x + seqio</A>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/datasets.html">Datasets — NVIDIA NeMo (AIStore)</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-compute</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2007.00072">[2007.00072] Data Movement Is All You Need: A Case Study on Optimizing Transformers</A>
							<DT><A HREF="https://github.com/spcl/dace">spcl/dace: DaCe - Data Centric Parallel Programming</A>
							<DT><A HREF="https://arxiv.org/abs/2210.17323">[2210.17323] GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</A>
							<DT><A HREF="https://research.nvidia.com/publication/2021-04_vs-quant-vector-scaled-quantization-accurate-low-precision-neural-network">VS-QUANT: Per-Vector Scaled Quantization for Accurate Low-Precision Neural Network Inference | Research</A>
							<DT><A HREF="https://arxiv.org/pdf/2102.04503.pdf">VS-QUANT: PER-VECTOR SCALED QUANTIZATION FOR ACCURATE LOW-PRECISION NEURAL NETWORK INFERENCE</A>
							<DT><A HREF="https://arxiv.org/abs/2102.00554">[2102.00554] Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks</A>
							<DT><A HREF="https://arxiv.org/abs/2306.03078">[2306.03078] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</A>
							<DT><A HREF="https://sparse.tamu.edu/">SuiteSparse Matrix Collection</A>
							<DT><A HREF="https://arxiv.org/abs/2304.07613">[2304.07613] STen: Productive and Efficient Sparsity in PyTorch</A>
							<DT><A HREF="https://github.com/spcl/sten">spcl/sten: Sparsity support for PyTorch</A>
							<DT><A HREF="https://sc23.supercomputing.org/presentation/?id=pap438&sess=sess178">VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores</A>
							<DT><A HREF="https://arxiv.org/abs/1712.05877">[1712.05877] Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</A>
							<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu">Kobzol/hardware-effects-gpu: Demonstration of various hardware effects on CUDA GPUs.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=q38V66bqhfU">EfficientML.ai lecture - YouTube</A>
							<DT><A HREF="https://github.com/srush/LLM-Training-Puzzles">srush/LLM-Training-Puzzles: What would you do with 1000 H100s...</A>
							<DT><A HREF="https://arxiv.org/abs/2310.07707">[2310.07707] MatFormer: Nested Transformer for Elastic Inference</A>
							<DT><A HREF="https://arxiv.org/abs/2007.00072">Data Movement Is All You Need: Case Study on Optimizing Transformers</A>
							<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu">hardware-effects-gpu: Demonstration of various hardware effects on CUDA GPUs.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=mP4BL6URdxc">EfficientML.ai Lecture 18: Distributed Training (Part II) (MIT Fall 2023)</A>
							<DT><A HREF="https://arxiv.org/abs/2306.03078">SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</A>
							<DT><A HREF="https://research.nvidia.com/publication/2021-04_vs-quant-vector-scaled-quantization-accurate-low-precision-neural-network">VS-QUANT: Per-Vector Scaled Quantization for Accurate Low-Precision Neural Network Inference</A>
							<DT><A HREF="https://arxiv.org/abs/2102.00554">Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks</A>
							<DT><A HREF="https://arxiv.org/abs/1712.05877">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</A>
							<DT><A HREF="https://twitter.com/tianle_cai/status/1770587583086764288">compute-memory bandwidth bottleneck</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-communication</H3>
						<DL><p>
							<DT><H3 FOLDED>juniper</H3>
							<DL><p>
								<DT><A HREF="https://community.juniper.net/blogs/sharada-yeluri/2024/01/02/gpu-fabrics-for-genai-workloads">GPU Fabrics for GenAI Workloads (Juniper)</A>
							</DL><p>
							<DT><A HREF="https://insujang.github.io/2022-06-11/parallelism-in-distributed-deep-learning/">Parallelism in Distributed Deep Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2201.12023">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</A>
							<DT><A HREF="https://arxiv.org/abs/1802.09941">Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis</A>
							<DT><A HREF="https://arxiv.org/abs/1802.08021">SparCML: High-Performance Sparse Communication for Machine Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2106.15565">[2106.15565] Flare: Flexible In-Network Allreduce</A>
							<DT><A HREF="https://arxiv.org/abs/2209.01346">[TPUv4] HammingMesh: A Network Topology for Large-Scale Deep Learning</A>
							<DT><A HREF="https://arxiv.org/abs/2302.03337">[Network Cloud Standard] Datacenter Ethernet and RDMA: Issues at scale</A>
							<DT><A HREF="https://www.youtube.com/watch?v=KYNHe_XyUBc">NVIDIA Networking: Understanding Ethernet Switches - YouTube</A>
							<DT><A HREF="https://dl.acm.org/doi/10.1145/3544216.3544265">Jupiter evolving | Proceedings of the ACM SIGCOMM 2022 Conference</A>
							<DT><A HREF="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41684.pdf">Google Omega: flexible, scalable schedulers for large compute clusters</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Collective_operation">Collective operation</A>
							<DT><A HREF="https://github.com/facebookresearch/ucc">Unified Collective Communication (UCC)</A>
							<DT><A HREF="https://network.nvidia.com/pdf/whitepapers/IB_Intro_WP_190.pdf">Introduction to InfiniBand</A>
							<DT><A HREF="https://github.com/open-mpi/ompi">open-mpi/ompi: Open MPI main development repository</A>
							<DT><A HREF="https://github.com/leandromoreira/linux-network-performance-parameters">leandromoreira/linux-network-performance-parameters</A>
							<DT><A HREF="https://arxiv.org/abs/2311.08105">[2311.08105] DiLoCo: Distributed Low-Communication Training LM</A>
							<DT><A HREF="https://arxiv.org/abs/1802.09941">Demystifying Parallel and Distributed Deep Learning: An In-Depth Analysis</A>
							<DT><A HREF="https://arxiv.org/abs/1802.08021">SparCML: High-Performance Sparse Communication for ML</A>
							<DT><A HREF="https://www.youtube.com/watch?v=KYNHe_XyUBc">NVIDIA Networking: Understanding Ethernet Switches</A>
							<DT><A HREF="https://developer.nvidia.com/blog/doubling-all2all-performance-with-nvidia-collective-communication-library-2-12/">Doubling all2all Performance with NCCL</A>
							<DT><A HREF="https://chatgpt.com/c/e57a5ce7-00de-486a-a8c2-c95db90cf417">non-blocking flat tree</A>
							<DT><A HREF="https://arxiv.org/abs/2305.06942">[2305.06942] Optimizing Distributed ML Communication with Fused Computation-Collective Operations</A>
							<DT><A HREF="https://le.qun.ch/en/blog/2024/12/25/libfabric-efa-0-intro/">Harnessing 3200 Gbps Network: A Journey with RDMA, EFA, and libfabric</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-supercomputing</H3>
						<DL><p>
							<DT><H3 FOLDED>supercomputing-high-performance-transaction-systems</H3>
							<DL><p>
								<DT><A HREF="https://perspectives.mvdirona.com/about-perspectives/">About Perspectives – Perspectives</A>
							</DL><p>
							<DT><H3 FOLDED>superpod</H3>
							<DL><p>
								<DT><A HREF="https://docs.nvidia.com/https:/docs.nvidia.com/dgx-superpod-reference-architecture-dgx-h100.pdf">NVIDIA DGX SuperPOD</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/tim_zaman/status/1636981863477809152?t=_WTQgylsaggk5zy5JJrlAA&s=31">Tim Zaman: Azure next-gen AI datacenter (training &amp; inference)</A>
							<DT><A HREF="https://coreweave.com/events/supercomputing-2023?utm_content=270153292&utm_medium=social&utm_source=twitter&hss_channel=tw-979803443681349632">CoreWeave @ SC23 — Events — CoreWeave</A>
							<DT><A HREF="https://colocatedeventsna2023.sched.com/event/1Rj1g">CNCF-Hosted Co-located Events North America 2023: How We Power the Largest AI Deployments</A>
							<DT><A HREF="https://www.youtube.com/@cncf">CNCF [Cloud Native Computing Foundation] - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=aSwfT4oRrvI">From Zero to Infinity: How AI-Powered Hedge Fund Build Cloud-Native AI</A>
							<DT><A HREF="https://www.youtube.com/watch?v=hHZxjH8u84s">Practice of Building AI Training Cluster Based on Kubernetes+RoCEv2</A>
							<DT><A HREF="https://cs217.stanford.edu/">Hardware Accelerators for Machine Learning (CS 217) by cs217</A>
							<DT><A HREF="https://blogs.nvidia.com/blog/nemo-amazon-titan/">NVIDIA Powers Training for Some of the Largest Amazon Titan Foundation Models | NVIDIA Blogs</A>
							<DT><A HREF="https://github.com/baidu-research/DeepBench">baidu-research/DeepBench: Benchmarking Deep Learning operations on different hardware (MUST)</A>
							<DT><A HREF="https://www.youtube.com/@cncf">CNCF [Cloud Native Computing Foundation]</A>
							<DT><A HREF="https://blogs.nvidia.com/blog/nemo-amazon-titan/">NVIDIA Training for Some of the Largest Amazon Fronteir Models</A>
							<DT><A HREF="https://github.com/baidu-research/DeepBench">DeepBench: Benchmarking Deep Learning operations on different hardware (MUST)</A>
							<DT><A HREF="https://gpulist.ai/">gpulist</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-people</H3>
						<DL><p>
							<DT><H3 FOLDED>efficient-ai-people-coreweave</H3>
							<DL><p>
								<DT><A HREF="https://github.com/anthr76">anthr76 (Anthony Rabbito)</A>
								<DT><A HREF="https://github.com/bradbeam">bradbeam (Brad Beam)</A>
								<DT><A HREF="https://github.com/ddymko">ddymko (David Dymko)</A>
								<DT><A HREF="https://github.com/dfinster">dfinster (David Finster)</A>
							</DL><p>
							<DT><A HREF="http://htor.ethz.ch/">Torsten Hoefler's Home Page</A>
							<DT><A HREF="https://arxiv.org/search/cs?searchtype=author&query=Hoefler%2C+T">Search | arXiv e-print repository</A>
							<DT><A HREF="https://github.com/suryabhupa">suryabhupa (Surya Bhupatiraju) (DeepMind)</A>
							<DT><A HREF="https://github.com/ranxian">ranxian (Ran Xian) CTO at Metabit Trading</A>
							<DT><A HREF="https://twitter.com/songhan_mit?lang=en">Instructor: Prof. Song Han</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-algorithms-structures</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=7WeraZ0LLlg&list=LL&index=840&t=3s">EfficientML.ai Lecture 13 - Transformer and LLM (Part II) (MIT 6.5940, Fall 2023) - YouTube</A>
							<DT><A HREF="https://hanlab.mit.edu/courses/2023-fall-65940">MIT 6.5940 Fall 2023 TinyML and Efficient Deep Learning Computing</A>
							<DT><A HREF="https://github.com/BearNinja123/data_structures/blob/main/stack.c">data_structures/stack.c at main</A>
						</DL><p>
						<DT><H3 FOLDED>MLPerf</H3>
						<DL><p>
							<DT><H3 FOLDED>mlperf-membership</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://github.com/neuralmagic/inference?tab=readme-ov-file">neuralmagic/inference: Reference implementations of MLPerf™ inference benchmarks</A>
							<DT><A HREF="https://mlcommons.org/">MLCommons - Better AI for Everyone</A>
						</DL><p>
						<DT><H3 FOLDED>efficient-ai-labs</H3>
						<DL><p>
							<DT><A HREF="https://huggingface.co/Efficient-Large-Model">Efficient-Large-Model (Efficient-Large-Model)</A>
						</DL><p>
						<DT><A HREF="https://arxiv.org/abs/2211.05102">[2211.05102] Efficiently Scaling Transformer Inference</A>
						<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1773268740806705266">optimal batch size (bytes per param, flops, bytes/token, arithmetic intensity )</A>
						<DT><A HREF="https://www.youtube.com/watch?v=gofI47kfD28">Bill Dally | Directions in Deep Learning Hardware - YouTube</A>
						<DT><A HREF="https://www.coreweave.com/blog/coreweaves-tensorizer-decrease-pytorch-model-load-times">Decrease PyTorch Model Load Times with CoreWeave’s Tensorizer</A>
						<DT><A HREF="https://www.youtube.com/watch?v=eZdOkDtYMoo">Song Han: Efficient Methods and Hardware for Deep Learning</A>
						<DT><A HREF="https://github.com/coreweave/tensorizer/#available-pre-tensorized-models-on-the-coreweave-cloud">coreweave/tensorizer: Module, Model, &amp; Tensor Serialization/Deserialization</A>
						<DT><A HREF="https://www.p99conf.io/?utm_source=google&utm_medium=cpc&utm_campaign=20420977396&utm_content=P99_Custom-Intent_P99-2023&utm_placement=youtube.com&gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdqVhRjjyo7gkpYIqZexrbTarZXUuFDYlrFHrlUkpWLbWgMlC8t1mv4aAoY2EALw_wcB">P99 CONF - The Event on All Things Performance</A>
						<DT><A HREF="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/pages/lecture-slides/">Lecture Slides | Performance Engineering of Software Systems</A>
						<DT><A HREF="https://arxiv.org/pdf/2211.05102.pdf">Efficiently Scaling Transformer Inference</A>
						<DT><A HREF="https://www.youtube.com/watch?v=iXSfUw7VfNw">Using Pytorch 2.0 Compile in IBM's Watsonx.AI Inference</A>
						<DT><A HREF="https://www.youtube.com/watch?v=1pg1JE2CPaA">Quatization Scaling</A>
						<DT><A HREF="https://github.com/cloud-hypervisor/cloud-hypervisor">cloud-hypervisor/cloud-hypervisor: A Virtual Machine Monitor</A>
						<DT><A HREF="https://horace.io/brrr_intro.html">Making Deep Learning go Brrrr From First Principles</A>
						<DT><A HREF="https://www.youtube.com/@rutgersefficientaiseminar9909">Rutgers Efficient AI Seminar - YouTube</A>
						<DT><A HREF="https://hanlab.mit.edu/courses/2024-fall-65940">MIT 6.5940 Fall 2024 TinyML and Efficient Deep Learning Computing</A>
						<DT><A HREF="https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB">EfficientML.ai Lecture, Fall 2023, MIT 6.5940 - YouTube</A>
						<DT><A HREF="https://research.google/blog/speed-matters/">Speed Matters</A>
						<DT><A HREF="https://2024resumedropco-design.splashthat.com/">Meta Research AI and Systems Co-Design Resume Drop (PhD New Grad/Intern)</A>
						<DT><A HREF="https://x.com/PytorchToAtoms/status/1867013852279255095">(1) Pytorch To Atoms en X: "BREAKING NEWS FROM @dylan522p vs @jefrankle debate: databricks disbanded their pretraining team &amp;amp; re-org'ed it to "training efficiency team"" / X</A>
						<DT><A HREF="https://cacm.acm.org/research/metas-hyperscale-infrastructure-overview-and-insights/">(DaaC): Meta’s Hyperscale Infrastructure: Overview and Insights – Communications of the ACM</A>
						<DT><A HREF="https://www.supercluster.blog/p/15-ai-supercluster-advanced-training">15. AI Supercluster: Advanced Training &amp; Optimization for Trillion Parameter Models</A>
					</DL><p>
					<DT><H3 FOLDED>papers-ai-compilers-pl</H3>
					<DL><p>
						<DT><H3 FOLDED>papers-ai-compilers-pl-correctness</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2306.06884">[2306.06884] A Survey of Modern Compiler Fuzzing</A>
							<DT><A HREF="https://comby.dev/blog/2022/04/11/comby-decomposer-compiler-fuzzing">Deconstructing programs for compiler fuzzing · Comby</A>
							<DT><A HREF="https://agroce.github.io/cc22.pdf">Making No-Fuss Compiler Fuzzing Effective</A>
						</DL><p>
						<DT><H3 FOLDED>papers-ai-compilers-pl-MLIR</H3>
						<DL><p>
							<DT><A HREF="https://mlir.llvm.org/docs/LangRef/">MLIR Language Reference - MLIR</A>
							<DT><A HREF="https://www.youtube.com/watch?v=5OSP5DNAozU">Building domain-specific compilers quickly with MLIR compiler infrastructure | Chris Lattner</A>
							<DT><A HREF="https://www.youtube.com/watch?v=A3qbcwasEUY">Groq Spotlight: Groq™ Compiler Overview - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=3LLzHKeL2hs">MLIR-based code generation for GPU tensor cores - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=LPlRLt9w4b0&t=306s">2023 EuroLLVM - What's new in MLIR?</A>
							<DT><A HREF="https://www.youtube.com/watch?v=xNe9fPvU7-U">Open MLIR Meeting 11-16-2023: Targeting H100 with NVGPU and NVVM Dialects - YouTube</A>
							<DT><A HREF="https://github.com/llvm/llvm-project/pull/87065">[mlir][nvgpu] NVGPU Tutorials by grypp · Pull Request #87065 · llvm/llvm-project</A>
							<DT><A HREF="https://github.com/llvm/llvm-project/pull/87065/files">[mlir][nvgpu] NVGPU Tutorials by grypp · Pull Request #87065 · llvm/llvm-project</A>
							<DT><A HREF="https://grypp.github.io/papers/nvdsl.pdf">Zero to Hero: Programming Nvidia Hopper with MLIR’s NVGPU Dialect</A>
							<DT><H3 FOLDED>byteIR</H3>
							<DL><p>
								<DT><A HREF="https://github.com/bytedance/byteir">bytedance/byteir: A model compilation solution for various hardware</A>
								<DT><A HREF="https://github.com/bytedance/byteir/blob/main/talks/ChinaSoftCon-ByteIR.pdf">byteir/talks/ChinaSoftCon-ByteIR.pdf at main · bytedance/byteir</A>
								<DT><A HREF="https://github.com/bytedance/byteir/blob/main/talks/c4ml23_poster.pdf">Linalg is All You Need to Optimize Attention</A>
							</DL><p>
							<DT><H3 FOLDED>NVGPU</H3>
							<DL><p>
								<DT><A HREF="https://mlir.llvm.org/docs/Dialects/NVGPU/">'nvgpu' Dialect - MLIR</A>
								<DT><A HREF="https://github.com/llvm/llvm-project/pull/87065">[mlir][nvgpu] NVGPU Tutorials by grypp · Pull Request #87065 · llvm/llvm-project</A>
								<DT><A HREF="https://github.com/llvm/llvm-project/pull/87065/files">[mlir][nvgpu] NVGPU Tutorials by grypp · Pull Request #87065 · llvm/llvm-project</A>
								<DT><A HREF="https://grypp.github.io/papers/nvdsl.pdf">Programming
Nvidia Hopper with MLIR’s NVGPU Dialect</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>papers-ai-compilers-pl-lectures</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=lPX1H3jW8ZQ">AI Hardware w/ Jim Keller - YouTube</A>
							<DT><A HREF="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/pages/lecture-slides/">Lecture Slides | Performance Engineering of Software Systems</A>
						</DL><p>
						<DT><H3 FOLDED>papers-ai-compilers-pl-dtypes</H3>
						<DL><p>
							<DT><H3 FOLDED>papers-ai-compilers-pl-dtypes-mx</H3>
							<DL><p>
								<DT><H3 FOLDED>fp8</H3>
								<DL><p>
									<DT><A HREF="https://tesla-cdn.thron.com/static/MXMU3S_tesla-dojo-technology_1WDVZN.pdf">A Guide to Tesla's Configurable Floating Point foRMATS &amp; Arithmetic</A>
									<DT><A HREF="https://arxiv.org/pdf/2209.05433.pdf">FP8 FORMATS FOR DEEP LEARNING</A>
									<DT><A HREF="https://github.com/pytorch/rfcs/blob/master/RFC-0030-native-fp8-dtype.md">rfcs/RFC-0030-native-fp8-dtype.md at master · pytorch/rfcs</A>
									<DT><A HREF="https://arxiv.org/pdf/1710.03740.pdf">MIXED PRECISION TRAINING</A>
									<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization — PyTorch 2.0 documentation</A>
									<DT><A HREF="https://github.com/Azure/MS-AMP">Azure/MS-AMP: Microsoft Automatic Mixed Precision Library</A>
									<DT><A HREF="https://azure.github.io/MS-AMP/">MS-AMP Documentation | MS-AMP</A>
									<DT><A HREF="https://arxiv.org/abs/2309.17224">[2309.17224] Training and inference of large language models using 8-bit floating point</A>
									<DT><A HREF="https://github.com/pytorch-labs/float8_experimental">pytorch-labs/float8_experimental</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Primitive_data_type">Primitive data type - Wikipedia</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">Single-precision floating-point format - Wikipedia</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/757275f2796bb901575c633e2a32bc76ca84ffec/include/cutlass/numeric_conversion.h#L760">numeric_conversion.h</A>
									<DT><A HREF="https://research.colfax-intl.com/adding-fp8-to-flashattention/">Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2 – Colfax Research</A>
									<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/606">FP8 Unable to achieve the expected FLOPS indicator in 4090 · Issue #606 · NVIDIA/TransformerEngine</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Vp1zZGBUy9o">🤗 Hugging Cast S2E2 - Accelerating AI with NVIDIA! - YouTube</A>
									<DT><A HREF="https://github.com/P3109/Public/blob/main/Shared%20Reports/P3109%20WG%20Interim%20Report.pdf">Public/Shared Reports/P3109 WG Interim Report.pdf at main · P3109/Public</A>
									<DT><A HREF="https://arxiv.org/pdf/2303.17951">FP8 versus INT8 for efficient deep learning inference</A>
									<DT><A HREF="https://01-ai.github.io/blog.html?post=zh/2024-07-30-%E5%A4%A7%E6%A8%A1%E5%9E%8B%20FP8%20%E4%BD%8E%E7%B2%BE%E5%BA%A6%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86.md">大模型 FP8 低精度量化推理 - 01.AI Blog</A>
									<DT><A HREF="https://gist.github.com/malfet/d9aaf3faf8b62e073f963085aa7d629b">float8_mm.cu</A>
									<DT><A HREF="https://github.com/enp1s0/simple_fp8/blob/main/include/simple_fp8.hpp">simple_fp8/include/simple_fp8.hpp at main · enp1s0/simple_fp8</A>
									<DT><A HREF="https://arxiv.org/pdf/2410.00907">ADDITION IS ALL YOU NEED
FOR ENERGY-EFFICIENT LANGUAGE MODELS</A>
								</DL><p>
								<DT><H3 FOLDED>fp6</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2401.14112">FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design</A>
									<DT><A HREF="https://github.com/usyd-fsalab/fp6_llm?tab=readme-ov-file">usyd-fsalab/fp6_llm: An efficient GPU support for LLM inference with x-bit quantization (e.g. FP6,FP5).</A>
									<DT><A HREF="https://github.com/usyd-fsalab/fp6_llm">usyd-fsalab/fp6_llm: An efficient GPU support for LLM inference with x-bit quantization (e.g. FP6,FP5).</A>
								</DL><p>
								<DT><H3 FOLDED>fp4</H3>
								<DL><p>
									<DT><A HREF="https://x.com/arankomatsuzaki/status/1884446877837582598">(Microsoft Research) Optimizing Large Language Model Training Using FP4 Quantization</A>
									<DT><A HREF="https://arxiv.org/abs/2501.17116">[2501.17116] Optimizing Large Language Model Training Using FP4 Quantization</A>
								</DL><p>
								<DT><A HREF="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">Open Compute Project • OCP Microscaling Formats (MX) Specification</A>
								<DT><A HREF="https://azure.microsoft.com/en-us/blog/fostering-ai-infrastructure-advancements-through-standardization/">Fostering AI infrastructure advancements through standardization | Microsoft Azure Blog</A>
								<DT><A HREF="https://arxiv.org/abs/2302.08007">[2302.08007] With Shared Microexponents, A Little Shifting Goes a Long Way</A>
								<DT><A HREF="https://arxiv.org/abs/2310.10537">[2310.10537] Microscaling Data Formats for Deep Learning</A>
								<DT><A HREF="https://github.com/microsoft/microxcaling">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024">DeepSpeed/blogs/deepspeed-fp6/03-05-2024 at master · microsoft/DeepSpeed</A>
								<DT><A HREF="https://www.opencompute.org/blog/open-compute-project-tackles-data-center-hardware-and-firmware-security">Open Compute Project Tackles Data Center Hardware and Firmware Security » Open Compute Project</A>
								<DT><A HREF="https://github.com/usyd-fsalab/fp6_llm">usyd-fsalab/fp6_llm: An efficient GPU support for LLM inference with x-bit quantization (e.g. FP6,FP5).</A>
								<DT><A HREF="https://github.com/microsoft/microxcaling?tab=readme-ov-file#Spec-Configuration">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
								<DT><A HREF="https://github.com/chengzeyi/AdaptiveFloat4">chengzeyi/AdaptiveFloat4: A novel high-precision 4bit quantization format</A>
							</DL><p>
							<DT><H3 FOLDED>INT8</H3>
							<DL><p>
							</DL><p>
						</DL><p>
						<DT><A HREF="https://github.com/llvm/circt">llvm/circt: Circuit IR Compilers and Tools</A>
						<DT><A HREF="https://www.youtube.com/watch?v=ZI198eFghJk">Modernizing Compiler Design for Carbon Toolchain - Chandler Carruth - CppNow 2023 - YouTube</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/FLOPS">FLOPS</A>
						<DT><A HREF="https://dl.acm.org/doi/pdf/10.1145/3620666.3651369">EVT: Accelerating Deep Learning Training with Epilogue Visitor Tree</A>
					</DL><p>
					<DT><H3 FOLDED>recommendation systems</H3>
					<DL><p>
						<DT><H3 FOLDED>torch-rec</H3>
						<DL><p>
							<DT><H3 FOLDED>generative-recommendation-systems</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/generative-recommenders">facebookresearch/generative-recommenders: Repository hosting code used to reproduce results in "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations" (https://arxiv.org/abs/2402.17152).</A>
							</DL><p>
							<DT><A HREF="https://github.com/pytorch/torchrec">pytorch/torchrec: Pytorch domain library for recommendation systems</A>
						</DL><p>
						<DT><H3 FOLDED>paddle-rec</H3>
						<DL><p>
							<DT><A HREF="https://github.com/PaddlePaddle/PaddleRec">PaddlePaddle/PaddleRec: Recommendation Algorithm大规模推荐算法库，包含推荐系统经典及最新算法LR、Wide&amp;Deep、DSSM、TDM、MIND、Word2Vec、Bert4Rec、DeepWalk、SSR、AITM，DSIN，SIGN，IPREC、GRU4Rec、Youtube_dnn、NCF、GNN、FM、FFM、DeepFM、DCN、DIN、DIEN、DLRM、MMOE、PLE、ESMM、ESCMM, MAML、xDeepFM、DeepFEFM、NFM、AFM、RALM、DMR、GateNet、NAML、DIFM、Deep Crossing、PNN、BST、AutoInt、FGCNN、FLEN、Fibinet、ListWise、DeepRec、ENSFM，TiSAS，AutoFIS等，包含经典推荐系统数据集criteo 、movielens等</A>
							<DT><A HREF="https://github.com/PaddleJitLab/DeepRecommender">PaddleJitLab/DeepRecommender: Deep learning for recommender systems</A>
						</DL><p>
						<DT><H3 FOLDED>bytedance-monolith</H3>
						<DL><p>
							<DT><A HREF="https://github.com/bytedance/monolith">bytedance/monolith: ByteDance's Recommendation System</A>
						</DL><p>
						<DT><H3 FOLDED>VideoRecSys</H3>
						<DL><p>
							<DT><A HREF="https://videorecsys.com/">VideoRecSys 2024: Large-Scale Video Recommender Systems Workshop</A>
						</DL><p>
						<DT><H3 FOLDED>recommendation-systems-netflix</H3>
						<DL><p>
							<DT><A HREF="https://netflixtechblog.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39">Foundation Model for Personalized Recommendation | by Netflix Technology Blog | Mar, 2025 | Netflix TechBlog</A>
						</DL><p>
						<DT><A HREF="https://github.com/bytedance/AffineQuant">bytedance/AffineQuant: Official implementation of the ICLR 2024 paper AffineQuant</A>
						<DT><A HREF="https://arxiv.org/abs/2402.17152">[2402.17152] Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations</A>
						<DT><A HREF="https://github.com/PaddlePaddle/PaddleRec">PaddlePaddle/PaddleRec: Recommendation Algorithm大规模推荐算法库，包含推荐系统经典及最新算法LR、Wide&amp;Deep、DSSM、TDM、MIND、Word2Vec、Bert4Rec、DeepWalk、SSR、AITM，DSIN，SIGN，IPREC、GRU4Rec、Youtube_dnn、NCF、GNN、FM、FFM、DeepFM、DCN、DIN、DIEN、DLRM、MMOE、PLE、ESMM、ESCMM, MAML、xDeepFM、DeepFEFM、NFM、AFM、RALM、DMR、GateNet、NAML、DIFM、Deep Crossing、PNN、BST、AutoInt、FGCNN、FLEN、Fibinet、ListWise、DeepRec、ENSFM，TiSAS，AutoFIS等，包含经典推荐系统数据集criteo 、movielens等</A>
						<DT><A HREF="https://github.com/PaddleJitLab/DeepRecommender">PaddleJitLab/DeepRecommender: Deep learning for recommender systems</A>
						<DT><A HREF="https://kumo.ai/resources/blog/improving-predictions-with-large-language-models/">Improving Predictions with Large Language Models - Kumo.ai</A>
						<DT><A HREF="https://github.com/facebookresearch/generative-recommenders">facebookresearch/generative-recommenders: Repository hosting code used to reproduce results in "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations" (https://arxiv.org/abs/2402.17152).</A>
						<DT><A HREF="https://videorecsys.com/">VideoRecSys 2024: Large-Scale Video Recommender Systems Workshop</A>
						<DT><A HREF="https://github.com/bytedance/monolith">bytedance/monolith: ByteDance's Recommendation System</A>
					</DL><p>
					<DT><H3 FOLDED>Conferences</H3>
					<DL><p>
						<DT><A HREF="https://scie.lcc.uma.es:8443/ratingSearch.jsf">Conference Rating - Search the GGS Rating 2021</A>
						<DT><H3 FOLDED>Call For Papers (CFP)</H3>
						<DL><p>
							<DT><A HREF="https://groups.google.com/g/ml-news/c/780yWzTWR1g/m/5ho81CbCAQAJ">European Workshop on Reinforcement Learning (EWRL 2022)</A>
						</DL><p>
						<DT><H3 FOLDED>LoG: Learning on Graphs (non rated)</H3>
						<DL><p>
							<DT><A HREF="https://logconference.github.io/cfp/">Learning on Graphs Conference</A>
							<DT><A HREF="https://towardsdatascience.com/announcing-the-learning-on-graphs-conference-c63caed7347">Announcing the Learning on Graphs Conference | by Michael Bronstein | Apr, 2022</A>
							<DT><A HREF="https://docs.google.com/forms/d/e/1FAIpQLSfacoaCBxzMXPz-AisU1DV6qya6Q1Hj3idgqWwYV61B4jC8Uw/viewform">LoG 2022 Reviewer Sign-up Form</A>
						</DL><p>
						<DT><H3 FOLDED>ICLR (A++)</H3>
						<DL><p>
							<DT><A HREF="https://openreview.net/group?id=ICLR.cc/2022/Conference">ICLR 2022 Conference | OpenReview</A>
						</DL><p>
						<DT><H3 FOLDED>NeurIPS (A++)</H3>
						<DL><p>
							<DT><A HREF="https://nips.cc/">NeurIPS</A>
							<DT><A HREF="https://ogb.stanford.edu/neurips2022/">OGB-LSC @ NeurIPS 2022 | Open Graph Benchmark</A>
						</DL><p>
						<DT><H3 FOLDED>ICML (A++)</H3>
						<DL><p>
							<DT><A HREF="https://icml.cc/">ICML</A>
						</DL><p>
						<DT><H3 FOLDED>CVPR (A++)</H3>
						<DL><p>
							<DT><A HREF="https://cvpr2022.thecvf.com/">Home | CVPR 2022</A>
						</DL><p>
						<DT><H3 FOLDED>ICCV (A++)</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>NAACL (A+)</H3>
						<DL><p>
							<DT><A HREF="https://huggingface.co/spaces/NAACL2022/papers">NAACL 2022 Papers - a Hugging Face Space by NAACL2022</A>
						</DL><p>
						<DT><H3 FOLDED>ICSE (NIER) (A++)</H3>
						<DL><p>
							<DT><A HREF="https://conf.researchr.org/track/icse-2023/icse-2023-NIER">ICSE 2023 - NIER - New Ideas and Emerging Results - ICSE 2023</A>
							<DT><A HREF="https://www.ieee.org/conferences/publishing/templates.html">IEEE - Manuscript Templates for Conference Proceedings</A>
							<DT><A HREF="https://osl.ugr.es/CTAN/macros/latex/contrib/IEEEtran/IEEEtran_HOWTO.pdf">How to Use thE IEEEtran LaTeX Class</A>
							<DT><A HREF="https://www.connectedpapers.com/">Connected Papers | Find and explore academic papers</A>
						</DL><p>
						<DT><H3 FOLDED>ASPLOS (ML SYS)</H3>
						<DL><p>
							<DT><A HREF="https://www.asplos-conference.org/">ASPLOS 2024 – San Diego, USA — April 27- May 1, 2024</A>
							<DT><A HREF="https://www.asplos-conference.org/asplos2024/">ASPLOS 2024 – ASPLOS 2024</A>
						</DL><p>
						<DT><A HREF="https://openreview.net/group?id=ICLR.cc/2022/Conference">ICLR 2022 Conference | OpenReview</A>
						<DT><A HREF="https://towardsdatascience.com/announcing-the-learning-on-graphs-conference-c63caed7347">Announcing the Learning on Graphs Conference | by Michael Bronstein | Apr, 2022</A>
					</DL><p>
					<DT><H3 FOLDED>Robotics &amp; Manufacturing</H3>
					<DL><p>
						<DT><H3 FOLDED>SLAM</H3>
						<DL><p>
							<DT><A HREF="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">Simultaneous localization and mapping - Wikipedia</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Particle_filter">Particle filter - Wikipedia</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter - Wikipedia</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation–maximization algorithm - Wikipedia</A>
							<DT><A HREF="https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html#ac4107fb146a762454a8a87715d9b7c96">OpenCV: cv::VideoCapture Class Reference</A>
							<DT><A HREF="https://www.amazon.com/Introduction-Simulation-SLAM-Alan-Pritsker/dp/0470265884">Introduction to simulation and SLAM: Pritsker, A. Alan B: 9780470265888: Amazon.com: Books</A>
							<DT><A HREF="https://fama.us.es/discovery/fulldisplay?docid=alma991000286959704987&context=L&vid=34CBUA_US:VU1&lang=es&search_scope=all_data_not_idus&adaptor=Local%20Search%20Engine&tab=all_data_not_idus&query=any,contains,Introduction%20to%20simulation%20and%20SLAM">Introduction to simulation and SLAM II</A>
							<DT><A HREF="https://pjreddie.com/darknet/">Darknet: Open Source Neural Networks in C</A>
						</DL><p>
						<DT><H3 FOLDED>Aerospace</H3>
						<DL><p>
							<DT><H3 FOLDED>Rockets</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=1reS83Njcio">Actively Stabilized Model Rocket with Real-Time Telemetry - YouTube</A>
							<DT><A HREF="https://shield.ai/">Shield AI - Building The World’s Best AI Pilot</A>
							<DT><A HREF="https://vnav.mit.edu/">MIT16.485 - Visual Navigation for Autonomous Vehicles</A>
							<DT><A HREF="https://www.youtube.com/watch?v=m04G-rskO58">China’s hypersonic drone launches revealed on video for first time - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=dYFYZ-g7fzA">From the construction of the Turbo Jet engine to the flight - just one step - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=QIK6SX2wZo4">China's company successfully tests 'JINDOU' high-speed engine for near-space travel - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Humanoid</H3>
						<DL><p>
							<DT><H3 FOLDED>Sparsh</H3>
							<DL><p>
								<DT><A HREF="https://ai.meta.com/blog/fair-robotics-open-source/">Advancing embodied AI through progress in touch perception, dexterity, and human-robot interaction</A>
								<DT><A HREF="https://ai.meta.com/research/publications/sparsh-self-supervised-touch-representations-for-vision-based-tactile-sensing/">Sparsh: Self-supervised touch representations for vision-based tactile sensing | Research - AI at Meta</A>
								<DT><A HREF="https://sparsh-ssl.github.io/">Sparsh | Self-supervised touch representations for vision-based tactile sensing</A>
							</DL><p>
							<DT><H3 FOLDED>GR00T</H3>
							<DL><p>
								<DT><A HREF="https://x.com/DrJimFan/status/1869092155991773681">(1) Jim Fan en X: "I believe solving robotics = 90% engineering + 10% research vision. Project GR00T is NVIDIA's moonshot initiative to build physical AGI for humanoid robots. The GEAR Lab is assembling a crack team right now. Join us! Openings: - Sr. Research Engineer, Robotics Systems - Sr. RE, https://t.co/hrkmVqfeEn" / X</A>
								<DT><A HREF="https://developer.nvidia.com/project-gr00t">Project GR00T Robotic Foundation Model | NVIDIA Developer</A>
								<DT><A HREF="https://x.com/TheHumanoidHub/status/1876494120761577519">(1) The Humanoid Hub en X: "Jensen, at the CES 2025 stage with 14 humanoid robots standing in the background, announced NVIDIA Isaac GR00T Blueprint. It's a simulation workflow for synthetic motion generation, enabling developers to create large datasets for training humanoids using imitation learning. https://t.co/F8BOZLrktV" / X</A>
							</DL><p>
							<DT><A HREF="https://ai.meta.com/blog/fair-robotics-open-source/">Advancing embodied AI through progress in touch perception, dexterity, and human-robot interaction</A>
							<DT><A HREF="https://www.youtube.com/watch?v=P8WwXSnKq1g">OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation - YouTube</A>
							<DT><A HREF="https://www.figure.ai/careers#careers-listing">Careers | Figure</A>
							<DT><A HREF="https://github.com/thu-ml/RoboticsDiffusionTransformer">thu-ml/RoboticsDiffusionTransformer: RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation</A>
							<DT><A HREF="http://zlethic.com/">Lingkang Zhang</A>
							<DT><A HREF="https://x.com/DrJimFan/status/1876462220806422881/photo/1">People ask me what's next. The GOAT points the way. 
Physical AI. Embodied Agents. Robotics. That's what's next.</A>
							<DT><A HREF="https://x.com/bobmcgrewai/status/1903199829943259494">manipulation &gt;&gt; locomotion</A>
						</DL><p>
						<DT><H3 FOLDED>Simulation</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/GoogleDeepMind/status/1714627619742683245">MuJoCO 3.0: GPU &amp; TPU acceleration through JAX</A>
							<DT><A HREF="https://www.olcf.ornl.gov/2024/02/29/planning-for-a-smooth-landing-on-mars/">Planning for a Smooth Landing on Mars – Oak Ridge Leadership Computing Facility</A>
						</DL><p>
						<DT><H3 FOLDED>Additive Manufacturing</H3>
						<DL><p>
							<DT><H3 FOLDED>Hadrian</H3>
							<DL><p>
								<DT><A HREF="https://www.hadrian.co/">Hadrian • Manufacturing the future</A>
							</DL><p>
							<DT><H3 FOLDED>Relativity Space</H3>
							<DL><p>
								<DT><A HREF="https://www.relativityspace.com/additive">Relativity Space - Additive</A>
							</DL><p>
							<DT><A HREF="https://dim.etsii.upm.es/metalia/">METALIA | División de Ingeniería de Máquinas</A>
						</DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=6cbayQAuvvw&t=305s">S3: They're Reimagining How to Build Anything | Hadrian</A>
						<DT><A HREF="https://twitter.com/lethic1">(1) Lethic (@lethic1) / X</A>
						<DT><A HREF="https://www.youtube.com/@lethic">Lethic Z - YouTube</A>
						<DT><A HREF="http://zlethic.com/">Lingkang Zhang</A>
						<DT><A HREF="https://github.com/zlingkang">zlingkang (Lingkang Zhang)</A>
						<DT><A HREF="https://www.anduril.com/">Anduril - Home</A>
						<DT><A HREF="https://x.com/SpaceX/status/1819772716339339664">(1) SpaceX en X: "Raptor 3 (sea level variant) Thrust: 280tf Specific impulse: 350s Engine mass: 1525kg Engine + vehicle-side commodities and hardware mass : 1720kg https://t.co/zormSroZyh" / X</A>
						<DT><A HREF="https://www.youtube.com/watch?v=2ckJt7bSqgQ">Jobs of the Future: Where Hardware Meets Software - YouTube</A>
						<DT><A HREF="https://www.youtube.com/shorts/O0qlNowndRw">3d printed robot arm, 50% speed. All joints in motion. - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>papers-miscelaneous</H3>
					<DL><p>
						<DT><A HREF="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem">Kolmogorov–Arnold representation theorem - Wikipedia</A>
						<DT><A HREF="https://blog.ntrlab.com/why-do-neural-networks-need-an-activation-function/">Why do neural networks need an activation function?</A>
						<DT><A HREF="https://hadrien-montanelli.github.io/2019-06-25.html">Deep networks and the Kolmogorov–Arnold theorem</A>
						<DT><A HREF="https://blog.ntrlab.com/">NTRLab.Blog | Turning ideas into software</A>
						<DT><A HREF="https://math.stackexchange.com/questions/2518664/are-there-any-simple-examples-of-kolmogorov-arnold-representation">simple examples of Kolmogorov-Arnold representation?</A>
					</DL><p>
					<DT><H3 FOLDED>experiments</H3>
					<DL><p>
						<DT><H3 FOLDED>wandb</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=hmewPDNUNJs">Log Your First Run With W&amp;B - YouTube</A>
							<DT><A HREF="https://wandb.ai/antferdom/min-max-gpt?nw=nwuserantferdom">min-max-gpt Workspace – Weights &amp; Biases</A>
						</DL><p>
						<DT><A HREF="https://wandb.ai/neuralink/exp14_mup_grid_search/reports/-Spectral-Transfer-MLP-s-Experiment-Results--Vmlldzo3NDQ0NTQw?accessToken=xe0mkunx3y8t0xzbzxu9caqcre57or5la58d9o209hinanlmzoaj7es24m4elvdj">[Spectral µTransfer] MLP's Experiment Results | exp14_mup_grid_search – Weights &amp; Biases</A>
					</DL><p>
					<DT><H3 FOLDED>Finance</H3>
					<DL><p>
						<DT><H3 FOLDED>semiconductors</H3>
						<DL><p>
							<DT><A HREF="https://irrationalanalysis.substack.com/p/a-background-proof-guide-on-communication">A Background-Proof Guide on Communication Systems</A>
						</DL><p>
						<DT><H3 FOLDED>quantitative-research</H3>
						<DL><p>
							<DT><H3 FOLDED>Jane Street</H3>
							<DL><p>
								<DT><A HREF="https://www.janestreet.com/join-jane-street/position/7449077002/">Machine Learning Performance Engineer :: Jane Street</A>
							</DL><p>
							<DT><A HREF="https://www.gresearch.com/">A Leading Quantitative Research and Technology Business</A>
							<DT><A HREF="https://www.fancyquantnation.com/contact">Contact — Fancy Quant NATION</A>
							<DT><A HREF="https://www.youtube.com/watch?v=qAiaBIpv60g">My Quant Career Performance in 2024 - YouTube</A>
						</DL><p>
						<DT><A HREF="https://godelterminal.com/">Godel Terminal: We don't use the mouse in finance</A>
						<DT><A HREF="https://www.google.com/search?q=Quantum+Computing+Inc+stock&client=safari&sca_esv=eccea50fd53f25b3&rls=en&sxsrf=ADLYWIJEgUFo3u4xOkLEDjip6lZdEs2wKA%3A1731622090576&ei=ynQ2Z8nqIsXVkdUPpp_C4Q0&ved=0ahUKEwjJ99_86tyJAxXFaqQEHaaPMNwQ4dUDCA8&uact=5&oq=Quantum+Computing+Inc+stock&gs_lp=Egxnd3Mtd2l6LXNlcnAiG1F1YW50dW0gQ29tcHV0aW5nIEluYyBzdG9jazIKEAAYgAQYRhj6ATIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgYQABgWGB4yBhAAGBYYHjIGEAAYFhgeMgYQABgWGB4yFhAAGIAEGEYY-gEYlwUYjAUY3QTYAQFIlBpQyANY7hhwBXgBkAEAmAHHAaAB9guqAQMwLjm4AQPIAQD4AQGYAg6gAucMwgIKEAAYsAMY1gQYR8ICDRAAGIAEGLADGEMYigXCAhkQLhiABBiwAxjRAxhDGMcBGMgDGIoF2AEBwgILEAAYgAQYhgMYigXCAggQABiABBiiBMICBRAhGKABwgIMEAAYgAQYDRhGGPoBwgIHEAAYgAQYDcICGBAAGIAEGA0YRhj6ARiXBRiMBRjdBNgBAZgDAIgGAZAGFLoGBggBEAEYCJIHBTUuNy4yoAefSA&sclient=gws-wiz-serp">Quantum Computing Inc stock - Google Search</A>
						<DT><A HREF="https://www.youtube.com/watch?v=Tqnj4lT6xv8">(1) Martin Shkreli Finance Lesson Part 1 (Full Lecture) - YouTube</A>
						<DT><A HREF="https://www.sec.gov/ix?doc=/Archives/edgar/data/0001045810/000104581024000124/nvda-20240428.htm">XBRL Viewer</A>
						<DT><A HREF="https://www.sec.gov/edgar/browse/?CIK=1045810&owner=exclude">EDGAR Entity Landing Page</A>
						<DT><A HREF="https://www.youtube.com/watch?v=HCd2UWZbn-E">(1) Martin Shkreli reacts to James Simons - Numberphile - YouTube</A>
						<DT><A HREF="https://irrationalanalysis.substack.com/p/a-background-proof-guide-on-communication">A Background-Proof Guide on Communication Systems</A>
					</DL><p>
					<DT><A HREF="https://www.youtube.com/@ArxivPapers/videos">Arxiv Papers - YouTube</A>
					<DT><A HREF="https://logconference.github.io/cfp/">Learning on Graphs Conference</A>
					<DT><A HREF="https://www.hotaipapers.com/papers">Hot AI Informations</A>
				</DL><p>
				<DT><H3 FOLDED>Software</H3>
				<DL><p>
					<DT><H3 FOLDED>AI Compilers &amp; PL</H3>
					<DL><p>
						<DT><H3 FOLDED>CUDA</H3>
						<DL><p>
							<DT><H3 FOLDED>cuda-installation</H3>
							<DL><p>
								<DT><H3 FOLDED>symlinks &amp; /etc/alternatives</H3>
								<DL><p>
									<DT><A HREF="https://chat.openai.com/c/21c4fe05-b3de-4a20-b96c-6d5c1bc7bea4">update_alternatives: multiple cuda installations</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-conda-installation</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#conda-installation">CUDA Installation Guide for Linux</A>
									<DT><A HREF="https://anaconda.org/nvidia">https://anaconda.org/nvidia</A>
									<DT><A HREF="https://anaconda.org/nvidia/repo?label=cuda-12.4.1">Package repository for nvidia :: Anaconda.org</A>
									<DT><A HREF="https://chatgpt.com/c/fc80f38a-f3a1-46a5-8a48-537e6fefe62b">LD_LIBRARY_PATH &amp; cuda lib</A>
									<DT><A HREF="https://cmake.org/cmake/help/latest/envvar/LDFLAGS.html">LDFLAGS — CMake 3.29.3 Documentation</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-12.6</H3>
								<DL><p>
									<DT><A HREF="https://developer.nvidia.com/cuda-12-6-0-download-archive?target_os=Linux&target_arch=arm64-sbsa&Compilation=Native&Distribution=Ubuntu&target_version=22.04&target_type=deb_local">CUDA Toolkit 12.6 Downloads | NVIDIA Developer</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-12.4</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>cuda-env-vars</H3>
								<DL><p>
									<DT><A HREF="https://faculty.cc.gatech.edu/~hyesoon/spr09/installcuda.html">CUDA PATH, LD_LIBRARY_PATH and CUDA_INSTALL_PATH</A>
									<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/blob/bfa2da709d619de7eb62cf58a12ecfeb24f300a0/env.src">ThunderKittens/env.src at bfa2da709d619de7eb62cf58a12ecfeb24f300a0 · HazyResearch/ThunderKittens</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/">CUDA Installation Guide for Linux</A>
								<DT><A HREF="https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local">CUDA Toolkit 12.4 Downloads | NVIDIA Developer</A>
								<DT><A HREF="https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation">cuda - How to verify CuDNN installation</A>
								<DT><A HREF="https://nvidia.github.io/cuda-python/install.html">Installation - CUDA Python 12.1.0 documentation</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/archive/10.2/cuda-installation-guide-linux/index.html">nvcc</A>
								<DT><A HREF="https://developer.download.nvidia.com/compute/cuda/redist/">Index of /compute/cuda/redist</A>
								<DT><A HREF="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/">Index of /compute/cuda/repos/ubuntu2204</A>
								<DT><A HREF="https://chat.openai.com/c/21c4fe05-b3de-4a20-b96c-6d5c1bc7bea4">CUDA_HOME</A>
								<DT><A HREF="https://github.com/phohenecker/switch-cuda/blob/master/switch-cuda.sh">switch-cuda/switch-cuda.sh at master · phohenecker/switch-cuda</A>
								<DT><A HREF="https://github.com/pytorch-labs/tritonbench/blob/main/utils/cuda_utils.py">tritonbench/utils/cuda_utils.py at main · pytorch-labs/tritonbench</A>
								<DT><A HREF="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive | NVIDIA Developer</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/archive/12.6.0/cuda-installation-guide-linux/index.html#system-requirements">CUDA Installation Guide for Linux</A>
								<DT><A HREF="https://github.com/imbue-ai/cluster-health/blob/master/health_checks/health_check_fixes/reinstall_nvidia.sh">cluster-health/health_checks/health_check_fixes/reinstall_nvidia.sh at master · imbue-ai/cluster-health</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-dev</H3>
							<DL><p>
								<DT><A HREF="https://github.com/leimao/CUTLASS-Examples/blob/f06835a42eecd52f30c8292b4b911e01fe1b02da/docker/cuda.Dockerfile">CUTLASS-Examples/docker/cuda.Dockerfile at f06835a42eecd52f30c8292b4b911e01fe1b02da · leimao/CUTLASS-Examples</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/Dockerfile">pytorch/Dockerfile at main · pytorch/pytorch</A>
								<DT><A HREF="https://hub.docker.com/r/chengzeyi/ubuntu-desktop">chengzeyi/ubuntu-desktop - Docker Image | Docker Hub</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-architecture</H3>
							<DL><p>
								<DT><H3 FOLDED>Ampere</H3>
								<DL><p>
									<DT><A HREF="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/">NVIDIA Ampere Architecture In-Depth</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/pdf/Ampere_Tuning_Guide.pdf">Ampere Tuning Guide</A>
									<DT><A HREF="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">A100 Tensor Core GPU Architecture</A>
								</DL><p>
								<DT><H3 FOLDED>Ada Lovelace</H3>
								<DL><p>
									<DT><H3 FOLDED>4090-fp16</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/123558">Support FP16 accumulation for faster LLM inference on 4090 like GPUs · Issue #123558 · pytorch/pytorch</A>
										<DT><A HREF="https://x.com/Birchlabs/status/1877537416224882770">(1) Birchlabs en X: "fp16 accumulation matmul coming to pytorch; gets around the nerf Nvidia put on the 3090/4090, allowing them to compete with datacenter-class GPUs https://t.co/nAE9UyYiuM" / X</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/pull/144441">[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt by eqy · Pull Request #144441 · pytorch/pytorch</A>
									</DL><p>
									<DT><A HREF="https://forums.developer.nvidia.com/t/ada-geforce-rtx-4090-fp8-cublaslt-performance/250737">Ada GeForce (RTX 4090) FP8 cuBLASLt performance</A>
									<DT><A HREF="https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf">NVIDIA ADA GPU ARCHITECTURE</A>
									<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/606">FP8 Unable to achieve the expected FLOPS indicator in 4090 · Issue #606 · NVIDIA/TransformerEngine</A>
								</DL><p>
								<DT><H3 FOLDED>Hopper</H3>
								<DL><p>
									<DT><H3 FOLDED>H100</H3>
									<DL><p>
										<DT><H3 FOLDED>WGMMA</H3>
										<DL><p>
											<DT><A HREF="https://research.colfax-intl.com/wp-content/uploads/2023/12/colfax-flashattention.pdf">A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library</A>
											<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">GPUs Go Brrr · Hazy Research</A>
											<DT><A HREF="https://arxiv.org/html/2402.13499v1">Without the wgmma instruction, the older mma.sync instruction can only reach about ⅔ the peak throughput of Hopper Tensor Cores:</A>
											<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/">PTX ISA 8.5</A>
											<DT><A HREF="https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/">CUTLASS Tutorial: Fast Matrix-Multiplication with WGMMA on NVIDIA® Hopper™ GPUs – Colfax Research</A>
											<DT><A HREF="https://x.com/KuterDinel/status/1808460651494293580">(1) Kuter Dinel en X: "Investigating Warpgroup MMA (Matrix Multiply Accumulate, aka Tensor Core) instructions Nvidia added with the Hopper architecture. Wonder what `gdesc` is maybe it's global descriptor ? https://t.co/CPksrNPDgm" / X</A>
										</DL><p>
										<DT><H3 FOLDED>h100-asynchrony</H3>
										<DL><p>
											<DT><H3 FOLDED>TMA</H3>
											<DL><p>
												<DT><A HREF="https://research.colfax-intl.com/tutorial-hopper-tma/">CUTLASS Tutorial: Mastering the NVIDIA® Tensor Memory Accelerator (TMA) – Colfax Research</A>
												<DT><A HREF="https://github.com/lcy-seso/DLFrameworkTest/pull/43/files#diff-6278ea2f3ba2c5944e0975de46068b9ef1f6451e2c54899d03172264dc3b3f6b">fix: refine and test tma copy. by lcy-seso · Pull Request #43 · lcy-seso/DLFrameworkTest</A>
											</DL><p>
											<DT><A HREF="https://www.youtube.com/watch?v=MC223HlPdK0">Stanford Seminar - Nvidia’s H100 GPU - YouTube</A>
										</DL><p>
										<DT><H3 FOLDED>confidential computing</H3>
										<DL><p>
											<DT><A HREF="https://github.com/NVIDIA/nvtrust">NVIDIA/nvtrust: Ancillary open source software to support confidential computing on NVIDIA GPUs</A>
											<DT><A HREF="https://learn.microsoft.com/en-us/azure/confidential-computing/confidential-computing-deployment-models">Choose Between Deployment Models | Microsoft Learn</A>
										</DL><p>
										<DT><H3 FOLDED>h100-benchmarking</H3>
										<DL><p>
										</DL><p>
										<DT><A HREF="https://kuterdinel.com/nv_isa/">https://kuterdinel.com/nv_isa/</A>
										<DT><A HREF="https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper">NVIDIA H100 Tensor Core GPU Architecture Overview</A>
										<DT><A HREF="https://lambdalabs.com/blog/flashattention-2-on-lambda-cloud-h100-vs-a100">NVIDIA H100 vs A100 Benchmarks for FlashAttention-2 on Lambda Cloud</A>
										<DT><A HREF="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">NVIDIA Hopper Architecture In-Depth</A>
										<DT><A HREF="https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper?ncid=no-ncid">NVIDIA H100 Tensor Core GPU Architecture Overview (NVIDIA H100 Tensor Core GPU Performance Specs)</A>
										<DT><A HREF="https://www.youtube.com/watch?v=USMnKuyXBFM">Developing Optimal CUDA Kernels on Hopper Tensor Cores NVIDIA On Demand - YouTube</A>
										<DT><A HREF="https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html">NVIDIA Hopper Tuning Guide</A>
										<DT><A HREF="https://mlir.llvm.org/OpenMeetings/2023-11-16-Targeting_H100_in_MLIR_ODM.pdf">Targeting H100 in MLIR</A>
									</DL><p>
									<DT><H3 FOLDED>GH200</H3>
									<DL><p>
										<DT><H3 FOLDED>GH200-pytorch</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/124807">[RFC] Mix and Match CUDA Allocators using Private Pools · Issue #124807 · pytorch/pytorch</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/pull/125722">Uses memory pools for mixing CUDA allocators by syed-ahmed · Pull Request #125722 · pytorch/pytorch</A>
											<DT><A HREF="https://discuss.pytorch.org/t/gh200-cuda-not-available-on-pytorch/200109">GH200 Cuda not available on pytorch - deployment - PyTorch Forums</A>
											<DT><A HREF="https://github.com/ggerganov/llama.cpp/issues/5026">Allow oversubscription of GPU memory through cudaMallocManaged on cuBLAS builds for systems like GH200 · Issue #5026 · ggerganov/llama.cpp</A>
										</DL><p>
										<DT><H3 FOLDED>Unified memory</H3>
										<DL><p>
											<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-memory-programming">CUDA C++ Programming Guide</A>
											<DT><A HREF="https://homepages.dcc.ufmg.br/~sylvain.collange/gpucourse/gpu_ufmg_2015_6.pdf">GPU programming: Unified memory models and more</A>
											<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-memory-programming">CUDA: 19. Unified Memory Programming</A>
											<DT><A HREF="https://www.hpcwire.com/2024/07/15/researchers-say-memory-bandwidth-and-nvlink-speeds-in-hopper-not-so-simple/">Researchers Say Memory Bandwidth and NVLink Speeds in Hopper Not So Simple</A>
											<DT><A HREF="https://arxiv.org/abs/2407.07850">[2407.07850] Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper</A>
										</DL><p>
										<DT><A HREF="https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/">NVIDIA Grace Hopper Superchip Architecture In-Depth</A>
										<DT><A HREF="https://cf-store.widencdn.net/nvdam/6/9/4/694f8b68-0f19-4f59-af54-84cb33f1e27b.pdf?response-content-disposition=inline%3B%20filename%3D%22grace-hopper-superchip-datasheet-2705455.pdf%22&response-content-type=application%2Fpdf&Expires=1710177055&Signature=lsuy6fnxVz6h-NHXjUsDtFA~pGYyWOT5uF3CZgu8H7C9rROz4w2dvCVLHg2Z4D-TFW-aqq3Z4GmPs6MGcZ6Xqj7nfwegQDQP~~9Ew-TMxsRhiEjbbQqscqnDRysCaqJph6VxudMlh-kU-c67Thwxb1VrlkhHgBOEYIO2x4nQs0LFYs92nANSSCHe71~HWCEM35rl1Kzk2yLJbAlFNUpb-c0lTPP9HoYFAq4c5tJbWcH~nOwHAjcd~uqi3EVqJk8QuHwiVysZ-GSJL7HnbVh~5lnEkBmGFvRQT7lSBcNCCWovWqX~NAhmx7VC7tvkyX5IcC7NlyirXrGeOxdHGkFOzg__&Key-Pair-Id=APKAJD5XONOBVWWOA65A">NVIDIA GH200 GraceHopper Superchip</A>
										<DT><A HREF="https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-hopper?ncid=so-link-825427-vt25#cid=hpc012_so-link_en-us">NVIDIA Grace Hopper Superchip Architecture Whitepaper</A>
										<DT><A HREF="https://resources.nvidia.com/en-us-grace-cpu/nvidia-grace-cpu-superchip?ncid=so-link-825427-vt25">NVIDIA Grace CPU Superchip Whitepaper</A>
										<DT><A HREF="https://greennode.ai/blog/nvidia-dgx-gh200-decoding-the-language-of-massive-memory">NVIDIA DGX GH200: Decoding the Language of Massive Memory</A>
										<DT><A HREF="https://github.com/abacusai/gh200-llm/pkgs/container/gh200-llm%2Fllm-train-serve">Package gh200-llm/llm-train-serve</A>
										<DT><A HREF="https://docs.nvidia.com/gh200-superchip-benchmark-guide.pdf">NVIDIA GH200 Grace Hopper Superchip Benchmark Step-by-Step Guide</A>
										<DT><A HREF="https://arxiv.org/abs/2407.07850">[2407.07850] Harnessing Integrated CPU-GPU System Memory for HPC: a first look into Grace Hopper</A>
										<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc24-s61159/">Grace Hopper Superchip Architecture and Performance Optimizations for Deep Learning Applications | NVIDIA On-Demand</A>
										<DT><A HREF="https://www.fibermall.com/blog/analysis-nvidia-gh200-chip-servers.htm?srsltid=AfmBOoqZKIicWc2wawR9tGVsqsr7cSvzkul69oWeB_1XThmuna1YzNy6">Detailed Analysis of NVIDIA GH200 Chip, Servers, and Cluster Networking - fibermall.com</A>
									</DL><p>
									<DT><H3 FOLDED>H200</H3>
									<DL><p>
										<DT><H3 FOLDED>h200-benchmarking</H3>
										<DL><p>
											<DT><A HREF="https://lambdalabs.com/blog/partner-spotlight-evaluating-nvidia-h200-gpus-for-ai-inference-with-baseten?utm_campaign=2024-10-od-cloud-baseten&utm_content=post-a&utm_medium=social&utm_source=twitter&hss_channel=tw-708922429">Partner Spotlight: Evaluating NVIDIA H200 Tensor Core GPUs for AI Inference with Baseten</A>
											<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/b171e879563ff0ba4eb35b94cf0e59a471e13d80/docs/source/blogs/H200launch.md">TensorRT-LLM/docs/source/blogs/H200launch.md at b171e879563ff0ba4eb35b94cf0e59a471e13d80 · NVIDIA/TensorRT-LLM</A>
										</DL><p>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/b171e879563ff0ba4eb35b94cf0e59a471e13d80/docs/source/blogs/H200launch.md">TensorRT-LLM/docs/source/blogs/H200launch.md at b171e879563ff0ba4eb35b94cf0e59a471e13d80 · NVIDIA/TensorRT-LLM</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2402.13499.pdf">Benchmarking and Dissecting the Nvidia Hopper GPU Architecture</A>
									<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-flashattention-2/">A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library – Colfax Research</A>
									<DT><A HREF="https://docs.nvidia.com/grace-performance-tuning-guide.pdf">NVIDIA Grace Performance Tuning Guide</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/pdf/Hopper_Tuning_Guide.pdf">Hopper Tuning Guide</A>
									<DT><A HREF="https://gist.github.com/edecoux/a0dd3f446bb901be25f2b11be231820e">Tensor Core GPU.md</A>
									<DT><A HREF="https://github.com/kuterd/nv_isa_solver">kuterd/nv_isa_solver: Nvidia Instruction Set Specification Generator</A>
									<DT><A HREF="https://github.com/cloudcores/CuAssembler">cloudcores/CuAssembler: An unofficial cuda assembler, for all generations of SASS, hopefully ：）</A>
									<DT><A HREF="https://kuterdinel.com/nv_isa/">Nvidia SM90a Instruction Set Architecture</A>
									<DT><A HREF="https://github.com/lenLRX/HopperTest">lenLRX/HopperTest</A>
									<DT><A HREF="https://github.com/lenLRX/HopperTest/blob/main/wgmma_test/power_test/test_wgmma_e4m3.cu">HopperTest/wgmma_test/power_test/test_wgmma_e4m3.cu at main · lenLRX/HopperTest</A>
								</DL><p>
								<DT><H3 FOLDED>Blackwell</H3>
								<DL><p>
									<DT><H3 FOLDED>NVL72</H3>
									<DL><p>
										<DT><H3 FOLDED>GB200</H3>
										<DL><p>
											<DT><A HREF="https://x.com/itsclivetime/status/1910026068746289286">(1) Clive Chan en X: "Google TPUv7: - 4.6 PFLOP/s FP8 - 192 GB HBM @ 7.4 TB/s - 600 GB/s (unidi) ICI - ~1000 watts Nvidia GB200: - 5 PFLOP/s FP8 / 10 PFLOP/s FP4 - 192 GB HBM @ 8 TB/s - 900 GB/s (unidi) NVLink - ~1200 watts https://t.co/mqvQfP9YyV" / X</A>
											<DT><A HREF="https://zhangguoxian.substack.com/p/whats-difference-between-gb200-and">What's difference between GB200 and GB300 - Wukong</A>
										</DL><p>
										<DT><A HREF="https://www.nvidia.com/en-us/data-center/gb200-nvl72/">GB200 NVL72 | NVIDIA</A>
										<DT><A HREF="https://blogs.nvidia.com/blog/blackwell-mlperf-inference/">NVIDIA Blackwell Takes Pole Position in Latest MLPerf Inference Results | NVIDIA Blog</A>
									</DL><p>
									<DT><H3 FOLDED>B200</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>TMEM</H3>
									<DL><p>
										<DT><H3 FOLDED>tcgen05.mma</H3>
										<DL><p>
											<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/#tcgen05-cta-pair">tcgen05 PTX ISA 8.7</A>
										</DL><p>
										<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/df8a550d3917b0e97f416b2ed8c2d786f7f686a3/examples/cute/tutorial/blackwell/01_mma_sm100.cu#L86">cutlass/examples/cute/tutorial/blackwell/01_mma_sm100.cu</A>
										<DT><A HREF="https://www.together.ai/blog/thunderkittens-nvidia-blackwell-gpus#:~:text=There%E2%80%99s%20also%20a%20new%20layer,to%20227KB%20of%20shared%20memory">ThunderKittens Now Optimized for NVIDIA Blackwell GPUs</A>
										<DT><A HREF="https://chatgpt.com/c/67efb9fd-8fb4-800c-8698-9c9dae8b22ed">Understanding TMEM in Blackwell</A>
									</DL><p>
									<DT><H3 FOLDED>2CTA</H3>
									<DL><p>
										<DT><A HREF="https://www.together.ai/blog/thunderkittens-nvidia-blackwell-gpus#:~:text=There%E2%80%99s%20also%20a%20new%20layer,to%20227KB%20of%20shared%20memory">ThunderKittens Now Optimized for NVIDIA Blackwell GPUs</A>
									</DL><p>
									<DT><A HREF="https://www.together.ai/blog/thunderkittens-nvidia-blackwell-gpus#:~:text=There%E2%80%99s%20also%20a%20new%20layer,to%20227KB%20of%20shared%20memory">ThunderKittens Now Optimized for NVIDIA Blackwell GPUs</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/blackwell_functionality.md">cutlass/media/docs/blackwell_functionality.md at main · NVIDIA/cutlass</A>
									<DT><A HREF="https://nvdam.widen.net/s/xqt56dflgh/nvidia-blackwell-architecture-technical-brief">nvidia-blackwell-architecture-technical-brief.pdf</A>
									<DT><A HREF="https://www.youtube.com/watch?v=7GV_OdqzmIU&t=430s">Cerebras Co-Founder Deconstructs Blackwell GPU Delay - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=RW2-HtWaOS0">Accelerating the Future: Triton on Blackwell Architecture - YouTube</A>
									<DT><A HREF="https://x.com/ogawa_tter/status/1843999601390678416">OGAWA: Microsoft Azure NVL72 GB200 server design</A>
									<DT><A HREF="https://developer.nvidia.com/blog/nvidia-blackwell-doubles-llm-training-performance-in-mlperf-training-v4-1/">NVIDIA Blackwell Doubles LLM Training Performance in MLPerf Training v4.1 | NVIDIA Technical Blog</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/commit/389e493055f981bfdc6d4348f823191ca7b9fddd">CUTLASS 3.8 Release (#2059) · NVIDIA/cutlass@389e493</A>
									<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc25-s74639/">Enable Tensor Core Programming in Python with CUTLASS 4.0 | GTC 25 2025 | NVIDIA On-Demand</A>
								</DL><p>
								<DT><H3 FOLDED>Rubin</H3>
								<DL><p>
									<DT><A HREF="https://www.techspot.com/news/105852-nvidia-blackwell-ai-successor-rubin-moves-forward-six.html">Next-gen Nvidia GPU "Rubin" is ahead of schedule, uses 3nm manufacturing and HBM4 | TechSpot</A>
								</DL><p>
								<DT><H3 FOLDED>gpu-reverse-engineer</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2503.20481">Analyzing Modern NVIDIA GPU cores</A>
									<DT><A HREF="https://www.youtube.com/watch?v=OUzm06YaUsI&t=8016s">George Hotz | how do GPUs work? (noob) + paper reading (not noob) | tinycorp.myshopify.com - YouTube</A>
								</DL><p>
								<DT><A HREF="https://github.com/ai-compiler-study/triton-kernels/blob/main/scripts/gpu_properties.cu">triton-kernels/scripts/gpu_properties.cu at gpu_properties</A>
								<DT><A HREF="https://arxiv.org/abs/2503.20481">[2503.20481] Analyzing Modern NVIDIA GPU cores</A>
								<DT><A HREF="https://x.com/RajaXg/status/1812721241985610147">GPU Architecture Impact</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/66ef1df492f7bc9c8eeb01d7e14db01838e3f0bd/tensorrt_llm/auto_parallel/cluster_info.py">TensorRT-LLM/tensorrt_llm/auto_parallel/cluster_info.py</A>
								<DT><A HREF="https://news.futunn.com/en/post/42190604/nvidia-s-next-generation-gpu-revealed-integrating-eight-hbm-4?level=1&data_ticket=1715948857342483">Nvidia's next-generation GPU revealed: integrating eight HBM 4, TSMC N3 process</A>
								<DT><A HREF="https://github.com/kuterd/nv_isa_solver?tab=readme-ov-file">kuterd/nv_isa_solver: Nvidia Instruction Set Specification Generator</A>
								<DT><A HREF="https://github.com/Jokeren/Awesome-GPU">Jokeren/Awesome-GPU: Awesome resources for GPUs</A>
								<DT><A HREF="https://www.youtube.com/watch?v=h9Z4oGN89MU">How do Graphics Cards Work? Exploring GPU Architecture - YouTube</A>
								<DT><A HREF="https://x.com/itsclivetime/status/1911543695473516689">HW-SW codesign: CSR, LLVM, two-path adder, CoWoS, DfT, STCO, SMPS</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-programming-model</H3>
							<DL><p>
								<DT><H3 FOLDED>cuda-memory-model</H3>
								<DL><p>
									<DT><H3 FOLDED>cuda-indexing</H3>
									<DL><p>
										<DT><A HREF="https://veitner.bearblog.dev/indexing-in-cuda/">Indexing in CUDA | simons blog</A>
									</DL><p>
									<DT><H3 FOLDED>cuda-swizzling</H3>
									<DL><p>
										<DT><A HREF="https://leimao.github.io/blog/CUDA-Shared-Memory-Swizzling/">CUDA Shared Memory Swizzling - Lei Mao's Log Book</A>
									</DL><p>
									<DT><H3 FOLDED>shared-memory</H3>
									<DL><p>
										<DT><A HREF="https://github.com/lcy-seso/vq-experiments/pull/8/files#diff-3105931cb25596ad015fdb28c71374aef0605adacc69fc28cd758e21ae63f5fd">feat: utility function for calculating shared memory usage. by lcy-seso · Pull Request #8 · lcy-seso/vq-experiments</A>
										<DT><A HREF="https://github.com/lcy-seso/vq-experiments/blob/3aef9f38da6dd19f1c9b3aa03410724a2457e0ac/shared_memory_calculator/README.md">vq-experiments/shared_memory_calculator/README.md: shared memory per block (H100 228 KB)</A>
									</DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy">Memory Hierarchy</A>
									<DT><A HREF="https://www.youtube.com/watch?v=uHN5fpfu8As">Memory hierarchy</A>
									<DT><A HREF="https://developer.nvidia.com/gtc/2020/video/cwe21754">GTC 2020: Memory Management on Modern GPU | NVIDIA Developer</A>
									<DT><A HREF="https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21819-optimizing-applications-for-nvidia-ampere-gpu-architecture.pdf">GPU memory architecture</A>
									<DT><A HREF="https://developer.download.nvidia.com/CUDA/training/register_spilling.pdf">Local memory/register spilling</A>
									<DT><A HREF="https://lwn.net/Articles/253361/">Memory part 3: Virtual Memory [LWN.net]</A>
									<DT><A HREF="https://christianjmills.com/posts/cuda-mode-notes/lecture-004/">Christian Mills - CUDA MODE Lecture 4: Compute and Memory Basics</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-thread-divergence</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=95fFNzV4sd0">Parallel C++: Thread Affinity - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-branchless-computing</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>cuda-array-interface</H3>
								<DL><p>
									<DT><A HREF="https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html">CUDA Array Interface (Version 3) — Numba 0+untagged.2155.g9ce83ef.dirty documentation</A>
									<DT><A HREF="https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.interface.html#__array_interface__">The Array Interface — NumPy v1.13 Manual</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-dev</H3>
								<DL><p>
									<DT><A HREF="https://github.com/PaddleJitLab/CUDATutorial/tree/develop/docs/01_build_dev_env">CUDATutorial/docs/01_build_dev_env at develop · PaddleJitLab/CUDATutorial</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-learning</H3>
								<DL><p>
									<DT><H3 FOLDED>PMPP</H3>
									<DL><p>
										<DT><A HREF="https://github.com/drisspg/simple_cuda">drisspg/simple_cuda: Learnings + Exercises from the PMPP book!</A>
										<DT><A HREF="https://github.com/olcf/cuda-training-series">olcf/cuda-training-series: Training materials associated with NVIDIA's CUDA Training Series (www.olcf.ornl.gov/cuda-training-series/)</A>
										<DT><A HREF="https://www.amazon.es/Programming-Massively-Parallel-Processors-Hands/dp/0323912311">Programming Massively Parallel Processors: A Hands-on Approach : Hwu, Wen-mei W., Kirk, David B., El Hajj, Izzat: Amazon.es: Libros</A>
										<DT><A HREF="https://shop.elsevier.com/books/programming-massively-parallel-processors/hwu/978-0-323-91231-0">Programming Massively Parallel Processors - 4th Edition | Elsevier Shop</A>
										<DT><A HREF="http://gpu.di.unimi.it/books/PMPP-3rd-Edition.pdf">‎gpu.di.unimi.it/books/PMPP-3rd-Edition.pdf</A>
										<DT><A HREF="https://www.cse.iitd.ac.in/~rijurekha/col730_2022/cudabook.pdf">NVIDIA version</A>
										<DT><A HREF="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311">Programming Massively Parallel Processors: A Hands-on Approach: Hwu, Wen-mei W., Kirk, David B., El Hajj, Izzat: 9780323912310: Amazon.com: Books</A>
										<DT><A HREF="https://github.com/drisspg/simple_cuda/tree/main/examples">simple_cuda/examples at main · drisspg/simple_cuda</A>
									</DL><p>
									<DT><H3 FOLDED>cuda-examples</H3>
									<DL><p>
										<DT><A HREF="https://github.com/KuangjuX/CUDAKernels">KuangjuX/CUDAKernels: 🎉My Collections of CUDA Kernels~</A>
										<DT><A HREF="https://github.com/tensor-fusion/CUDA-Transformer/blob/main/transformer_cuda.py#L37">CUDA-Transformer/transformer_cuda.py at main · tensor-fusion/CUDA-Transformer</A>
										<DT><A HREF="https://github.com/abhisheknair10/Llama3.cu">abhisheknair10/Llama3.cu: Lightweight Llama 3-8B Inference Engine in CUDA C</A>
									</DL><p>
									<DT><A HREF="https://github.com/cuda-mode">CUDA MODE</A>
									<DT><A HREF="http://giantpandacv.com/project/CUDA/%E3%80%90BBuf%E7%9A%84CUDA%E7%AC%94%E8%AE%B0%E3%80%91%E4%B8%80%EF%BC%8C%E8%A7%A3%E6%9E%90OneFlow%20Element-Wise%20%E7%AE%97%E5%AD%90%E5%AE%9E%E7%8E%B0/">【BBuf的CUDA笔记】一，解析OneFlow Element-Wise 算子实现 - GiantPandaCV</A>
									<DT><A HREF="https://github.com/BBuf/how-to-optim-algorithm-in-cuda">BBuf/how-to-optim-algorithm-in-cuda: how to optimize some algorithm in cuda.</A>
									<DT><A HREF="https://github.com/NVIDIA/cuda-samples">NVIDIA/cuda-samples: Samples for CUDA Developers which demonstrates features in CUDA Toolkit</A>
									<DT><A HREF="https://github.com/cuda-mode/lectures">cuda-mode/lectures: Material for cuda-mode lectures</A>
									<DT><A HREF="https://github.com/mlecauchois/micrograd-cuda/blob/main/micrograd_cuda/operations.py">micrograd-cuda/micrograd_cuda/operations.py at main · mlecauchois/micrograd-cuda</A>
									<DT><A HREF="https://github.com/shineyruan/CUDA-Stream-Compaction">shineyruan/CUDA-Stream-Compaction</A>
									<DT><A HREF="https://ahgamut.github.io/">Blog Needs a Name</A>
									<DT><A HREF="https://www.youtube.com/playlist?list=PL5Q2soXY2Zi_OwkTgEyA6tk3UsoPBH737">Livestream - P&amp;S Hands-on Acceleration on Heterogeneous Computing Systems (Fall 2021) - YouTube</A>
									<DT><A HREF="https://www.youtube.com/playlist?list=PL5Q2soXY2Zi-Mnk1PxjEIG32HAGILkTOF">Livestream - Computer Architecture - ETH Zürich (Fall 2021) - YouTube</A>
									<DT><A HREF="https://github.com/FZJ-JSC/tutorial-multi-gpu">FZJ-JSC/tutorial-multi-gpu: Efficient Distributed GPU Programming for Exascale, an SC/ISC Tutorial</A>
									<DT><A HREF="https://homepages.dcc.ufmg.br/~sylvain.collange/gpucourse/">Index of /~sylvain.collange/gpucourse</A>
									<DT><A HREF="https://www.youtube.com/watch?v=qYqrfq452ig&list=LL&index=19&t=4582s">Hardcore CUDA Hackathon Talks at AGI House SF - YouTube</A>
									<DT><A HREF="https://github.com/PacktPublishing/Learn-CUDA-Programming">PacktPublishing/Learn-CUDA-Programming: Learn CUDA Programming, published by Packt</A>
									<DT><A HREF="https://github.com/bongwonjang/ImageNet-CUDA?tab=readme-ov-file">bongwonjang/ImageNet-CUDA</A>
									<DT><A HREF="https://github.com/olcf/cuda-training-series">olcf/cuda-training-series: Training materials associated with NVIDIA's CUDA Training Series (www.olcf.ornl.gov/cuda-training-series/)</A>
									<DT><A HREF="https://github.com/PaddleJitLab/CUDATutorial/blob/develop/docs/00_prev_concept/README.md">CUDATutorial/docs/00_prev_concept/README.md at develop · PaddleJitLab/CUDATutorial</A>
									<DT><A HREF="https://github.com/ifromeast/cuda_learning/blob/main/04_transformer/train_gpt2.cu">cuda_learning/04_transformer/train_gpt2.cu at main · ifromeast/cuda_learning</A>
									<DT><A HREF="https://github.com/Tony-Tan/CUDA_Freshman">Tony-Tan/CUDA_Freshman</A>
									<DT><A HREF="https://github.com/cloneofsimo/torchcu/blob/main/.kernel_cache/wp___main___8109c9e/module_codegen.cu">torchcu/.kernel_cache/wp___main___8109c9e/module_codegen.cu at main · cloneofsimo/torchcu</A>
									<DT><A HREF="https://github.com/MuGdxy/muda">MuGdxy/muda: μ-Cuda, COVER THE LAST MILE OF CUDA. With features: intellisense-friendly, structured launch, automatic cuda graph generation and updating.</A>
									<DT><A HREF="https://github.com/DefTruth/CUDA-Learn-Notes">DefTruth/CUDA-Learn-Notes: 🎉 Modern CUDA Learn Notes with PyTorch: fp32, fp16, bf16, fp8/int8, flash_attn, sgemm, sgemv, warp/block reduce, dot, elementwise, softmax, layernorm, rmsnorm.</A>
									<DT><A HREF="https://github.com/moderngpu/moderngpu/wiki/Getting-started#cloning-the-source">Getting started · moderngpu/moderngpu Wiki</A>
									<DT><A HREF="https://www.youtube.com/watch?v=eqkAaplKBc4&t=3159s">(1) A Taste of GPU Compute - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=86FAWCzIe_4">CUDA Programming Course – High-Performance Computing with GPUs - YouTube</A>
									<DT><A HREF="https://github.com/SzymonOzog/GPU_Programming">SzymonOzog/GPU_Programming</A>
									<DT><A HREF="http://www.cudahandbook.com/">‎www.cudahandbook.com</A>
									<DT><A HREF="https://www.youtube.com/watch?v=4pkbXmE4POc&list=PLRRuQYjFhpmubuwx-w8X964ofVkW1T8O4">Lecture 01 - Introduction - YouTube</A>
									<DT><A HREF="https://mlforsystems.org/assets/papers/neurips2024/paper32.pdf">WarpDrive: An Agentic Workflow for Ninja GPU Transformations</A>
									<DT><A HREF="https://www.youtube.com/watch?v=4pkbXmE4POc&t=1s">Lecture 01 - Introduction - YouTube</A>
									<DT><A HREF="https://github.com/PaddleJitLab/CUDATutorial">PaddleJitLab/CUDATutorial: A self-learning tutorail for CUDA High Performance Programing.</A>
									<DT><A HREF="https://github.com/lcy-seso/DLFrameworkTest">lcy-seso/DLFrameworkTest: My tests and experiments with some popular dl frameworks.</A>
									<DT><A HREF="https://leimao.github.io/">Lei Mao - Lei Mao's Log Book</A>
								</DL><p>
								<DT><H3 FOLDED>Systolic array</H3>
								<DL><p>
									<DT><A HREF="https://en.wikipedia.org/wiki/Systolic_array">Systolic array - Wikipedia</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-cooperative-groups</H3>
								<DL><p>
									<DT><A HREF="https://leimao.github.io/blog/CUDA-Cooperative-Groups/">CUDA Cooperative Groups - Lei Mao's Log Book</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">1. Introduction — CUDA C Programming Guide</A>
								<DT><A HREF="https://x.com/RajaXg/status/1812721241985610147">GPU Architecture Impact</A>
								<DT><A HREF="https://github.com/IonThruster/CudaTutorials/blob/master/strategy.md">CudaTutorials/strategy.md</A>
								<DT><A HREF="https://jhui.github.io/2017/03/06/CUDA/">“CUDA Tutorial”</A>
								<DT><A HREF="https://www.olcf.ornl.gov/calendar/cuda-shared-memory/">CUDA Shared Memory – Oak Ridge Leadership Computing Facility</A>
								<DT><A HREF="https://www.olcf.ornl.gov/cuda-training-series/">CUDA Training Series – Oak Ridge Leadership Computing Facility</A>
								<DT><A HREF="https://github.com/olcf/cuda-training-series">olcf/cuda-training-series: Training materials associated with NVIDIA's CUDA Training Series (www.olcf.ornl.gov/cuda-training-series/)</A>
								<DT><A HREF="https://github.com/NVIDIA/multi-gpu-programming-models">NVIDIA/multi-gpu-programming-models: Examples demonstrating available options to program multiple GPUs in a single node or a cluster</A>
								<DT><A HREF="https://github.com/BBuf/how-to-optim-algorithm-in-cuda">BBuf/how-to-optim-algorithm-in-cuda: how to optimize some algorithm in cuda.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=KHa-OSrZPGo">CppCon 2016: “Bringing Clang and C++ to GPUs: An Open-Source, CUDA-Compatible GPU C++ Compiler" - YouTube</A>
								<DT><A HREF="https://tripack45.github.io/2019/10/20/intern2019/">Parallelism on Its Own Feet: 90 Days as an NVidia Intern | Patricium</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md">cutlass/media/docs/cute/00_quickstart.md at main · NVIDIA/cutlass</A>
								<DT><A HREF="https://github.com/NVIDIA/cuda-samples">NVIDIA/cuda-samples: Samples for CUDA Developers which demonstrates features in CUDA Toolkit</A>
								<DT><A HREF="https://github.com/Tony-Tan/CUDA_Freshman">Tony-Tan/CUDA_Freshman</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cXpTDKjjKZE">03 CUDA Fundamental Optimization Part 1 - YouTube</A>
								<DT><A HREF="https://github.com/cuda-mode/lecture2">cuda-mode/lecture2: lecture 2 - 2024-01-20</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NQ-0D5Ti2dc">Lecture 2 Ch1-3 PMPP book</A>
								<DT><A HREF="https://www.youtube.com/watch?v=OSpy-HoR0ac">Intro to CUDA (part 5): Memory Model - YouTube</A>
								<DT><A HREF="https://github.com/torstem/demo-cuda-pybind11">demo-cuda-pybind11</A>
								<DT><A HREF="https://github.com/cuda-mode/resource-stream">cuda-mode/resource-stream: CUDA related news and material links</A>
								<DT><A HREF="https://github.com/cuda-mode/lectures">cuda-mode/lectures: Material for cuda-mode lectures</A>
								<DT><A HREF="https://www.youtube.com/watch?v=u70a9ssZnjI&list=LL&index=17">George Hotz | Programming | cherry computer: no more superscalar? the thneed lesson - YouTube</A>
								<DT><A HREF="https://blog.speechmatics.com/pointless-gpu-optimization-exercise">An Almost Pointless Exercise in GPU Optimization</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/">CUDA C++ Best Practices Guide</A>
								<DT><A HREF="https://github.com/NVIDIA/multi-gpu-programming-models">NVIDIA/multi-gpu-programming-models: Examples demonstrating available options to program multiple GPUs</A>
								<DT><A HREF="https://leimao.github.io/blog/Row-Major-VS-Column-Major/">Row-Major VS Column-Major - Lei Mao's Log Book</A>
								<DT><A HREF="https://homepages.dcc.ufmg.br/~sylvain.collange/gpucourse/">Index of /~sylvain.collange/gpucourse</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#introduction">CUDA C++ Programming Guide</A>
								<DT><A HREF="https://www.youtube.com/watch?v=c8mQYGbT310">Introduction | GPU Programming | Episode 0 - YouTube</A>
								<DT><A HREF="https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/">CUDA Refresher: The CUDA Programming Model | NVIDIA Technical Blog</A>
								<DT><A HREF="https://www.youtube.com/watch?v=0-ztm8SKq70&t=977s">Stanford CS149 I Parallel Computing I 2023 I Lecture 4 - Parallel Programming Basics - YouTube</A>
								<DT><A HREF="https://github.com/chenzomi12/AISystem/blob/main/03Compiler/04Backend/04LoopOpt.pdf">AISystem/03Compiler/04Backend/04LoopOpt.pdf at main · chenzomi12/AISystem</A>
								<DT><A HREF="https://www.youtube.com/watch?v=31ZyYkoClT4">Lecture 05 - Memory and Tiling - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-eoUw8fTy2E&t=4s">Lecture 11 - Scan (Kogge Stone) - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=XTfH6Ll9KaA">Lecture 15 - Sort - YouTube</A>
								<DT><A HREF="https://github.com/PaddleJitLab/CUDATutorial">PaddleJitLab/CUDATutorial: A self-learning tutorail for CUDA High Performance Programing.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=OUzm06YaUsI">George Hotz | how do GPUs work? (noob) + paper reading (not noob) | tinycorp.myshopify.com - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>PTX-Parallel Thread Execution</H3>
							<DL><p>
								<DT><H3 FOLDED>NVVM IR</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/7_libNVVM">cuda-samples/Samples/7_libNVVM at master · NVIDIA/cuda-samples</A>
								</DL><p>
								<DT><H3 FOLDED>CTA Cooperative Thread Array</H3>
								<DL><p>
									<DT><A HREF="https://www.together.ai/blog/thunderkittens-nvidia-blackwell-gpus#:~:text=There%E2%80%99s%20also%20a%20new%20layer,to%20227KB%20of%20shared%20memory">ThunderKittens Now Optimized for NVIDIA Blackwell GPUs</A>
								</DL><p>
								<DT><H3 FOLDED>source-in-ptx</H3>
								<DL><p>
									<DT><A HREF="https://forums.developer.nvidia.com/t/how-to-see-ptx-cu-source-code/220043">How to see PTX/CU/source code? - Developer Tools / Nsight Compute - NVIDIA Developer Forums</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/pdf/CUDA_Compiler_Driver_NVCC.pdf">NVIDIA CUDA Compiler Driver Release 12.4</A>
								</DL><p>
								<DT><H3 FOLDED>ptx-isa-8.7</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/#warp-level-matrix-multiply-accumulate-instructions">1. Introduction — PTX ISA 8.7 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>ptx-isa-8.5</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html">PTX ISA 8.5</A>
								</DL><p>
								<DT><H3 FOLDED>opal</H3>
								<DL><p>
									<DT><A HREF="https://github.com/kuterd/opal_ptx">kuterd/opal_ptx: Experimental GPU language with meta-programming</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html">PTX Writers Guide to Interoperability</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma">matrix-multiply-accumulate (mma)</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix">ldmatrix</A>
								<DT><A HREF="https://github.com/openai/triton/blob/8a83b141fc832dfb83dac3f335b21f3efb888afd/python/triton/tools/compile.py#L18">triton: compile.py</A>
								<DT><A HREF="https://segmentfault.com/a/1190000041878026/en">深度学习 - Practice torch.fx Part 1 - Pytorch-based Model Optimization Quantization Artifact - 个人文章 - SegmentFault 思否</A>
								<DT><A HREF="https://segmentfault.com/a/1190000041878026/en">Practice torch.fx Part 1 - Pytorch-based Model Optimization</A>
								<DT><A HREF="https://twitter.com/cis_female/status/1660390038226751490">microcode: ncu -k "kernel" --list-page sass --page source python3 file.py</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk">cp.async.bulk</A>
								<DT><A HREF="https://bruce-lee-ly.medium.com/nvidia-tensor-core-getting-started-with-mma-ptx-programming-508e44a6cb7d">Nvidia Tensor Core-Getting Started with MMA PTX Programming | by Bruce-Lee-LY | Medium</A>
								<DT><A HREF="https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseeks-ai-breakthrough-bypasses-industry-standard-cuda-uses-assembly-like-ptx-programming-instead">DeepSeek's AI breakthrough bypasses industry-standard CUDA, uses Nvidia's assembly-like PTX programming instead | Tom's Hardware</A>
							</DL><p>
							<DT><H3 FOLDED>NVCC</H3>
							<DL><p>
								<DT><H3 FOLDED>NVVM IR</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/7_libNVVM">cuda-samples/Samples/7_libNVVM at master · NVIDIA/cuda-samples</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-compilation</H3>
								<DL><p>
									<DT><A HREF="https://github.com/TiledTensor/TiledCUDA/blob/43de49e62fc0583e66d4561bf10698e640c6be21/examples/python/gemm/compile.py#L112">TiledCUDA/examples/python/gemm/compile.py at 43de49e62fc0583e66d4561bf10698e640c6be21 · TiledTensor/TiledCUDA</A>
									<DT><A HREF="https://github.com/TiledTensor/TiledCUDA/blob/43de49e62fc0583e66d4561bf10698e640c6be21/examples/python/gemm/compile.py#L112">compile.py#L112</A>
									<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/bdf733be55f0b323a8cf7cc6745a81c3f43cd7f0/hopper/setup.py#L163">flash-attention/hopper/setup.py at bdf733be55f0b323a8cf7cc6745a81c3f43cd7f0 · Dao-AILab/flash-attention</A>
									<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/bdf733be55f0b323a8cf7cc6745a81c3f43cd7f0/hopper/setup.py#L130">FlastAttention nvcc flags</A>
									<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/blob/master/pipeline-gemm/Makefile">cfx-article-src/pipeline-gemm/Makefile at master · ColfaxResearch/cfx-article-src</A>
									<DT><A HREF="https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/ops/utils.py">Liger-Kernel/src/liger_kernel/ops/utils.py at main · linkedin/Liger-Kernel</A>
									<DT><A HREF="https://github.com/moderngpu/moderngpu/wiki/Getting-started#cloning-the-source">Getting started · moderngpu/moderngpu Wiki</A>
									<DT><A HREF="https://github.com/pranjalssh/fast.cu/blob/main/logs.txt">fast.cu/logs.txt at main · pranjalssh/fast.cu</A>
									<DT><A HREF="https://docs.cedana.ai/setup/gpu-checkpointing">Cedana</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">CUDA NVCC</A>
								<DT><A HREF="https://leimao.github.io/blog/CUDA-Compilation/">CUDA Compilation - Lei Mao's Log Book</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#nvcc-command-options">NVIDIA CUDA Compiler Driver</A>
							</DL><p>
							<DT><H3 FOLDED>asm-volatile</H3>
							<DL><p>
								<DT><A HREF="https://gist.github.com/sophiawisdom/88b48f7146deb0d35c09506dd3a9c09e">invocation: TORCH_CUDA_ARCH_LIST=9.0a PYTORCH_NO_CUDA_MEMORY_CACHING=1 compute-sanitizer python3 test.py</A>
								<DT><A HREF="https://triton-lang.org/main/python-api/generated/triton.language.inline_asm_elementwise.html#triton.language.inline_asm_elementwise">triton.language.inline_asm_elementwise — Triton documentation</A>
							</DL><p>
							<DT><H3 FOLDED>cubin</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/cHHillee/status/1779141387876962469">Triton kernels can be precompiled into .cubin files</A>
								<DT><A HREF="https://github.com/VivekPanyam/cudaparsers">VivekPanyam/cudaparsers: Parsers for CUDA binary files</A>
								<DT><A HREF="https://github.com/Narsil/kernels_triton/blob/main/src/main.rs">kernels_triton/src/main.rs at main · Narsil/kernels_triton</A>
							</DL><p>
							<DT><H3 FOLDED>cudarc</H3>
							<DL><p>
								<DT><A HREF="https://github.com/akhildevelops/cudaz">akhildevelops/cudaz: A Zig Cuda wrapper</A>
								<DT><A HREF="https://github.com/coreylowman/cudarc/tree/main">coreylowman/cudarc: Safe rust wrapper around CUDA toolkit</A>
								<DT><A HREF="https://leimao.github.io/blog/Proper-CUDA-Error-Checking/">Proper CUDA Error Checking - Lei Mao's Log Book</A>
								<DT><A HREF="https://github.com/Narsil/axum_cudarc/blob/main/src/main.rs">axum_cudarc/src/main.rs at main · Narsil/axum_cudarc</A>
							</DL><p>
							<DT><H3 FOLDED>sass</H3>
							<DL><p>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">1. Overview — cuda-binary-utilities (SaSS)</A>
								<DT><A HREF="https://github.com/openai/triton/blob/8a83b141fc832dfb83dac3f335b21f3efb888afd/python/triton/tools/disasm.py#L69">triton: disasm.py#L69</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1825109774620311700">Sass syntax highlighting and CFG visualization in browser. (Kuter Dinel)</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1821658410804506752">Python DSL to directly write ptx</A>
								<DT><A HREF="https://github.com/kuterd/sass_graph/tree/master">kuterd/sass_graph: NVIDIA Sass binary CFG graph visualization tool.</A>
								<DT><A HREF="https://kuterdinel.com/nvidia-sass-control-code-viewer.html">Nvidia SASS Control Code Viewer • Kuter Dinel's blog</A>
								<DT><A HREF="https://www.youtube.com/watch?v=we3i5VuoPWk">Lecture 37: Introduction to SASS &amp; GPU Microarchitecture - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-kernels</H3>
							<DL><p>
								<DT><H3 FOLDED>Apex</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/apex">NVIDIA/apex: A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch</A>
									<DT><A HREF="https://nvidia.github.io/apex/">Apex (A PyTorch Extension) — Apex 0.1.0 documentation</A>
									<DT><A HREF="https://github.com/cat-state/tinypar">cat-state/tinypar</A>
									<DT><A HREF="https://github.com/stas00/tinypar">stas00/tinypar: TP/PP/DP implementation of llama using apex blocks</A>
									<DT><A HREF="https://github.com/cat-state/tinypar/blob/main/llama.py#L130">tinypar/llama.py at main · cat-state/tinypar</A>
								</DL><p>
								<DT><H3 FOLDED>cuda-tensor-cores</H3>
								<DL><p>
									<DT><H3 FOLDED>Transformer Engine</H3>
									<DL><p>
										<DT><H3 FOLDED>transformer-engine-blackwell</H3>
										<DL><p>
											<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/pull/1418/files">Initial Support Blackwell Build by johnnynunez · Pull Request #1418 · NVIDIA/TransformerEngine</A>
										</DL><p>
										<DT><A HREF="https://github.com/cchan/nanoGPT-fp8">cchan/nanoGPT-fp8</A>
										<DT><A HREF="https://twitter.com/itsclivetime/status/1655515089506820097">(1) Clive Chan en Twitter: "WIP FP8 training on consumer graphics cards - 🧵/4 I hacked nanoGPT to use TransformerEngine on RTX 4090 and ran a few iterations of GPT-2 training: - nanoGPT Block (+flashattn) =&amp;gt; TE TransformerLayer (both BF16): 15% faster - BF16 =&amp;gt; FP8: additional +18% https://t.co/cJNWehoGeu" / Twitter</A>
										<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/te_llama/tutorial_accelerate_hf_llama_with_te.ipynb">TransformerEngine/docs/examples/te_llama/tutorial_accelerate_hf_llama_with_te.ipynb</A>
										<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/15">Ada Lovelace support</A>
										<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
										<DT><A HREF="http://giantpandacv.com/project/CUDA/%E8%AF%A6%E8%A7%A3%20NVIDIA%20H100%20TransformerEngine/">详解 NVIDIA H100 TransformerEngine - GiantPandaCV</A>
									</DL><p>
									<DT><H3 FOLDED>tensor-cores-V100</H3>
									<DL><p>
										<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2019/11/ORNL_Tensor_Core_Training_Aug2019.pdf">VOLTA TENSOR CORE TRAINING (ORNL)</A>
									</DL><p>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/discussions/4066">Tinygrad: CUDA Tensor Core Integration #2684</A>
									<DT><A HREF="https://github.com/jbaron34/torchwindow">jbaron34/torchwindow: Display tensors directly from GPU</A>
									<DT><A HREF="https://developer.nvidia.com/gtc/2020/video/s21745-vid">GTC 2020: Developing CUDA kernels to push Tensor | NVIDIA Developer</A>
									<DT><A HREF="https://ieeexplore.ieee.org/abstract/document/9139835">Demystifying Tensor Cores to Optimize Half-Precision Matrix Multiply | IEEE Conference Publication | IEEE Xplore</A>
									<DT><A HREF="https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/">CUTLASS Tutorial: Fast Matrix-Multiplication with WGMMA on NVIDIA® Hopper™ GPUs – Colfax Research</A>
									<DT><A HREF="https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/">NVIDIA Tensor Core Programming - Lei Mao's Log Book</A>
									<DT><A HREF="http://iliasmirnov.com/tensor/">Comparing CUDA and Tensor Cores for Training Neural Networks</A>
									<DT><A HREF="https://blog.sina.com.cn/s/blog_56ab14d50102yv82.html">理解TensorCore_豆饭的小窝_新浪博客</A>
									<DT><A HREF="https://github.com/lcy-seso/DLFrameworkTest/blob/master/Cuda/tensor_core/README.md">DLFrameworkTest/Cuda/tensor_core/README.md at master · lcy-seso/DLFrameworkTest</A>
								</DL><p>
								<DT><A HREF="https://github.com/enp1s0/cutf">enp1s0/cutf: CUDA Template Functions</A>
								<DT><A HREF="https://jhui.github.io/2017/03/06/CUDA/">“CUDA Tutorial”</A>
								<DT><A HREF="https://github.com/mcarilli/cuda-memory/tree/master">mcarilli/cuda-memory: Playing around with GPU memory optimization</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index">cuda-mode/triton-index: Cataloging released Triton kernels.</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index/blob/main/kernel_overview.md">triton-index/kernel_overview.md</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-graphs</H3>
							<DL><p>
								<DT><H3 FOLDED>cuda-graphs-debug</H3>
								<DL><p>
									<DT><A HREF="https://github.com/flashinfer-ai/debug-print?tab=readme-ov-file">flashinfer-ai/debug-print: Debug print operator for cudagraph debugging</A>
									<DT><A HREF="https://github.com/flashinfer-ai/debug-print/blob/main/example.py">debug-print/example.py at main · flashinfer-ai/debug-print</A>
								</DL><p>
								<DT><A HREF="https://developer.nvidia.com/blog/cuda-graphs/">Getting Started with CUDA Graphs | NVIDIA Technical Blog</A>
								<DT><A HREF="https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/">Accelerating PyTorch with CUDA Graphs | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/issues/99397">Internal errors with cuda graph (CUBLAS_STATUS_NOT_INITIALIZED and jit failure) · Issue #99397 · pytorch/pytorch</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/understanding-cudagraph-trees/1967">Understanding CUDAGraph Trees - compiler - PyTorch Developer Mailing List</A>
								<DT><A HREF="https://fireworks.ai/blog/speed-python-pick-two-how-cuda-graphs-enable-fast-python-code-for-deep-learning">Speed, Python: Pick Two. How CUDA Graphs Enable Fast Python Code for Deep Learning</A>
								<DT><A HREF="https://github.com/flashinfer-ai/debug-print">flashinfer-ai/debug-print: Debug print operator for cudagraph debugging</A>
							</DL><p>
							<DT><H3 FOLDED>cccl</H3>
							<DL><p>
								<DT><H3 FOLDED>llm.cpp</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=WiB_3Csfj_Q">CUDA C++ llm.cpp - YouTube</A>
									<DT><A HREF="https://github.com/karpathy/llama2.c">karpathy/llama2.c: Inference Llama 2 in one file of pure C</A>
									<DT><A HREF="https://github.com/gevtushenko/llm.c">gevtushenko/llm.c: LLM training in simple, raw C/CUDA</A>
								</DL><p>
								<DT><H3 FOLDED>Thrust</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/thrust/index.html">Thrust</A>
									<DT><A HREF="https://thrust.github.io/">Thrust - Parallel Algorithms Library</A>
									<DT><A HREF="https://github.com/NVIDIA/thrust">NVIDIA/thrust: The C++ parallel algorithms library.</A>
									<DT><H3 FOLDED>Fancy Iterators</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/archive/12.0.0/pdf/Thrust_Quick_Start_Guide.pdf">Thrust_Quick_Start_Guide.pdf</A>
									<DT><A HREF="https://www.youtube.com/watch?v=zlJg9mCNfkQ">Thrust and the C++ Standard Algorithms - Conor Hoekstra - GTC 2021 - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=W2tWOdzgXHA">GoingNative 2013 C++ Seasoning</A>
									<DT><A HREF="https://www.youtube.com/watch?v=h4Jl1fk3MkQ">CppCon 2016: Marshall Clow “STL Algorithms - why you should use them, and how to write your own" - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=2olsGf6JIkU">CppCon 2018: Jonathan Boccara “105 STL Algorithms in Less Than an Hour” - YouTube</A>
									<DT><A HREF="https://github.com/codereport/Content/tree/main/Talks">Content/Talks at main · codereport/Content</A>
									<DT><A HREF="https://github.com/codereport/Content/tree/main/Talks/2021-04-GTC/ThrustAndTheCppStandardAlgorithms">Content/Talks/2021-04-GTC/ThrustAndTheCppStandardAlgorithms</A>
									<DT><A HREF="https://research.nvidia.com/publication/2011-10_thrust-productivity-oriented-library-cuda">Thrust: A Productivity-Oriented Library for CUDA | Research</A>
									<DT><A HREF="https://developer.nvidia.com/blog/expressive-algorithmic-programming-thrust/">Expressive Algorithmic Programming with Thrust | NVIDIA Technical Blog</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=WiB_3Csfj_Q">CUDA C++ llm.cpp - YouTube</A>
								<DT><A HREF="https://github.com/NVIDIA/cccl">NVIDIA/cccl: CUDA C++ Core Libraries</A>
								<DT><A HREF="https://github.com/eyalroz/cuda-api-wrappers">eyalroz/cuda-api-wrappers: Thin, unified, C++-flavored wrappers for the CUDA APIs</A>
								<DT><A HREF="https://nyc2024.pydata.org/cfp/talk/TX3CBB/">Accelerating GPU Algorithms in pure Python :: PyData NYC 2024 :: pretalx</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-python</H3>
							<DL><p>
								<DT><H3 FOLDED>pycuda</H3>
								<DL><p>
									<DT><A HREF="https://github.com/inducer/pycuda">inducer/pycuda: CUDA integration for Python, plus shiny features</A>
									<DT><A HREF="https://documen.tician.de/pycuda/driver.html">Device Interface</A>
									<DT><A HREF="https://forums.developer.nvidia.com/t/pycuda-best-practice-to-keep-c-kernels-separate-from-python-code/72829/4">load_kernels</A>
								</DL><p>
								<DT><A HREF="https://github.com/NVIDIA/cuda-python">NVIDIA/cuda-python: CUDA Python Low-level Bindings</A>
								<DT><A HREF="https://github.com/inducer/pyopencl">inducer/pyopencl: OpenCL integration for Python, plus shiny features</A>
								<DT><A HREF="https://github.com/MatthieuDartiailh/pyclibrary">MatthieuDartiailh/pyclibrary: C parser and ctypes automation for python</A>
								<DT><A HREF="https://github.com/trolldbois/ctypeslib/blob/master/ctypeslib/clang2py.py">clang2py.py</A>
							</DL><p>
							<DT><H3 FOLDED>CUTLASS</H3>
							<DL><p>
								<DT><H3 FOLDED>cutlass-build</H3>
								<DL><p>
									<DT><A HREF="https://claude.ai/chat/8ff889d1-bd26-4939-94b4-7bdaa22b63c6">Troubleshooting CUTLASS Source Code Build - Claude</A>
								</DL><p>
								<DT><H3 FOLDED>cutlass-learning</H3>
								<DL><p>
									<DT><H3 FOLDED>cutlass-examples</H3>
									<DL><p>
										<DT><A HREF="https://github.com/leimao/CUTLASS-Examples">leimao/CUTLASS-Examples: CUTLASS and CuTe Examples</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>cutlass-blackwell</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cutlass">NVIDIA/cutlass: CUTLASS 3.8 first release that supports the NVIDIA Blackwell SM100 architecture</A>
									<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72720/">Programming Blackwell Tensor Cores with CUTLASS | GTC 25 2025 | NVIDIA On-Demand</A>
									<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc25-s74639/">Enable Tensor Core Programming in Python with CUTLASS 4.0 | GTC 25 2025 | NVIDIA On-Demand</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/blackwell_functionality.md">Blackwell functionality: Blackwell SM100 GEMMs</A>
								</DL><p>
								<DT><H3 FOLDED>Mixed-input matrix multiplication performance optimizations</H3>
								<DL><p>
									<DT><A HREF="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">Mixed-input matrix multiplication performance optimizations – Google Research Blog</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/pull/1084">Support for Mixed Input TensorOp by manishucsd · Pull Request #1084 · NVIDIA/cutlass</A>
								</DL><p>
								<DT><H3 FOLDED>TMA (Tensor Memory Accelerator)</H3>
								<DL><p>
									<DT><A HREF="https://research.colfax-intl.com/tutorial-hopper-tma/">CUTLASS Tutorial: Mastering the NVIDIA® Tensor Memory Accelerator (TMA) – Colfax Research</A>
									<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/tree/master/tma">cfx-article-src/tma at master · ColfaxResearch/cfx-article-src</A>
								</DL><p>
								<DT><H3 FOLDED>CuTe</H3>
								<DL><p>
									<DT><H3 FOLDED>cute-learning</H3>
									<DL><p>
										<DT><A HREF="https://github.com/DD-DuDa/Cute-Learning">DD-DuDa/Cute-Learning: Examples of CUDA implementations by Cutlass CuTe</A>
										<DT><A HREF="https://github.com/CalebDu/Awesome-Cute">CalebDu/Awesome-Cute</A>
										<DT><A HREF="https://github.com/leimao/CUTLASS-Examples/blob/f06835a42eecd52f30c8292b4b911e01fe1b02da/README.md">CUTLASS-Examples/README.md</A>
									</DL><p>
									<DT><H3 FOLDED>cute-layout-algebra</H3>
									<DL><p>
										<DT><A HREF="https://leimao.github.io/article/CuTe-Layout-Algebra/">CuTe Layout Algebra - Lei Mao's Log Book</A>
										<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md">CuTe Layouts</A>
										<DT><A HREF="https://research.colfax-intl.com/a-note-on-the-algebra-of-cute-layouts/">A note on the algebra of CuTe Layouts – Colfax Research</A>
										<DT><A HREF="https://github.com/leimao/CUTLASS-Examples/blob/f06835a42eecd52f30c8292b4b911e01fe1b02da/examples/cute_tiled_mma_preview/cute_tiled_mma_preview.cu">CUTLASS-Examples/examples/cute_tiled_mma_preview/cute_tiled_mma_preview.cu at f06835a42eecd52f30c8292b4b911e01fe1b02da · leimao/CUTLASS-Examples</A>
									</DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md">CuTe Layouts</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md">Getting Started With CuTe</A>
									<DT><A HREF="https://research.colfax-intl.com/a-note-on-the-algebra-of-cute-layouts/">A note on the algebra of CuTe Layouts – Colfax Research</A>
									<DT><A HREF="https://github.com/reed-lau/cute-gemm/blob/main/gemm-simple.cu">cute-gemm/gemm-simple.cu at main · reed-lau/cute-gemm</A>
									<DT><A HREF="https://github.com/reed-lau/cute-gemm">reed-lau/cute-gemm</A>
									<DT><A HREF="https://github.com/weishengying/cute_gemm">weishengying/cute_gemm</A>
									<DT><A HREF="https://veitner.bearblog.dev/predication-in-cutlass/">Predication in Cutlass | simons blog</A>
									<DT><A HREF="https://leimao.github.io/blog/CuTe-Tiled-MMA/">CuTe Tiled MMA - Lei Mao's Log Book</A>
									<DT><A HREF="https://github.com/leimao/CUTLASS-Examples/blob/f06835a42eecd52f30c8292b4b911e01fe1b02da/examples/cute_tiled_mma_preview/cute_tiled_mma_preview.cu">CUTLASS-Examples/examples/cute_tiled_mma_preview/cute_tiled_mma_preview.cu at f06835a42eecd52f30c8292b4b911e01fe1b02da · leimao/CUTLASS-Examples</A>
								</DL><p>
								<DT><H3 FOLDED>nvidia-cutlass-dsl</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/tree/main/python">cutlass/python at main · NVIDIA/cutlass</A>
									<DT><A HREF="https://www.piwheels.org/project/nvidia-cutlass-dsl/">piwheels - nvidia-cutlass-dsl</A>
									<DT><A HREF="https://colab.research.google.com/github/NVIDIA/cutlass/blob/main/examples/python/03_basic_conv2d.ipynb">03_basic_conv2d.ipynb - Colab</A>
								</DL><p>
								<DT><H3 FOLDED>cutlass-visualization</H3>
								<DL><p>
									<DT><H3 FOLDED>print_latex</H3>
									<DL><p>
										<DT><A HREF="https://github.com/NVIDIA/cutlass/pull/1656">feat: allow print_latex(TiledMMA) to colorize sliced thread and add print_latex(ThrMMA) by cloudhan · Pull Request #1656 · NVIDIA/cutlass</A>
										<DT><A HREF="https://leimao.github.io/blog/CuTe-Tiled-MMA/">CuTe Tiled MMA cute::print_latex - (Lei Mao's Log)</A>
									</DL><p>
									<DT><H3 FOLDED>cutlass-viz</H3>
									<DL><p>
										<DT><A HREF="https://github.com/flashinfer-ai/cutlass-viz">flashinfer-ai/cutlass-viz</A>
									</DL><p>
									<DT><A HREF="https://github.com/flashinfer-ai/cutlass-viz">flashinfer-ai/cutlass-viz</A>
								</DL><p>
								<DT><H3 FOLDED>cutlass-predication</H3>
								<DL><p>
									<DT><A HREF="https://veitner.bearblog.dev/predication-in-cutlass/">Predication in Cutlass | simons blog</A>
								</DL><p>
								<DT><A HREF="https://github.com/NVIDIA/cutlass">NVIDIA/cutlass: CUDA Templates for Linear Algebra Subroutines</A>
								<DT><A HREF="https://www.youtube.com/watch?v=PWWOGrLZtZg">CUTLASS: A CUDA C++ Template Library for Accelerating Deep Learning... Aniket Shivam &amp; Vijay Thakkar - YouTube</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md">Getting Started With CuTe</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md">CuTe Layouts</A>
								<DT><A HREF="https://www.youtube.com/watch?v=yCyZEJrlrfY&t=126s">Lightning Talk: Harnessing NVIDIA Tensor Cores: An Exploration of CUTLASS &amp; OpenAI..- Matthew Nicely - YouTube</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/python/README.md">cutlass/python/README.md</A>
								<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31883/">Accelerating Convolution with Tensor Cores in CUTLASS</A>
								<DT><A HREF="https://www.nvidia.com/ko-kr/on-demand/session/gtcspring22-s41996/">Accelerating Backward Data Gradient by Increasing Tensor Core Utilization in CUTLASS</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/pull/1084">Support for Mixed Input TensorOp by manishucsd (Google PR)</A>
								<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-flashattention-2/">A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library – Colfax Research</A>
								<DT><A HREF="https://research.colfax-intl.com/wp-content/uploads/2023/12/colfax-flashattention.pdf">A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library</A>
								<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/">GEMM kernels Hopper</A>
								<DT><A HREF="https://www.youtube.com/watch?v=G6q719ck7ww">Lecture 15: CUTLASS - YouTube</A>
								<DT><A HREF="https://pypi.org/project/nvidia-cutlass/">nvidia-cutlass · PyPI</A>
								<DT><A HREF="https://dl.acm.org/doi/pdf/10.1145/3620666.3651369">EVT: Accelerating Deep Learning Training with Epilogue Visitor Tree</A>
								<DT><A HREF="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS: Fast Linear Algebra in CUDA C++ | NVIDIA Technical Blog</A>
								<DT><A HREF="https://github.com/ericauld/cutlass-playground">ericauld/cutlass-playground</A>
								<DT><A HREF="https://research.colfax-intl.com/tutorial-matrix-transpose-in-cutlass/">Tutorial: Matrix Transpose in CUTLASS – Colfax Research</A>
								<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/tree/master/transpose-cute">cfx-article-src/transpose-cute at master · ColfaxResearch/cfx-article-src</A>
								<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/tree/master/cutlass_gemm">cfx-article-src/cutlass_gemm at master · ColfaxResearch/cfx-article-src</A>
								<DT><A HREF="https://github.com/yester31/Cutlass_EX">yester31/Cutlass_EX: study of cutlass</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/issues/792">[QST] Should I use 3.x or 2.x API for gmem/smem reading/writing? · Issue #792 · NVIDIA/cutlass</A>
								<DT><A HREF="https://github.com/tlc-pack/cutlass_fpA_intB_gemm">tlc-pack/cutlass_fpA_intB_gemm: A standalone GEMM kernel for fp16 activation and quantized weight, extracted from FasterTransformer</A>
								<DT><A HREF="https://file.notion.so/f/f/51cb76f9-c458-4b5a-83bf-5a2d5c61edfc/894cc1c7-1dbc-4be2-85ee-2f4d76944a99/s21745-developing-cuda-kernels-to-push-tensor-cores-to-the-absolute-limit-on-nvidia-a100.pdf?table=block&id=ab3624dc-7812-40b0-8b58-15ad1afaccd2&spaceId=51cb76f9-c458-4b5a-83bf-5a2d5c61edfc&expirationTimestamp=1724169600000&signature=MpFjqZDTXGju4jogSU720ZXu1JegKfD1mu1DJX0kwnw&downloadName=s21745-developing-cuda-kernels-to-push-tensor-cores-to-the-absolute-limit-on-nvidia-a100.pdf">A100 Tensor Cores</A>
								<DT><A HREF="https://github.com/ColfaxResearch/cutlass-kernels">ColfaxResearch/cutlass-kernels</A>
								<DT><A HREF="https://github.com/weishengying/cutlass_flash_atten_fp8">weishengying/cutlass_flash_atten_fp8: 使用 cutlass 仓库在 ada 架构上实现 fp8 的 flash attention</A>
								<DT><A HREF="https://www.youtube.com/watch?v=adA9AMu4_Kc">vLLM Office Hours - Using NVIDIA CUTLASS for High-Performance Inference - September 05, 2024 - YouTube</A>
								<DT><A HREF="https://research.colfax-intl.com/cutlass-tutorial-design-of-a-gemm-kernel/">CUTLASS Tutorial: Efficient GEMM kernel designs with Pipelining – Colfax Research</A>
								<DT><A HREF="https://leimao.github.io/blog/Build-Develop-CUTLASS-CUDA-Kernels/">Build and Develop CUTLASS CUDA Kernels - Lei Mao's Log Book</A>
							</DL><p>
							<DT><H3 FOLDED>cuDNN</H3>
							<DL><p>
								<DT><H3 FOLDED>cuDNN-attention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cudnn-frontend/issues/52">What's the difference of flash attention implement between cudnn and Dao-AILab? · Issue #52 · NVIDIA/cudnn-frontend</A>
									<DT><A HREF="https://github.com/NVIDIA/cudnn-frontend/blob/1.0/release/samples/python/test_mhas.py">cudnn-frontend/samples/python/test_mhas.py at 1.0/release · NVIDIA/cudnn-frontend</A>
									<DT><A HREF="https://github.com/NVIDIA/cudnn-frontend/blob/9f82dda5c029d15a5f371f0fe003dc0c74a0c987/samples/python/test_mhas.py#L431">test_mhas.py#L431: sdpa</A>
									<DT><A HREF="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-894/developer-guide/index.html">Developer Guide :: NVIDIA cuDNN Documentation</A>
									<DT><A HREF="https://github.com/pytorch-labs/tritonbench/issues/41">[performance] Torch SDPA cuDNN backend vs FlashAttention v3 · Issue #41 · pytorch-labs/tritonbench</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/cudnn/installation/latest/linux.html#installing-cudnn-using-conda">Installing cuDNN Backend on Linux — NVIDIA cuDNN Installation</A>
							</DL><p>
							<DT><H3 FOLDED>cuBLAS</H3>
							<DL><p>
								<DT><A HREF="https://github.com/wangzyon/NVIDIA_SGEMM_PRACTICE">wangzyon/NVIDIA_SGEMM_PRACTICE: Step-by-step optimization of CUDA SGEMM</A>
								<DT><A HREF="https://github.com/Bruce-Lee-LY/cuda_hgemm">Bruce-Lee-LY/cuda_hgemm: Several optimization methods of half-precision general matrix multiplication (HGEMM) using tensor core with WMMA API and MMA PTX instruction.</A>
								<DT><A HREF="https://siboehm.com/articles/22/CUDA-MMM">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</A>
								<DT><A HREF="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/tinyblas.cu">llamafile/llamafile/tinyblas.cu at main · Mozilla-Ocho/llamafile</A>
							</DL><p>
							<DT><H3 FOLDED>NCCL</H3>
							<DL><p>
								<DT><H3 FOLDED>NVLINK</H3>
								<DL><p>
									<DT><A HREF="https://www.nvidia.com/en-us/data-center/nvlink/">nvlink</A>
								</DL><p>
								<DT><H3 FOLDED>nccl-installation</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html">Installation Guide :: NVIDIA Deep Learning NCCL Documentation</A>
								</DL><p>
								<DT><H3 FOLDED>nccl-multi-node</H3>
								<DL><p>
									<DT><A HREF="https://github.com/stas00/ml-engineering/blob/58bdecd9d245d4275b78f38a869631f9f08be168/network/benchmarks/all_reduce_bench.py#L49">ml-engineering/network/benchmarks/all_reduce_bench.py at 58bdecd9d245d4275b78f38a869631f9f08be168 · stas00/ml-engineering</A>
									<DT><A HREF="https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html">Multinode Training — PyTorch Tutorials 2.4.0+cu121 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>nccl-profiler</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/NPKit">microsoft/NPKit: NCCL Profiling Kit</A>
								</DL><p>
								<DT><H3 FOLDED>tccl</H3>
								<DL><p>
									<DT><A HREF="https://github.com/cchan/tccl">cchan/tccl: extensible collectives library in triton</A>
									<DT><A HREF="https://github.com/yifuwang/symm-mem-recipes">yifuwang/symm-mem-recipes</A>
									<DT><A HREF="https://github.com/cchan/tccl/blob/main/presentation.pdf">tccl/presentation.pdf at main · cchan/tccl</A>
								</DL><p>
								<DT><H3 FOLDED>nccl-flux</H3>
								<DL><p>
									<DT><A HREF="https://github.com/bytedance/flux">bytedance/flux: A fast communication-overlapping library for tensor parallelism on GPUs.</A>
								</DL><p>
								<DT><A HREF="https://le.qun.ch/en/blog/2024/12/25/libfabric-efa-0-intro/">Harnessing 3200 Gbps Network: A Journey with RDMA, EFA, and libfabric</A>
								<DT><A HREF="https://twitter.com/ProjectPhysX/status/1637789116363407362">(1) Dr. Moritz Lehmann en X: "There is a supppsed vendor-independent (but not cross-vendor) way of #GPU P2P communication in #OpenCL: have all GPUs in the same context &amp;amp; pass buffers to other devices' kernels.💡 Drivers should automatically handle P2P comm via PCIe/SLI/NVLink/CrossFire/InfFabric. 🧵1/9 https://t.co/h0DsEPPkFq" / X</A>
								<DT><A HREF="https://github.com/coreweave/nccl-tests">coreweave/nccl-tests: NVIDIA NCCL Tests for Distributed Training</A>
								<DT><A HREF="https://github.com/open-mpi/ompi">open-mpi/ompi: Open MPI main development repository</A>
								<DT><A HREF="https://github.com/horovod/horovod/blob/master/docs/concepts.rst">horovod/docs/concepts.rst at master · horovod/horovod</A>
								<DT><A HREF="https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/">MPI Scatter, Gather, and Allgather · MPI Tutorial</A>
								<DT><A HREF="https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/">MPI Broadcast and Collective Communication · MPI Tutorial</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/tests/onebit/test_nccl_perf.py">DeepSpeed/tests: test_nccl_perf.py</A>
								<DT><A HREF="https://www.youtube.com/watch?v=nD6PUe3d6ec">Lecture #2 - Scatter-to-Gather Transformation - YouTube</A>
								<DT><A HREF="https://github.com/cuda-mode/p2p-perf">cuda-mode/p2p-perf: measuring peer-to-peer (p2p) transfer on different cuda devices</A>
								<DT><A HREF="https://www.youtube.com/watch?v=T22e3fgit-A">Lecture 17: NCCL - YouTube</A>
								<DT><A HREF="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/p2pBandwidthLatencyTest">cuda-samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest at master · NVIDIA/cuda-samples</A>
								<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/network/debug">ml-engineering/network/debug</A>
								<DT><A HREF="https://github.com/microsoft/mscclpp">microsoft/mscclpp: MSCCL++: A GPU-driven communication stack for scalable AI applications</A>
								<DT><A HREF="https://github.com/microsoft/msccl">microsoft/msccl: Microsoft Collective Communication Library</A>
								<DT><A HREF="https://github.com/Azure/msccl">Azure/msccl: Microsoft Collective Communication Library</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Collective_operation">Collective operation - Wikipedia</A>
								<DT><A HREF="https://github.com/Vchitect/FasterCache/blob/main/fastercache/dsp/comm.py">FasterCache/fastercache/dsp/comm.py at main · Vchitect/FasterCache</A>
								<DT><A HREF="https://github.com/chengzeyi/ParaAttention/blob/16ab3b17c415b80bb757fe48fa6e95d0adda9430/src/para_attn/primitives.py">ParaAttention/src/para_attn/primitives.py</A>
								<DT><A HREF="https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/#:~:text=We%20tested%20NCCL%202,180x%20improvement%20at%2024k%20GPUs">Massively Scale Your Deep Learning Training with NCCL 2.4 | NVIDIA Technical Blog</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-profiling</H3>
							<DL><p>
								<DT><H3 FOLDED>ncu</H3>
								<DL><p>
									<DT><H3 FOLDED>ncu-scripts</H3>
									<DL><p>
										<DT><A HREF="https://github.com/lcy-seso/vq-experiments/blob/3aef9f38da6dd19f1c9b3aa03410724a2457e0ac/bench_quant_gemv/ncu.sh">vq-experiments/bench_quant_gemv/ncu.sh ncu profiling script example</A>
										<DT><A HREF="https://gist.github.com/mcarilli/376821aa1a7182dfcf59928a7cde3223">Favorite nsight systems profiling commands for Pytorch scripts</A>
										<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/15">Proper benchmarkign with CUDA synchronization NVIDIA/TransformerEngine</A>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch-labs/tritonbench/blob/42da900e856473218e12f07e91ac176752ffa80a/tritonbench/components/ncu/analyzer.py">tritonbench/tritonbench/components/ncu/analyzer.py</A>
									<DT><A HREF="https://github.com/pytorch-labs/tritonbench/blob/42da900e856473218e12f07e91ac176752ffa80a/tritonbench/utils/triton_op.py">tritonbench/tritonbench/utils/triton_op.py at 42da900e856473218e12f07e91ac176752ffa80a · pytorch-labs/tritonbench</A>
								</DL><p>
								<DT><H3 FOLDED>DrGPU: A Top-Down Profiler for GPU</H3>
								<DL><p>
									<DT><A HREF="https://github.com/FindHao/drgpu">FindHao/drgpu</A>
									<DT><A HREF="https://about.findhao.net/ICPE2023.pdf">DrGPU: A Top-Down Profiler for GPU</A>
									<DT><A HREF="https://github.com/FindHao/drgpu">FindHao/drgpu (Optimization suggestions)</A>
									<DT><A HREF="https://chat.openai.com/c/74ad9a19-e6f0-4ba3-a0d7-0d7a0b6c5c61">Performance Logging &amp; Saving Kernel ASM &amp; IR</A>
									<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/de17730a993b1d2cce4fd09e3654b5f79fd23c96/kernels/triton/inference/gptq/a100_qlinear.py#L8">applied-ai/kernels/triton/inference/gptq/a100_qlinear.py (ASM &amp; IR)</A>
								</DL><p>
								<DT><H3 FOLDED>Nsight</H3>
								<DL><p>
									<DT><H3 FOLDED>High Pipe Utilization</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>Issue Slot Utilization</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>CPI Stall ‘Long Scoreboard’</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>nsight-videos</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=GCkdiHk6fUY">Memory Analysis with NVIDIA Nsight Compute (memory chart)</A>
										<DT><A HREF="https://www.youtube.com/watch?v=uHN5fpfu8As">Memory hierarchy</A>
										<DT><A HREF="https://www.youtube.com/watch?v=Iuy_RAvguBM">Intro to NVIDIA Nsight Compute</A>
										<DT><A HREF="https://www.youtube.com/watch?v=nhTjq0P9uc8">08 GPU Performance Analysis</A>
										<DT><A HREF="https://www.youtube.com/watch?v=fsC3QeZHM1U">Introduction to Kernel Performance Analysis with NVIDIA Nsight Compute</A>
										<DT><A HREF="https://www.youtube.com/watch?v=3DAYN-onSzY">GPU Series: Hands-On Session with NSight Systems and Compute - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=nhTjq0P9uc8">08 GPU Performance Analysis - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=fsC3QeZHM1U">Introduction to Kernel Performance Analysis with NVIDIA Nsight Compute - YouTube</A>
									</DL><p>
									<DT><A HREF="https://gist.github.com/mcarilli/376821aa1a7182dfcf59928a7cde3223">Favorite nsight systems profiling commands for Pytorch scripts</A>
									<DT><A HREF="https://gpuhackshef.readthedocs.io/en/latest/">Sheffield GPU Hackathon — GPUHackSheffield documentation</A>
									<DT><A HREF="https://gpuhackshef.readthedocs.io/en/latest/tools/nvidia-profiling-tools.html">NVIDIA Profiling Tools — GPUHackSheffield documentation</A>
									<DT><A HREF="https://docs.nvidia.com/nsight-systems/UserGuide/index.html">User Guide :: Nsight Systems Documentation</A>
									<DT><A HREF="https://gist.github.com/mcarilli/213a4e698e4a0ae2234ddee56f4f3f95">Single- and multiprocess profiling workflow with nvprof and NVVP (Nsight Systems coming soon...)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=nhTjq0P9uc8">08 GPU Performance Analysis - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=fsC3QeZHM1U">Introduction to Kernel Performance Analysis with NVIDIA Nsight Compute - YouTube</A>
									<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf">https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf</A>
									<DT><A HREF="https://blog.speechmatics.com/pointless-gpu-optimization-exercise">An Almost Pointless Exercise in GPU Optimization</A>
									<DT><A HREF="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#memory-tables">Memory tables Profiling Guide</A>
									<DT><A HREF="https://github.com/NVIDIA/nsight-training/tree/master/cuda/nsight_compute/vlog_memory_workload">nsight-training/cuda/nsight_compute/vlog_memory_workload</A>
									<DT><A HREF="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">2. Kernel Profiling Guide — NsightCompute 12.4 documentation</A>
									<DT><A HREF="http://tspeterkim.github.io/posts/nsight-setup-on-ec2">How to set up Nsight Compute Locally to profile Remote GPUs | Taeksang Peter Kim</A>
									<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf">ornl: Nsight Compute OLCF Webinar</A>
									<DT><A HREF="https://github.com/reed-lau/cute-gemm/blob/main/profile.sh">cute-gemm/profile.sh at main · reed-lau/cute-gemm</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/using-nsight-systems-to-profile-gpu-workload/59">Using Nsight Systems to profile GPU workload - hardware-backends / NVIDIA CUDA - PyTorch Developer Mailing List</A>
								</DL><p>
								<DT><H3 FOLDED>NVML NVIDIA Management Library</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html">NVML API Reference Guide :: GPU Deployment and Management Documentation</A>
								</DL><p>
								<DT><H3 FOLDED>nvprof</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/cuda/profiler-users-guide/">nvprof: Profiler Users Guide</A>
								</DL><p>
								<DT><H3 FOLDED>TensorBoard</H3>
								<DL><p>
									<DT><A HREF="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">TensorFlow Profiler: Profile model performance  |  TensorBoard</A>
									<DT><A HREF="https://github.com/pytorch/kineto/blob/main/tb_plugin/docs/gpu_utilization.md">pytorch gpu_utilization.md</A>
									<DT><A HREF="https://github.com/pytorch/kineto/tree/main/tb_plugin">tb_plugin</A>
									<DT><A HREF="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard — PyTorch Tutorials 2.0.1+cu117 documentation</A>
									<DT><A HREF="https://github.com/LaurentMazare/tboard-rs">LaurentMazare/tboard-rs: Read and write tensorboard data using Rust</A>
								</DL><p>
								<DT><A HREF="https://gist.github.com/mcarilli/376821aa1a7182dfcf59928a7cde3223">Favorite nsight systems profiling commands for Pytorch scripts</A>
								<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/15">Proper benchmarkign with CUDA synchronization NVIDIA/TransformerEngine</A>
								<DT><A HREF="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">TensorFlow Profiler: Profile model performance  |  TensorBoard</A>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html">GPU Performance Background User's Guide - NVIDIA Docs</A>
								<DT><A HREF="https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py">Step Demo — Matplotlib 3.7.2 documentation</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_latency.py">vllm/benchmarks/benchmark_latency.py at main · vllm-project/vllm</A>
								<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu">Kobzol/hardware-effects-gpu: Demonstration of various hardware effects on CUDA GPUs.</A>
								<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu/tree/master/memory-coalescing">hardware-effects-gpu/memory-coalescing at master · Kobzol/hardware-effects-gpu</A>
								<DT><A HREF="https://github.com/mcarilli/cuda-memory/tree/master">mcarilli/cuda-memory: Playing around with GPU memory optimization</A>
								<DT><A HREF="https://github.com/cupy/cupy/blob/5a4c7a1c461776c779afc1e614aa06db7be594fa/docs/source/user_guide/performance.rst#L10">Performance Best Practices: CuPy</A>
								<DT><A HREF="https://github.com/Lin-Mao/DrGPUM">Lin-Mao/DrGPUM: A memory profiler for NVIDIA GPUs to explore memory inefficiencies in GPU-accelerated applications.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch - YouTube</A>
								<DT><A HREF="https://docs.google.com/presentation/d/110dnMW94LX1ySWxu9La17AVUxjgSaQDLOotFC3BZZD4/edit#slide=id.p">Lecture 1 How to profile CUDA kernels in PyTorch</A>
								<DT><A HREF="https://www.speechmatics.com/company/articles-and-news/timing-operations-in-pytorch">How to Accurately Time CUDA Kernels in Pytorch</A>
								<DT><A HREF="https://developer.nvidia.com/tools-overview">NVIDIA Developer Tools Overview | NVIDIA Developer</A>
								<DT><A HREF="https://developer.nvidia.com/nsight-visual-studio-code-edition">Nsight Visual Studio Code Edition</A>
								<DT><A HREF="https://developer.nvidia.com/nsight-dl-designer">Nsight Deep Learning Designer (earlier access)</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_latency.py">vllm/benchmarks/benchmark_latency.py at main</A>
								<DT><A HREF="https://github.com/Kobzol/hardware-effects-gpu/tree/master/memory-coalescing">hardware-effects-gpu/memory-coalescing at master</A>
								<DT><A HREF="https://github.com/Lin-Mao/DrGPUM">Lin-Mao/DrGPUM: A memory profiler for NVIDIA GPUs to explore memory inefficiencies in GPU</A>
								<DT><A HREF="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.pdf">Understanding Latency Hiding on GPUs (2016)</A>
								<DT><A HREF="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives</A>
								<DT><A HREF="https://github.com/NVIDIA/nvbandwidth">NVIDIA/nvbandwidth: A tool for bandwidth measurements on NVIDIA GPUs.</A>
								<DT><A HREF="https://christianjmills.com/posts/cuda-mode-notes/lecture-001/">Christian Mills - CUDA MODE Lecture 1: How to profile CUDA kernels in PyTorch</A>
								<DT><A HREF="http://arthurchiao.art/blog/understanding-gpu-performance/">Understanding NVIDIA GPU Performance: Utilization vs. Saturation (2023)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=zHY7iF_2RyU">Lecture 07 - Profiling - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-debug</H3>
							<DL><p>
								<DT><H3 FOLDED>cuda-env-vars</H3>
								<DL><p>
									<DT><H3 FOLDED>CUDA_LAUNCH_BLOCKING</H3>
									<DL><p>
										<DT><A HREF="https://claude.ai/chat/483c79f7-bbc0-4cc5-bcae-24b26675cf9a">CUDA_LAUNCH_BLOCKING kernel correctness and profiling</A>
									</DL><p>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>cuda-micro-benchmarking</H3>
							<DL><p>
								<DT><H3 FOLDED>TF32</H3>
								<DL><p>
									<DT><A HREF="https://x.com/BenTheEgg/status/1813898551145173346">TF32 is FP32 range with FP16 accuracy (BF16 is FP32 range with reduced accuracy)</A>
								</DL><p>
								<DT><H3 FOLDED>hopper-microbenchmarking</H3>
								<DL><p>
									<DT><A HREF="https://gist.github.com/Chillee/2ec89696db8b7ed1c24461159e325405">H100 peak matmul FLOPS</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2503.20481">[2503.20481] Analyzing Modern NVIDIA GPU cores</A>
								<DT><A HREF="https://arxiv.org/pdf/1903.07486.pdf">Dissecting the NVidia Turing T4 GPU via Microbenchmarking</A>
								<DT><A HREF="https://arxiv.org/pdf/1912.03413.pdf">IPU</A>
								<DT><A HREF="https://arxiv.org/pdf/1804.06826.pdf%5B/url%5D">Volta</A>
								<DT><A HREF="https://github.com/sophiawisdom/benchmarks">sophiawisdom/benchmarks</A>
								<DT><A HREF="https://github.com/Yinghan-Li/YHs_Sample/tree/master/cuda/microbenchmark">YHs_Sample/cuda/microbenchmark at master · Yinghan-Li/YHs_Sample</A>
								<DT><A HREF="https://www.ece.lsu.edu/koppel/gp/notes/set-nv-org.pdf">slides</A>
								<DT><A HREF="https://github.com/mag-/gpu_benchmark?tab=readme-ov-file">mag-/gpu_benchmark: Gpu benchmark</A>
								<DT><A HREF="https://github.com/stas00/ml-engineering/blob/master/compute/accelerator/benchmarks/mamf-finder.py">ml-engineering/compute/accelerator/benchmarks/mamf-finder.py at master · stas00/ml-engineering</A>
								<DT><A HREF="https://github.com/EleutherAI/cookbook/tree/main/benchmarks/sizing">cookbook/benchmarks/sizing at main · EleutherAI/cookbook</A>
								<DT><A HREF="https://github.com/ROCm/pytorch-micro-benchmarking">ROCm/pytorch-micro-benchmarking</A>
								<DT><A HREF="https://github.com/NVIDIA/cuda-samples/blob/master/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/README.md">cuda-samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/README.md at master · NVIDIA/cuda-samples</A>
								<DT><A HREF="https://x.com/cHHillee/status/1884743684928962884">(2) Horace He en X: "@PytorchToAtoms @giffmana https://t.co/yCWveA7zNp Also need to set vboost slider to 1. With this, I hit 780 TFLOPS. You also need a new-enough version of PyTorch to have this PR + have cutlass installed lol: https://t.co/Kmh6LxdNEp https://t.co/UyBoD5QgIg" / X</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-perf</H3>
							<DL><p>
								<DT><H3 FOLDED>arithmetic intensity</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/cis_female/status/1771746532892586388">arithmetic intensity: easy appro min(m,n,k)</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=USMnKuyXBFM">Developing Optimal CUDA Kernels on Hopper Tensor Cores NVIDIA On Demand - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=OIOx3CJP2Es&t=559s">Introduction to CUDA Programming and Performance Optimization NVIDIA On Demand - YouTube</A>
								<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62192/?playlistId=playList-d59c3dc3-9e5a-404d-8725-4b567f4dfe77">Advanced Performance Optimization in CUDA | NVIDIA On-Demand</A>
								<DT><A HREF="https://forums.developer.nvidia.com/t/structures-of-arrays-vs-arrays-of-structures/13581">Structures of Arrays vs Arrays of Structures? - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/AoS_and_SoA">AoS and SoA - Wikipedia</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#getting-started">CUDA C++ Best Practices Guide</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-reproducibility</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/docs/stable/notes/randomness.html">Reproducibility — PyTorch 1.13 documentation</A>
								<DT><A HREF="https://vandurajan91.medium.com/random-seeds-and-reproducible-results-in-pytorch-211620301eba">Random seeds and reproducible results in PyTorch | by Vandana Rajan | Medium</A>
								<DT><A HREF="https://developer.nvidia.com/deep-learning-performance-training-inference">Reproducible Performance</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-error-correction-code</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>MGI</H3>
							<DL><p>
								<DT><A HREF="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html">NVIDIA Multi-Instance GPU User Guide :: NVIDIA Tesla Documentation</A>
								<DT><A HREF="https://developer.nvidia.com/blog/getting-the-most-out-of-the-a100-gpu-with-multi-instance-gpu/">Getting the Most Out of the NVIDIA A100 GPU with Multi-Instance GPU | NVIDIA Technical Blog</A>
								<DT><A HREF="https://pytorch.org/docs/stable/cuda.html">torch.cuda — PyTorch 1.13 documentation</A>
								<DT><A HREF="https://discuss.pytorch.org/t/access-gpu-partitions-in-mig/142272">Access GPU partitions in MIG - PyTorch Forums</A>
							</DL><p>
							<DT><H3 FOLDED>IB</H3>
							<DL><p>
								<DT><H3 FOLDED>ib-lectures</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=Pgy4wAw6eEo">InfiniBand Principles Every HPC Expert MUST Know (Part 2) - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=2gidd6lLiH8">27 Aug 18: Webinar: Introduction to InfiniBand Networks - YouTube</A>
								</DL><p>
								<DT><A HREF="https://pytorchtoatoms.substack.com/p/nvidia-quantum-x800-next-generation">NVIDIA Quantum-X800: Next Generation Infiniband 800Gbit/s Network Topology</A>
								<DT><A HREF="https://pytorchtoatoms.substack.com/p/benchmarking-nvlink-and-infiniband">Benchmarking Nvlink &amp; Infiniband Network Speeds</A>
								<DT><A HREF="http://mvapich.cse.ohio-state.edu/benchmarks/">MVAPICH :: Benchmarks</A>
								<DT><A HREF="https://github.com/imbue-ai/cluster-health/blob/master/ufm_events/find_problematic_events.py">cluster-health/ufm_events/find_problematic_events.py at master · imbue-ai/cluster-health</A>
								<DT><A HREF="https://github.com/imbue-ai/cluster-health/tree/master/ib_burn">cluster-health/ib_burn at master · imbue-ai/cluster-health</A>
								<DT><A HREF="https://github.com/ppl-ai/pplx-kernels">ppl-ai/pplx-kernels: Perplexity GPU Kernels</A>
							</DL><p>
							<DT><H3 FOLDED>NVLink</H3>
							<DL><p>
								<DT><A HREF="https://pytorchtoatoms.substack.com/p/benchmarking-nvlink-and-infiniband">Benchmarking Nvlink &amp; Infiniband Network Speeds</A>
							</DL><p>
							<DT><H3 FOLDED>GPUDirect</H3>
							<DL><p>
								<DT><H3 FOLDED>gds-installation</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">NVIDIA GPUDirect Storage Installation and Troubleshooting Guide - NVIDIA Docs</A>
									<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mofed-req-install">NVIDIA GPUDirect Storage Installation and Troubleshooting Guide - NVIDIA Docs</A>
									<DT><A HREF="https://docs.nvidia.com/networking/display/mlnxofedv531001">NVIDIA MLNX_OFED Documentation Rev 5.3-1.0.0.1 - NVIDIA Docs</A>
									<DT><A HREF="https://docs.nvidia.com/networking/display/mlnxofedv461000/downloading+mellanox+ofed">Downloading Mellanox OFED - NVIDIA Docs</A>
									<DT><A HREF="https://docs.nvidia.com/networking/display/mlnxofedv461000/general+support+in+mlnx_ofed">General Support in MLNX_OFED - NVIDIA Docs</A>
									<DT><A HREF="https://network.nvidia.com/products/infiniband-drivers/linux/mlnx_ofed/">Linux InfiniBand Drivers</A>
									<DT><A HREF="https://ubuntu.com/server/docs/nvidia-drivers-installation">NVIDIA drivers installation | Ubuntu</A>
									<DT><A HREF="https://github.com/developer-onizuka/gpudirect_storage">developer-onizuka/gpudirect_storage (global view)</A>
								</DL><p>
								<DT><H3 FOLDED>cuFile</H3>
								<DL><p>
									<DT><H3 FOLDED>kvikio</H3>
									<DL><p>
										<DT><A HREF="https://github.com/rapidsai/kvikio/blob/branch-24.04/python/tests/test_defaults.py">kvikio/python/tests/test_defaults.py (compat_mode)</A>
										<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/topics/cufile-compatibility.html">cuFile Compatibility Mode - NVIDIA Docs</A>
										<DT><A HREF="https://docs.rapids.ai/api/libkvikio/nightly/">Compatibility Mode (KVIKIO_COMPAT_MODE)</A>
									</DL><p>
									<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html">cuFile API Reference Guide - NVIDIA Docs</A>
									<DT><A HREF="https://github.com/rapidsai/kvikio">rapidsai/kvikio</A>
									<DT><A HREF="https://github.com/rapidsai/kvikio/pull/135">Overload `numpy.fromfile()` and `cupy.fromfile()` by madsbk · Pull Request #135 · rapidsai/kvikio</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py at main · alpa-projects/alpa</A>
									<DT><A HREF="https://github.com/pfnet/pytorch-pfn-extras/blob/f6b127063ec910b71788db2ae6ef96a3d89832b1/tests/pytorch_pfn_extras_tests/cuda_tests/test_allocator.py">pytorch-pfn-extras/tests/pytorch_pfn_extras_tests/cuda_tests/test_allocator.py at f6b127063ec910b71788db2ae6ef96a3d89832b1 · pfnet/pytorch-pfn-extras</A>
									<DT><A HREF="https://pytorch-pfn-extras.readthedocs.io/en/latest/user_guide/cuda.html">CUDA (CuPy Interoperability) — pytorch-pfn-extras documentation</A>
									<DT><A HREF="https://github.com/NVIDIA/apex/blob/810ffae374a2b9cb4b5c5e28eaeca7d7998fca0c/apex/contrib/csrc/gpu_direct_storage/gds.cpp">apex/apex/contrib/csrc/gpu_direct_storage/gds.cpp</A>
									<DT><A HREF="https://github.com/NVIDIA/MagnumIO/blob/main/gds/samples/cufile_sample_022.cc">cuFile Batch APIs</A>
									<DT><A HREF="https://chat.openai.com/c/61bb588b-35a7-42a6-90e9-c2355e5646a9">Identify Disk Storage Type</A>
								</DL><p>
								<DT><A HREF="https://github.com/NVIDIA/gds-nvidia-fs">NVIDIA/gds-nvidia-fs: NVIDIA GPUDirect Storage Driver</A>
								<DT><A HREF="https://developer.nvidia.com/blog/gpudirect-storage/">GPUDirect Storage: A Direct Path Between Storage and GPU Memory</A>
								<DT><A HREF="https://arxiv.org/pdf/2203.04910.pdf?">GPU-Initiated On-Demand High-Throughput Storage Access in the BaM System Architecture</A>
								<DT><A HREF="https://on-demand.gputechconf.com/supercomputing/2019/pdf/sc1922-gpudirect-storage-transfer-data-directly-to-gpu-memory-alleviating-io-bottlenecks.pdf">GPUDIRECT STORAGE:A DIRECT GPU-STORAGE DATA PATH</A>
								<DT><A HREF="https://www.snia-j.org/cmm/images/2022/10/3-1NVIDIA.pdf">NVIDIA GPUDirect Storage</A>
								<DT><A HREF="https://github.com/michaelbe2/write_to_gpu">michaelbe2/write_to_gpu: Using RC or DC with new post send APIs</A>
								<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py at main · alpa-projects/alpa</A>
								<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html">NVIDIA GPUDirect Storage O_DIRECT Requirements Guide - NVIDIA Docs</A>
								<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html">NVIDIA GPUDirect Storage Best Practices Guide - NVIDIA Docs</A>
								<DT><A HREF="https://medium.com/@kaiyongx2/quick-guide-to-gpudirect-storage-gds-592037bdc046">Learning Nvidia GPUDirect. Impetus of Using GPUDirect Storage | by KY | Medium</A>
								<DT><A HREF="https://chat.openai.com/c/61bb588b-35a7-42a6-90e9-c2355e5646a9">Identify Disk Storage Type</A>
								<DT><A HREF="https://man7.org/linux/man-pages/man8/lsblk.8.html">lsblk(8) - Linux manual page</A>
								<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">NVIDIA GPUDirect Storage Overview Guide - NVIDIA Docs</A>
								<DT><A HREF="https://ieeexplore.ieee.org/document/7973709">Offloading Communication Control Logic in GPU</A>
								<DT><A HREF="https://github.com/Mellanox/gpu_direct_rdma_access">Mellanox/gpu_direct_rdma_access: example code for using DC QP for providing RDMA READ and WRITE operations to remote GPU memory</A>
								<DT><A HREF="https://github.com/NVIDIA/gdrcopy">NVIDIA/gdrcopy: A fast GPU memory copy library based on NVIDIA GPUDirect RDMA technology</A>
								<DT><A HREF="https://github.com/lw?tab=stars">lw (Luca Wehrstedt)</A>
								<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/compare/main...mikaylagawarecki:pytorch:gds">Comparing pytorch:main...mikaylagawarecki:gds · pytorch/pytorch</A>
							</DL><p>
							<DT><H3 FOLDED>nvidia-driver</H3>
							<DL><p>
								<DT><H3 FOLDED>cuda-driver-installation</H3>
								<DL><p>
									<DT><A HREF="https://developer.nvidia.com/cuda-12-6-0-download-archive?target_os=Linux&target_arch=arm64-sbsa&Compilation=Native&Distribution=Ubuntu&target_version=22.04&target_type=deb_local">CUDA Toolkit 12.6 Downloads | NVIDIA Developer</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#driver-installation">4. Driver Installation— Installation Guide for Linux 12.8 documentation</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#uninstallation">8.6 Uninstallation (official docs)</A>
									<DT><A HREF="https://ubuntu.com/server/docs/nvidia-drivers-installation">NVIDIA drivers installation | Ubuntu</A>
									<DT><A HREF="https://help.ubuntu.com/community/NvidiaDriversInstallation">NvidiaDriversInstallation - Community Help Wiki</A>
									<DT><A HREF="https://gist.github.com/jhelgert/b50d2b33d59eb935d8c37cd7d5d891d1">Installing the NVIDIA driver, CUDA, cuDNN, NCCL and Tensorflow on Linux</A>
									<DT><A HREF="https://linux.die.net/man/8/modprobe">modprobe(8): add/remove modules from Kernel - Linux man page</A>
									<DT><A HREF="https://www.nvidia.com/Download/index.aspx">NVIDIA Driver Downloads</A>
									<DT><A HREF="https://fossies.org/linux/misc/">Linux: Free open source software (misc) | Fossies Archive</A>
									<DT><A HREF="https://ubuntuforums.org/showthread.php?t=2454347">[SOLVED] dpkg: error processing archive /var/cache/apt/archives/libnvidia-compute-440_440.118.</A>
									<DT><A HREF="https://github.com/imbue-ai/cluster-health/blob/master/health_checks/health_check_fixes/reinstall_nvidia.sh">cluster-health/health_checks/health_check_fixes/reinstall_nvidia.sh at master · imbue-ai/cluster-health</A>
								</DL><p>
								<DT><H3 FOLDED>open-gpu-kernel-modules</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tinygrad/tinyos/blob/main/setup/driverinstall.sh">tinyos/setup/driverinstall.sh at main · tinygrad/tinyos</A>
									<DT><A HREF="https://morgangiraud.medium.com/multi-gpu-tinygrad-patch-4904a75f8e16">Multi-GPU Tinygrad Patch. Unlocking Multi-GPU P2P Capabilities... | by Morgan | May, 2024 | Medium</A>
									<DT><A HREF="https://github.com/tinygrad/open-gpu-kernel-modules">tinygrad/open-gpu-kernel-modules: NVIDIA Linux open GPU with P2P support</A>
									<DT><A HREF="https://developer.nvidia.com/blog/nvidia-transitions-fully-towards-open-source-gpu-kernel-modules/">NVIDIA Transitions Fully Towards Open-Source GPU Kernel Modules | NVIDIA Technical Blog</A>
								</DL><p>
								<DT><H3 FOLDED>nvidia-driver-570.86.15</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-570-86-15/index.html">Version 570.86.15(Linux)/572.13(Windows) :: NVIDIA Data Center GPU Driver Documentation</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/datacenter/tesla/index.html">NVIDIA Data Center GPU Driver Documentation</A>
								<DT><A HREF="https://download.nvidia.com/XFree86/Linux-x86_64/470.223.02/README/">NVIDIA Accelerated Linux Graphics Driver README and Installation Guide</A>
								<DT><A HREF="https://github.com/tinygrad/open-gpu-kernel-modules">tinygrad/open-gpu-kernel-modules: NVIDIA Linux open GPU with P2P support</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/acc466751b2723eb913fd3148b4f054189bbf1ab/.devcontainer/README.md">pytorch/.devcontainer/README.md: NVIDIA Container Toolkit for GPU Usage</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">CUDA 12.5 Update 1 Release Notes</A>
								<DT><A HREF="https://github.com/pytorch-labs/tritonbench/tree/main/docker/infra">tritonbench/docker/infra at main · pytorch-labs/tritonbench</A>
								<DT><A HREF="https://github.com/facebookresearch/xformers/pull/1157">[FA3] Link to cuda library to fix the FA3 extension build by xuzhao9 · Pull Request #1157 · facebookresearch/xformers</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/pull/1700">Use CUDA runtime API to retrieve function pointer to driver API by shunfan-shao · Pull Request #1700 · NVIDIA/cutlass</A>
								<DT><A HREF="https://docs.nvidia.com/deploy/cuda-compatibility/">CUDA Compatibility</A>
								<DT><A HREF="https://docs.nvidia.com/datacenter/tesla/drivers/index.html">NVIDIA Data Center Drivers</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-monitoring</H3>
							<DL><p>
								<DT><H3 FOLDED>NVML</H3>
								<DL><p>
									<DT><A HREF="https://github.com/gpuopenanalytics/pynvml">gpuopenanalytics/pynvml: Provide Python access to the NVML library for GPU diagnostics</A>
									<DT><A HREF="https://pypi.org/project/pynvml/">pynvml · PyPI</A>
									<DT><A HREF="https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries">NVML API Reference</A>
								</DL><p>
								<DT><H3 FOLDED>nvidia-smi</H3>
								<DL><p>
									<DT><H3 FOLDED>nvidia-smi-kill</H3>
									<DL><p>
										<DT><A HREF="https://chatgpt.com/c/679cbf8b-5348-800c-9bc6-c99e41df17a7">Kill GPU Processes Ubuntu nvidia-smi</A>
										<DT><A HREF="https://claude.ai/chat/fcb0ea6d-a81c-4de0-a5ee-e6803c72e48e">sudo nvidia-smi --query-compute-apps=pid --format=csv,noheader | xargs -r sudo kill -9</A>
									</DL><p>
									<DT><A HREF="https://forums.developer.nvidia.com/t/nvidia-smi-drain-failed-to-parse-device-specified-at-the-command-line/180402">Nvidia-smi drain "Failed to parse device specified at the command-line" - CUDA / CUDA Programming and Performance - NVIDIA Developer Forums</A>
									<DT><A HREF="https://unix.stackexchange.com/questions/654075/how-can-i-disable-and-later-re-enable-one-of-my-nvidia-gpus">linux - How can I disable (and later re-enable) one of my NVIDIA GPUs? - Unix &amp; Linux Stack Exchange</A>
									<DT><A HREF="https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries">Useful nvidia-smi Queries | NVIDIA</A>
									<DT><A HREF="https://x.com/pranjalssh">watch -n 0.1 nvidia-smi</A>
									<DT><A HREF="https://docs.nvidia.com/deploy/nvidia-smi/index.html">https://docs.nvidia.com/deploy/nvidia-smi/index.html</A>
									<DT><A HREF="https://github.com/gpuopenanalytics/pynvml">gpuopenanalytics/pynvml: Provide Python access to the NVML library for GPU diagnostics</A>
									<DT><A HREF="https://pypi.org/project/pynvml/">pynvml · PyPI</A>
								</DL><p>
								<DT><H3 FOLDED>gpu-monitoring</H3>
								<DL><p>
									<DT><H3 FOLDED>nvitop</H3>
									<DL><p>
										<DT><A HREF="https://github.com/XuehaiPan/nvitop?tab=readme-ov-file#resource-metric-collector">XuehaiPan/nvitop: An interactive NVIDIA-GPU process viewer and beyond, the one-stop solution for GPU process management.</A>
									</DL><p>
									<DT><H3 FOLDED>nvtop</H3>
									<DL><p>
										<DT><A HREF="https://github.com/Syllo/nvtop">Syllo/nvtop: GPU &amp; Accelerator process monitoring for AMD, Apple, Huawei, Intel, NVIDIA and Qualcomm</A>
									</DL><p>
									<DT><A HREF="https://github.com/wookayin/gpustat">wookayin/gpustat: 📊 A simple command-line utility for querying and monitoring GPU status</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>Nvidia-instruction-set-architecture</H3>
							<DL><p>
								<DT><H3 FOLDED>Nvidia SM90a Instruction Set Architecture</H3>
								<DL><p>
									<DT><A HREF="https://kuterdinel.com/nv_isa/">https://kuterdinel.com/nv_isa/</A>
									<DT><A HREF="https://x.com/KuterDinel">(1) Kuter Dinel (@KuterDinel) / X</A>
									<DT><A HREF="https://kuterdinel.com/nvidia-sass-control-code-viewer.html">Nvidia SASS Control Code Viewer • Kuter Dinel's blog</A>
									<DT><A HREF="https://kuterdinel.com/python-bytecode-and-ast-explorer.html">Python Bytecode and AST Explorer • Kuter Dinel's blog</A>
									<DT><A HREF="https://github.com/kuterd/nv_isa_solver">kuterd/nv_isa_solver: Nvidia Instruction Set Specification Generator</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>cupy</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>cuda-sharing</H3>
							<DL><p>
								<DT><H3 FOLDED>Multi-Process Service (MPS)</H3>
								<DL><p>
									<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2021/06/MPS_ORNL_20210817.pdf">Introduction (slides)</A>
									<DT><A HREF="https://docs.nvidia.com/deploy/mps/index.html">Multi-Process Service :: GPU Deployment and Management Documentation</A>
								</DL><p>
								<DT><H3 FOLDED>Time-Slicing</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html">Time-Slicing GPUs in Kubernetes — NVIDIA GPU Operator 23.9.2 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>NVIDIA Multi-Instance GPU</H3>
								<DL><p>
									<DT><A HREF="https://www.nvidia.com/en-us/technologies/multi-instance-gpu/">Multi-Instance GPU (MIG) | NVIDIA</A>
								</DL><p>
								<DT><A HREF="https://research.colfax-intl.com/sharing-nvidia-gpus-at-the-system-level-time-sliced-and-mig-backed-vgpus/">Sharing NVIDIA® GPUs at the System Level: Time-Sliced and MIG-Backed vGPUs – Colfax Research</A>
								<DT><A HREF="https://www.youtube.com/watch?v=8VQHwNwX-BU">NSDI '23 - Transparent GPU Sharing in Container Clouds for Deep Learning Workloads - YouTube</A>
								<DT><A HREF="https://github.com/pkusys/TGS">pkusys/TGS: Artifacts for our NSDI'23 paper TGS</A>
							</DL><p>
							<DT><H3 FOLDED>cuda-people</H3>
							<DL><p>
								<DT><A HREF="https://github.com/wangzyon">wangzyon (Wang Zhiyong)</A>
								<DT><A HREF="https://www.youtube.com/@OnurMutluLectures/videos">Onur Mutlu Lectures - YouTube</A>
								<DT><A HREF="https://siboehm.com/">siboehm</A>
								<DT><A HREF="https://leimao.github.io/tags/CUDA/">Tag: CUDA - Lei Mao's Log Book</A>
								<DT><A HREF="https://github.com/depaulmillz">dePaul Miller (@depaulmillz)</A>
								<DT><A HREF="https://github.com/Sam3077">Sam3077 (Samantha)</A>
								<DT><A HREF="https://github.com/yzhaiustc">yzhaiustc (Yujia Zhai)</A>
								<DT><A HREF="https://github.com/shangz-ai">shangz-ai (Shang Zhang)</A>
								<DT><A HREF="https://github.com/IonThruster">IonThruster (Pradeep Ramani)</A>
								<DT><A HREF="https://github.com/mbrookhart">mbrookhart (Matthew Brookhart)</A>
								<DT><A HREF="https://github.com/masahi">masahi</A>
								<DT><A HREF="https://github.com/binarybana">binarybana (Jason Knight)</A>
								<DT><A HREF="https://github.com/CliveUnger">CliveUnger (Clive Unger)</A>
							</DL><p>
							<DT><H3 FOLDED>compute-capability</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch/pytorch/pull/109168/files#diff-6eceb58c0b35f53fc6d289165f9ff56fa419155920dfdef45a82296f171c96fa">Basic fp8 support in Inductor by ipiszy · Pull Request #109168 · pytorch/pytorch: isSM90orLaterDevice</A>
								<DT><A HREF="https://github.com/ai-compiler-study/triton-kernels/blob/main/scripts/gpu_properties.cu">triton-kernels/scripts/gpu_properties.cu at gpu_properties</A>
							</DL><p>
							<DT><A HREF="https://github.com/geohot/cuda_ioctl_sniffer">geohot/cuda_ioctl_sniffer: Sniff CUDA ioctls</A>
							<DT><A HREF="https://jhui.github.io/2017/03/06/CUDA/">“CUDA Tutorial”</A>
							<DT><A HREF="https://developer.nvidia.com/deep-learning-performance-training-inference">Reproducible Performance</A>
							<DT><A HREF="https://catalog.ngc.nvidia.com/collections">Collections - Use-Case Based AI Software Packages | NVIDIA NGC</A>
							<DT><A HREF="https://forums.fast.ai/t/clearing-gpu-memory-pytorch/14637">Clearing GPU Memory - PyTorch - Part 1 (2018) / Beginner (2018) - Deep Learning Course Forums</A>
							<DT><A HREF="https://discuss.pytorch.org/t/why-moving-model-and-tensors-to-gpu/41498">Moving memory from CPU-RAM to GPU VRAM</A>
							<DT><A HREF="https://www.programcreek.com/python/?CodeExample=clear+memory">Python clear memory</A>
							<DT><A HREF="https://groups.google.com/a/anaconda.com/g/numba-users/c/7jkf-X_U7B8">Best way to clean up GPU memory</A>
							<DT><A HREF="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094">CUDA Runtime API :: CUDA Toolkit Documentation</A>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html">cuDNN Documentation</A>
							<DT><A HREF="https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation">cuda - How to verify CuDNN installation</A>
							<DT><A HREF="https://nvidia.github.io/cuda-python/install.html">Installation - CUDA Python 12.1.0 documentation</A>
							<DT><A HREF="https://twitter.com/marius/status/1657530968801181696/photo/1">GPU Computing</A>
							<DT><A HREF="https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics">CUDA semantics — PyTorch 2.0 documentation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=QQceTDjA4f4">GTC 2022 - How CUDA Programming Works - Stephen Jones, CUDA Architect, NVIDIA - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=-J8YyfrSwTk">Effective ML - YouTube</A>
							<DT><A HREF="https://github.com/enp1s0/cutf">enp1s0/cutf: CUDA Template Functions</A>
							<DT><A HREF="https://arxiv.org/pdf/1903.07486.pdf">https://arxiv.org/pdf/1903.07486.pdf</A>
							<DT><A HREF="https://arxiv.org/abs/1903.07486">[1903.07486] Dissecting the NVidia Turing T4 GPU via Microbenchmarking</A>
							<DT><A HREF="https://gist.github.com/mcarilli">mcarilli’s gists</A>
							<DT><A HREF="https://futhark-lang.org/">Why Futhark?</A>
							<DT><A HREF="https://www.youtube.com/watch?v=q38V66bqhfU">EfficientML.ai lecture - YouTube</A>
							<DT><A HREF="https://github.com/coreylowman/cudarc">coreylowman/cudarc: Safe rust wrapper around CUDA toolkit</A>
							<DT><A HREF="https://research.colfax-intl.com/adding-fp8-to-flashattention/">Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2 – Colfax Research</A>
							<DT><A HREF="https://forums.fast.ai/t/clearing-gpu-memory-pytorch/14637">Clearing GPU Memory</A>
							<DT><A HREF="https://github.com/microsoft/onnxruntime/tree/main/onnxruntime/core/providers/cuda">onnxruntime/onnxruntime/core/providers/cuda at main</A>
							<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/blob/df4a7534860137e060e18d2ebf019906120ea204/src/fastertransformer/kernels/matrix_transpose_kernels.cu#L4">FasterTransformer/src/fastertransformer/kernels/matrix_transpose_kernels.cu at df4a7534860137e060e18d2ebf019906120ea204 · NVIDIA/FasterTransformer</A>
							<DT><A HREF="https://github.com/HigherOrderCO/HVM/blob/5de3e7ed8f1fcee6f267841a24119ffd569c714d/src/hvm.cu#L4">HVM/src/hvm.cu</A>
							<DT><A HREF="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">NVIDIA CUDA Compiler Driver</A>
							<DT><A HREF="https://github.com/Rust-GPU/Rust-CUDA">Rust-GPU/Rust-CUDA: Ecosystem of libraries and tools for writing and executing fast GPU code fully in Rust.</A>
						</DL><p>
						<DT><H3 FOLDED>rocm</H3>
						<DL><p>
							<DT><H3 FOLDED>HIP</H3>
							<DL><p>
								<DT><A HREF="https://github.com/geohot/tinygrad/pull/750">HIP backend by nanamiwang · Pull Request #750 · geohot/tinygrad</A>
								<DT><A HREF="https://www.semianalysis.com/p/nvidiaopenaitritonpytorch">TorchInductor &amp; Triton</A>
								<DT><A HREF="https://github.com/openai/triton/issues/46">Support for HIP backend / AMD GPUs · Issue #46 · openai/triton</A>
								<DT><A HREF="https://github.com/openai/triton/blob/a2433f3135c312c05fbbcd98083896c93bf0c504/python/triton/runtime/driver.py#L111">triton/driver.py</A>
								<DT><A HREF="https://nv-adlr.github.io/MegatronLM">MegatronLM: NCCL</A>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/mpi.html">NCCL and MPI — NCCL 2.18.1 documentation</A>
								<DT><A HREF="https://pytorch.org/docs/stable/notes/hip.html">HIP (ROCm) semantics — PyTorch 2.0 documentation</A>
								<DT><A HREF="https://github.com/ROCm-Developer-Tools/HIP">ROCm-Developer-Tools/HIP: HIP: C++ Heterogeneous-Compute Interface for Portability</A>
								<DT><A HREF="https://github.com/ROCm-Developer-Tools/HIPIFY">HIPIFY</A>
								<DT><A HREF="https://docs.python.org/3/library/ctypes.html">ctypes — A foreign function library for Python — Python 3.11.3 documentation</A>
								<DT><A HREF="https://gist.github.com/geohot/6232fb00527de161a5c8ce8a635dd4f3">Wrapper for HIP</A>
								<DT><A HREF="https://www.youtube.com/playlist?list=PLB1fSi1mbw6IKbZSPz9a2r2DbnHWnLbF-">AMD HIP Tutorial - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>RDNA3</H3>
							<DL><p>
								<DT><A HREF="https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf">Instruction Set Architecture</A>
								<DT><A HREF="https://github.com/geohot/tinygrad/blob/ed038ba12906bb1980956b1e31e02e77ec0524ee/extra/rocm/rdna3/asm.py">tinygrad/asm.py</A>
							</DL><p>
							<DT><H3 FOLDED>rocm-llvm</H3>
							<DL><p>
								<DT><A HREF="https://llvm.org/docs/CommandGuide/llvm-mc.html">llvm-mc - LLVM Machine Code Playground — LLVM 17.0.0git documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=avRWPe1MXPk">LLVM Isn't Always The Best Choice For Compilers. - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>rocm-lumi</H3>
							<DL><p>
								<DT><H3 FOLDED>user information</H3>
								<DL><p>
									<DT><A HREF="https://docs.csc.fi/accounts/how-to-manage-user-information/#how-to-link-your-csc-user-account-to-external-authentication-sources">Managing user information - Docs CSC</A>
									<DT><A HREF="https://my.csc.fi/profile">My CSC</A>
									<DT><A HREF="https://mms.myaccessid.org/fed-apps/profile/#settings_sshkeys">Perun - User profile</A>
									<DT><A HREF="https://puhuri-portal.neic.no/login/">Puhuri Portal |</A>
									<DT><A HREF="https://md.sigma2.no/lumi-general-course-oct23?both#DAY-1-%E2%80%93-Tuesday-3102023">LUMI General Course - HedgeDoc</A>
									<DT><A HREF="https://lumi-supercomputer.github.io/LUMI-training-materials/4day-20230530/extra_1_05_Compilers_and_Parallel_Programming_Models/">Compilers and Parallel Programming Models - LUMI training materials</A>
								</DL><p>
								<DT><A HREF="https://openfam.github.io/">OpenFAM: A library for programming Fabric-Attached Memory</A>
								<DT><A HREF="https://cpe.ext.hpe.com/docs/performance-tools/perftools-lite.html#description">perftools-lite — HPE Cray Programming Environment 10.1.0 documentation</A>
							</DL><p>
							<DT><H3 FOLDED>rocm-instinct</H3>
							<DL><p>
								<DT><H3 FOLDED>rocm-amd-mi300x</H3>
								<DL><p>
									<DT><A HREF="https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance">AMD MI300 – Taming The Hype – AI Performance, Volume Ramp, Customers, Cost, IO, Networking, Software</A>
								</DL><p>
								<DT><H3 FOLDED>rocm-mi250x</H3>
								<DL><p>
									<DT><A HREF="https://www.lumi-supercomputer.eu/lumis-full-system-architecture-revealed/">LUMI’s full system architecture revealed - LUMI</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://rocm.github.io/rocncloc.html">ROCm, A New Era in Open GPU Computing</A>
							<DT><A HREF="https://github.com/ROCm-Developer-Tools/rocprofiler">ROCm-Developer-Tools/rocprofiler: ROC profiler library. Profiling with perf-counters and derived metrics.</A>
							<DT><A HREF="https://github.com/ROCmSoftwarePlatform/hip-python">ROCmSoftwarePlatform/hip-python: HIP Python Low-level Bindings</A>
							<DT><A HREF="https://www.youtube.com/watch?v=lnVQsJJFcdg&list=LL&index=296&t=1336s">George Hotz | Programming | writing a Qualcomm GPU driver | Freedreno | Mesa for compute | part 2 - YouTube</A>
							<DT><A HREF="https://github.com/tinygrad/remu?tab=readme-ov-file">tinygrad/remu: RDNA3 emulator</A>
							<DT><A HREF="https://docs.amd.com/">AMD Documentation - Portal</A>
							<DT><A HREF="https://pytorch.org/docs/stable/elastic/run.html#launcher-api">torchrun (Elastic Launch) — PyTorch 2.0 documentation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=LG9G4aA28rU">GPU Programming Concepts (Part 1) - YouTube</A>
							<DT><A HREF="https://lambdalabs.com/blog/multi-node-pytorch-distributed-training-guide#distributed-pytorch-underthehood">(MPI): Multi node PyTorch Distributed Training</A>
							<DT><A HREF="https://github.com/openai/triton/pull/1983">[ROCM] Core Functionality for AMD by micmelesse · Pull Request #1983 · openai/triton</A>
							<DT><A HREF="https://www.mosaicml.com/blog/amd-mi250">Training LLMs with AMD MI250 GPUs and MosaicML</A>
							<DT><A HREF="https://tenstorrent.com/category/research/">Category: Research - Tenstorrent</A>
							<DT><A HREF="https://github.com/ROCm/rocm-blogs">ROCm/rocm-blogs</A>
						</DL><p>
						<DT><H3 FOLDED>Kernels</H3>
						<DL><p>
							<DT><H3 FOLDED>matmul</H3>
							<DL><p>
								<DT><H3 FOLDED>matmul-sparsity</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/cHHillee/status/1785021827871596581">(1) Horace He en X: "Many don't know that GPUs automatically leverage ternary and fine-grained sparsity to accelerate your matmuls! e.g. A matmul with ternary + 90% sparsity results in 33% more FLOPs in my benchmark. (not joking) I explore this "optimization" here: https://t.co/YD3CTq1i7J (1/3) https://t.co/QQ7x8fmbfZ" / X</A>
									<DT><A HREF="https://www.thonking.ai/p/strangely-matrix-multiplications">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</A>
									<DT><A HREF="https://gist.github.com/Chillee/42e4635c59760a74cb3b4ba7ea5ad9f8#file-mm_weird-py">Strangely, Matrix Multiplications Run Faster When Given "Predictable" Data!</A>
								</DL><p>
								<DT><H3 FOLDED>matmul-quant</H3>
								<DL><p>
									<DT><A HREF="https://github.com/oadirt/quant-matmul">oadirt/quant-matmul</A>
								</DL><p>
								<DT><H3 FOLDED>matmul-sm90</H3>
								<DL><p>
									<DT><H3 FOLDED>matmul-sm90-triton</H3>
									<DL><p>
										<DT><A HREF="https://github.com/KnowingNothing/MatmulTutorial/blob/main/examples/matmul/triton/triton_matmul_sm90.py">MatmulTutorial/examples/matmul/triton/triton_matmul_sm90.py at main · KnowingNothing/MatmulTutorial</A>
										<DT><A HREF="https://github.com/KnowingNothing/MatmulTutorial/blob/main/examples/matmul/triton/complex_matmul.py">MatmulTutorial/examples/matmul/triton/complex_matmul.py at main · KnowingNothing/MatmulTutorial</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/extra/gemm/triton_nv_matmul.py#L4">tinygrad/extra/gemm/triton_nv_matmul.py at master · tinygrad/tinygrad</A>
										<DT><A HREF="https://github.com/pytorch-labs/applied-ai/commit/4d6a670431f1af8c25e8d1f1745a8e4981c6130f#diff-e1dc3f6561669f4e7d7cf69bb2ef4fb3bcd0ab1cce84e553cb4c779d50e7dc91">add triton tma gemm and cutlass pingpong gemm · pytorch-labs/applied-ai@4d6a670</A>
										<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/_triton/tiled_matmul_kernels.py#L152">xformers/xformers/ops/_triton/tiled_matmul_kernels.py at 0004c67c7e9ec3c9e7b3907db0e0b2957430b35b · facebookresearch/xformers</A>
										<DT><A HREF="https://github.com/stanford-futuredata/stk/blob/main/stk/backend/triton_kernels.py#L222">stk/stk/backend/triton_kernels.py at main</A>
									</DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/e1976daacc7b030ba672217eb5d96f5a663df4ab/media/docs/efficient_gemm.md">cutlass/media/docs/efficient_gemm.md</A>
									<DT><A HREF="https://github.com/c3sr/tcu_scope/blob/master/src/gemm/wmma.cu">tcu_scope/src/gemm/wmma.cu at master · c3sr/tcu_scope</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide: wmma::mma_sync</A>
									<DT><A HREF="https://github.com/wmmae/wmma_extension">wmmae/wmma_extension: An extension library of WMMA API (Tensor Core API)</A>
								</DL><p>
								<DT><H3 FOLDED>torch-matmul</H3>
								<DL><p>
									<DT><H3 FOLDED>_inductor/kernel/mm_scaled</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/354fe48db9ef94c69db6d03d997a374048824f83/torch/_inductor/kernel/mm_scaled.py#L504">pytorch/torch/_inductor/kernel/mm_scaled.py at 354fe48db9ef94c69db6d03d997a374048824f83 · pytorch/pytorch</A>
										<DT><A HREF="https://gist.github.com/drisspg/783616821043ab4594b9784f556c6714">Scaled MM API: This doc servers as a quick reference for the _scaled_mm API and how it has changed overtime for each major version of PyTorch.</A>
										<DT><A HREF="https://research.colfax-intl.com/deepseek-r1-and-fp8-mixed-precision-training/">DeepSeek-R1 and FP8 Mixed-Precision Training – Colfax Research</A>
										<DT><A HREF="https://github.com/pytorch/ao/blob/4d1c7741842a1dfbd479b3481fcdc93c64db703e/torchao/dtypes/floatx/float8_layout.py#L279">ao/torchao/dtypes/floatx/float8_layout.py preprocess_data row-major and colum-major</A>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/4c1dd13ba33d0fcd1f039ea67b026979a09ae00a/benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py#L8">pytorch/benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py at 4c1dd13ba33d0fcd1f039ea67b026979a09ae00a · pytorch/pytorch</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/c2ff9fe042ffe39a2684aecc6c6062a066489f54/torch/_inductor/kernel/mm_scaled.py#L190">pytorch/torch/_inductor/kernel/mm_scaled.py at c2ff9fe042ffe39a2684aecc6c6062a066489f54 · pytorch/pytorch</A>
									<DT><A HREF="https://gist.github.com/Chillee/f86675147366a7a0c6e244eaa78660f7#file-4-matmul-bench-py">PT 2.0 Benchmarks</A>
									<DT><A HREF="https://gist.github.com/Chillee/2ec89696db8b7ed1c24461159e325405">H100 peak matmul FLOPS</A>
								</DL><p>
								<DT><H3 FOLDED>matmul-tiling</H3>
								<DL><p>
									<DT><A HREF="https://x.com/cHHillee/status/1630274862345490432">(2) Horace He en X: "Let's say I have a [M x K] @ [K x N] matmul. Which one of these configurations will have the best perf? Think about the actual ramifications of tiling! A: M=2047, K=N=2048 B: K=2047, M=N=2048 C: N=2047, M=K=2048 19/19" / X</A>
								</DL><p>
								<DT><H3 FOLDED>cuBLAS-matmul</H3>
								<DL><p>
									<DT><A HREF="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/tinyblas.cu">llamafile/llamafile/tinyblas.cu at main · Mozilla-Ocho/llamafile</A>
									<DT><A HREF="https://siboehm.com/articles/22/CUDA-MMM">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</A>
								</DL><p>
								<DT><H3 FOLDED>torch-matmul</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>matmul-theory</H3>
								<DL><p>
									<DT><A HREF="https://siboehm.com/articles/22/CUDA-MMM">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</A>
									<DT><A HREF="https://www.twitch.tv/spikedoanz">spikedoanz - Twitch</A>
									<DT><A HREF="https://github.com/spikedoanz/matmul">spikedoanz/matmul: can you multiply two matrices?</A>
									<DT><A HREF="https://en.algorithmica.org/hpc/algorithms/matmul/">Matrix Multiplication - Algorithmica</A>
									<DT><A HREF="https://www.youtube.com/watch?v=VgSQ1GOC86s">George Hotz | Programming | can you multiply a matrix? (noob lesson) | geohot/tinygrad/tree/gemm - YouTube</A>
									<DT><A HREF="https://salykova.github.io/matmul-cpu">Beating OpenBLAS and MKL in 150 lines of C Code: A Tutorial on High-Performance Matrix Multiplication</A>
								</DL><p>
								<DT><A HREF="https://siboehm.com/articles/22/CUDA-MMM">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</A>
								<DT><A HREF="https://gist.github.com/nadavrot/5b35d44e8ba3dd718e595e40184d03f0">Efficient matrix multiplication</A>
								<DT><A HREF="https://github.com/KnowingNothing/MatmulTutorial">KnowingNothing/MatmulTutorial: A Easy-to-understand TensorOp Matmul Tutorial</A>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html">Matrix Multiplication — JAX documentation</A>
								<DT><A HREF="https://github.com/openai/openai-gemm">openai/openai-gemm: Open single and half precision gemm implementations</A>
								<DT><A HREF="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">Mixed-input matrix multiplication performance optimizations – Google Research Blog</A>
								<DT><A HREF="https://arxiv.org/pdf/2006.16668.pdf">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma">matrix-multiply-accumulate (mma)</A>
								<DT><A HREF="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications?utm_source=post-email-title&publication_id=1781836&post_id=142904770&utm_campaign=email-post-title&isFreemail=true&r=1mqy6n&triedRedirect=true&utm_medium=email">What Shapes Do Matrix Multiplications Like? [medium]</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Sk35MKtCXfQ&t=4743s">tinygrad: matrix multiplication, a@b, cube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=VgSQ1GOC86s&t=19510s">George Hotz | Programming | can you multiply a matrix? (noob lesson) | geohot/tinygrad/tree/gemm - YouTube</A>
								<DT><A HREF="https://github.com/matiaslindgren/cuda-memory-access-recorder/tree/master/examples">cuda-memory-access-recorder/examples at master · matiaslindgren/cuda-memory-access-recorder</A>
								<DT><A HREF="https://github.com/BearNinja123/matmul">BearNinja123/matmul: A collection of matrix multiplication techniques in C, from naive to BLAS</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/examples/llm.c/ubench/matmul.c">tinygrad/examples/llm.c/ubench/matmul.c</A>
								<DT><A HREF="https://gist.github.com/geohot/0cad05378fcbaeb0dceec3e89e0d4d7b">A 1024x1024x1024 matmul with a 2x2x2 core in OpenCL</A>
								<DT><A HREF="https://twitter.com/cis_female/status/1771746532892586388">arithmetic intensity: easy appro min(m,n,k)</A>
								<DT><A HREF="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications">What Shapes Do Matrix Multiplications Like? [medium]</A>
								<DT><A HREF="https://gist.github.com/Chillee/abc38703f88fcb64683b6ccb0ae9d8ba">What Shapes Do Matrix Multiplications Like?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=VgSQ1GOC86s&t=19510s">George Hotz | Programming | can you multiply a matrix? (noob lesson)</A>
								<DT><A HREF="https://pytorch.org/blog/inside-the-matrix/">Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond | PyTorch</A>
								<DT><A HREF="https://github.com/ridgerchu/matmulfreellm">ridgerchu/matmulfreellm: Implementation for MatMul-free LM.</A>
								<DT><A HREF="https://github.com/tspeterkim/cuda-matmult">tspeterkim/cuda-matmult</A>
								<DT><A HREF="https://github.com/yester31/Matrix_Multiplication">yester31/Matrix_Multiplication: Diverse Matrix multiplication algorithms</A>
								<DT><A HREF="https://www.youtube.com/watch?v=oQT7IC0x254">2678x Faster with CUDA C: How GPUs enabled Deep Learning Revolution - YouTube</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/14108c1677e5a2f93a822a41cb579b572977d0d3/torch/_inductor/kernel/mm.py#L585">pytorch/torch/_inductor/kernel/mm.py at 14108c1677e5a2f93a822a41cb579b572977d0d3 · pytorch/pytorch</A>
								<DT><A HREF="https://gist.github.com/malfet/029aadee65629bcfcc7285c608a3bd79">Swift example that runs matrix multiplicaiton on MPS</A>
								<DT><A HREF="https://github.com/nadavrot/bistra">nadavrot/bistra: Bistra is a domain-specific language designed to generate high-performance kernels (such as GEMMs, convolutions, etc). The program is designed to allow powerful compiler optimizations and code generation that are not possible in C. The tool can auto-tune GEMM kernels to around 90% of peak performance (on X86/AVX2) within seconds.</A>
								<DT><A HREF="https://medium.com/@nevinbaiju_77488/optimizing-llama-2-faster-matmul-using-avx-8ca0b02258d5">Optimizing Llama 2: Faster matmul using AVX | by Nevin Baiju | Medium</A>
								<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/tiled_matmul.py">xformers/xformers/ops/tiled_matmul.py at 0004c67c7e9ec3c9e7b3907db0e0b2957430b35b · facebookresearch/xformers</A>
								<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/_triton/tiled_matmul_kernels.py#L152">xformers/xformers/ops/_triton/tiled_matmul_kernels.py at 0004c67c7e9ec3c9e7b3907db0e0b2957430b35b · facebookresearch/xformers</A>
								<DT><A HREF="https://github.com/mobiusml/gemlite/blob/master/examples/cuda_example.py">gemlite/examples/cuda_example.py at master · mobiusml/gemlite</A>
							</DL><p>
							<DT><H3 FOLDED>GEMM</H3>
							<DL><p>
								<DT><H3 FOLDED>HGEMM</H3>
								<DL><p>
									<DT><A HREF="https://github.com/Bruce-Lee-LY/cuda_hgemm">Bruce-Lee-LY/cuda_hgemm: Several optimization methods of half-precision general matrix multiplication (HGEMM) using tensor core with WMMA API and MMA PTX instruction.</A>
								</DL><p>
								<DT><H3 FOLDED>SGEMM</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NervanaSystems/maxas/wiki/SGEMM">SGEMM · NervanaSystems/maxas Wiki</A>
									<DT><A HREF="https://github.com/siboehm/SGEMM_CUDA">siboehm/SGEMM_CUDA: Fast CUDA matrix multiplication from scratch</A>
									<DT><A HREF="https://github.com/wangzyon/NVIDIA_SGEMM_PRACTICE">wangzyon/NVIDIA_SGEMM_PRACTICE: Step-by-step optimization of CUDA SGEMM</A>
									<DT><A HREF="https://github.com/Mozilla-Ocho/llamafile/blob/main/llamafile/sgemm.cpp">llamafile/llamafile/sgemm.cpp at main · Mozilla-Ocho/llamafile</A>
									<DT><A HREF="https://github.com/yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs">yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs: Optimizing SGEMM kernel functions on NVIDIA GPUs to a close-to-cuBLAS performance.</A>
								</DL><p>
								<DT><H3 FOLDED>Grouped-gemm</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tgale96/grouped_gemm">tgale96/grouped_gemm: PyTorch bindings for CUTLASS grouped GEMM.</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/examples/57_hopper_grouped_gemm/57_hopper_grouped_gemm.cu">cutlass/examples/57_hopper_grouped_gemm/57_hopper_grouped_gemm.cu at main · NVIDIA/cutlass</A>
									<DT><A HREF="https://github.com/google/jax/pull/20462/">Add MegaBlox grouped matrix multiplication kernels for TPU. by copybara-service[bot] · Pull Request #20462 · google/jax</A>
								</DL><p>
								<DT><H3 FOLDED>scaled-gemm</H3>
								<DL><p>
									<DT><H3 FOLDED>FBGEMM</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/docs/transformers/quantization/fbgemm_fp8">FBGEMM FP8</A>
										<DT><A HREF="https://github.com/pytorch/FBGEMM">pytorch/FBGEMM: FB (Facebook) + GEMM (General Matrix-Matrix Multiplication)</A>
										<DT><A HREF="https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/experimental/gen_ai/bench/quantize_bench.py">FBGEMM/fbgemm_gpu/experimental/gen_ai/bench/quantize_bench.py at main · pytorch/FBGEMM</A>
									</DL><p>
									<DT><H3 FOLDED>gemm-fp8</H3>
									<DL><p>
										<DT><H3 FOLDED>GridQuant</H3>
										<DL><p>
											<DT><A HREF="https://pytorch.org/blog/hadacore/?utm_content=318142990&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">HadaCore: Tensor Core Accelerated Hadamard Transform Kernel | PyTorch</A>
											<DT><A HREF="https://pytorch.org/blog/accelerating-gemms-triton/?utm_content=318580331&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating 2D Dynamic Block Quantized Float8 GEMMs in Triton | PyTorch</A>
										</DL><p>
										<DT><A HREF="https://gist.github.com/wkcn/232d2cf8d50e15cdb38be3e577cc4e3a">FP8GEMM</A>
										<DT><A HREF="https://github.com/pytorch/FBGEMM/blob/669b6c7167f7ea21eb049c621b5408cb03240e44/fbgemm_gpu/experimental/gemm/triton_gemm/fp8_gemm.py#L1448">FBGEMM/fbgemm_gpu/experimental/gemm/triton_gemm/fp8_gemm.py at 669b6c7167f7ea21eb049c621b5408cb03240e44 · pytorch/FBGEMM</A>
										<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/53b48f35ad87603df882d036c1a2e3f7d88f2bd8/kernels/needs_perf_help/fp8_gemm_bench.py#L102">applied-ai/kernels/needs_perf_help/fp8_gemm_bench.py at 53b48f35ad87603df882d036c1a2e3f7d88f2bd8 · pytorch-labs/applied-ai</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/c2ff9fe042ffe39a2684aecc6c6062a066489f54/torch/_inductor/kernel/mm_scaled.py#L190">pytorch/torch/_inductor/kernel/mm_scaled.py at c2ff9fe042ffe39a2684aecc6c6062a066489f54 · pytorch/pytorch</A>
										<DT><A HREF="https://research.colfax-intl.com/adding-fp8-to-flashattention/">Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2 – Colfax Research</A>
										<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/main/kernels/needs_perf_help/fp8_rowwise_tma_persistent.py">applied-ai/kernels/needs_perf_help/fp8_rowwise_tma_persistent.py</A>
									</DL><p>
									<DT><H3 FOLDED>gemm-fp4</H3>
									<DL><p>
										<DT><H3 FOLDED>fbgemm-fp4</H3>
										<DL><p>
											<DT><A HREF="https://github.com/meta-llama/llama-models/blob/main/models/quantize_impls.py#L50">llama-models/models/quantize_impls.py: Int4ScaledWeights</A>
											<DT><A HREF="https://gist.github.com/drisspg/783616821043ab4594b9784f556c6714">Scaled MM API</A>
										</DL><p>
										<DT><H3 FOLDED>gemm-fp4-triton</H3>
										<DL><p>
											<DT><A HREF="https://triton-lang.org/main/getting-started/tutorials/10-block-scaled-matmul.html">Block Scaled Matrix Multiplication — Triton documentation: nvfp4, mxfp4</A>
											<DT><A HREF="https://github.com/triton-lang/triton/blob/32b42821c75bcb17d0f58ddabc479b95df8dbcbf/python/tutorials/10-block-scaled-matmul.py#L311">triton/python/tutorials/10-block-scaled-matmul.py</A>
										</DL><p>
										<DT><H3 FOLDED>cutlass-gemm-fp4</H3>
										<DL><p>
											<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/5e497243f7ad13a2aa842143f9b10bbb23d98292/examples/72_blackwell_narrow_precision_gemm/72b_blackwell_nvfp4_nvfp4_gemm.cu#L35">cutlass/examples/72b_blackwell_nvfp4_nvfp4_gemm.cu</A>
										</DL><p>
										<DT><A HREF="https://github.com/NVIDIA/cutlass/commit/79fc51f4b853d031babb0c7229b4efa99190f14f">v3.9 update (#2213) · NVIDIA/cutlass@79fc51f</A>
									</DL><p>
									<DT><A HREF="https://gist.github.com/drisspg/783616821043ab4594b9784f556c6714">Scaled MM API</A>
								</DL><p>
								<DT><H3 FOLDED>gemm-distributed</H3>
								<DL><p>
									<DT><A HREF="https://blog.shi-labs.com/distributed-gemm-88be6a481e2b">Distributed GEMM: CUTLASS-native Tensor Parallelism | SHI Labs</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/tree/main/examples/65_distributed_gemm">cutlass/examples/65_distributed_gemm at main · NVIDIA/cutlass</A>
								</DL><p>
								<DT><H3 FOLDED>mix-precission-gemm</H3>
								<DL><p>
									<DT><A HREF="https://github.com/Bruce-Lee-LY/cuda_hgemm">Bruce-Lee-LY/cuda_hgemm: Several optimization methods of half-precision general matrix multiplication (HGEMM) using tensor core with WMMA API and MMA PTX instruction.</A>
									<DT><A HREF="https://www.spatters.ca/two-stage-fp16-mma">Improving FP16/16 matmul accuracy with two-stage accumulation | spatters.ca</A>
								</DL><p>
								<DT><H3 FOLDED>gemm-benchmark</H3>
								<DL><p>
									<DT><A HREF="https://github.com/EleutherAI/cookbook/tree/main/benchmarks/sizing">cookbook/benchmarks/sizing at main · EleutherAI/cookbook</A>
									<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/main/benchmarks/benchmark_gemm.py">flash-attention/benchmarks/benchmark_gemm.py at main · Dao-AILab/flash-attention</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/pull/138555">[WIP] Prototype Triton kernel for torch.bmm(NJT, T) by cpuhrsch · Pull Request #138555 · pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>openai-gemm</H3>
								<DL><p>
									<DT><A HREF="https://github.com/openai/openai-gemm">openai/openai-gemm: Open single and half precision gemm implementations</A>
								</DL><p>
								<DT><H3 FOLDED>tinygrad-gemm</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/tree/c4fdb9c725924fd1bc8a89ca07a1f405953b4d54/extra/gemm">tinygrad/extra/gemm</A>
								</DL><p>
								<DT><H3 FOLDED>cutlass-gemm</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/5c447dd84f8ae0e1d48ff9a2eae26ce8c4958101/python/cutlass/op/gemm.py#L137">cutlass/python/cutlass/op/gemm.py</A>
									<DT><A HREF="https://research.colfax-intl.com/cutlass-tutorial-design-of-a-gemm-kernel/">CUTLASS Tutorial: Efficient GEMM kernel designs with Pipelining – Colfax Research</A>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/pipeline.md">cutlass/media/docs/pipeline.md at main · NVIDIA/cutlass</A>
									<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/tree/master/pipeline-gemm">cfx-article-src/pipeline-gemm at master · ColfaxResearch/cfx-article-src</A>
									<DT><A HREF="https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/">Deep Dive on Cutlass Ping-Pong GEMM Kernel | PyTorch</A>
								</DL><p>
								<DT><H3 FOLDED>Threadblock Rasterization</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#threadblock-rasterization">cutlass/media/docs/efficient_gemm.md at main · NVIDIA/cutlass</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Cache_prefetching">Cache prefetching - Wikipedia</A>
								</DL><p>
								<DT><H3 FOLDED>TiledCuda</H3>
								<DL><p>
									<DT><A HREF="https://github.com/TiledTensor/TiledCUDA/blob/master/src/kernels/gemm.cu">TiledCUDA/src/kernels/gemm.cu at master · TiledTensor/TiledCUDA</A>
									<DT><A HREF="https://github.com/TiledTensor/benchmarks">TiledTensor/benchmarks: Benchmark tests supporting the TiledCUDA library.</A>
								</DL><p>
								<DT><H3 FOLDED>gemm-hopper</H3>
								<DL><p>
									<DT><H3 FOLDED>gemm-hopper-benchmark</H3>
									<DL><p>
										<DT><A HREF="https://gist.github.com/Chillee/2ec89696db8b7ed1c24461159e325405">H100 peak matmul FLOPS</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/pull/144829/files#diff-90b954bddfc0db29441328610022aa85c98d3e2647875f7982a6b42acdf2b183">Added swizzle searching, disabled fp16 accum, and enabled ping-pong for cutlass by masnesral · Pull Request #144829 · pytorch/pytorch</A>
										<DT><A HREF="https://x.com/cHHillee/status/1884743684928962884">Horace He Hopper GEMM microbenchmarking cutlass</A>
									</DL><p>
									<DT><H3 FOLDED>DeepGEMM</H3>
									<DL><p>
										<DT><A HREF="https://github.com/deepseek-ai/DeepGEMM">deepseek-ai/DeepGEMM: DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling</A>
										<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72876/">Blackwell Programming for the Masses With OpenAI Triton | GTC 25 2025 | NVIDIA On-Demand</A>
									</DL><p>
									<DT><A HREF="https://github.com/pranjalssh/fast.cu">pranjalssh/fast.cu: Fastest kernels written from scratch</A>
									<DT><A HREF="https://github.com/weishengying/cute_gemm">weishengying/cute_gemm</A>
									<DT><A HREF="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">Outperforming cuBLAS on H100: a Worklog</A>
									<DT><A HREF="https://github.com/lcy-seso/hopper-gemm-101">lcy-seso/hopper-gemm-101</A>
								</DL><p>
								<DT><H3 FOLDED>gemm-blackwell</H3>
								<DL><p>
									<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72876/">Blackwell Programming for the Masses With OpenAI Triton | GTC 25 2025 | NVIDIA On-Demand</A>
									<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72720/">Programming Blackwell Tensor Cores with CUTLASS | GTC 25 2025 | NVIDIA On-Demand</A>
								</DL><p>
								<DT><A HREF="https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog">Outperforming cuBLAS on H100: a Worklog</A>
								<DT><A HREF="https://x.com/pranjalssh/status/1862657709994385694">Pranjal en X: "I implemented H100 cuda matmul kernel from scratch</A>
								<DT><A HREF="https://research.colfax-intl.com/cutlass-tutorial-design-of-a-gemm-kernel/">CUTLASS Tutorial: Efficient GEMM kernel designs with Pipelining – Colfax Research</A>
								<DT><A HREF="https://siboehm.com/articles/22/CUDA-MMM">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</A>
								<DT><A HREF="https://leimao.github.io/article/CUDA-Matrix-Multiplication-Optimization/">CUDA Matrix Multiplication Optimization - Lei Mao's Log Book</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/e1976daacc7b030ba672217eb5d96f5a663df4ab/media/docs/efficient_gemm.md">cutlass/media/docs/efficient_gemm.md</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/tree/c4fdb9c725924fd1bc8a89ca07a1f405953b4d54/extra/gemm">tinygrad/extra/gemm</A>
								<DT><A HREF="https://github.com/mobiusml/gemlite/">mobiusml/gemlite: Simple and fast low-bit matmul kernels in CUDA / Triton</A>
								<DT><A HREF="http://giantpandacv.com/project/CUDA/%E5%8F%AF%E8%83%BD%E6%98%AF%E8%AE%B2%E5%BE%97%E6%9C%80%E6%B8%85%E6%A5%9A%E7%9A%84WeightOnlyGEMM/">可能是讲得最清楚的WeightOnlyGEMM - GiantPandaCV</A>
								<DT><A HREF="https://engineering.fb.com/2018/11/07/ml-applications/fbgemm/">Open-sourcing FBGEMM for server-side inference - Engineering at Meta</A>
								<DT><A HREF="https://github.com/pytorch/FBGEMM">pytorch/FBGEMM: FB (Facebook) + GEMM (General Matrix-Matrix Multiplication) - https://code.fb.com/ml-applications/fbgemm/</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/3930f709ce01ada61b7d7a57935b5503ab72f1ed/media/docs/cute/0x_gemm_tutorial.md">cutlass/media/docs/cute/0x_gemm_tutorial.md</A>
								<DT><A HREF="https://arxiv.org/abs/1804.06826">[1804.06826] Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking</A>
								<DT><A HREF="https://scholar.google.com/citations?hl=en&user=8d0x03EAAAAJ&view_op=list_works&sortby=pubdate">‪Zhe Jia‬ - ‪Google Scholar‬</A>
								<DT><A HREF="https://arxiv.org/pdf/1912.03413.pdf">Dissecting the Graphcore IPUArchitecture via Microbenchmarking</A>
								<DT><A HREF="https://github.com/vdumoulin/conv_arithmetic">vdumoulin/conv_arithmetic: A technical report on convolution arithmetic in the context of deep learning</A>
								<DT><A HREF="https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/enums.py">FBGEMM/fbgemm_gpu/fbgemm_gpu/enums.py at main · pytorch/FBGEMM</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/native/cuda">pytorch/aten/src/ATen/native/cuda at main · pytorch/pytorch</A>
								<DT><A HREF="https://github.com/sjfeng1999/gpu-arch-microbenchmark">sjfeng1999/gpu-arch-microbenchmark: Dissecting NVIDIA GPU Architecture</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/extra/gemm/torch_gemm.py">tinygrad/extra/gemm/torch_gemm.py at master · tinygrad/tinygrad</A>
								<DT><A HREF="https://github.com/LaurentMazare/gemm">LaurentMazare/gemm</A>
								<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/">GEMM kernels Hopper</A>
								<DT><A HREF="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications?utm_source=post-email-title&publication_id=1781836&post_id=142904770&utm_campaign=email-post-title&isFreemail=true&r=1mqy6n&triedRedirect=true&utm_medium=email">What Shapes Do Matrix Multiplications Like? [medium]</A>
								<DT><A HREF="https://twitter.com/Si_Boehm/status/1610335205767933952">Iterative CUDA matrix multiply optimization (80% cuBLAS perf)</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/examples/00_basic_gemm/basic_gemm.cu">cutlass/examples/00_basic_gemm/basic_gemm.cu</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/66ef1df492f7bc9c8eeb01d7e14db01838e3f0bd/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py#L69">generate_kernels.py#L69</A>
								<DT><A HREF="https://github.com/microsoft/microxcaling">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
								<DT><A HREF="https://github.com/flame/how-to-optimize-gemm">flame/how-to-optimize-gemm</A>
								<DT><A HREF="https://github.com/flame/how-to-optimize-gemm/wiki#step-by-step-optimizations">Approach to Optimizing Matrix-Matrix Multiplication - Step-by-Step</A>
								<DT><A HREF="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS: Fast Linear Algebra in CUDA C++ | NVIDIA Technical Blog</A>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/5c447dd84f8ae0e1d48ff9a2eae26ce8c4958101/python/cutlass/op/gemm.py#L137">cutlass/python/cutlass/op/gemm.py</A>
								<DT><A HREF="https://gist.github.com/Chillee/abc38703f88fcb64683b6ccb0ae9d8ba">What Shapes Do Matrix Multiplications Like?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Sk35MKtCXfQ&t=4743s">matrix multiplication, a@b, cube</A>
								<DT><A HREF="https://pytorch.org/blog/inside-the-matrix/">Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch/ao/blob/cb3bd8c674f2123af232a0231b5e38ddafa756a8/torchao/dtypes/aqt.py#L526">ao/torchao/dtypes/aqt.py</A>
								<DT><A HREF="https://github.com/openai/openai-gemm">openai/openai-gemm: Open single and half precision gemm implementations</A>
								<DT><A HREF="https://github.com/Yinghan-Li/YHs_Sample/tree/master/cuda/gemm">YHs_Sample/cuda/gemm</A>
								<DT><A HREF="https://github.com/leimao/CUDA-GEMM-Optimization">leimao/CUDA-GEMM-Optimization: CUDA Matrix Multiplication Optimization</A>
								<DT><A HREF="https://github.com/tlc-pack/cutlass_fpA_intB_gemm">tlc-pack/cutlass_fpA_intB_gemm: A standalone GEMM kernel for fp16 activation and quantized weight, extracted from FasterTransformer</A>
								<DT><A HREF="https://github.com/sarah-ek/small-gemm/blob/main/src/lib.rs">small-gemm/src/lib.rs at main · sarah-ek/small-gemm</A>
								<DT><A HREF="https://github.com/LaurentMazare/gemm-metal">LaurentMazare/gemm-metal</A>
								<DT><A HREF="https://github.com/TiledTensor/TiledCUDA/blob/master/src/kernels/gemm.cu">TiledCUDA/src/kernels/gemm.cu at master · TiledTensor/TiledCUDA</A>
								<DT><A HREF="https://github.com/reed-lau/cute-gemm">reed-lau/cute-gemm</A>
								<DT><A HREF="https://github.com/weishengying/cute_gemm">weishengying/cute_gemm</A>
								<DT><A HREF="https://github.com/TiledTensor/benchmarks/blob/62e50b7597fe6fe1c4775bc9e83dde9df0b4f050/gemm/cutlass/gemm.py#L5">benchmarks/gemm/cutlass/gemm.py</A>
								<DT><A HREF="https://alexarmbr.github.io/2024/08/10/How-To-Write-A-Fast-Matrix-Multiplication-From-Scratch-With-Tensor-Cores.html">How To Write A Fast Matrix Multiplication From Scratch With Tensor Cores | Alex Armbruster</A>
								<DT><A HREF="https://github.com/ColfaxResearch/cfx-article-src/blob/master/pipeline-gemm/README.md">cfx-article-src/pipeline-gemm/README.md at master · ColfaxResearch/cfx-article-src</A>
								<DT><A HREF="https://github.com/LeiWang1999/Stream-k.tvm?tab=readme-ov-file">LeiWang1999/Stream-k.tvm</A>
								<DT><A HREF="https://github.com/pranjalssh/fast.cu">pranjalssh/fast.cu: Fastest kernels written from scratch</A>
								<DT><A HREF="https://www.youtube.com/watch?v=XAr_iVE8uUk">Matmul | Triton GPU Kernels 101 Lesson #6 - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>BMM</H3>
							<DL><p>
								<DT><A HREF="https://github.com/EleutherAI/cookbook/tree/main/benchmarks/sizing">cookbook/benchmarks/sizing at main · EleutherAI/cookbook</A>
								<DT><A HREF="https://github.com/EleutherAI/cookbook/blob/main/benchmarks/sizing/bmm_flops.py">cookbook/benchmarks/sizing/bmm_flops.py at main · EleutherAI/cookbook</A>
							</DL><p>
							<DT><H3 FOLDED>linalg: standard Linear Algebra</H3>
							<DL><p>
								<DT><H3 FOLDED>BLAS</H3>
								<DL><p>
									<DT><H3 FOLDED>colmajor-rowmajor</H3>
									<DL><p>
										<DT><A HREF="https://www.netlib.org/blas/blast-forum/blas-report.pdf">Conventional Storage</A>
										<DT><A HREF="https://leimao.github.io/blog/Row-Major-VS-Column-Major/">Row-Major VS Column-Major - Lei Mao's Log Book</A>
										<DT><A HREF="https://www.youtube.com/watch?v=USMnKuyXBFM">Developing Optimal CUDA Kernels on Hopper Tensor Cores NVIDIA On Demand - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>BLAS-level-1</H3>
									<DL><p>
										<DT><A HREF="https://www.netlib.org/blas/snrm2.f90">SNRM2</A>
										<DT><A HREF="https://netlib.org/lapack/explore-html/d6/d12/snrm2_8f90_source.html">LAPACK: BLAS/SRC/snrm2.f90 Source File</A>
									</DL><p>
									<DT><A HREF="https://www.netlib.org/blas/">BLAS (Basic Linear Algebra Subprograms)</A>
									<DT><A HREF="https://www.netlib.org/blas/blast-forum/blas-report.pdf">Basic Linear Algebra Subprograms Technial (BLAST)</A>
									<DT><A HREF="https://github.com/flame/blis/">flame/blis: BLAS-like Library Instantiation Software Framework (main)</A>
									<DT><A HREF="https://github.com/flame">flame</A>
									<DT><A HREF="https://github.com/flame/blislab?tab=readme-ov-file">flame/blislab: BLISlab: A Sandbox for Optimizing GEMM</A>
									<DT><A HREF="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS: Fast Linear Algebra in CUDA C++ | NVIDIA Technical Blog</A>
									<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html">Matrix Multiplication Background User's Guide - NVIDIA Docs</A>
									<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/blob/df4a7534860137e060e18d2ebf019906120ea204/src/fastertransformer/kernels/matrix_transpose_kernels.cu#L4">FasterTransformer/src/fastertransformer/kernels/matrix_transpose_kernels.cu</A>
									<DT><A HREF="https://github.com/microsoft/BitBLAS">microsoft/BitBLAS: BitBLAS is a library to support mixed-precision matrix multiplications, especially for quantized LLM deployment.</A>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/fd27f19e9231160939da8a3a23434b4ae8ce51ee/docs/tensor/ops.md?plain=1">tinygrad/docs/tensor/ops.md</A>
								</DL><p>
								<DT><A HREF="https://www.netlib.org/blas/blast-forum/blas-report.pdf">Basic Linear Algebra Subprograms Technial (BLAST)</A>
								<DT><A HREF="https://en.cppreference.com/w/cpp/numeric/linalg">Basic linear algebra algorithms (since C++26) - cppreference.com</A>
								<DT><A HREF="https://numpy.org/doc/stable/reference/routines.linalg.html">Linear algebra (numpy.linalg) — NumPy v1.26 Manual</A>
								<DT><A HREF="https://github.com/bytedance/byteir/blob/main/talks/c4ml23_poster.pdf">Linalg is All You Need to Optimize Attention</A>
								<DT><A HREF="https://www.openblas.net/">OpenBLAS : An optimized BLAS library</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cublas/">cuBLAS</A>
								<DT><A HREF="https://github.com/flame/blislab?tab=readme-ov-file">flame/blislab: BLISlab: A Sandbox for Optimizing GEMM</A>
								<DT><A HREF="https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/">CUTLASS: Fast Linear Algebra in CUDA C++ | NVIDIA Technical Blog</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/c4fdb9c725924fd1bc8a89ca07a1f405953b4d54/docs/tensor/ops.md?plain=1#L2">tinygrad/docs/tensor/ops.md</A>
							</DL><p>
							<DT><H3 FOLDED>kernels-theory</H3>
							<DL><p>
								<DT><H3 FOLDED>kernel-grid</H3>
								<DL><p>
									<DT><H3 FOLDED>dynamic grid</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=liKrhX2gm44">High Performance LLMs in Jax 2024</A>
									</DL><p>
									<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/grid_blockspec.html#pallas-grid">Grids and BlockSpecs — JAX documentation</A>
									<DT><A HREF="https://indico.fysik.su.se/event/6537/contributions/9386/attachments/4028/4627/2.CUDA-FromLoops2Grids-Markidis.pdf">CUDA – From Loops to Grids</A>
									<DT><A HREF="https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/">CUDA Pro Tip: Write Flexible Kernels with Grid-Stride Loops | NVIDIA Technical Blog</A>
									<DT><A HREF="https://erangad.medium.com/1d-2d-and-3d-thread-allocation-for-loops-in-cuda-e0f908537a52">1D, 2D and 3D thread allocation for loops in CUDA | by Eranga Dulshan | Medium</A>
									<DT><A HREF="https://www.youtube.com/watch?v=sRpWrTBOXCc">Kernel Grid | GPU Programming | Episode 2 - YouTube</A>
									<DT><A HREF="https://github.com/SzymonOzog/GPU_Programming">SzymonOzog/GPU_Programming</A>
								</DL><p>
								<DT><H3 FOLDED>softmax-kernel</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=IpHjDoW4ffw">How to write a fast Softmax kernel - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=p-6wUOXaVqs">Why Do Neural Networks Love the Softmax? - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>tiling</H3>
								<DL><p>
									<DT><A HREF="https://github.com/lcy-seso/VPTQ/pull/5">feat: use more warps to improve instruction latency hiding. by lcy-seso · Pull Request #5 · lcy-seso/VPTQ</A>
								</DL><p>
								<DT><A HREF="https://x.com/RajaXg/status/1812721241985610147">GPU Architecture Impact</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-05-12-quick-tk">ThunderKittens: A Simple Embedded DSL for AI kernels · Hazy Research</A>
								<DT><A HREF="https://twitter.com/karpathy/status/1789666350878601581">(1) Andrej Karpathy en X: "@Kartikayb77 I read this book and then I was surprised that I still understood so little of the kernels that started to appear as llm.c contributions, beating mine. It's a pretty good 101 intro. Learning CUDA is like that horse meme, all the learning resources you can find on the left, then... https://t.co/C0k1WZqkQM" / X</A>
								<DT><A HREF="https://siboehm.com/articles/22/CUDA-MMM">How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index">cuda-mode/triton-index: Cataloging released Triton kernels.</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index/blob/main/kernel_overview.md">triton-index/kernel_overview.md</A>
								<DT><A HREF="https://github.com/mgmalek/efficient_cross_entropy/blob/main/modules.py#L67">efficient_cross_entropy/modules.py at main · mgmalek/efficient_cross_entropy</A>
								<DT><A HREF="https://mp.weixin.qq.com/s/G_XvnB4CeEBWTLNefi0Riw">【BBuf的CUDA笔记】十二，LayerNorm/RMSNorm的重计算实现</A>
								<DT><A HREF="http://giantpandacv.com/project/CUDA/%E3%80%90BBuf%E7%9A%84CUDA%E7%AC%94%E8%AE%B0%E3%80%91%E4%B8%80%EF%BC%8C%E8%A7%A3%E6%9E%90OneFlow%20Element-Wise%20%E7%AE%97%E5%AD%90%E5%AE%9E%E7%8E%B0/">【BBuf的CUDA笔记】一，解析OneFlow Element-Wise 算子实现 - GiantPandaCV</A>
								<DT><A HREF="https://www.youtube.com/watch?v=qYqrfq452ig&list=LL&index=19&t=4582s">Hardcore CUDA Hackathon Talks at AGI House SF (Tri Dao)</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">GPUs Go Brrr · Hazy Research</A>
								<DT><A HREF="https://github.com/microsoft/microxcaling">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
								<DT><A HREF="https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/">CUTLASS Tutorial: Fast Matrix-Multiplication with WGMMA on NVIDIA® Hopper™ GPUs – Colfax Research</A>
								<DT><A HREF="https://github.com/lw/kernels/tree/main">lw/kernels</A>
								<DT><A HREF="https://github.com/triton-lang/kernels">triton-lang/kernels</A>
								<DT><A HREF="https://gist.github.com/nadavrot/5b35d44e8ba3dd718e595e40184d03f0">Efficient matrix multiplication</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/6c4ddd62601cfb6d839cdc50cae27881f7a2d106/tinygrad/codegen/kernel.py#L388">tinygrad/tinygrad/codegen/kernel.py at 6c4ddd62601cfb6d839cdc50cae27881f7a2d106 · tinygrad/tinygrad</A>
								<DT><A HREF="https://github.com/Tony-Tan/CUDA_Freshman">Tony-Tan/CUDA_Freshman</A>
								<DT><A HREF="https://github.com/KuangjuX/CUDAKernels">KuangjuX/CUDAKernels: 🎉My Collections of CUDA Kernels~</A>
								<DT><A HREF="https://research.colfax-intl.com/cutlass-tutorial-design-of-a-gemm-kernel/">CUTLASS Tutorial: Efficient GEMM kernel designs with Pipelining – Colfax Research</A>
								<DT><A HREF="https://www.youtube.com/watch?v=RW2-HtWaOS0">Accelerating the Future: Triton on Blackwell Architecture - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NZz5sczZ_30&t=16s">Triton Conference 2024: Morning Session - YouTube</A>
								<DT><A HREF="https://docs.nersc.gov/tools/performance/roofline/#:~:text=The%20Roofline%20performance%20model%20offers,software%20implementations%20and%20architecture%20designs.">Roofline Performance Model - NERSC Documentation</A>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html">Linear/Fully-Connected Layers User's Guide - NVIDIA Docs</A>
								<DT><A HREF="https://x.com/zealandic1/status/1856110612084076752">Extract fast Torch kernels (geohot)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=zy8ChVd_oTM">Flash Attention derived and coded from first principles with Triton (Python) - YouTube</A>
								<DT><A HREF="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">From Online Softmax to FlashAttention</A>
								<DT><A HREF="https://pytorch.org/blog/hadacore/?utm_content=318142989&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024">HadaCore: Tensor Core Accelerated Hadamard Transform Kernel | PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>Marlin</H3>
							<DL><p>
								<DT><H3 FOLDED>flashinfer</H3>
								<DL><p>
									<DT><A HREF="https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/layers/activation.py#L26">sglang/python/sglang/srt/layers/activation.py: gelu_and_mul</A>
									<DT><A HREF="https://github.com/yzh119/flashinfer-dev">yzh119/flashinfer-dev: FlashInfer: Kernel Library for LLM Serving</A>
									<DT><A HREF="https://x.com/cHHillee/status/1869839444179816588">(1) Horace He en X: "Although FlexAttention required custom kernel work, in our view the "main" contribution of FlexAttention is introducing an API that is both: 1. Flexible enough to support a wide variety of attention variants, and 2. easy to codegen into an efficient kernel. FlashInfer is one of" / X</A>
								</DL><p>
								<DT><A HREF="https://github.com/IST-DASLab/marlin">IST-DASLab/marlin: FP16xINT4 LLM inference kernel that can achieve near-ideal ~4x speedups up to medium batchsizes of 16-32 tokens.</A>
								<DT><A HREF="https://github.com/flashinfer-ai/flashinfer">flashinfer-ai/flashinfer: FlashInfer: Kernel Library for LLM Serving</A>
								<DT><A HREF="https://github.com/IST-DASLab/Sparse-Marlin">IST-DASLab/Sparse-Marlin</A>
							</DL><p>
							<DT><H3 FOLDED>kernel fusion</H3>
							<DL><p>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM">NVIDIA/TensorRT-LLM: TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines.</A>
								<DT><A HREF="https://nvidia.github.io/TensorRT-LLM/overview.html#about-tensorrt-llm">Overview — tensorrt_llm documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=m6BSREnQ84U">Fusing Kernels - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>microxcaling</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/microxcaling">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024">deepspeed-fp6</A>
							</DL><p>
							<DT><H3 FOLDED>XQA-kernel</H3>
							<DL><p>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/XQA-kernel.md">TensorRT-LLM/docs/source/blogs/XQA-kernel.md</A>
							</DL><p>
							<DT><H3 FOLDED>split-k-gemm</H3>
							<DL><p>
								<DT><A HREF="https://github.com/NVIDIA/cutlass/blob/main/examples/06_splitK_gemm/splitk_gemm.cu">cutlass/examples/06_splitK_gemm/splitk_gemm.cu</A>
								<DT><A HREF="https://arxiv.org/abs/2402.00025">[2402.00025] Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK work decomposition</A>
								<DT><A HREF="https://pytorch.org/blog/accelerating-moe-model//#30-work-decomposition---splitk">Accelerating MoE model inference with Locality-Aware Kernel Design | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch-labs/applied-ai/tree/main/kernels/triton/inference/col_major_moe_gemm">applied-ai/kernels/triton/inference/col_major_moe_gemm at main · pytorch-labs/applied-ai</A>
							</DL><p>
							<DT><H3 FOLDED>Flash-Attention</H3>
							<DL><p>
								<DT><H3 FOLDED>flashattention-theory</H3>
								<DL><p>
									<DT><A HREF="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">From Online Softmax to FlashAttention</A>
									<DT><A HREF="https://www.youtube.com/watch?v=zy8ChVd_oTM&list=LL&index=9&t=2s">Flash Attention derived and coded from first principles with Triton (Python) - YouTube</A>
									<DT><A HREF="https://kuterdinel.com/online-softmax.html">Online Softmax for dummies • Kuter Dinel's blog</A>
									<DT><A HREF="https://github.com/tspeterkim/flash-attention-minimal">tspeterkim/flash-attention-minimal: Flash Attention in ~100 lines of CUDA (forward pass only)</A>
									<DT><A HREF="https://research.colfax-intl.com/gpu-mode-cutlass-and-flashattention-3/">GPU Mode: CUTLASS and FlashAttention-3 – Colfax Research</A>
									<DT><A HREF="https://www.youtube.com/watch?v=zy8ChVd_oTM">Flash Attention derived and coded from first principles with Triton (Python) - YouTube</A>
									<DT><A HREF="https://colab.research.google.com/drive/1X-x6PCRydNY9LZBPLA0DZh3Tj2Dyz60M?usp=sharing">naive_flash.ipynb - Colab</A>
								</DL><p>
								<DT><H3 FOLDED>flash-attention-issues</H3>
								<DL><p>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/253">ModuleNotFoundError: No module named 'torch' · Issue #253 · HazyResearch/flash-attention</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/246">No Module Named 'torch' · Issue #246 · HazyResearch/flash-attention</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/131">installing dropout_layer_norm · Issue #131 · HazyResearch/flash-attention</A>
									<DT><A HREF="https://github.com/HazyResearch/flash-attention/issues/250">ModuleNotFoundError: No module named 'dropout_layer_norm' when trying to import flash_attn.ops.layer_norm · Issue #250 · HazyResearch/flash-attention</A>
								</DL><p>
								<DT><H3 FOLDED>flash-attention-mosaic</H3>
								<DL><p>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1812897008031617493">FA3 mosaic</A>
									<DT><A HREF="https://github.com/google/jax/blob/main/jax/experimental/mosaic/gpu/examples/flash_attention.py#L146-L354">jax/jax/experimental/mosaic/gpu/examples/flash_attention.py at main · google/jax</A>
								</DL><p>
								<DT><H3 FOLDED>FlashAttention-2</H3>
								<DL><p>
									<DT><A HREF="https://mp.weixin.qq.com/s/5K6yNj23NmNLcAQofHcT4Q">Flash Attention v2</A>
									<DT><A HREF="http://giantpandacv.com/project/CUDA/%E3%80%90BBuf%E7%9A%84CUDA%E7%AC%94%E8%AE%B0%E3%80%91%E5%8D%81%E4%BA%94%EF%BC%8COpenAI%20Triton%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%E4%B8%89%20FusedAttention/">FlashAttention V2 (chinese blog)</A>
									<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/main/kernels/cuda/tutorials/flash2.cu">applied-ai/kernels/cuda/tutorials/flash2.cu</A>
									<DT><A HREF="https://princeton-nlp.github.io/flash-atttention-2/">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning | Princeton NLP Group</A>
									<DT><A HREF="https://github.com/TiledTensor/TiledCUDA/blob/master/src/kernels/flash_attn.cu">TiledCUDA/src/kernels/flash_attn.cu at master · TiledTensor/TiledCUDA</A>
								</DL><p>
								<DT><H3 FOLDED>FlashAttention-3</H3>
								<DL><p>
									<DT><H3 FOLDED>FlashAttention-3-xformers</H3>
									<DL><p>
										<DT><A HREF="https://github.com/facebookresearch/xformers/blob/main/xformers/ops/fmha/flash3.py">xformers/xformers/ops/fmha/flash3.py at main · facebookresearch/xformers</A>
									</DL><p>
									<DT><H3 FOLDED>FlashAttention-3-flops</H3>
									<DL><p>
										<DT><A HREF="https://github.com/facebookresearch/xformers/commit/536363ecc38809cec9c6227e6ceef88255538fea">Provide FLOPs formula for FlashAttention3 · facebookresearch/xformers@536363e</A>
									</DL><p>
									<DT><A HREF="https://www.together.ai/blog/flashattention-3">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</A>
									<DT><A HREF="https://github.com/LaurentMazare/jax-flash-attn3">LaurentMazare/jax-flash-attn3: JAX bindings for the flash-attention3 kernels</A>
									<DT><A HREF="https://github.com/triton-lang/triton/blob/3ae95a858eac26088102075500e3860864432106/python/test/unit/hopper/test_flashattention.py#L294">triton/python/test/unit/hopper/test_flashattention.py: torch.autograd.Function</A>
									<DT><A HREF="https://github.com/Dao-AILab/flash-attention/pull/1112">Add how to import FA3 to documentation. by AdamLouly · Pull Request #1112 · Dao-AILab/flash-attention</A>
									<DT><A HREF="https://research.character.ai/optimizing-ai-inference-at-character-ai-part-deux/">Optimizing AI Inference at Character.AI (Part Deux)</A>
									<DT><A HREF="https://research.colfax-intl.com/gpu-mode-cutlass-and-flashattention-3/">GPU Mode: CUTLASS and FlashAttention-3 – Colfax Research</A>
									<DT><A HREF="https://gist.github.com/sophiawisdom/88b48f7146deb0d35c09506dd3a9c09e">invocation: TORCH_CUDA_ARCH_LIST=9.0a PYTORCH_NO_CUDA_MEMORY_CACHING=1 compute-sanitizer python3 test.py</A>
								</DL><p>
								<DT><H3 FOLDED>torch-scaled-dot-product-attention</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA) — PyTorch Tutorials 2.1.0+cu121 documentation</A>
									<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">torch.nn.functional.scaled_dot_product_attention — PyTorch 2.4 documentation</A>
									<DT><A HREF="https://github.com/thecharlieblake/lovely-llama/blob/08b8b068dbf2e8e9aea3328328f3910cd954a596/lovely_llama.py#L68">lovely_llama.py#L68</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/3b5e2689a1b8a3137e65b0c0bc8bfb96260c9bfe/torch/nn/functional.py">pytorch/torch/nn/functional.py at 3b5e2689a1b8a3137e65b0c0bc8bfb96260c9bfe · pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>FlashAttention-cuda</H3>
								<DL><p>
									<DT><A HREF="https://x.com/__tinygrad__/status/1910574691284300167">(1) the tiny corp en X: ".fuse() is now supported on Tensors. Automatic fusing puts one reduce in a kernel, but if you want more, you can fuse back to the nearest contiguous with fuse. Try it, it's fun! Single kernel softmax works, and with a few tweaks, this is flash attention ($500 bounty). https://t.co/9iI4csuBKI" / X</A>
									<DT><A HREF="https://gist.github.com/sophiawisdom/88b48f7146deb0d35c09506dd3a9c09e">invocation: TORCH_CUDA_ARCH_LIST=9.0a PYTORCH_NO_CUDA_MEMORY_CACHING=1 compute-sanitizer python3 test.py</A>
								</DL><p>
								<DT><H3 FOLDED>triton-attention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/sgl-project/sglang/blob/5e6c32657e384b023faf03d79e06f7727feedb7c/python/sglang/srt/layers/attention/triton_backend.py#L17">sglang/python/sglang/srt/layers/attention/triton_backend.py</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/layers/attention/flash_attn_triton.py">text-generation-inference/server/text_generation_server/layers/attention/flash_attn_triton.py at main · huggingface/text-generation-inference</A>
									<DT><A HREF="https://www.youtube.com/watch?v=6ap2QVWKFH0">Flash-attention forward pass | Triton GPU Kernels 101 Lesson #9 - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>cudnn-flashattention</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/cudnn-frontend/issues/52">What's the difference of flash attention implement between cudnn and Dao-AILab? · Issue #52 · NVIDIA/cudnn-frontend</A>
									<DT><A HREF="https://github.com/NVIDIA/cudnn-frontend/blob/1.0/release/samples/python/test_mhas.py#L431">cudnn-frontend/samples/python/test_mhas.py at 1.0/release · NVIDIA/cudnn-frontend</A>
								</DL><p>
								<DT><H3 FOLDED>flex-attention</H3>
								<DL><p>
									<DT><H3 FOLDED>flex-attention-benchmark</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ai-compiler-study/test_attn/blob/main/benchmark_attention.py#L211">test_attn/benchmark_attention.py at main · ai-compiler-study/test_attn</A>
									</DL><p>
									<DT><H3 FOLDED>flex-attention-examples</H3>
									<DL><p>
										<DT><A HREF="https://gist.github.com/Chillee/2e270fc5413dbbce58c779f8c4eac66c">flex_attention_tutorial.py</A>
									</DL><p>
									<DT><A HREF="https://gist.github.com/Chillee/2e270fc5413dbbce58c779f8c4eac66c">flex_attention_tutorial.py</A>
									<DT><A HREF="https://github.com/drisspg/attention-gym">drisspg/attention-gym: Helpful tools and examples for working with flex-attention</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/14108c1677e5a2f93a822a41cb579b572977d0d3/torch/_inductor/kernel/flex_decoding.py#L3">pytorch/torch/_inductor/kernel/flex_decoding.py at 14108c1677e5a2f93a822a41cb579b572977d0d3 · pytorch/pytorch</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/14108c1677e5a2f93a822a41cb579b572977d0d3/torch/_inductor/kernel/flex_attention.py">pytorch/torch/_inductor/kernel/flex_attention.py at 14108c1677e5a2f93a822a41cb579b572977d0d3 · pytorch/pytorch</A>
									<DT><A HREF="https://x.com/drisspg/status/1821949403302637667">option for specifying backend (e.g. cuDNN)</A>
									<DT><A HREF="https://x.com/cHHillee/status/1821253769147118004">Introducing FlexAttention, a new PyTorch API allowing for many attention variants to enjoy fused kernels in a few lines of PyTorch.</A>
									<DT><A HREF="https://pytorch.org/blog/flexattention/?utm_content=303215489&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024">FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention | PyTorch</A>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1833131508564914295">Longformer: On long sequence lengths, the speedup is even more significant. Added visualizations and repo link in the replies</A>
									<DT><A HREF="https://github.com/cccntu/t5-flex-attention">cccntu/t5-flex-attention: T5 model optimized with FlexAttention</A>
									<DT><A HREF="https://arxiv.org/abs/2412.05496">[2412.05496] Flex Attention: A Programming Model for Generating Optimized Attention Kernels</A>
									<DT><A HREF="https://pytorch.org/blog/flexattention/">FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention | PyTorch</A>
									<DT><A HREF="https://x.com/ye_combinator/status/1869826615146053904">(1) Zihao Ye en X: "@tri_dao @__tensorcore__ (3/n) Inspired by the awesome FlexAttention (from @cHHillee @JoyChew_d etc), we added JIT support in FlashInfer to allow user to customize their own attention variants, using user-defined Query/Logits/... Transform functors, and generate code to the CUDA/Cutlass template for LLM https://t.co/5oiBbLjLga" / X</A>
									<DT><A HREF="https://github.com/shreyansh26/Attention-Mask-Patterns">shreyansh26/Attention-Mask-Patterns: Using FlexAttention to compute attention with different masking patterns</A>
								</DL><p>
								<DT><H3 FOLDED>libflash</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tlc-pack/libflash_attn/tree/master">tlc-pack/libflash_attn: Standalone Flash Attention v2 kernel without libtorch dependency</A>
								</DL><p>
								<DT><H3 FOLDED>AttentionEngine</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/AttentionEngine">microsoft/AttentionEngine</A>
								</DL><p>
								<DT><H3 FOLDED>QuantumAttention</H3>
								<DL><p>
									<DT><H3 FOLDED>QuantumAttention-TK</H3>
									<DL><p>
										<DT><A HREF="https://github.com/WaveSpeedAI/QuantumAttention/blob/e7b647eb86596609ea68d372c9291a2e63e16d16/src/quantum_attn/tk/attention.py">QuantumAttention/src/quantum_attn/tk/attention.py at e7b647eb86596609ea68d372c9291a2e63e16d16 · WaveSpeedAI/QuantumAttention</A>
									</DL><p>
									<DT><A HREF="https://github.com/chengzeyi/QuantumAttention/commit/35abd08f042314e01bad999a0521faee24ca4d6a">Dev kernel (#1) · chengzeyi/QuantumAttention@35abd08</A>
									<DT><A HREF="https://github.com/chengzeyi/QuantumAttention/blob/main/src/quantum_attn/config.py">QuantumAttention/src/quantum_attn/config.py at main env vars patch</A>
									<DT><A HREF="https://github.com/search?q=repo%3Achengzeyi%2FQuantumAttention%20attention.force_eager_fallback&type=code">config patching (e.g. attention.force_eager_fallback)</A>
									<DT><A HREF="https://github.com/WaveSpeedAI/QuantumAttention/commit/e7b647eb86596609ea68d372c9291a2e63e16d16">Dev fp8 attn per head (#3) · WaveSpeedAI/QuantumAttention@e7b647e</A>
								</DL><p>
								<DT><H3 FOLDED>FlashMLA</H3>
								<DL><p>
									<DT><A HREF="https://github.com/deepseek-ai/FlashMLA">deepseek-ai/FlashMLA</A>
									<DT><A HREF="https://github.com/deepseek-ai/FlashMLA/commit/4da4dbd303eabbcdb5806051d82430ded46625d6">feat: add benchmark for flash_infer vs flash_mla · deepseek-ai/FlashMLA@4da4dbd</A>
									<DT><A HREF="https://github.com/deepseek-ai/open-infra-index?tab=readme-ov-file">deepseek-ai/open-infra-index</A>
								</DL><p>
								<DT><A HREF="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">From Online Softmax to FlashAttention</A>
								<DT><A HREF="https://github.com/NVIDIA/online-softmax">NVIDIA/online-softmax: Benchmark code for the "Online normalizer calculation for softmax" paper</A>
								<DT><A HREF="https://github.com/HazyResearch/flash-attention">HazyResearch/flash-attention: Fast and memory-efficient exact attention</A>
								<DT><A HREF="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA) — PyTorch Tutorials 2.1.0+cu121 documentation</A>
								<DT><A HREF="https://gist.github.com/Chillee/41baf11aac8036d25d637321c48dad20">You Could Have Invented Flash-Attention!</A>
								<DT><A HREF="https://github.com/tspeterkim/flash-attention-minimal">tspeterkim/flash-attention-minimal: Flash Attention in ~100 lines of CUDA (forward pass only)</A>
								<DT><A HREF="https://github.com/lucidrains/flash-cosine-sim-attention">lucidrains/flash-cosine-sim-attention: Implementation of fused cosine similarity attention in the same style as Flash Attention</A>
								<DT><A HREF="https://github.com/jundaf2/INT8-Flash-Attention-FMHA-Quantization">jundaf2/INT8-Flash-Attention-FMHA-Quantization</A>
								<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/main/src/turbomind/kernels/flash_attention/flash_attention.h">lmdeploy/src/turbomind/kernels/flash_attention/flash_attention.h at main · InternLM/lmdeploy</A>
								<DT><A HREF="https://mp.weixin.qq.com/s?__biz=Mzg2NjcwNjcxNQ==&mid=2247485453&idx=1&sn=beb642f06f3501bd235a8f42973e39fb&chksm=ce47fc79f930756f991d93f69cad36409e3dcb58e517f41f583d29609b360e9b46f6777a42b4&scene=21#wechat_redirect">图解大模型计算加速系列：Flash Attention V1，从硬件到计算逻辑</A>
								<DT><A HREF="https://arxiv.org/abs/1805.02867">[1805.02867] Online normalizer calculation for softmax</A>
								<DT><A HREF="https://gist.github.com/wkcn/65bbf94037222a38af78169f7f2c206b">test_flash_attn.py</A>
								<DT><A HREF="https://www.thonking.ai/p/pytorch-blog-flexattention-the-flexibility?utm_source=post-email-title&publication_id=1781836&post_id=147464468&utm_campaign=email-post-title&isFreemail=false&r=1tutvb&triedRedirect=true&utm_medium=email">[PyTorch Blog] FlexAttention: The Flexibility of PyTorch with the Performance of FlashAttention</A>
								<DT><A HREF="https://x.com/cHHillee/status/1821253769147118004">FlexAttention</A>
								<DT><A HREF="https://github.com/pytorch-labs/attention-gym/">pytorch-labs/attention-gym: Helpful tools and examples for working with flex-attention</A>
								<DT><A HREF="https://arxiv.org/pdf/1805.02867">Online normalizer calculation for softmax</A>
								<DT><A HREF="https://github.com/lancerts/Algo-ML-Kernels/blob/main/flash_attn/naive_flash.ipynb">Algo-ML-Kernels/flash_attn/naive_flash.ipynb at main · lancerts/Algo-ML-Kernels</A>
								<DT><A HREF="https://github.com/zhuzilin/ring-flash-attention">zhuzilin/ring-flash-attention: Ring attention implementation with flash attention</A>
								<DT><A HREF="https://github.com/KnowingNothing/MatmulTutorial/blob/main/examples/attention/triton/fused_linear_attention_complex.py">MatmulTutorial/examples/attention/triton/fused_linear_attention_complex.py</A>
								<DT><A HREF="https://github.com/weishengying/cutlass_flash_atten_fp8">weishengying/cutlass_flash_atten_fp8: 使用 cutlass 仓库在 ada 架构上实现 fp8 的 flash attention</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/7efcb5e0ed6403f08965b999626c5807680b46ed/server/text_generation_server/layers/attention/flash_attn_triton.py#L811">text-generation-inference/server/text_generation_server/layers/attention/flash_attn_triton.py at 7efcb5e0ed6403f08965b999626c5807680b46ed · huggingface/text-generation-inference</A>
								<DT><A HREF="https://gist.github.com/cloneofsimo/af610ff8aa11a3f57956e7d7f578409c">FlashAttention comparison</A>
								<DT><A HREF="https://github.com/INT-FlashAttention2024/INT-FlashAttention/blob/main/flash_atten_fp.py">INT-FlashAttention/flash_atten_fp.py at main · INT-FlashAttention2024/INT-FlashAttention</A>
								<DT><A HREF="https://github.com/KuangjuX/PyKernelCollection/blob/main/src/pytorch/flashattention/flashattention.py">PyKernelCollection/src/pytorch/flashattention/flashattention.py at main · KuangjuX/PyKernelCollection</A>
								<DT><A HREF="https://www.youtube.com/watch?v=zy8ChVd_oTM">Flash Attention derived and coded from first principles with Triton (Python) - YouTube</A>
								<DT><A HREF="https://x.com/papers_anon/status/1857271245349490945">Computes the cross-entropy loss without materializing the logis for all tokens into global memory</A>
								<DT><A HREF="https://x.com/ogawa_tter/status/1858431835560460684">OGAWA, Tadashi en X: "=&amp;gt; "CUTLASS and Flash Attention 3", Jay Shah, Colfax Research, GPU Mode, Nov 17, 2024 (1:49:15) https://t.co/dKVCsw7RGx https://t.co/Wl3LCuydFp BF16: up to 850 TFLOPS FP8: up to 1.3 PFLOPS FlashAttention-3, Jul 12 https://t.co/PttAXd4rzt CUTLASS, Jun 7 https://t.co/j32ViFdKt7 https://t.co/vkaYgvPXR1" / X</A>
								<DT><A HREF="https://github.com/FlagOpen/FlagAttention">FlagOpen/FlagAttention: A collection of memory efficient attention operators implemented in the Triton language.</A>
								<DT><A HREF="https://github.com/lcy-seso/DLFrameworkTest/blob/master/fused_mha/README.md">DLFrameworkTest/fused_mha/README.md at master · lcy-seso/DLFrameworkTest</A>
								<DT><A HREF="https://x.com/__tinygrad__/status/1910574691284300167?s=12">Tinygrad automatic searched softmax optimized kenrel with fusion</A>
							</DL><p>
							<DT><H3 FOLDED>inductor-kernels</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch/pytorch/tree/14108c1677e5a2f93a822a41cb579b572977d0d3/torch/_inductor/kernel">pytorch/torch/_inductor/kernel</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/14108c1677e5a2f93a822a41cb579b572977d0d3/torch/_inductor/kernel/flex_attention.py">pytorch/torch/_inductor/kernel/flex_attention.py at 14108c1677e5a2f93a822a41cb579b572977d0d3 · pytorch/pytorch</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/14108c1677e5a2f93a822a41cb579b572977d0d3/torch/_inductor/kernel/mm.py#L585">pytorch/torch/_inductor/kernel/mm.py at 14108c1677e5a2f93a822a41cb579b572977d0d3 · pytorch/pytorch</A>
							</DL><p>
							<DT><H3 FOLDED>deterministic-attention</H3>
							<DL><p>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-894/developer-guide/index.html">Developer Guide :: NVIDIA cuDNN Documentation</A>
								<DT><A HREF="https://github.com/NVIDIA/cudnn-frontend/blob/de355c7094af70467f2b264f531ab5c5f4401c42/test/python_fe/test_mhas.py#L1198">test_mhas.py#L1198: use_deterministic_algorithm=is_deterministic</A>
							</DL><p>
							<DT><H3 FOLDED>FlashFFTConv</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/realDanFu/status/1724127071902011611">FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores</A>
							</DL><p>
							<DT><H3 FOLDED>kernels-dit</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ai-compiler-study/sd3-triton-kernels">ai-compiler-study/sd3-triton-kernels: Triton kernels for Stable Diffusion 3</A>
							</DL><p>
							<DT><H3 FOLDED>kernels-quantization</H3>
							<DL><p>
								<DT><H3 FOLDED>gptq</H3>
								<DL><p>
									<DT><A HREF="https://github.com/IST-DASLab/gptq">IST-DASLab/gptq: Code for the ICLR 2023 paper "GPTQ: Accurate Post-training Quantization of Generative Pretrained Transformers".</A>
								</DL><p>
								<DT><H3 FOLDED>awq</H3>
								<DL><p>
									<DT><A HREF="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq: [MLSys 2024 Best Paper Award] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</A>
								</DL><p>
								<DT><A HREF="https://github.com/hahnyuan/RPTQ4LLM">hahnyuan/RPTQ4LLM: Reorder-based post-training quantization for large language model</A>
								<DT><A HREF="https://github.com/IST-DASLab/QUIK">IST-DASLab/QUIK: Repository for the QUIK project, enabling the use of 4bit kernels for generative inference</A>
							</DL><p>
							<DT><H3 FOLDED>kernels-sparsity</H3>
							<DL><p>
								<DT><H3 FOLDED>dynamic sparse training</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/blog/accelerating-neural-network-training/?utm_content=297933946&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity | PyTorch</A>
									<DT><A HREF="https://arxiv.org/pdf/1903.05662">UNDERSTANDING STRAIGHT-THROUGH ESTIMATOR IN TRAINING ACTIVATION QUANTIZED NEURAL NETS</A>
									<DT><A HREF="https://arxiv.org/pdf/2310.06927">Sparse Fine-tuning for Inference Acceleration of Large Language Models</A>
								</DL><p>
								<DT><H3 FOLDED>V:N:M</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/pdf/2310.02065">VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores</A>
								</DL><p>
								<DT><A HREF="https://pytorch.org/blog/accelerating-neural-network-training/?utm_content=297933946&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch/ao/tree/main/torchao/sparsity/training#benchmarking">ao/torchao/sparsity/training at main · pytorch/ao</A>
								<DT><A HREF="https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/">Structured Sparsity in the NVIDIA Ampere Architecture and Applications in Search Engines | NVIDIA Technical Blog</A>
								<DT><A HREF="https://docs.nvidia.com/cuda/cusparselt/index.html">cuSPARSELt: A High-Performance CUDA Library for Sparse Matrix-Matrix Multiplication — NVIDIA cuSPARSELt 0.6.1 documentation</A>
								<DT><A HREF="https://arxiv.org/pdf/2104.08378">Accelerating Sparse Deep Neural Networks</A>
								<DT><A HREF="https://pytorch.org/tutorials/advanced/semi_structured_sparse.html?highlight=beta">(beta) Accelerating BERT with semi-structured (2:4) sparsity — PyTorch Tutorials 2.3.0+cu121 documentation</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/pull/122350">[sparse] Add fast semi-structured spasification kernels by jcaip · Pull Request #122350 · pytorch/pytorch</A>
								<DT><A HREF="https://github.com/pytorch/ao/commit/d97ae74f46bb92bc26d01b2b5aed11197c275dd9">training acceleration via runtime semi-structured sparsity (#184) · pytorch/ao@d97ae74</A>
								<DT><A HREF="https://github.com/IST-DASLab/Sparse-Marlin">IST-DASLab/Sparse-Marlin</A>
								<DT><A HREF="https://github.com/AlibabaResearch/flash-llm">AlibabaResearch/flash-llm: Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity</A>
							</DL><p>
							<DT><H3 FOLDED>Warp Divergence</H3>
							<DL><p>
								<DT><A HREF="https://www.reddit.com/r/CUDA/comments/gkpjxe/what_is_warp_divergence/#">What is Warp Divergence ? : r/CUDA</A>
								<DT><A HREF="https://pytorch.org/blog/accelerating-neural-network-training/?utm_content=297933946&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity | PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>arithmetic-intensity</H3>
							<DL><p>
								<DT><A HREF="https://zhuanlan.zhihu.com/p/638468472">LLM（十七）：从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能 - 知乎</A>
								<DT><A HREF="https://www.youtube.com/watch?v=qYqrfq452ig&t=4548s">Hardcore CUDA Hackathon Talks at AGI House SF - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>kernels-tma</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/main/kernels/triton/inference/fp8/tma_gemm.py">applied-ai/kernels/triton/inference/fp8/tma_gemm.py at main · pytorch-labs/applied-ai</A>
							</DL><p>
							<DT><H3 FOLDED>kernel-benchmarking</H3>
							<DL><p>
								<DT><A HREF="https://github.com/oadirt/quant-matmul">oadirt/quant-matmul</A>
								<DT><A HREF="https://github.com/oadirt/quant-matmul/blob/main/tests/test_quant_matmul.py">quant-matmul/tests/test_quant_matmul.py at main · oadirt/quant-matmul</A>
								<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/main/tests/test_flash_attn.py">flash-attention/tests/test_flash_attn.py at main · Dao-AILab/flash-attention</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/7eb2a99585b683e5486bfbf2e78c8165f08c392b/benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py#L92">pytorch/benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py</A>
								<DT><A HREF="https://github.com/drisspg/driss_torch/blob/main/test/test_amax.py">driss_torch/test/test_amax.py at main · drisspg/driss_torch</A>
								<DT><A HREF="https://github.com/tgale96/grouped_gemm/pull/14#issuecomment-2211362572">Use CUTLASS for both `trans_a` and `trans_b` on Ampere by dfyz · Pull Request #14 · tgale96/grouped_gemm</A>
								<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN/blob/main/benchmark/benchmark_dynamic_quant.py">FLASHNN/benchmark/benchmark_dynamic_quant.py at main · AlibabaPAI/FLASHNN</A>
								<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN/blob/main/benchmark/benchmark_flash_attn.py">FLASHNN/benchmark/benchmark_flash_attn.py at main · AlibabaPAI/FLASHNN</A>
								<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN/blob/main/benchmark/benchmark_gemm_a8w8.py">FLASHNN/benchmark/benchmark_gemm_a8w8.py at main · AlibabaPAI/FLASHNN</A>
								<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN/blob/main/benchmark/benchmark_paged_attn.py">FLASHNN/benchmark/benchmark_paged_attn.py at main · AlibabaPAI/FLASHNN</A>
								<DT><A HREF="https://github.com/FlagOpen/FlagGems/blob/master/benchmark/performance_utils.py#L15">FlagGems/benchmark/performance_utils.py at master · FlagOpen/FlagGems</A>
								<DT><A HREF="https://github.com/triton-lang/kernels/tree/main/benchmarking">kernels/benchmarking at main · triton-lang/kernels</A>
								<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/blob/699857ab8940ff704f6c087f971dfde6f7172af9/demos/based_demo/benchmark_kernel.py#L48">ThunderKittens/demos/based_demo/benchmark_kernel.py at 699857ab8940ff704f6c087f971dfde6f7172af9 · HazyResearch/ThunderKittens</A>
								<DT><A HREF="https://github.com/drisspg/transformer_nuggets/blob/83f754eb670b73aa789a924b6b9fab67784ca28f/transformer_nuggets/utils/benchmark.py#L60">benchmark_torch_function_in_microseconds</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/f3c54ccf8f6139807f4623037c0174964a286652/torch/_inductor/utils.py#L121">do_bench_using_profiling</A>
								<DT><A HREF="https://docs.nersc.gov/tools/performance/roofline/#:~:text=The%20Roofline%20performance%20model%20offers,software%20implementations%20and%20architecture%20designs.">Roofline Performance Model - NERSC Documentation</A>
								<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/main/kernels/needs_perf_help/fp8_gemm_bench.py">applied-ai/kernels/needs_perf_help/fp8_gemm_bench.py at main · pytorch-labs/applied-ai</A>
							</DL><p>
							<DT><H3 FOLDED>kernel-tuner</H3>
							<DL><p>
								<DT><A HREF="https://github.com/KernelTuner/kernel_tuner">KernelTuner/kernel_tuner: Kernel Tuner</A>
								<DT><A HREF="https://github.com/KernelTuner/kernel_tuner_tutorial/blob/master/hands-on/cuda/03_Kernel_Tuner_Advanced.ipynb">kernel_tuner_tutorial/hands-on/cuda/03_Kernel_Tuner_Advanced.ipynb at master · KernelTuner/kernel_tuner_tutorial</A>
							</DL><p>
							<DT><H3 FOLDED>scan</H3>
							<DL><p>
								<DT><A HREF="https://gist.github.com/Chillee/e3089e7a11419c6b85f68de170e0ba0c">Higher Order Kernel - associative scan</A>
							</DL><p>
							<DT><H3 FOLDED>ThunderKittens</H3>
							<DL><p>
								<DT><H3 FOLDED>ThunderKittens-lectures</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=xcpEl0cGCC4">CUDA + ThunderKittens, but increasingly drunk. - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=IAwLzkldxUk">ThunderKittens goes live: AMA and library walkthrough - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>ThunderKittens-benchmark</H3>
								<DL><p>
									<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/blob/699857ab8940ff704f6c087f971dfde6f7172af9/demos/based_demo/README.md">ThunderKittens/demos/based_demo/README.md at 699857ab8940ff704f6c087f971dfde6f7172af9 · HazyResearch/ThunderKittens</A>
									<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/blob/699857ab8940ff704f6c087f971dfde6f7172af9/demos/based_demo/benchmark_kernel.py">ThunderKittens/demos/based_demo/benchmark_kernel.py at 699857ab8940ff704f6c087f971dfde6f7172af9 · HazyResearch/ThunderKittens</A>
								</DL><p>
								<DT><H3 FOLDED>ThunderKittens-cubinding</H3>
								<DL><p>
									<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/blob/main/kernels/example_bind/example_bind.cu">ThunderKittens/kernels/example_bind/example_bind.cu at main · HazyResearch/ThunderKittens</A>
								</DL><p>
								<DT><H3 FOLDED>ThunderKittens-blackwell</H3>
								<DL><p>
									<DT><H3 FOLDED>ThunderKittens-blackwell-attention</H3>
									<DL><p>
										<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/tree/blackwell/kernels/attn/b200">ThunderKittens/kernels/attn/b200 at blackwell · HazyResearch/ThunderKittens</A>
									</DL><p>
									<DT><H3 FOLDED>ThunderKittens-blackwell-gemm</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://x.com/bfspector/status/1883051606369001873">(1) Benjamin F Spector en X: "We got early access to some of the very first Nvidia B200’s. We share initial benchmark results and wrote the fastest (public) attention kernel with 925+ BF16 TFLOPs: Since the PTX instruction set released yesterday, @aaryan04 and I have been hard at work at @HazyResearch https://t.co/ZhlsBMvRl1" / X</A>
									<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/tree/blackwell">HazyResearch/ThunderKittens at blackwell</A>
									<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/tree/blackwell_tuning">HazyResearch/ThunderKittens at blackwell_tuning</A>
									<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/tree/blackwell_shapes">HazyResearch/ThunderKittens at blackwell_shapes</A>
									<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/tree/blackwell_fp4">HazyResearch/ThunderKittens at blackwell_fp4</A>
									<DT><A HREF="https://www.together.ai/blog/nvidia-hgx-b200-with-together-kernel-collection">Together AI Achieves 90% Faster BF16 Training with NVIDIA Blackwell Platform and Together Kernel Collection</A>
									<DT><A HREF="https://www.together.ai/blog/thunderkittens-nvidia-blackwell-gpus#:~:text=There%E2%80%99s%20also%20a%20new%20layer,to%20227KB%20of%20shared%20memory">ThunderKittens Now Optimized for NVIDIA Blackwell GPUs</A>
									<DT><A HREF="https://hazyresearch.stanford.edu/blog/2025-03-15-tk-blackwell">ThunderKittens Now on Blackwells! · Hazy Research</A>
								</DL><p>
								<DT><H3 FOLDED>TK-fp8</H3>
								<DL><p>
									<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/commit/1719fb72641b965d26155a0515d413b007f9dc72">[wip] fp8 bindings · HazyResearch/ThunderKittens@1719fb7</A>
								</DL><p>
								<DT><H3 FOLDED>ThunderKittens-MLA</H3>
								<DL><p>
									<DT><A HREF="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla">ThunderMLA: FlashMLA, Faster and Fused-er! · Hazy Research</A>
								</DL><p>
								<DT><A HREF="https://arxiv.org/abs/2410.20399">[2410.20399] ThunderKittens: Simple, Fast, and Adorable AI Kernels</A>
								<DT><A HREF="https://twitter.com/bfspector/status/1789749117104894179">(1) Benjamin F Spector en X: "(1/7) Happy mother’s day! We think what the mothers of America really want is a Flash Attention implementation that’s just 100 lines of code and 30% faster, and we’re happy to provide. We're excited to introduce ThunderKittens (TK), a simple DSL embedded within CUDA that makes... https://t.co/7Nupt8B4hq" / X</A>
								<DT><A HREF="https://github.com/HazyResearch/ThunderKittens">HazyResearch/ThunderKittens: Tile primitives for speedy kernels</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-05-12-quick-tk">ThunderKittens: A Simple Embedded DSL for AI kernels · Hazy Research</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-05-12-tk">GPUs Go Brrr · Hazy Research</A>
								<DT><A HREF="https://github.com/Narsil/zandle/tree/main/src">zandle/src at main · Narsil/zandle</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-10-29-tk2">Easier, Better, Faster, Cuter · Hazy Research</A>
								<DT><A HREF="https://hazyresearch.stanford.edu/blog/2024-11-27-tk-fp8">ThunderKittens: Bringing fp8 to theaters near you · Hazy Research</A>
								<DT><A HREF="https://github.com/tile-ai/tilelang">tile-ai/tilelang: Domain-specific language designed to streamline the development of high-performance GPU/CPU kernels</A>
								<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/pull/28/files">[feat] add simple half gemm example by luliyucoordinate · Pull Request #28 · HazyResearch/ThunderKittens</A>
								<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/tree/blackwell">HazyResearch/ThunderKittens at blackwell</A>
								<DT><A HREF="https://github.com/fla-org/ThunderKittens">fla-org/ThunderKittens: Tile primitives for speedy kernels</A>
							</DL><p>
							<DT><H3 FOLDED>TiledCuda</H3>
							<DL><p>
								<DT><H3 FOLDED>TileFusion</H3>
								<DL><p>
									<DT><H3 FOLDED>tilefusion-shared-memory</H3>
									<DL><p>
										<DT><A HREF="https://github.com/lcy-seso/vq-experiments/blob/3aef9f38da6dd19f1c9b3aa03410724a2457e0ac/shared_memory_calculator/README.md">vq-experiments/shared_memory_calculator/README.md at 3aef9f38da6dd19f1c9b3aa03410724a2457e0ac · lcy-seso/vq-experiments</A>
										<DT><A HREF="https://github.com/lcy-seso/vq-experiments/pull/8/files#diff-d90aac6207a1bb13e0c2b012acae231b944bf643e570f1afc35f33f6f542ad2f">feat: utility function for calculating shared memory usage. by lcy-seso · Pull Request #8 · lcy-seso/vq-experiments</A>
									</DL><p>
									<DT><A HREF="https://github.com/microsoft/TileFusion">microsoft/TileFusion</A>
									<DT><A HREF="https://github.com/lcy-seso/vq-experiments/blob/3aef9f38da6dd19f1c9b3aa03410724a2457e0ac/bench_quant_gemv/ncu.sh">vq-experiments/bench_quant_gemv/ncu.sh ncu profiling script example</A>
									<DT><A HREF="https://github.com/VPTQ/benchmarks">VPTQ/benchmarks: Benchmark tests for VPTQ.</A>
								</DL><p>
								<DT><A HREF="https://github.com/TiledTensor/TiledCUDA/tree/master">TiledTensor/TiledCUDA: TiledCUDA is a highly efficient kernel template library designed to elevate CUDA C’s level of abstraction for processing tiles.</A>
								<DT><A HREF="https://github.com/microsoft/TileFusion">microsoft/TileFusion</A>
							</DL><p>
							<DT><H3 FOLDED>tilelang</H3>
							<DL><p>
								<DT><A HREF="https://github.com/tile-ai/tilelang">tile-ai/tilelang: Domain-specific language designed to streamline the development of high-performance GPU/CPU/Accelerators kernels</A>
								<DT><A HREF="https://github.com/LeiWang1999/tilelang/blob/6574ac58fd5b9ee964a9479d61bc56af5235068a/tilelang/jit/adapter/libgen.py">tilelang/tilelang/jit/adapter/libgen.py CUTLASS_INCLUDE_DIR</A>
								<DT><A HREF="https://x.com/Lei_Wang_1999/status/1907752700952908228">tilelang analyzer</A>
								<DT><A HREF="https://github.com/tile-ai/tilelang/tree/main/examples/analyze">TVM IR Performance Analyzer</A>
							</DL><p>
							<DT><H3 FOLDED>mirage</H3>
							<DL><p>
								<DT><A HREF="https://mirage-project.readthedocs.io/en/latest/tutorials/index.html">Tutorials — Mirage documentation</A>
								<DT><A HREF="https://mirage-project.readthedocs.io/en/latest/index.html">Welcome to Mirage’s documentation! — Mirage documentation</A>
								<DT><A HREF="https://arxiv.org/abs/2405.05751">[2405.05751] A Multi-Level Superoptimizer for Tensor Programs</A>
							</DL><p>
							<DT><H3 FOLDED>Tritonbench</H3>
							<DL><p>
								<DT><H3 FOLDED>generative-recommenders</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookresearch/generative-recommenders">facebookresearch/generative-recommenders: Repository hosting code used to reproduce results in "Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations" (https://arxiv.org/abs/2402.17152).</A>
									<DT><A HREF="https://github.com/facebookresearch/generative-recommenders/issues/115">How to install hammer package used in triton_ragged_hstu_attention? · Issue #115 · facebookresearch/generative-recommenders</A>
								</DL><p>
								<DT><H3 FOLDED>tritonbench-proton</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch-labs/tritonbench/commit/794bc77d2c960c6317091c5dfa4b5d701a789d30">Support flops metric in proton profiling (#111) · pytorch-labs/tritonbench@794bc77</A>
									<DT><A HREF="https://github.com/pytorch-labs/tritonbench/issues/41">$ python run.py --op flash_attention --batch 1 --n-heads 24 --seq-len 4608 --d-head 128 --only cudnn,sdpa,flash_v3 --metrics proton --native-sdpa --pt2-sdpa</A>
								</DL><p>
								<DT><H3 FOLDED>nvtx</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch-labs/tritonbench/commit/e985049aa6e80429845a2b560252a53b7cf64937">Use nvtx.range_start (#116) · pytorch-labs/tritonbench@e985049</A>
									<DT><A HREF="https://gist.github.com/mcarilli/376821aa1a7182dfcf59928a7cde3223">Favorite nsight systems profiling commands for Pytorch scripts</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/using-nsight-systems-to-profile-gpu-workload/59">Using Nsight Systems to profile GPU workload - hardware-backends / NVIDIA CUDA - PyTorch Developer Mailing List</A>
								</DL><p>
								<DT><A HREF="https://github.com/pytorch-labs/tritonbench/tree/main">pytorch-labs/tritonbench: Tritonbench is a collection of PyTorch custom operators with example inputs to measure their performance.</A>
								<DT><A HREF="https://github.com/pytorch/fbgemm">pytorch/FBGEMM: FB (Facebook) + GEMM (General Matrix-Matrix Multiplication) - https://code.fb.com/ml-applications/fbgemm/</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/issues/136168">OperatorBench Plan · Issue #136168 · pytorch/pytorch</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/397a2e6eb6cff0d956b22c46f63e109694695391/test/test_speed_v_torch.py#L4">tinygrad/test/test_speed_v_torch.py at 397a2e6eb6cff0d956b22c46f63e109694695391 · tinygrad/tinygrad</A>
							</DL><p>
							<DT><A HREF="https://github.com/cuda-mode/triton-index">cuda-mode/triton-index: Cataloging released Triton kernels.</A>
							<DT><A HREF="https://openai.com/research/block-sparse-gpu-kernels">OpenAI: Block-sparse GPU kernels</A>
							<DT><A HREF="https://github.com/IST-DASLab/marlin">IST-DASLab/marlin: FP16xINT4 LLM inference kernel that can achieve near-ideal ~4x speedups up to medium batchsizes of 16-32 tokens.</A>
							<DT><A HREF="https://hta.readthedocs.io/en/latest/source/features/kernel_breakdown.html">Kernel Breakdown — Holistic Trace Analysis 0.2.0 documentation</A>
							<DT><A HREF="https://github.com/efeslab/Atom">efeslab/Atom: [MLSys'24] Atom: Low-bit Quantization for Efficient and Accurate LLM Serving</A>
							<DT><A HREF="https://github.com/huggingface/candle-paged-attention">huggingface/candle-paged-attention</A>
							<DT><A HREF="https://github.com/microsoft/onnxscript">microsoft/onnxscript: ONNX Script enables developers to naturally author ONNX functions and models using a subset of Python.</A>
							<DT><A HREF="https://github.com/flashinfer-ai/flashinfer">flashinfer-ai/flashinfer: FlashInfer: Kernel Library for LLM Serving</A>
							<DT><A HREF="https://github.com/microsoft/onnxscript">microsoft/onnxscript: programm ONNX functions and models using a subset of Python.</A>
							<DT><A HREF="https://ai.meta.com/research/publications/accelerating-a-triton-fused-kernel-for-w4a16-quantized-inference-with-splitk-work-decomposition/">Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK Work Decomposition</A>
							<DT><A HREF="https://github.com/turboderp/exllama">turboderp/exllama: A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.</A>
							<DT><A HREF="https://github.com/turboderp/exllamav2">turboderp/exllamav2: LLMs locally on modern consumer-class GPUs</A>
							<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/de17730a993b1d2cce4fd09e3654b5f79fd23c96/kernels/triton/inference/gptq/a100_qlinear.py#L109">applied-ai: Triton GPTQ a100_qlinear.py (print perf stats)</A>
							<DT><A HREF="https://github.com/BearNinja123/channels-last-groupnorm">BearNinja123/channels-last-groupnorm: A CUDA kernel for NHWC GroupNorm for PyTorch</A>
							<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/66ef1df492f7bc9c8eeb01d7e14db01838e3f0bd/cpp/tensorrt_llm/kernels">TensorRT-LLM/cpp/tensorrt_llm/kernels</A>
							<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/66ef1df492f7bc9c8eeb01d7e14db01838e3f0bd/cpp/tensorrt_llm/kernels/cutlass_kernels">TensorRT-LLM/cpp/tensorrt_llm/kernels/cutlass_kernels</A>
							<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
							<DT><A HREF="https://pytorch.org/blog/accelerating-moe-model/#30-work-decomposition---splitk">Accelerating MoE model inference with Locality-Aware Kernel Design | PyTorch</A>
							<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/main/kernels/triton/inference/gptq/splitk_dequant_gemm.py">applied-ai/kernels/triton/inference/gptq/splitk_dequant_gemm.py at main · pytorch-labs/applied-ai</A>
							<DT><A HREF="https://github.com/vedantroy/gpu_kernels/">vedantroy/gpu_kernels</A>
							<DT><A HREF="https://github.com/BBuf/how-to-optim-algorithm-in-cuda">BBuf/how-to-optim-algorithm-in-cuda: how to optimize some algorithm in cuda.</A>
							<DT><A HREF="https://github.com/mgmalek/efficient_cross_entropy">mgmalek/efficient_cross_entropy</A>
							<DT><A HREF="https://github.com/openai/openai-gemm">openai/openai-gemm: Open single and half precision gemm implementations</A>
							<DT><A HREF="https://github.com/RulinShao/LightSeq/blob/main/lightseq/lightseq_async_attn.py#L436">LightSeq/lightseq/lightseq_async_attn.py at main · RulinShao/LightSeq</A>
							<DT><A HREF="https://github.com/pytorch/ao/blob/main/torchao/kernel/intmm_triton.py">ao/torchao/kernel/intmm_triton.py at main · pytorch/ao</A>
							<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN">AlibabaPAI/FLASHNN</A>
							<DT><A HREF="https://github.com/FlagOpen/FlagGems">FlagOpen/FlagGems: FlagGems is an operator library for large language models implemented in Triton Language.</A>
							<DT><A HREF="https://github.com/microsoft/DeepSpeed-Kernels/tree/main/dskernels/ft_gemm/gemm_variants">DeepSpeed-Kernels/dskernels/ft_gemm/gemm_variants at main · microsoft/DeepSpeed-Kernels</A>
							<DT><A HREF="https://gist.github.com/msaroufim/087c2a358c505e287a926e6a27b3e3b0">Project Popcorn: Generate SOTA kernels with LLMs in public</A>
							<DT><A HREF="https://github.com/ai-compiler-study/triton-kernels/blob/main/scripts/gpu_properties.cu">triton-kernels/scripts/gpu_properties.cu at gpu_properties</A>
						</DL><p>
						<DT><H3 FOLDED>computational graph explorer</H3>
						<DL><p>
							<DT><A HREF="https://research.google/blog/model-explorer/">Model Explorer: Graph visualization for large model development</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/45e7400e3c86bc147cd1ab7b1b068068bdfe0cb2/docs-legacy/env_vars.md">tinygrad: env var GRAPH create a graph of all operations</A>
							<DT><A HREF="https://github.com/ezyang/torchdbg">ezyang/torchdbg: PyTorch centric eager mode debugger</A>
							<DT><A HREF="https://bbycroft.net/llm">LLM Visualization</A>
							<DT><A HREF="https://pytorch.org/blog/inside-the-matrix/">Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond | PyTorch</A>
							<DT><A HREF="https://ai.google.dev/edge/model-explorer#two_ways_to_use_model_explorer">Model Explorer  |  Edge  |  Google for Developers</A>
							<DT><A HREF="https://github.com/google-ai-edge/model-explorer">google-ai-edge/model-explorer: A modern model graph visualizer and debugger</A>
							<DT><A HREF="https://ieeexplore.ieee.org/document/8019861">Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow | IEEE Journals &amp; Magazine | IEEE Xplore</A>
							<DT><A HREF="https://idl.cs.washington.edu/files/2018-TensorFlowGraph-VAST.pdf">Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow</A>
							<DT><A HREF="https://github.com/google-ai-edge/model-explorer/tree/main">google-ai-edge/model-explorer: A modern model graph visualizer and debugger</A>
						</DL><p>
						<DT><H3 FOLDED>PyTorch</H3>
						<DL><p>
							<DT><H3 FOLDED>torch-installation</H3>
							<DL><p>
								<DT><H3 FOLDED>pytorch-from-source</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/issues/77867">ccmake list</A>
									<DT><A HREF="https://gist.github.com/mhubii/1c1049fb5043b8be262259efac4b89d5">A guide to install and use the PyTorch C++ API with Anaconda.md</A>
								</DL><p>
								<DT><A HREF="https://pytorch.org/get-started/locally/">Start Locally | PyTorch</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/pytorch-release-2-6-0-final-rc-is-available/2748">PyTorch Release 2.6.0 - Final RC is available - Release Announcements - PyTorch Developer Mailing List</A>
								<DT><A HREF="https://chrisdare.medium.com/running-pytorch-on-apple-silicon-m1-gpus-a8bb6f680b02">Installing and running pytorch on M1 GPUs (Apple metal/MPS)</A>
								<DT><A HREF="https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/">Introducing Accelerated PyTorch Training on Mac | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/tree/main/.devcontainer">pytorch/.devcontainer at main · pytorch/pytorch</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/89ba1b1a67d570e41b03da87e5518eaff0d31fbf/docker/common/install_pytorch.sh">TensorRT-LLM/docker/common/install_pytorch.sh at 89ba1b1a67d570e41b03da87e5518eaff0d31fbf · NVIDIA/TensorRT-LLM</A>
								<DT><A HREF="https://github.com/pytorch-labs/tritonbench/tree/main/utils">tritonbench/utils at main · pytorch-labs/tritonbench</A>
								<DT><A HREF="https://github.com/pytorch-labs/tritonbench/blob/main/utils/cuda_utils.py">tritonbench/utils/cuda_utils.py at main · pytorch-labs/tritonbench</A>
								<DT><A HREF="https://github.com/pytorch-labs/tritonbench/pull/163/files">[install] install pytorch nightly if does not exist by xuzhao9 · Pull Request #163 · pytorch-labs/tritonbench</A>
							</DL><p>
							<DT><H3 FOLDED>torch-lightning</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Lightning-AI/pytorch-lightning">Lightning-AI/pytorch-lightning: Pretrain, finetune and deploy AI models on multiple GPUs, TPUs with zero code changes.</A>
								<DT><A HREF="https://github.com/Lightning-AI/litgpt">Lightning-AI/litgpt: Load, pretrain, finetune, deploy 20+ LLMs on your own data. Uses state-of-the-art techniques: flash attention, FSDP, 4-bit, LoRA, and more.</A>
								<DT><A HREF="https://github.com/Lightning-AI/LitServe">Lightning-AI/LitServe: Deploy AI models at scale. High-throughput serving engine for AI/ML models that uses the latest state-of-the-art model deployment techniques.</A>
								<DT><A HREF="https://x.com/_willfalcon/status/1859964670129422391">(1) William Falcon ⚡️ en X: "Yesterday we announced our $50M new funding. Don’t know why our users love Lightning? check out these 3 products: I swear these 3 products will EASILY 10x your iteration speed. 1. PyTorch Lightning: EASILY train and finetune ANY model of ANY size. With over 160 million https://t.co/IMGknJnO0k" / X</A>
							</DL><p>
							<DT><H3 FOLDED>torch-nightly</H3>
							<DL><p>
								<DT><A HREF="https://twitter.com/karpathy/status/1779354343013269929">"compound" F.sdpa (scaled dot product attention)</A>
								<DT><A HREF="https://github.com/Chillee/llm.c?tab=readme-ov-file#some-benchmark-numbers-with-newer-version-of-pytorch">PyTorch nightly + F.sdpa + coordinate descent tuning vs llm.c</A>
								<DT><A HREF="https://github.com/triton-lang/triton/issues/4310">torch pip nighly wheels binaries release</A>
							</DL><p>
							<DT><H3 FOLDED>torch-docs</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-composability-meeting-notes</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/document/d/1QTR3t3KdRu5JT1lvAuJLsPd3LruCfv0LecpsgR8eWhg/edit#heading=h.8y2jwyieg2yh">Composability meeting notes - Google Docs</A>
								</DL><p>
								<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
								<DT><A HREF="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py">Tensors — PyTorch Tutorials 1.11.0+cu102 documentation</A>
								<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">CrossEntropyLoss — PyTorch 1.11.0 documentation</A>
								<DT><A HREF="https://pytorch.org/docs/stable/optim.html">torch.optim — PyTorch 1.11.0 documentation</A>
								<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.Tensor.to.html">torch.Tensor.to — PyTorch 1.12 documentation</A>
								<DT><A HREF="https://github.com/BBuf/how-to-learn-deep-learning-framework?tab=readme-ov-file">BBuf/how-to-learn-deep-learning-framework: how to learn PyTorch and OneFlow</A>
								<DT><A HREF="https://github.com/pytorch/workshops/blob/master/ASPLOS_2024/README.md">workshops/ASPLOS_2024/README.md at master · pytorch/workshops</A>
								<DT><A HREF="https://github.com/pytorch/workshops/blob/master/ASPLOS_2024/inductor.pdf">workshops/ASPLOS_2024/inductor.pdf at master · pytorch/workshops</A>
								<DT><A HREF="https://colab.research.google.com/drive/1XQwio7DsqB5LP2D574f_uIb8G7KhirNa?usp=sharing#scrollTo=fMsppme9eqnl">PT2-Benchmark - Colab</A>
							</DL><p>
							<DT><H3 FOLDED>torch-release-notes</H3>
							<DL><p>
								<DT><H3 FOLDED>PyTorch 2.0</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>PyTorch 2.1</H3>
								<DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/pytorch-2-1-automatic-dynamic-shape-compilation-torch-distributed-checkpoint-torch-compile-numpy-torch-export-prototype-and-more/1548">PyTorch 2.1: automatic dynamic shape compilation, torch.distributed.checkpoint, torch.compile + NumPy, torch.export prototype, and more! - Release Announcements - PyTorch Dev Discussions</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/releases/tag/v2.1.0">Release PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing · pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>PyTorch 2.2</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/releases/tag/v2.2.0">PyTorch 2.2: FlashAttention-v2, AOTInductor</A>
								</DL><p>
								<DT><H3 FOLDED>PyTorch 2.3</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/blog/pytorch2-3/">PyTorch 2.3 Release Blog | PyTorch</A>
									<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile — PyTorch Tutorials 2.3.0+cu121 documentation</A>
									<DT><A HREF="https://cloud.google.com/blog/products/ai-machine-learning/introducing-pytorch-xla-2-3">Introducing PyTorch/XLA 2.3 | Google Cloud Blog</A>
								</DL><p>
								<DT><H3 FOLDED>PyTorch 2.4</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>PyTorch 2.5</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/releases/tag/v2.5.0">Release PyTorch 2.5.0 Release, SDPA CuDNN backend, Flex Attention · pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>PyTorch 2.6</H3>
								<DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/pytorch-release-2-6-0-final-rc-is-available/2748">PyTorch Release 2.6.0 - Final RC is available - Release Announcements - PyTorch Developer Mailing List</A>
								</DL><p>
								<DT><H3 FOLDED>torch 2.7</H3>
								<DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/pytorch-2-7-rc1-produced-for-pytorch-audio-vision/2855">PyTorch 2.7 RC1 produced for pytorch, audio, vision - Release Announcements - PyTorch Developer Mailing List</A>
								</DL><p>
								<DT><H3 FOLDED>torch-roadmap</H3>
								<DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/meta-pytorch-team-2024-h2-roadmaps/2226">Meta PyTorch Team 2024 H2 Roadmaps - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/meta-pytorch-team-2025-h1-roadmaps/2794">Meta PyTorch Team 2025 H1 Roadmaps - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://drive.google.com/file/d/1vTy9e5Vwk4xOb8tmhlICapT4x2KGbPQ7/view">[PUBLIC] Triton v-team OKRs H1 2025.pdf - Google Drive</A>
								</DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/pytorch-2-1-automatic-dynamic-shape-compilation-torch-distributed-checkpoint-torch-compile-numpy-torch-export-prototype-and-more/1548">PyTorch 2.1: automatic dynamic shape compilation, torch.distributed.checkpoint, torch.compile + NumPy, torch.export prototype, and more! - Release Announcements - PyTorch Dev Discussions</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/releases/tag/v2.1.0">Release PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing · pytorch/pytorch</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/releases/tag/v2.2.0">PyTorch 2.2: FlashAttention-v2, AOTInductor</A>
								<DT><A HREF="https://pytorch.org/blog/pytorch2-5/">PyTorch 2.5 Release Blog | PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>torch-dev</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-composability</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/document/d/1QTR3t3KdRu5JT1lvAuJLsPd3LruCfv0LecpsgR8eWhg/edit">Composability meeting notes - Google Docs</A>
								</DL><p>
								<DT><A HREF="https://github.com/albanD/pytorch_dev_env_setup">albanD/pytorch_dev_env_setup</A>
								<DT><A HREF="https://direnv.net/">direnv – unclutter your .profile | direnv</A>
								<DT><A HREF="http://giantpandacv.com/project/PyTorch/%E3%80%8APytorchConference2023%20%E7%BF%BB%E8%AF%91%E7%B3%BB%E5%88%97%E3%80%8B2-PyTorch%E5%BC%80%E5%8F%91%E8%80%85%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD/">PyTorch 2.0 SW infra</A>
								<DT><A HREF="https://docs.google.com/document/d/1QTR3t3KdRu5JT1lvAuJLsPd3LruCfv0LecpsgR8eWhg/edit">Composability meeting notes - Google Docs</A>
								<DT><A HREF="https://github.com/pytorch/builder">pytorch/builder: Continuous builder and binary build scripts for pytorch</A>
								<DT><A HREF="https://github.com/alihassanijr/PyTorch-CUDA12">alihassanijr/PyTorch-CUDA12: Nightly PyTorch + CUDA 12 Dockerfile</A>
								<DT><A HREF="https://fkong.tech/posts/2023-05-24-torch-source/">Some thoughts on reading PyTorch source code</A>
							</DL><p>
							<DT><H3 FOLDED>torch-compiler</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-compiler-docs</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit#heading=h.ivdr7fmrbeab">torch.compile, the missing manual - Google Docs</A>
									<DT><A HREF="http://blog.ezyang.com/2024/11/ways-to-use-torch-compile/">Ways to use torch.compile : ezyang’s blog</A>
									<DT><A HREF="https://github.com/pytorch/workshops/tree/master/ASPLOS_2024">workshops/ASPLOS_2024 at master · pytorch/workshops</A>
									<DT><A HREF="https://fkong.tech/posts/2024-09-01-torch-compile-stack/">torch.compile 重要步骤函数调用栈 · fkong' tech blog</A>
									<DT><A HREF="https://x.com/ezyang/status/1809766173849821669">what to expect from the compiler</A>
									<DT><A HREF="https://www.slideshare.net/slideshow/pytorch-2-internals/264537964">PyTorch 2 Internals | PPT</A>
									<DT><A HREF="https://strint.notion.site/torch-compile-backend-9f80a637dc0c4025abc207829bece666">torch.compile 自定义 backend 调研</A>
									<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler.html">torch.compiler — PyTorch 2.4 documentation</A>
									<DT><A HREF="https://github.com/apuaaChen/EVT_AE/blob/main/benchmark/ops/gemm.py">EVT_AE/benchmark/ops/gemm.py at main · apuaaChen/EVT_AE</A>
									<DT><A HREF="https://gist.github.com/gradjitta/aa84759af89223e0baae1f9afaeea8e0">torch.compile Troubleshooting Guide</A>
								</DL><p>
								<DT><H3 FOLDED>Inductor</H3>
								<DL><p>
									<DT><H3 FOLDED>inductor-env-vars</H3>
									<DL><p>
										<DT><H3 FOLDED>inductor-mode</H3>
										<DL><p>
											<DT><A HREF="https://pytorch.org/get-started/pytorch-2.0/">max-autotune</A>
											<DT><A HREF="https://twitter.com/jxmnop/status/1778835034079691008">max-autotune description</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/31c0467594c7c41c8e8ff1828bf01fa31fc4454f/torch/_inductor/__init__.py#L179">torch/_inductor/__init__.py: list_mode_options</A>
										</DL><p>
										<DT><A HREF="https://twitter.com/cHHillee/status/1777825367954432114/photo/1">TORCH_LOGS="output_code" python t.py</A>
										<DT><A HREF="https://twitter.com/karpathy/status/1779354343013269929">TORCHINDUCTOR_COORDINATE_DESCENT_TUNING=1</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py">pytorch/torch/_inductor/config.py at main · pytorch/pytorch</A>
										<DT><A HREF="https://github.com/Chillee/llm.c">Chillee/llm.c: LLM training in simple, raw C/CUDA</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/f3c54ccf8f6139807f4623037c0174964a286652/torch/_inductor/config.py#L682">TORCHINDUCTOR_PROFILE_WITH_DO_BENCH_USING_PROFILING</A>
										<DT><A HREF="https://github.com/discus0434/faster-flux/blob/main/src/faster_flux/pipeline_wrapper.py">triton.cudagraphs</A>
										<DT><A HREF="https://pytorch.org/docs/stable/generated/torch._logging.set_logs.html">torch._logging.set_logs — PyTorch 2.5 documentation</A>
										<DT><A HREF="https://github.com/ROCm/pytorch-micro-benchmarking">ROCm/pytorch-micro-benchmarking</A>
									</DL><p>
									<DT><H3 FOLDED>inductor-codegen</H3>
									<DL><p>
										<DT><H3 FOLDED>inductor-codegen-debug</H3>
										<DL><p>
											<DT><H3 FOLDED>cudalive</H3>
											<DL><p>
												<DT><A HREF="https://x.com/traviscline/status/1812342671026852071">Live-rendering of pytorch torch.compile optimizations</A>
												<DT><A HREF="https://github.com/tmc/cudalive">tmc/cudalive</A>
												<DT><A HREF="https://www.youtube.com/watch?v=KE7qXPY1j28&t=915s">Hardcore CUDA Hackathon Demos at AGI House - YouTube</A>
											</DL><p>
											<DT><A HREF="https://discuss.pytorch.org/t/getting-triton-to-generate-all-kernels/189173">Getting Triton to generate all kernels - torch.compile / torch._inductor - PyTorch Forums</A>
											<DT><A HREF="https://www.youtube.com/watch?v=qYqrfq452ig&t=4548s">Hardcore CUDA Hackathon Talks at AGI House SF - YouTube</A>
											<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html#benchmark-individual-triton-kernel">TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1 TorchInductor GPU profiling</A>
											<DT><A HREF="https://colab.research.google.com/drive/1XQwio7DsqB5LP2D574f_uIb8G7KhirNa?usp=sharing#scrollTo=P5sS86KpdaTI">PT2-Benchmark - Colab: TORCHINDUCTOR_UNIQUE_KERNEL_NAMES</A>
										</DL><p>
										<DT><H3 FOLDED>inductor-codegen-triton</H3>
										<DL><p>
											<DT><A HREF="https://discuss.pytorch.org/t/getting-triton-to-generate-all-kernels/189173">Getting Triton to generate all kernels - torch.compile / torch._inductor - PyTorch Forums</A>
										</DL><p>
										<DT><H3 FOLDED>inductor-kernels</H3>
										<DL><p>
											<DT><H3 FOLDED>torch-cutlass</H3>
											<DL><p>
												<DT><A HREF="https://www.youtube.com/watch?v=USMnKuyXBFM">Developing Optimal CUDA Kernels on Hopper Tensor Cores NVIDIA On Demand - YouTube</A>
											</DL><p>
											<DT><H3 FOLDED>inductor-kernels-profiling</H3>
											<DL><p>
												<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html">TorchInductor GPU Profiling — PyTorch 2.4 documentation</A>
												<DT><A HREF="https://colab.research.google.com/drive/1XQwio7DsqB5LP2D574f_uIb8G7KhirNa?usp=sharing#scrollTo=5qmmcFxtePrz">PT2-Benchmark - Colab</A>
												<DT><A HREF="https://gist.github.com/chenyang78/703498efed5de6161fcf1514c2bc6531">parse_kernel_metadata_csv.py</A>
												<DT><A HREF="https://github.com/pytorch/workshops/blob/master/ASPLOS_2024/profiling.pdf">workshops/ASPLOS_2024/profiling.pdf at master · pytorch/workshops</A>
												<DT><A HREF="https://github.com/drisspg/transformer_nuggets/blob/83f754eb670b73aa789a924b6b9fab67784ca28f/transformer_nuggets/utils/benchmark.py#L60">transformer_nuggets/transformer_nuggets/utils/benchmark.py</A>
												<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html#benchmark-individual-triton-kernel">TORCHINDUCTOR_BENCHMARK_KERNEL</A>
												<DT><A HREF="https://gist.github.com/shunting314/96a0afef9dce53d6357bf1633094f358">cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py</A>
												<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html">Profiling to understand torch.compile performance — PyTorch 2.4 documentation</A>
												<DT><A HREF="https://github.com/pytorch/pytorch/issues/120423">Inefficient triton kernels generated by inductor · Issue #120423 · pytorch/pytorch</A>
											</DL><p>
											<DT><H3 FOLDED>torch-custom-op</H3>
											<DL><p>
												<DT><H3 FOLDED>torch-custom-op-examples</H3>
												<DL><p>
													<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/tiled_matmul.py#L201">tiled_matmul.py#L201 xformers</A>
													<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/fmha/flash3.py">xformers_flash3::flash_fwd</A>
												</DL><p>
												<DT><A HREF="https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit#heading=h.ptttacy8y1u9">The Custom Operators Manual - Google Docs</A>
												<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/tiled_matmul.py#L201">xformers_python::tiled_matmul_fwd</A>
												<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/fmha/flash3.py">xformers_flash3::flash_fwd</A>
												<DT><A HREF="https://github.com/triton-lang/triton/blob/3ae95a858eac26088102075500e3860864432106/python/test/unit/hopper/test_flashattention.py#L294">triton/python/test/unit/hopper/test_flashattention.py: torch.autograd.Function (only works on post-Ampere GPUs right now)</A>
												<DT><A HREF="https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html">PyTorch Custom Operators — PyTorch Tutorials 2.4.0+cu121 documentation</A>
												<DT><A HREF="https://pytorch.org/blog/cuda-free-inference-for-llms/?utm_content=306418723&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024">CUDA-Free Inference for LLMs | PyTorch</A>
												<DT><A HREF="https://github.com/triton-lang/triton/issues/34">Custom operation tutorial: module 'triton' has no attribute 'Function' · Issue #34 · triton-lang/triton</A>
												<DT><A HREF="http://152.67.113.27/articles/PyTorch+%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%EF%BC%8812%EF%BC%89+%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BF%90%E7%AE%97%E7%AC%A6_10977905_csdn.html">PyTorch Basic Learning (12) - Custom Operators</A>
												<DT><A HREF="https://static.sched.com/hosted_files/pytorch2024/36/PTC%202024_%20Extending%20PyTorch%20with%20Custom%20Operators.pdf">PyTorch conf 2024:  Extending PyTorch with Custom Operators</A>
												<DT><A HREF="https://github.com/drisspg/transformer_nuggets/blob/83f754eb670b73aa789a924b6b9fab67784ca28f/transformer_nuggets/flash/flash_attention.py">transformer_nuggets/transformer_nuggets/flash/flash_attention.py</A>
												<DT><A HREF="https://github.com/drisspg/driss_torch">drisspg/driss_torch: Cuda extensions for PyTorch</A>
												<DT><A HREF="https://github.com/triton-lang/triton/blob/3ae95a858eac26088102075500e3860864432106/python/tutorials/05-layer-norm.py#L226">triton/python/tutorials/05-layer-norm.py: torch.autograd.Function</A>
												<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/9cafd4ae140b52dc2c95be1a1c6aeb24925a883d/hopper/flash_attn_interface.py#L165">flash-attention/hopper/flash_attn_interface.py: FlashAttnFunc(torch.autograd.Function)</A>
												<DT><A HREF="https://www.youtube.com/watch?v=ACR1WnRScCc">Composability Sync - User defined Triton vs custom ops / C++ FX - YouTube</A>
												<DT><A HREF="https://github.com/pytorch/pytorch/pull/111434">[Inductor] Support user defined triton kernels in inductor by oulgen · Pull Request #111434 · pytorch/pytorch</A>
												<DT><A HREF="https://github.com/BobMcDear/attorch/blob/main/attorch/glu_layer.py">attorch/attorch/glu_layer.py: GLUAutoGrad.apply</A>
												<DT><A HREF="https://github.com/chengzeyi/stable-fast/blob/fffe290680ec2ddc01f511e8e7fc62357ed901d8/src/sfast/triton/torch_ops.py">sfast: register_custom_python_operator</A>
												<DT><A HREF="https://github.com/pytorch/pytorch/issues/136550">torch.compiled custom Triton kernels can output incorrect results · Issue #136550 · pytorch/pytorch</A>
												<DT><A HREF="https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#testing-an-operator">Custom C++ and CUDA Operators — PyTorch Tutorials 2.4.0+cu121 documentation</A>
												<DT><A HREF="https://github.com/TiledTensor/TiledCUDA/blob/master/pytiledcuda/__init__.py">TiledCUDA/pytiledcuda/__init__.py at master · TiledTensor/TiledCUDA</A>
												<DT><A HREF="https://github.com/pytorch/pytorch/blob/20af56d4359c3f5fed2e8f94e111a8502f2ebeb3/test/test_flop_counter.py#L801">pytorch/test/test_flop_counter.py: registration example</A>
												<DT><A HREF="https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst">tutorials/advanced_source/cpp_custom_ops.rst at main · pytorch/tutorials</A>
												<DT><A HREF="https://docs.google.com/document/d/1ZcxkW1FRNVm9AvDuTthRHvM09XnbkXOQzOE0yqSVMQI/edit?tab=t.0">torch_custom_operators - Google Docs</A>
												<DT><A HREF="https://github.com/chengzeyi/piflux/blob/main/src/piflux/ops/context_ops.py">piflux/src/piflux/ops/context_ops.py at main · chengzeyi/piflux</A>
												<DT><A HREF="https://github.com/lcy-seso/DLFrameworkTest/tree/master/PyTorch/test_pytorch_customize_op">DLFrameworkTest/PyTorch/test_pytorch_customize_op at master · lcy-seso/DLFrameworkTest</A>
											</DL><p>
											<DT><H3 FOLDED>TritonTemplate</H3>
											<DL><p>
												<DT><A HREF="https://github.com/triton-lang/triton/issues/4310">Latest nightly triton causes my custom fused attention kernel to output incorrect results. · Issue #4310 · triton-lang/triton</A>
												<DT><A HREF="https://github.com/pytorch/pytorch/blob/419a7e197d2579e699c2e730902d197a27df8deb/torch/_inductor/select_algorithm.py#L675">pytorch/torch/_inductor/select_algorithm.py</A>
												<DT><A HREF="https://github.com/pytorch/pytorch/blob/419a7e197d2579e699c2e730902d197a27df8deb/test/inductor/test_max_autotune.py#L880">pytorch/test/inductor/test_max_autotune.py</A>
												<DT><A HREF="https://github.com/pytorch/pytorch/blob/419a7e197d2579e699c2e730902d197a27df8deb/torch/_inductor/kernel/conv.py#L140">pytorch/torch/_inductor/kernel/conv.py</A>
												<DT><A HREF="https://discuss.pytorch.org/t/getting-triton-to-generate-all-kernels/189173/4">Getting Triton to generate all kernels - torch.compile / torch._inductor - PyTorch Forums</A>
											</DL><p>
											<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile — PyTorch Tutorials 2.3.0+cu121 documentation</A>
											<DT><A HREF="https://dev-discuss.pytorch.org/t/user-defined-kernels-vs-torch-library-custom-op/2113">User-defined Kernels vs. `torch.library` custom op - compiler - PyTorch Developer Mailing List</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/121367">`torch.compile` makes triton kernel slower in some cases · Issue #121367 · pytorch/pytorch</A>
											<DT><A HREF="https://github.com/drisspg/driss_torch">drisspg/driss_torch: Cuda extensions for PyTorch</A>
											<DT><A HREF="https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit#heading=h.ptttacy8y1u9">The Custom Operators Manual - Google Docs</A>
											<DT><A HREF="https://pytorch.org/blog/cuda-free-inference-for-llms/?utm_content=306418723&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024">CUDA-Free Inference for LLMs | PyTorch</A>
											<DT><A HREF="https://gist.github.com/chenyang78/703498efed5de6161fcf1514c2bc6531">parse_kernel_metadata_csv.py</A>
										</DL><p>
										<DT><H3 FOLDED>inductor-lowering</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch-labs/attention-gym/blob/bbf437e9ea7d802c0ee71d067787f7b57605f9ff/attn_gym/mods/softcapping.py#L23">register_lowering example: softcapping.py#L23</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/8dd380803c0e25786cba12801088c420a2ca071b/torch/_inductor/lowering.py#L2450">pytorch/torch/_inductor/lowering.py at 8dd380803c0e25786cba12801088c420a2ca071b · pytorch/pytorch</A>
										</DL><p>
										<DT><H3 FOLDED>_inductor-code-gen-cuda</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/ac87388e61e6af38e978458af0bcb03e605d1928/torch/_inductor/codegen/cuda/cuda_template.py#L23">pytorch/torch/_inductor/codegen/cuda/cuda_template.py at ac87388e61e6af38e978458af0bcb03e605d1928 · pytorch/pytorch</A>
										</DL><p>
										<DT><A HREF="https://twitter.com/cHHillee/status/1777825367954432114/photo/1">TORCH_LOGS="output_code" python t.py</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/wiki/Codegen-and-Structured-Kernels">Codegen and Structured Kernels · pytorch/pytorch Wiki</A>
										<DT><A HREF="https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">How Pytorch 2.0 Accelerates Deep Learning with Operator Fusion and CPU/GPU Code-Generation</A>
										<DT><A HREF="https://twitter.com/marksaroufim/status/1746307482904076374">torch.utils.cpp_extension import load_inline</A>
										<DT><A HREF="https://twitter.com/johnowhitaker/status/1746275479806742664/photo/1">IR Triton code</A>
										<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch</A>
										<DT><A HREF="https://twitter.com/cHHillee/status/1777825367954432114">Horace He: see what kernels are being executed under torch.compile</A>
										<DT><A HREF="https://github.com/Chillee/llm.c/blob/master/inductor_gpt2.cpp">torch compile can also generate and emit C++ code (llm.c)</A>
										<DT><A HREF="https://pytorch.org/tutorials/prototype/inductor_cpp_wrapper_tutorial.html">Inductor C++ Wrapper Tutorial</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/fea1b99d89204989db64d0d63f5e46fce60d1962/torch/_inductor/config.py#L34">TORCHINDUCTOR_CPP_WRAPPER</A>
									</DL><p>
									<DT><H3 FOLDED>inductor-caching</H3>
									<DL><p>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-bring-compile-time-down-to-zero-our-plans-and-direction-may-14th-edition/2089">How To Bring Compile Time Down to Zero: Our Plans and Direction (May 14th Edition) - compiler - PyTorch Developer Mailing List</A>
									</DL><p>
									<DT><H3 FOLDED>inductor-profiling</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ai-compiler-study/test_attn/blob/main/inductor_profiling.py">test_attn/inductor_profiling.py at main · ai-compiler-study/test_attn</A>
										<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html">TorchInductor GPU Profiling — PyTorch 2.4 documentation</A>
										<DT><A HREF="https://colab.research.google.com/drive/1XQwio7DsqB5LP2D574f_uIb8G7KhirNa?usp=sharing#scrollTo=5qmmcFxtePrz">PT2-Benchmark - Colab</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/7eb2a99585b683e5486bfbf2e78c8165f08c392b/benchmarks/dynamo/microbenchmarks/microbench.py">pytorch/benchmarks/dynamo/microbenchmarks/microbench.py</A>
										<DT><A HREF="https://gist.github.com/chenyang78/703498efed5de6161fcf1514c2bc6531">parse_kernel_metadata_csv.py</A>
									</DL><p>
									<DT><H3 FOLDED>inductor-tests</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/c2ff9fe042ffe39a2684aecc6c6062a066489f54/test/inductor/test_fp8.py#L441">test_fp8.py#L441 backend="inductor", mode="max-autotune"</A>
									</DL><p>
									<DT><H3 FOLDED>inductor-debug</H3>
									<DL><p>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/torch-compile-can-be-debugged-now/1595">Torch.compile can be debugged now! - compiler - PyTorch Dev Discussions</A>
										<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">Lightning Talk: PT2 Export - A Sound Full Graph Capture Mechanism for PyTorch - Avik Chaudhuri, Meta - YouTube</A>
										<DT><A HREF="https://github.com/youkaichao/hello_frame_eval">youkaichao/hello_frame_eval: A hello-world usage for frame eval api.</A>
										<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst">pytorch/docs/source/torch.compiler_troubleshooting.rst at main · pytorch/pytorch</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/torch-compile-can-be-debugged-now/1595">Torch.compile can be debugged now</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst">pytorch/docs/source/torch.compiler_troubleshooting.rst</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/31c0467594c7c41c8e8ff1828bf01fa31fc4454f/docs/source/torch.compiler_troubleshooting.rst#L20">pytorch/docs/source/torch.compiler_troubleshooting.rst at 31c0467594c7c41c8e8ff1828bf01fa31fc4454f · pytorch/pytorch</A>
										<DT><A HREF="https://github.com/alpha0422/study/blob/main/inductor/common.py">study/inductor/common.py at main · alpha0422/study</A>
										<DT><A HREF="https://discuss.pytorch.org/t/getting-triton-to-generate-all-kernels/189173/4">Getting Triton to generate all kernels - torch.compile / torch._inductor - PyTorch Forums</A>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch/workshops/blob/master/ASPLOS_2024/inductor.pdf">workshops/ASPLOS_2024/inductor.pdf at master · pytorch/workshops</A>
									<DT><A HREF="https://github.com/thuml/depyf">thuml/depyf: understand and adapt to PyTorch compiler</A>
									<DT><A HREF="https://github.com/alpha0422/study">alpha0422/study: Study for various topics.</A>
									<DT><A HREF="https://pytorch.org/docs/main/torch.compiler_aot_inductor.html">AHEAD-OF-TIME COMPILATION FOR TORCH.EXPORT</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/the-future-of-c-model-deployment/1282">The future of C++ model deployment</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/codegen/wrapper.py">codegen/wrapper.py</A>
									<DT><A HREF="https://www.youtube.com/watch?v=w7d4oWzwZ0c">AOTInductor: Ahead-of-Time Compilation for PT2 Exported Models</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747">TorchInductor: a PyTorch-native Compiler with Define-by-Run IR and Symbolic Shapes</A>
									<DT><A HREF="https://twitter.com/mathemakitten/status/1744861826528211323">inductor + triton = kernels &gt; handcoded FlashAttention 2</A>
									<DT><A HREF="https://www.youtube.com/watch?v=682pQYiS4cQ">Fixing an PyTorch Inductor bug: Views, Buffers, Realize - YouTube</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/pull/111434">[Inductor] Support user defined triton kernels in inductor by oulgen · Pull Request #111434 · pytorch/pytorch</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/inductor-file-structure-explanation/1860/2">Inductor file structure explanation - PyTorch Dev Discussions</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/pull/107802">[Inductor CUTLASS backend] Step 1: Inductor config for cuda / cutlass, util functions. by ipiszy · Pull Request #107802 · pytorch/pytorch</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/e91c37c1c39734359637e3fd999ba20e26b6bfa7/torch/_inductor/config.py">pytorch/torch/_inductor/config.py</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/pull/111434">[Inductor] Support user defined triton kernels in inductor</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/inductor-file-structure-explanation/1860/2">Inductor file structure explanation</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/pull/107802">[Inductor CUTLASS backend] Step 1: Inductor config for cuda / cutlass, util functions</A>
									<DT><A HREF="https://github.com/ngimel/inductor_generated">ngimel/inductor_generated</A>
									<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common deep learning models</A>
									<DT><A HREF="https://twitter.com/cHHillee/status/1777825367954432114/photo/1">TORCH_LOGS="output_code" python t.py</A>
									<DT><A HREF="https://gist.github.com/Chillee/f86675147366a7a0c6e244eaa78660f7">1-pw_op_fusion.py</A>
									<DT><A HREF="https://pytorch.org/blog/introducing-depyf/">Introducing depyf: mastering torch.compile with ease | PyTorch</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/when-does-the-inductor-code-run/2088/3">When does the inductor code run? - compiler - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://www.youtube.com/watch?v=qYqrfq452ig&t=4548s">Hardcore CUDA Hackathon Talks at AGI House SF - YouTube</A>
									<DT><A HREF="https://pytorch.org/assets/images/pytorch-2.0-img4.jpg">compilation process</A>
									<DT><A HREF="https://wentao.site/compiler_introduction/">Pytorch Compiler Introduction - Yewentao's Blog</A>
									<DT><A HREF="https://github.com/FindHao/ml_scripts/blob/main/inductor/filter_kernel_num.py">ml_scripts/inductor/filter_kernel_num.py at main · FindHao/ml_scripts</A>
								</DL><p>
								<DT><H3 FOLDED>Dynamo</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-symbolic-shapes</H3>
									<DL><p>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/state-of-symbolic-shapes-branch/777">State of Symbolic Shapes</A>
										<DT><A HREF="https://www.youtube.com/watch?v=R-AVYgBIZRY">Dynamic Shapes</A>
										<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes — Good Coding Style | by Noam Shazeer | Feb, 2024 | Medium</A>
										<DT><A HREF="https://github.com/ezyang/data-dependent-shape-puzzles">ezyang/data-dependent-shape-puzzles: Puzzlers regarding data-dependent shapes in PT2</A>
										<DT><A HREF="https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit#heading=h.44gwi83jepaj">Dealing with GuardOnDataDependentSymNode errors - Google Docs</A>
										<DT><A HREF="https://colab.research.google.com/github/ezyang/data-dependent-shape-puzzles/blob/main/data-dependent-shape-puzzles.ipynb#scrollTo=65eb2hyriPM0">data-dependent-shape-puzzles.ipynb - Colab</A>
										<DT><A HREF="https://static.sched.com/hosted_files/pytorch2024/a6/Data-dependent%20shapes%20in%20PT2.pdf">Data-dependent shapes in PT2</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/135859">torch.bmm, torch.topk with out variants set causing recompilations in torch.compile · Issue #135859 · pytorch/pytorch</A>
										<DT><A HREF="https://github.com/replicate/autocompile">replicate/autocompile: automatically infer dynamic shapes for Torch-Tensor compilation</A>
									</DL><p>
									<DT><H3 FOLDED>Guard Model</H3>
									<DL><p>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/understanding-dynamic-shapes-and-guards-and-when-it-does-does-not-cause-graph-breaks/2429">Understanding dynamic shapes and guards and when it does/does not cause graph breaks - compiler - PyTorch Developer Mailing List</A>
										<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html#the-guard-model">Dynamic shapes — PyTorch 2.4 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>dynamo-docs</H3>
									<DL><p>
										<DT><A HREF="https://fkong.tech/posts/2023-05-20-dynamo/">一文搞懂 TorchDynamo 原理 · fkong' tech blog</A>
										<DT><A HREF="https://fkong.tech/posts/2023-05-14-dynamo-01/">TorchDynamo 源码剖析 01 - Frame Evaluation 与字节码基础 · fkong' tech blog</A>
										<DT><A HREF="https://fkong.tech/posts/2023-05-14-dynamo-02/">TorchDynamo 源码剖析 02 - 字节码翻译 · fkong' tech blog</A>
										<DT><A HREF="https://fkong.tech/posts/2023-05-14-dynamo-03/">TorchDynamo 源码剖析 03 - Graph Break · fkong' tech blog</A>
										<DT><A HREF="https://github.com/chenzomi12/AISystem/blob/main/03Compiler/06PyTorch/02.torchscript.pdf">AISystem/03Compiler/06PyTorch/02.torchscript.pdf at main · chenzomi12/AISystem</A>
										<DT><A HREF="https://github.com/chenzomi12/AISystem/blob/main/03Compiler/06PyTorch/04.torchdynamo.pdf">AISystem/03Compiler/06PyTorch/04.torchdynamo.pdf at main · chenzomi12/AISystem</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/59cf4bc5ae64aea2c6a9b870243821695adfc30b/docs/source/torch.compiler_fine_grain_apis.rst#L29">pytorch/docs/source/torch.compiler_fine_grain_apis.rst: torch.compiler.is_compiling</A>
									</DL><p>
									<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html">Dynamo Deep-Dive — PyTorch 2.6 documentation</A>
									<DT><A HREF="https://github.com/pytorch/workshops/blob/master/ASPLOS_2024/dynamo_example.pdf">workshops/ASPLOS_2024/dynamo_example.pdf at master · pytorch/workshops</A>
									<DT><A HREF="https://www.youtube.com/watch?v=egZB5Uxki0I">torchdynamo deep dive - YouTube</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/torchdynamo-update-4-lazytensor-nvfuser-experiments/496">TorchDynamo Update 4: LazyTensor &amp; nvFuser Experiments - compiler - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/torchdynamo-update-7-inference-with-fx2trt/576">TorchDynamo Update 7: Inference with FX2TRT - compiler - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://strint.github.io/221203-torchdynamo.html">TorchDynamo 初探: Python ByteCode 的动态修改 | strint’s blog (nexfort)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">Lightning Talk: PT2 Export - A Sound Full Graph Capture Mechanism for PyTorch - Avik Chaudhuri, Meta - YouTube</A>
									<DT><A HREF="https://github.com/Skylion007/pytorch/blob/ad8448497290394b50f84a2932fcb4f078793b3f/test/dynamo/test_functions.py#L1603">pytorch/test/dynamo/test_functions.py</A>
									<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common deep learning models</A>
									<DT><A HREF="https://github.com/thuml/depyf">thuml/depyf: understand and adapt to PyTorch compiler</A>
									<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">Export - A Sound Full Graph Capture Mechanism for PyTorch</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/a-torchdynamo-trace-time-ablation-study/1961">A TorchDynamo trace time ablation study - compiler - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://www.youtube.com/watch?v=X6RXkTXQOb0">Debugging torchdynamo symbolic shapes: Part 2 - YouTube</A>
									<DT><A HREF="https://colab.research.google.com/github/ezyang/data-dependent-shape-puzzles/blob/main/data-dependent-shape-puzzles.ipynb#scrollTo=eXgUk-MfiPM2">data-dependent-shape-puzzles.ipynb - Colab</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/issues/117045">cache_entries empty after torch.compile() ? · Issue #117045 · pytorch/pytorch</A>
									<DT><A HREF="https://github.com/pytorch-labs/tritonbench/commit/9dcf57345999c2ce5d271680a7bbdac2c8e0f527#diff-7188946c2f4f60af53ffb2e2f55bd09a3b00c12b49989fe085933585edcb3c5cR26">torch.compiler.is_compiling()</A>
									<DT><A HREF="https://github.com/replicate/autocompile">replicate/autocompile: automatically infer dynamic shapes for Torch-Tensor compilation</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/pull/133137/files">[Dynamo] Trace torch function modes entered outside of torch.compile by mlazos · Pull Request #133137 · pytorch/pytorch</A>
									<DT><A HREF="https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit?tab=t.0#heading=h.fh8zzonyw8ng">The dynamic shapes manual - Google Docs</A>
								</DL><p>
								<DT><H3 FOLDED>torch-compiler-depyf</H3>
								<DL><p>
									<DT><H3 FOLDED>depyf-dynamo</H3>
									<DL><p>
										<DT><A HREF="https://github.com/thuml/depyf/blob/master/docs/_static/images/dynamo-workflow.svg">dynamo-workflow.svg</A>
									</DL><p>
									<DT><A HREF="https://github.com/thuml/depyf">thuml/depyf: depyf is a tool to help you understand and adapt to PyTorch compiler torch.compile.</A>
									<DT><A HREF="https://pytorch.org/blog/introducing-depyf/">Introducing depyf: mastering torch.compile with ease | PyTorch</A>
									<DT><A HREF="https://github.com/thuml/depyf/blob/master/docs/walk_through.rst">depyf/docs/walk_through.rst at master · thuml/depyf</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/interactively-explore-what-torch-compile-does-to-your-code/1576">Interactively explore what torch.compile does to your code! - compiler</A>
									<DT><A HREF="https://findhao.net/easycoding/2621">How to Debug PyTorch Compiler - FindHao</A>
									<DT><A HREF="https://depyf.readthedocs.io/en/latest/walk_through.html">A Walk Through Example of torch.compile — depyf documentation</A>
								</DL><p>
								<DT><H3 FOLDED>torch-compiler-benchmark</H3>
								<DL><p>
									<DT><A HREF="https://hud.pytorch.org/benchmark/compilers">compilers</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/7eb2a99585b683e5486bfbf2e78c8165f08c392b/benchmarks/dynamo/microbenchmarks/microbench.py">pytorch/benchmarks/dynamo/microbenchmarks/microbench.py</A>
									<DT><A HREF="https://github.com/iree-org/iree-comparative-benchmark/blob/main/comparative_benchmark/pt_inductor/run_benchmarks.py">iree-comparative-benchmark/comparative_benchmark/pt_inductor/run_benchmarks.py at main · iree-org/iree-comparative-benchmark</A>
									<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html#benchmark-individual-triton-kernel">TorchInductor GPU Profiling — PyTorch 2.4 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>torch-compiler-caching</H3>
								<DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-bring-compile-time-down-to-zero-our-plans-and-direction-may-14th-edition/2089">How To Bring Compile Time Down to Zero: Our Plans and Direction (May 14th Edition) - compiler - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html">Compile Time Caching in torch.compile — PyTorch Tutorials 2.3.0+cu121 documentation</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/remote-compilation-caching-system-testing/2179">Remote compilation caching system testing - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://github.com/siliconflow/nexfort-gpt/blob/6253c6bb054e658d67566150f87329b87815ae63/generate.py#L27">torch._inductor.config.fx_graph_cache = True</A>
									<DT><A HREF="https://github.com/siliconflow/onediff/blob/4bf583b51d5d5bc5232f6350f78c7822c0f07a78/onediff_diffusers_extensions/README.md#fast-lora-loading-and-switching">Compiled graph re-using</A>
								</DL><p>
								<DT><H3 FOLDED>torch-fx-graph</H3>
								<DL><p>
									<DT><A HREF="http://giantpandacv.com/project/PyTorch/%E7%94%A8%E6%B2%90%E7%A5%9E%E7%9A%84%E6%96%B9%E6%B3%95%E9%98%85%E8%AF%BBPyTorch%20FX%E8%AE%BA%E6%96%87/">Reading PyTorch FX Papers Using Mushen’s Method</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/different-points-at-which-fusion-occurs/2099">Different points at which fusion occurs? - compiler / FX - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://arxiv.org/pdf/2112.08429">TORCH.FX: PRACTICAL PROGRAM CAPTURE AND TRANSFORMATION FOR DEEP LEARNING IN PYTHON</A>
									<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_transformations.html">Writing Graph Transformations on ATen IR — PyTorch 2.4 documentation</A>
									<DT><A HREF="https://github.com/pytorch/tutorials/blob/main/intermediate_source/fx_conv_bn_fuser.py">tutorials/intermediate_source/fx_conv_bn_fuser.py at main · pytorch/tutorials</A>
								</DL><p>
								<DT><H3 FOLDED>torch-compiler-fusion</H3>
								<DL><p>
									<DT><A HREF="https://towardsdatascience.com/how-pytorch-2-0-accelerates-deep-learning-with-operator-fusion-and-cpu-gpu-code-generation-35132a85bd26">Operator Fusion &amp; Compilation</A>
								</DL><p>
								<DT><H3 FOLDED>torch-compiler-breaks</H3>
								<DL><p>
									<DT><A HREF="https://github.com/sayakpaul/diffusers-torchao">Benefitting from torch.compile(): torchao &amp; diffusers</A>
								</DL><p>
								<DT><H3 FOLDED>torch-compiler-examples</H3>
								<DL><p>
									<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common DNN</A>
									<DT><A HREF="https://github.com/xdit-project/xDiT/blob/94642cabbf570f159d8305f859fa018be4a0b561/xfuser/model_executor/pipelines/base_pipeline.py#L231">xDiT/xfuser/model_executor/pipelines/base_pipeline.py: _convert_transformer_backbone</A>
								</DL><p>
								<DT><H3 FOLDED>torch-compiler-export</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">PT2 Export - A Sound Full Graph Capture Mechanism</A>
									<DT><A HREF="https://www.youtube.com/watch?v=krwpwbH7NJc">PyTorch's Computational Graph</A>
									<DT><A HREF="https://github.com/microsoft/Olive">microsoft/Olive: Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation.</A>
									<DT><A HREF="https://pytorch.org/docs/stable/export.html">torch.export — PyTorch 2.5 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>torch-compiler-einops</H3>
								<DL><p>
									<DT><A HREF="https://github.com/arogozhnikov/einops/wiki/Using-torch.compile-with-einops">Using torch.compile with einops · arogozhnikov/einops Wiki</A>
								</DL><p>
								<DT><H3 FOLDED>torch-jit</H3>
								<DL><p>
									<DT><A HREF="https://strint.notion.site/torch-jit-trace-177dbc92e21d4c66ab8d208a2a86a27f">torch.jit.trace 简析</A>
								</DL><p>
								<DT><A HREF="https://github.com/thuml/depyf/blob/master/docs/walk_through.rst">depyf/docs/walk_through.rst at master · thuml/depyf</A>
								<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common deep learning models, can be used as a learning resource for torch.compile</A>
								<DT><A HREF="https://www.youtube.com/watch?v=kdZwgRghy3M">XLA Internal</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965">Functionalization in PyTorch: Everything You Wanted To Know - compiler - PyTorch Dev Discussions</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467">Min-cut optimal(*) recomputation (i.e. activation checkpointing) with AOTAutograd - compiler - PyTorch Dev Discussions</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/interactively-explore-what-torch-compile-does-to-your-code/1576">Interactively explore what torch.compile does to your code! - compiler</A>
								<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler">torch.compiler — PyTorch 2.3 documentation</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747">TorchInductor: a PyTorch-native Compiler with Define-by-Run IR and Symbolic Shapes</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/i-build-a-decompiler-to-convert-bytecode-generated-by-dynamo-into-readable-source-code/1471/4">I build a decompiler to convert bytecode generated by dynamo into readable source code</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-capture-nccl-communication-ops-in-faketensormode/1410">How to capture NCCL communication ops in FakeTensorMode</A>
								<DT><A HREF="https://pytorch.org/docs/stable/fx.html">torch.fx — PyTorch 2.1 documentation</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/torch-compile-tech-talks-at-ptc23/1625">Torch.compile() tech talks at PTC'23</A>
								<DT><A HREF="https://www.youtube.com/watch?v=krwpwbH7NJc">PyTorch's Computational Graph</A>
								<DT><A HREF="https://www.youtube.com/watch?v=w30xteQDeO8">PT2 Export - A Sound Full Graph Capture Mechanism</A>
								<DT><A HREF="https://peps.python.org/pep-0523/">PEP 523 – Adding a frame evaluation API to CPython | peps.python.org</A>
								<DT><A HREF="https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha/blob/main/app.py">HF Transformer simple example</A>
								<DT><A HREF="https://github.com/thuml/depyf">thuml/depyf: understand and adapt to PyTorch compiler</A>
								<DT><A HREF="https://github.com/thuml/learn_torch.compile">thuml/learn_torch.compile: torch.compile artifacts for common deep learning models</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965">Functionalization in PyTorch: Everything You Wanted To Know</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/min-cut-optimal-recomputation-i-e-activation-checkpointing-with-aotautograd/467">Min-cut optimal(*) recomputation (i.e. activation checkpointing) with AOTAutograd - compiler</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/interactively-explore-what-torch-compile-does-to-your-code/1576">Interactively explore what torch.compile does to your code</A>
								<DT><A HREF="https://peps.python.org/pep-0523/">PEP 523 – Adding a frame evaluation API to CPython</A>
								<DT><A HREF="https://www.youtube.com/watch?v=mEYzE1iIEDI">Composability sync - Hierarchical compilation, torchrec PT2, NF4, accuracy - YouTube</A>
								<DT><A HREF="https://en.cppreference.com/w/c/numeric/math/fma">fma, fmaf, fmal - cppreference.com</A>
								<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile — PyTorch Tutorials 2.3.0+cu121 documentation</A>
								<DT><A HREF="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">Introduction to torch.compile — PyTorch Tutorials 2.3.0+cu121 documentation</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-bring-compile-time-down-to-zero-our-plans-and-direction-may-14th-edition/2089">How To Bring Compile Time Down to Zero: Our Plans and Direction (May 14th Edition) - compiler - PyTorch Developer Mailing List</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/how-can-i-dump-the-prims-ir-triton-code-and-ptx-code-when-using-torch-compile/2057">How can I dump the prims IR, triton code, and ptx code when using torch.compile() - compiler - PyTorch Developer Mailing List</A>
								<DT><A HREF="https://github.com/Lightning-AI/lightning-thunder">Lightning-AI/lightning-thunder: Make PyTorch models up to 40% faster! Thunder is a source to source compiler for PyTorch. It enables using different hardware executors at once; across one or thousands of GPUs.</A>
								<DT><A HREF="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/96ad88eb476f41a5403dcdade086afb8/torch_compile_tutorial.ipynb">torch_compile_tutorial.ipynb - Colab</A>
								<DT><A HREF="https://blog.christianperone.com/wp-content/uploads/2023/12/talk_torch_v2.pdf">PyTorch 2 internals</A>
								<DT><A HREF="https://www.youtube.com/watch?v=139UPjoq7Kw&t=3115s">Building Machine Learning Systems for a Trillion Trillion Floating Point Operations - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>torch-internals</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-device</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/pull/4508">add available memory check to accelerators by jeffra · Pull Request #4508 · microsoft/DeepSpeed</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/pull/4508">add available memory check to accelerators</A>
								</DL><p>
								<DT><H3 FOLDED>torch/cuda</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-blackwell</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/145949">[CUDA][Blackwell] Blackwell Tracking Issue · Issue #145949 · pytorch/pytorch</A>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/cuda/__init__.py">pytorch/torch/cuda/__init__.py at main · pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>torch-backends</H3>
								<DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/weight-sharing-on-cuda/701">Weight sharing on cuda</A>
									<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile</A>
								</DL><p>
								<DT><H3 FOLDED>torch-hooks</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=syLFCVYua6Q">PyTorch Hooks Explained</A>
								</DL><p>
								<DT><H3 FOLDED>ATen</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-autograd</H3>
									<DL><p>
										<DT><A HREF="https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation">Automatic differentiation - Wikipedia</A>
										<DT><A HREF="https://x.com/i/bookmarks?post_id=1803963383018066272">94 lines of code are everything that is needed</A>
										<DT><A HREF="https://www.youtube.com/watch?v=dEnUP6_kpeo">04 PyTorch tutorial - How do computational graphs and autograd in PyTorch work - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=MswxJw-8PvE">PyTorch Autograd Explained - In-depth Tutorial - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=ne99laPUxN4">The Simple Essence of Automatic Differentiation - Conal Elliott - YouTube</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-read-the-autograd-codebase/383">How to read the autograd codebase - frontend API - PyTorch Dev Discussions</A>
										<DT><A HREF="https://www.youtube.com/watch?v=S7VG-0Tw6a4">Building an autograd engine with only Triton GPU kernels - live 2025.1.28 - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=MswxJw-8PvE&t=2s">PyTorch Autograd Explained - In-depth Tutorial - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>torch-tensor</H3>
									<DL><p>
										<DT><H3 FOLDED>torch-dtypes</H3>
										<DL><p>
											<DT><H3 FOLDED>Floating-Point Numbers</H3>
											<DL><p>
												<DT><A HREF="https://www.youtube.com/watch?v=PhYeUg1zXq4">An In-Depth Look at Floating-Point Numbers - YouTube</A>
												<DT><A HREF="https://www.youtube.com/watch?v=d1LNUvkRMEg&t=13076s">GPT-2 from Scratch in C (Day 1/2) - YouTube</A>
											</DL><p>
											<DT><H3 FOLDED>torch-fp8</H3>
											<DL><p>
												<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
												<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization — PyTorch 2.0 documentation</A>
												<DT><A HREF="https://github.com/pytorch/pytorch/pull/109168">Basic fp8 support in Inductor</A>
												<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/53b48f35ad87603df882d036c1a2e3f7d88f2bd8/kernels/needs_perf_help/fp8_gemm_bench.py#L102">applied-ai/kernels/needs_perf_help/fp8_gemm_bench.py</A>
												<DT><A HREF="https://github.com/Azure/MS-AMP">Azure/MS-AMP: Microsoft Automatic Mixed Precision Library</A>
												<DT><A HREF="https://github.com/vllm-project/llm-compressor">vllm-project/llm-compressor</A>
												<DT><A HREF="https://github.com/neuralmagic/AutoFP8">neuralmagic/AutoFP8</A>
												<DT><A HREF="https://x.com/i/bookmarks?post_id=1815769704415342890">Neural Magic: vLLM Llama-3.1-405B FP8</A>
												<DT><A HREF="https://github.com/vllm-project/vllm/pull/4749">[Kernel] Add w8a8 CUTLASS kernels by tlrmchlsmth · Pull Request #4749 · vllm-project/vllm</A>
												<DT><A HREF="https://github.com/vllm-project/vllm/pull/5975">[Kernel] Expand FP8 support to Ampere GPUs using FP8 Marlin by mgoin · Pull Request #5975 · vllm-project/vllm</A>
												<DT><A HREF="https://github.com/vllm-project/vllm/pull/6559">[ Misc ] `fbgemm` checkpoints by robertgshaw2-neuralmagic · Pull Request #6559 · vllm-project/vllm</A>
												<DT><A HREF="https://gist.github.com/wkcn/232d2cf8d50e15cdb38be3e577cc4e3a">FP8GEMM</A>
												<DT><A HREF="https://arxiv.org/pdf/2303.17951">FP8 versus INT8 for efficient deep learning inference</A>
												<DT><A HREF="https://github.com/drisspg/transformer_nuggets/blob/main/transformer_nuggets/fp8/scaled_quant.py">transformer_nuggets/transformer_nuggets/fp8/scaled_quant.py at main · drisspg/transformer_nuggets</A>
												<DT><A HREF="https://01-ai.github.io/blog.html?post=zh/2024-07-30-%E5%A4%A7%E6%A8%A1%E5%9E%8B%20FP8%20%E4%BD%8E%E7%B2%BE%E5%BA%A6%E9%87%8F%E5%8C%96%E6%8E%A8%E7%90%86.md">大模型 FP8 低精度量化推理 - 01.AI Blog</A>
												<DT><A HREF="https://arxiv.org/abs/2209.05433">[2209.05433] FP8 Formats for Deep Learning</A>
												<DT><A HREF="https://dev-discuss.pytorch.org/t/float8-in-pytorch-1-x/1815">Float8 in PyTorch [1/x] - PyTorch Developer Mailing List</A>
											</DL><p>
											<DT><H3 FOLDED>torch-bfloat16</H3>
											<DL><p>
												<DT><A HREF="https://dev-discuss.pytorch.org/t/summation-bfloat16/390">Summation bfloat16 - documentation - PyTorch Developer Mailing List</A>
												<DT><A HREF="https://www.youtube.com/watch?v=B1eFGn5nN84">Are Numerical Linear Algebra Algorithms Accurate at Extreme Scale and at Low Precisions?</A>
												<DT><A HREF="https://x.com/_xjdr/status/1817612850820788557">(1) xjdr en X: "I should probably explain this madness: numpy doesn't natively support bfloat16 but most (good) models are trained in it. If you are using torch or huggingface and you need to convert to something else (like jax as the gods intended) then you are going to have to copy, throw it https://t.co/SB6ISHTIrg" / X</A>
												<DT><A HREF="https://x.com/_xjdr/status/1817604436887498884">(1) xjdr en X: "protip for working with bfloat16 with torch and numpy bf16_np = param.cpu().view(dtype=torch.uint16).numpy().view(ml_dtypes.bfloat16)" / X</A>
												<DT><A HREF="https://github.com/jax-ml/ml_dtypes">jax-ml/ml_dtypes: A stand-alone implementation of several NumPy dtype extensions used in machine learning.</A>
												<DT><A HREF="https://x.com/torchcompiled/status/1862096374864687331">Ethan 🇦🇺 en X: "it's crazy to me that RoPE's issue with BF16 wasn't noticed earlier. For a reasonable N of 2048, these are the computed frequencies prior to cos(x) &amp;amp; sin(x) for fp32 above and bf16 below. At small values there is little loss of precision, but for larger values the difference is https://t.co/HoHrudJEj9" / X</A>
												<DT><A HREF="https://x.com/cloneofsimo/status/1862167638061023376">Simo Ryu en X: "Jesus christ, the problem is worse with 2d RoPE. Left is RoPE with bfloat visualized, (for w axis info, last half of RoPE) Right is float32 visualized. https://t.co/UzR0EO4V6l" / X</A>
											</DL><p>
											<DT><A HREF="https://pytorch.org/docs/stable/tensors.html">torch.Tensor — PyTorch 2.2 documentation</A>
											<DT><A HREF="https://dev-discuss.pytorch.org/t/summation-bfloat16/390/2">Summation bfloat16 - documentation - PyTorch Developer Mailing List</A>
											<DT><A HREF="http://giantpandacv.com/project/CUDA/%E5%9C%A8OneFlow%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%87%AA%E5%8A%A8%E6%8F%90%E5%8D%87/">torch dtypes promotion procedure</A>
										</DL><p>
										<DT><A HREF="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch internals : ezyang’s blog</A>
										<DT><A HREF="http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/">PyTorch dispatcher</A>
										<DT><A HREF="http://blog.ezyang.com/2020/05/a-brief-taxonomy-of-pytorch-operators-by-shape-behavior/">A brief taxonomy of PyTorch operators by shape behavior : ezyang’s blog</A>
										<DT><A HREF="https://ezyang.github.io/stride-visualizer/index.html">STRIDE Visualizer</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/c10/core/TensorImpl.h">TensorImpl.h at main</A>
										<DT><A HREF="https://theaisummer.com/einsum-attention/">Einsum: Transformer example</A>
										<DT><A HREF="https://pytorch.org/blog/a-tour-of-pytorch-internals-1/">A Tour of PyTorch Internals (Part I) | PyTorch</A>
										<DT><A HREF="https://colab.research.google.com/drive/1zjAisRrc8R6uixKsrs1DRm3lwz5MWN68#scrollTo=3KS38qNL0-E5">Tensor_subclasses.ipynb - Colab</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/what-and-why-is-torch-dispatch/557">What (and Why) is __torch_dispatch__? - frontend API - PyTorch Developer Mailing List</A>
									</DL><p>
									<DT><H3 FOLDED>aten-custom-ops</H3>
									<DL><p>
										<DT><A HREF="https://github.com/drisspg/driss_torch">drisspg/driss_torch: Cuda extensions for PyTorch</A>
										<DT><A HREF="https://github.com/TiledTensor/TiledCUDA/blob/master/pytiledcuda/__init__.py">TiledCUDA/pytiledcuda/__init__.py at master · TiledTensor/TiledCUDA</A>
									</DL><p>
									<DT><H3 FOLDED>aten-mm</H3>
									<DL><p>
										<DT><A HREF="https://gist.github.com/drisspg/783616821043ab4594b9784f556c6714">Scaled MM API</A>
										<DT><A HREF="https://gist.github.com/Chillee/2ec89696db8b7ed1c24461159e325405">H100 peak matmul FLOPS</A>
									</DL><p>
									<DT><A HREF="https://x.com/karpathy/status/1803963383018066272">(1) Andrej Karpathy en X: "These 94 lines of code are everything that is needed to train a neural network. Everything else is just efficiency. This is my earlier project Micrograd. It implements a scalar-valued auto-grad engine. You start with some numbers at the leafs (usually the input data and the https://t.co/2zVJP3cNJ0" / X</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-read-the-autograd-codebase/383">How to read the autograd codebase - frontend API - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC?usp=sharing">Simple Grad - Colab</A>
									<DT><A HREF="https://pytorch.org/docs/main/torch.compiler_ir.html#prims-ir">IRs — PyTorch main documentation</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/tree/main/aten/src#the-c-interface">low-level tensor libraries for PyTorch, as well as the new ATen C++ bindings</A>
									<DT><A HREF="https://github.com/FlagOpen/FlagGems">FlagOpen/FlagGems: FlagGems is an operator library for large language models implemented in Triton Language.</A>
									<DT><A HREF="https://x.com/__tinygrad__/status/1864162388754280820">(1) the tiny corp en X: "@giffmana Oh wait we have to add CUDA and METAL support: cpu_conv2d_fp16 cpu_conv2d_fp16_with_bias cpu_conv2d_fp32 cpu_conv2d_fp32_with_bias cpu_conv2d_int8 cpu_conv2d_int8_with_bias cuda_conv2d_fp16 cuda_conv2d_fp16_with_bias cuda_conv2d_fp32 cuda_conv2d_fp32_with_bias cuda_conv2d_int8" / X</A>
								</DL><p>
								<DT><H3 FOLDED>torch-nn</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/d04957c0c682d766987cad07dce20986ca4a5b78/torch/_refs/nn/functional/__init__.py#L304">pytorch/torch/_refs/nn/functional/__init__.py OPS</A>
								</DL><p>
								<DT><H3 FOLDED>torch-internals-logs</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ezyang/tlparse">ezyang/tlparse: TORCH_LOG parser for PT2</A>
									<DT><A HREF="https://twitter.com/johnowhitaker/status/1746275479806742664/photo/1">TORCH_LOGS="output_code" python compile_square.py</A>
									<DT><A HREF="https://github.com/ezyang/tlparse">ezyang/tlparse: TORCH_LOGS parser for PT2</A>
								</DL><p>
								<DT><H3 FOLDED>torch-profiling</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-benchmarking</H3>
									<DL><p>
										<DT><H3 FOLDED>torch-benchmarking-model-params</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/26b0a0c2f37a8ad376f261df7bb4fee65ff2f230/torch/nn/parallel/distributed.py#L1378">distributed.py#L1378</A>
											<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/e9024c691f27a41fabd94617d39d75813b649f26/tinygrad/nn/state.py#L87">tinygrad/tinygrad/nn/state.py: model number of parameters (get_parameters)</A>
											<DT><A HREF="https://github.com/huggingface/transformers/blob/b7ea171403d53a2aa9bce422f1fad8fb1150844b/examples/research_projects/movement-pruning/counts_parameters.py#L4">transformers/examples/research_projects/movement-pruning/counts_parameters.py at b7ea171403d53a2aa9bce422f1fad8fb1150844b · huggingface/transformers</A>
										</DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/9df4bc6a0dc72caccee142555d1668fad1621206/benchmarks/transformer/better_transformer_vs_mha_functional.py#L167">pytorch/benchmarks/transformer/better_transformer_vs_mha_functional.py at 9df4bc6a0dc72caccee142555d1668fad1621206 · pytorch/pytorch</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/9df4bc6a0dc72caccee142555d1668fad1621206/benchmarks/transformer/better_transformer_vs_mha_functional.py#L114">benchmark_torch_function</A>
										<DT><A HREF="https://github.com/triton-lang/triton/blob/8e0c7b425ac149c43183de966ffa423fd46e4762/python/triton/testing.py">triton/python/triton/testing.py systematic testing</A>
										<DT><A HREF="https://github.com/madsys-dev/deepseekv2-profile/blob/main/mla/benchmark.py#L164">deepseekv2-profile/mla/benchmark.py</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/c3a31d90e7d10a9b89b11396b6f8b20ed52bf394/torch/utils/throughput_benchmark.py">pytorch/torch/utils/throughput_benchmark.py</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/7eb2a99585b683e5486bfbf2e78c8165f08c392b/benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py#L92">pytorch/benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py at 7eb2a99585b683e5486bfbf2e78c8165f08c392b · pytorch/pytorch</A>
										<DT><A HREF="https://github.com/drisspg/transformer_nuggets/blob/main/transformer_nuggets/utils/benchmark.py">transformer_nuggets/transformer_nuggets/utils/benchmark.py</A>
										<DT><A HREF="https://gist.github.com/malfet/6a17156d7f5663b8b12054a1beff3fe1">Measure performance difference of `torch.mm` vs `torch.bmm`</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/d1f73fd844ba8b84dc1ac581f0c938c6a64d823f/torch/utils/throughput_benchmark.py">pytorch/torch/utils/throughput_benchmark.py</A>
										<DT><A HREF="https://blog.csdn.net/qq_40507857/article/details/118764782">Understanding of model computational capacity (FLOPs) and parameter quantity (Params) in deep learning and summary of four calculation methods</A>
										<DT><A HREF="https://github.com/EleutherAI/cookbook/blob/main/benchmarks/sizing/transformer_flops.py#L139">cookbook/benchmarks/sizing/transformer_flops.py at main · EleutherAI/cookbook</A>
										<DT><A HREF="https://github.com/tgale96/grouped_gemm/pull/14#issuecomment-2211362572">Use CUTLASS for both `trans_a` and `trans_b` on Ampere by dfyz · Pull Request #14 · tgale96/grouped_gemm</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/e9024c691f27a41fabd94617d39d75813b649f26/tinygrad/nn/state.py#L87">tinygrad/tinygrad/nn/state.py: model number of parameters (get_parameters)</A>
										<DT><A HREF="https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964">How to measure time in PyTorch - PyTorch Forums</A>
										<DT><A HREF="https://github.com/cchan/nanoGPT-fp8/blob/master/bench.py">nanoGPT-fp8/bench.py at master · cchan/nanoGPT-fp8</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/utils/benchmark/utils/compare.py#L270">pytorch/torch/utils/benchmark/utils/compare.py at main · pytorch/pytorch</A>
										<DT><A HREF="https://pytorch.org/tutorials/recipes/recipes/benchmark.html">PyTorch Benchmark — PyTorch Tutorials 2.4.0+cu121 documentation</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/4c1dd13ba33d0fcd1f039ea67b026979a09ae00a/benchmarks/transformer/sdp.py#L66">pytorch/benchmarks/transformer/sdp.py: PrettyTable</A>
										<DT><A HREF="https://github.com/facebookresearch/param/blob/main/inference/compute/pt/pytorch_linear.py">param/inference/compute/pt/pytorch_linear.py at main · facebookresearch/param</A>
										<DT><A HREF="https://github.com/drisspg/transformer_nuggets/blob/83f754eb670b73aa789a924b6b9fab67784ca28f/transformer_nuggets/utils/benchmark.py#L60">benchmark_torch_function_in_microseconds</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/f3c54ccf8f6139807f4623037c0174964a286652/torch/_inductor/utils.py#L121">do_bench_using_profiling</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/f3c54ccf8f6139807f4623037c0174964a286652/torch/_inductor/config.py#L682">TORCHINDUCTOR_PROFILE_WITH_DO_BENCH_USING_PROFILING</A>
										<DT><A HREF="http://arthurchiao.art/blog/understanding-gpu-performance/">Understanding NVIDIA GPU Performance: Utilization vs. Saturation (2023)</A>
										<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/utils/benchmark.py">flash-attention/flash_attn/utils/benchmark.py at main · Dao-AILab/flash-attention</A>
										<DT><A HREF="https://github.com/sayakpaul/diffusers-torchao/blob/bade7a6abb1cab9ef44782e6bcfab76d0237ae1f/inference/launch_image_benchmarks.sh#L5">diffusers-torchao/inference/launch_image_benchmarks.sh: Possibl values for each argument -&gt; loop over all combinations</A>
									</DL><p>
									<DT><H3 FOLDED>torch-utils</H3>
									<DL><p>
										<DT><H3 FOLDED>flop_counter</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/27d97b9649f26565c640f8710db740b71e615f0d/torch/utils/flop_counter.py#L597">pytorch/torch/utils/flop_counter.py</A>
											<DT><A HREF="https://gist.github.com/soumith/5f81c3d40d41bb9d08041431c656b233">Horace's flop counter, but with flops metric fixed correctly</A>
											<DT><A HREF="https://github.com/zugexiaodui/torch_flops/tree/dc72fb62934e107987fc3e9cb59d74d32b3910ef?tab=readme-ov-file">zugexiaodui/torch_flops at dc72fb62934e107987fc3e9cb59d74d32b3910ef</A>
											<DT><A HREF="https://github.com/zugexiaodui/torch_flops/tree/main?tab=readme-ov-file">zugexiaodui/torch_flops: A library for calculating the FLOPs in the forward() process based on torch.fx</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/123800">add `FlopCounterMode` documentation · Issue #123800 · pytorch/pytorch</A>
											<DT><A HREF="https://github.com/pytorch/ao/blob/e283743b3cc4612bb641b88dca3670231724d396/torchao/profiler/performance_counter.py">ao/torchao/profiler/performance_counter.py</A>
											<DT><A HREF="https://zhuanlan.zhihu.com/p/663566912">torch_flops: 准确捕获forward中所有算子的FLOPs计算库 - 知乎</A>
											<DT><A HREF="https://github.com/zugexiaodui/torch_flops?tab=readme-ov-file">zugexiaodui/torch_flops: A library for calculating the FLOPs in the forward() process based on torch.fx</A>
										</DL><p>
										<DT><A HREF="https://github.com/lucidrains/pytorch-custom-utils/blob/main/pytorch_custom_utils/total_parameters.py">pytorch-custom-utils/pytorch_custom_utils/total_parameters.py at main · lucidrains/pytorch-custom-utils</A>
										<DT><A HREF="https://github.com/triton-lang/triton/issues/4310">torch.utils.collect_env</A>
										<DT><A HREF="https://github.com/FindHao/ml_scripts">FindHao/ml_scripts: A collection of my own scripts to run pytorch models, debugging pytorch compiler, and filter the results</A>
										<DT><A HREF="https://github.com/FindHao/ml_scripts/blob/main/install_conda_env.sh">ml_scripts/install_conda_env.sh at main · FindHao/ml_scripts</A>
										<DT><A HREF="https://medium.com/the-owl/quick-tips-1-how-to-obtain-environment-information-using-pytorch-a6a8e03f4422">python -m torch.utils.collect_env</A>
									</DL><p>
									<DT><H3 FOLDED>Host-Device Synchronization</H3>
									<DL><p>
										<DT><A HREF="https://github.com/NVIDIA/TransformerEngine/issues/15">Proper benchmarkign with CUDA synchronization NVIDIA/TransformerEngine</A>
										<DT><A HREF="https://discuss.pytorch.org/t/torch-gets-slower-when-upgrading-the-version/186525/4">torch.cuda.symnchronize() before st and stop host timers (et)</A>
										<DT><A HREF="https://discuss.pytorch.org/t/torch-gets-slower-when-upgrading-the-version/186525">Torch gets slower when upgrading the version (good stats)</A>
										<DT><A HREF="https://github.com/Chillee/llm.c/blob/3f232c12f688233ae7949add457fd50192bba867/train_gpt2.py#L394C3-L394C4">llm.c: train_gpt2.py#L394C3-L394C4</A>
									</DL><p>
									<DT><H3 FOLDED>torch-profilling-cuda-events</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.cuda.Event.html#">Event — PyTorch 2.2 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>Warm-Up Steps</H3>
									<DL><p>
										<DT><A HREF="https://github.com/horseee/DeepCache/blob/97813f6406ab71e236bfeb2f8a0e58c6a25b7397/stable_diffusion.py#L29">DeepCache: stable_diffusion.py#L29 (benchmark)</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/78f87d5a0c7d82911a639c397577284868a53c42/server/text_generation_server/models/flash_causal_lm.py#L690">flash_causal_lm.py#L690</A>
									</DL><p>
									<DT><H3 FOLDED>Fixed Clocks</H3>
									<DL><p>
										<DT><A HREF="https://blog.speechmatics.com/cuda-timings">How to Accurately Time CUDA Kernels in Pytorch</A>
										<DT><A HREF="https://github.com/google-deepmind/alphatensor/blob/1949163da3bef7e3eb268a3ac015fd1c2dbfc767/benchmarking/run_gpu_benchmark.py#L60">alphatensor/benchmarking/run_gpu_benchmark.py --lock-gpu-clocks</A>
										<DT><A HREF="https://github.com/openai/triton/blob/8e0c7b425ac149c43183de966ffa423fd46e4762/python/triton/testing.py#L441">Triton testing.py#L441 set_gpu_clock</A>
										<DT><A HREF="https://github.com/openai/triton/blob/8e0c7b425ac149c43183de966ffa423fd46e4762/python/test/regression/test_performance.py#L25">Triton test_performance.py#L25</A>
										<DT><A HREF="https://twitter.com/mike64_t/status/1763239211254030365">RTX 4090 Core and Mem overclock</A>
										<DT><A HREF="https://www.thonking.ai/p/strangely-matrix-multiplications">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</A>
										<DT><A HREF="https://github.com/triton-lang/triton/blob/3c189dd306982e44db607c54195f625941ae16c1/python/triton/testing.py#L461">triton/python/triton/testing.py at 3c189dd306982e44db607c54195f625941ae16c1 · triton-lang/triton</A>
									</DL><p>
									<DT><H3 FOLDED>Cache Flush</H3>
									<DL><p>
										<DT><A HREF="https://github.com/openai/triton/blob/8e0c7b425ac149c43183de966ffa423fd46e4762/python/triton/testing.py#L141">A100 L2 cache 40 MB (Triton testing.py#L141)</A>
									</DL><p>
									<DT><H3 FOLDED>torch-profilling-cuda-graphs</H3>
									<DL><p>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/78f87d5a0c7d82911a639c397577284868a53c42/server/text_generation_server/models/flash_causal_lm.py#L690">flash_causal_lm.py#L690</A>
									</DL><p>
									<DT><H3 FOLDED>kineto</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch-labs/tritonbench/commit/1fb46a37d012ed1a59709afd9735e0bed88e430c#diff-c4ac117ad0a56f447b3f1a1c139113a637f01798caf1408824be2bb13694f957R22">Enable cudagraph mode for kineto_trace (#106) · pytorch-labs/tritonbench@1fb46a3</A>
										<DT><A HREF="https://github.com/pytorch-labs/tritonbench/issues/85">Explain the kineto trace with an example · Issue #85 · pytorch-labs/tritonbench</A>
										<DT><A HREF="https://github.com/pytorch-labs/tritonbench/blob/main/docs/kineto_trace.md">tritonbench/docs/kineto_trace.md at main · pytorch-labs/tritonbench</A>
									</DL><p>
									<DT><A HREF="https://blog.speechmatics.com/cuda-timings">How to Accurately Time CUDA Kernels in Pytorch</A>
									<DT><A HREF="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles</A>
									<DT><A HREF="https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics">CUDA semantics — PyTorch 2.0 documentation</A>
									<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch - YouTube</A>
									<DT><A HREF="https://github.com/quentinf00/article-memory-log?tab=readme-ov-file">quentinf00/article-memory-log</A>
									<DT><A HREF="https://docs.google.com/presentation/d/110dnMW94LX1ySWxu9La17AVUxjgSaQDLOotFC3BZZD4/edit#slide=id.p">Lecture 1 How to profile CUDA kernels in PyTorch</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#codebase-structure">Lecture 1 How to profile CUDA kernels in PyTorch</A>
									<DT><A HREF="https://github.com/pytorch/kineto">pytorch/kineto: A CPU+GPU Profiling library that provides access to timeline traces and hardware performance counters.</A>
									<DT><A HREF="https://www.speechmatics.com/company/articles-and-news/timing-operations-in-pytorch">How to Accurately Time CUDA Kernels in Pytorch</A>
									<DT><A HREF="https://github.com/cloneofsimo/reverse_eng_deepspeed_study/blob/main/day3/run.py">reverse_eng_deepspeed_study/day3/run.py at main</A>
									<DT><A HREF="https://github.com/pytorch/kineto?tab=readme-ov-file">pytorch/kineto: A CPU+GPU Profiling library that provides access to timeline traces and hardware performance counters.</A>
									<DT><A HREF="https://discuss.pytorch.org/t/torch-gets-slower-when-upgrading-the-version/186525/4">torch.cuda.symnchronize() before st and stop host timers (et)</A>
									<DT><A HREF="https://discuss.pytorch.org/t/torch-gets-slower-when-upgrading-the-version/186525">Torch gets slower when upgrading the version (good stats)</A>
									<DT><A HREF="https://twitter.com/neurosp1ke/status/1784239369949159740">CUDA-MODE 16: Profiling</A>
									<DT><A HREF="https://www.youtube.com/watch?v=SKV6kDk1s94">Lecture 16: On Hands Profiling - YouTube</A>
									<DT><A HREF="https://gist.github.com/Chillee/41baf11aac8036d25d637321c48dad20">You Could Have Invented Flash-Attention!</A>
									<DT><A HREF="https://gist.github.com/Chillee/07b36672a0ca2d1280e42b8d10f23174">Compute Flop Utilization in PyTorch</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/9df4bc6a0dc72caccee142555d1668fad1621206/benchmarks/transformer/better_transformer_vs_mha_functional.py#L114">pytorch/benchmarks/transformer/benchmark_torch_function</A>
									<DT><A HREF="https://github.com/tgale96/grouped_gemm/pull/14#issuecomment-2211362572">Use CUTLASS for both `trans_a` and `trans_b` on Ampere by dfyz · Pull Request #14 · tgale96/grouped_gemm</A>
									<DT><A HREF="https://christianjmills.com/posts/cuda-mode-notes/lecture-001/">Christian Mills - CUDA MODE Lecture 1: How to profile CUDA kernels in PyTorch</A>
									<DT><A HREF="https://github.com/drisspg/transformer_nuggets/blob/83f754eb670b73aa789a924b6b9fab67784ca28f/transformer_nuggets/utils/benchmark.py#L60">transformer_nuggets/transformer_nuggets/utils/benchmark.py at 83f754eb670b73aa789a924b6b9fab67784ca28f · drisspg/transformer_nuggets</A>
									<DT><A HREF="https://github.com/search?q=repo%3Apytorch%2Fao%20profiler_runner&type=code">torchao: profiler_runner</A>
									<DT><A HREF="https://github.com/FindHao/TorchExpert/blob/master/test_example.py">TorchExpert/test_example.py at master · FindHao/TorchExpert</A>
									<DT><A HREF="https://colab.research.google.com/drive/1XQwio7DsqB5LP2D574f_uIb8G7KhirNa?usp=sharing#scrollTo=fMsppme9eqnl">PT2-Benchmark - Colab</A>
									<DT><A HREF="https://github.com/pytorch/workshops/blob/master/ASPLOS_2024/profiling.pdf">workshops/ASPLOS_2024/profiling.pdf</A>
									<DT><A HREF="https://github.com/zugexiaodui/torch_flops?tab=readme-ov-file">zugexiaodui/torch_flops: A library for calculating the FLOPs in the forward() process based on torch.fx</A>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/using-nsight-systems-to-profile-gpu-workload/59">Using Nsight Systems to profile GPU workload - hardware-backends / NVIDIA CUDA - PyTorch Developer Mailing List</A>
									<DT><A HREF="https://gist.github.com/mcarilli/376821aa1a7182dfcf59928a7cde3223">Favorite nsight systems profiling commands for Pytorch scripts</A>
									<DT><A HREF="https://pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html">Profiling to understand torch.compile performance — PyTorch 2.6 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>torch-amp</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/docs/stable/amp.html">Automatic Mixed Precision package - torch.amp — PyTorch 2.3 documentation</A>
									<DT><A HREF="https://tspeterkim.github.io/posts/mixed-precision-from-scratch">Mixed Precision Training from Scratch | Taeksang Peter Kim</A>
									<DT><A HREF="https://github.com/tspeterkim/mixed-precision-from-scratch">tspeterkim/mixed-precision-from-scratch: Mixed precision training from scratch with Tensors and CUDA</A>
								</DL><p>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml">native_functions.yaml</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/wiki/Codegen-and-Structured-Kernels">Codegen and Structured Kernels · pytorch/pytorch Wiki</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#codebase-structure">Onboarding to PyTorch Internals (WIKI): Codebase structure</A>
								<DT><A HREF="http://blog.ezyang.com/2019/05/pytorch-internals/">PyTorch internals : ezyang’s blog</A>
								<DT><A HREF="http://blog.ezyang.com/2020/01/vmap-in-haskell/">vmap in Haskell : ezyang’s blog</A>
								<DT><A HREF="https://docs.google.com/spreadsheets/d/e/2PACX-1vQQFW0T_bucT5KZn0BHYTC1KYhkL6ZMG5ZxQWc6UmAkHUDYpqkpzXnsb59uv2TB0Jgc1Q6qO63bx6WQ/pubhtml">Copy of The PyTorch Operator Spreadsheet - Google Drive</A>
								<DT><A HREF="https://docs.google.com/document/d/1tlgPcR2YmC3PcQuYDPUORFmEaBPQEmo8dsh4eUjnlyI/edit#heading=h.unax8xdp403v">PT2 Manifesto - Google Docs</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-find-the-c-cuda-implementation-of-specific-operators-in-pytorch-source-code/1551/3">How to find the c++/cuda implementation of specific operators in pytorch source code - PyTorch Dev Discussions</A>
								<DT><A HREF="https://github.com/ToluClassics/candle-tutorial">Convert PyTorch Models to Candle</A>
								<DT><A HREF="https://github.com/albanD/pytorch_dev_env_setup">albanD/pytorch_dev_env_setup</A>
								<DT><A HREF="https://github.com/albanD/pytorchviz">albanD/pytorchviz: A small package to create visualizations of PyTorch execution graphs</A>
								<DT><A HREF="https://pytorch-dev-podcast.simplecast.com/episodes">Episodes | PyTorch Developer Podcast</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md">pytorch/torch/csrc/jit/docs/serialization.md at main · pytorch/pytorch</A>
								<DT><A HREF="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles | PyTorch</A>
								<DT><A HREF="https://www.youtube.com/watch?v=asWINANITgg">PyTorch composability sync: Full step capture, sub-byte dtypes - YouTube</A>
								<DT><A HREF="https://drive.google.com/file/d/1XBox0G3FI-71efQQjmqGh0-VkCd-AHPL/view">pytorch2_internals.pdf - Google Drive</A>
								<DT><A HREF="https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html">A guide to PyTorch’s CUDA Caching Allocator | Zach’s Blog</A>
								<DT><A HREF="https://www.youtube.com/watch?v=kSOmyARCbyM">PyTorch composability sync</A>
								<DT><A HREF="https://pytorch.org/blog/pytorch-2-paper-tutorial/?utm_content=282093849&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">PyTorch 2 paper and tutorial @ ASPLOS 2024 | PyTorch</A>
								<DT><A HREF="https://pytorch.org/assets/pytorch_2.pdf">PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation</A>
								<DT><A HREF="https://drive.google.com/file/d/1XBox0G3FI-71efQQjmqGh0-VkCd-AHPL/view">pytorch2_internals.pdf</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-find-the-c-cuda-implementation-of-specific-operators-in-pytorch-source-code/1551/3">How to find the c++/cuda implementation of specific operators in pytorch source code</A>
								<DT><A HREF="https://www.youtube.com/watch?v=asWINANITgg">PyTorch composability sync: Full step capture, sub-byte dtypes</A>
								<DT><A HREF="https://pytorch.org/blog/accelerating-generative-ai/">Accelerating Generative AI with PyTorch: Segment Anything, Fast</A>
								<DT><A HREF="https://docs.google.com/document/d/1QTR3t3KdRu5JT1lvAuJLsPd3LruCfv0LecpsgR8eWhg/edit">Composability meeting notes - Google Docs</A>
								<DT><A HREF="https://github.com/lernapparat/torchhacks/blob/main/test/test_lazyload.py">torchhacks/test/test_lazyload.py</A>
								<DT><A HREF="https://pytorch.org/blog/a-tour-of-pytorch-internals-2/">PyTorch Internals Part II - The Build System | PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>torch-research</H3>
							<DL><p>
								<DT><H3 FOLDED>PyTorch Labs</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-labs-fp8</H3>
									<DL><p>
										<DT><A HREF="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization">A Visual Guide to Quantization - by Maarten Grootendorst</A>
										<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
										<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization — PyTorch 2.0 documentation</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/pull/109168">Basic fp8 support in Inductor by ipiszy · Pull Request #109168 · pytorch/pytorch</A>
										<DT><A HREF="https://twitter.com/MSFTDeepSpeed/status/1765923648773525795">DeepSpeed-FP6</A>
										<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024">DeepSpeed/blogs/deepspeed-fp6/03-05-2024 at master · microsoft/DeepSpeed</A>
										<DT><A HREF="https://huggingface.co/docs/transformers/quantization/fbgemm_fp8">FBGEMM FP8</A>
										<DT><A HREF="https://github.com/pytorch/FBGEMM">pytorch/FBGEMM: FB (Facebook) + GEMM (General Matrix-Matrix Multiplication)</A>
										<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
									</DL><p>
									<DT><H3 FOLDED>torchao</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch-labs/ao/issues/47">[RFC] Plans for torchao · Issue #47 · pytorch-labs/ao</A>
										<DT><A HREF="https://github.com/pytorch-labs/ao">pytorch-labs/ao: api's and workflows for quantization and pruning gpu models.</A>
										<DT><A HREF="https://github.com/pytorch-labs/applied-ai">pytorch-labs/applied-ai: Applied AI experiments and examples for PyTorch</A>
										<DT><A HREF="https://pytorch.org/blog/accelerating-generative-ai-2/">Accelerating Generative AI with PyTorch II: GPT, Fast | PyTorch</A>
										<DT><A HREF="https://github.com/pytorch/ao">pytorch/ao: Create and integrate custom data types, layouts and kernels with up to 2x speedups with 65% less VRAM for inference and training</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/layers/torchao_utils.py">sglang/python/sglang/srt/layers/torchao_utils.py at main · sgl-project/sglang</A>
										<DT><A HREF="https://github.com/pytorch/benchmark/blob/main/userbenchmark/torchao/install.py">benchmark/userbenchmark/torchao/install.py</A>
										<DT><A HREF="https://github.com/microsoft/microxcaling?tab=readme-ov-file#Spec-Configuration">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
										<DT><A HREF="https://github.com/mobiusml/gemlite">mobiusml/gemlite: Fast low-bit matmul kernels in Triton</A>
										<DT><A HREF="https://github.com/pytorch/FBGEMM/tree/main/fbgemm_gpu/experimental/gen_ai">FBGEMM/fbgemm_gpu/experimental/gen_ai at main · pytorch/FBGEMM</A>
										<DT><A HREF="https://github.com/pytorch/ao/blob/4d1c7741842a1dfbd479b3481fcdc93c64db703e/torchao/dtypes/floatx/float8_layout.py#L279">ao/torchao/dtypes/floatx/float8_layout.py preprocess_data</A>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch-labs">PyTorch Labs</A>
									<DT><A HREF="https://github.com/pytorch-labs/gpt-fast">pytorch-labs/gpt-fast: Simple and efficient pytorch-native transformer text generation in &lt;1000 LOC of python.</A>
									<DT><A HREF="https://github.com/pytorch-labs/float8_experimental">pytorch-labs/float8_experimental</A>
									<DT><A HREF="https://github.com/pytorch-labs/ao/issues/47">[RFC] Plans for torchao · Issue #47 · pytorch-labs/ao</A>
									<DT><A HREF="https://github.com/pytorch-labs/segment-anything-fast">pytorch-labs/segment-anything-fast: A batched offline inference oriented version of SAM</A>
									<DT><A HREF="https://github.com/pytorch-labs/ao">pytorch-labs/ao: api's and workflows for quantization and pruning gpu models.</A>
									<DT><A HREF="https://github.com/Chillee/lit-llama">Chillee/lit-llama: Simple transformer inference in PyTorch with torch.compile + lit-llama code</A>
									<DT><A HREF="https://github.com/pytorch-labs/torchfix">pytorch-labs/torchfix: TorchFix - a linter for PyTorch-using code with autofix support</A>
									<DT><A HREF="https://github.com/pytorch-labs/applied-ai">pytorch-labs/applied-ai: Applied AI experiments and examples for PyTorch</A>
									<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/de17730a993b1d2cce4fd09e3654b5f79fd23c96/kernels/triton/inference/gptq/a100_qlinear.py#L109">applied-ai: Triton GPTQ a100_qlinear.py (print perf stats)</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-generative-ai-2/">Accelerating Generative AI with PyTorch II: GPT, Fast | PyTorch</A>
								</DL><p>
								<DT><H3 FOLDED>torch-research-facebookexperimental</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
									<DT><A HREF="https://github.com/youkaichao/TRUMPY">youkaichao/TRUMPY: Analyze backward memory usage in pytorch!</A>
									<DT><A HREF="https://github.com/lucidrains/pytorch-custom-utils">lucidrains/pytorch-custom-utils: Just some miscellaneous utility functions / decorators / modules related to Pytorch and Accelerate to help speed up implementation of new AI research</A>
									<DT><A HREF="https://github.com/pytorch-labs/ao">pytorch-labs/ao: The torchao repository contains api's and workflows for quantization and pruning gpu models.</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-triton/?utm_content=278887799&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Triton Dequantization Kernels for GPTQ | PyTorch</A>
									<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
									<DT><A HREF="https://github.com/facebookincubator">Meta Incubator</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
								</DL><p>
								<DT><A HREF="https://github.com/pytorch/tensordict">pytorch/tensordict: TensorDict is a pytorch dedicated tensor container.</A>
								<DT><A HREF="https://github.com/BobMcDear/attorch">BobMcDear/attorch: A subset of PyTorch's neural network modules, written in Python using OpenAI's Triton.</A>
								<DT><A HREF="https://github.com/lernapparat/torchhacks">lernapparat/torchhacks: Hacks for PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>torch-debug</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-debug-tracing</H3>
								<DL><p>
									<DT><A HREF="https://pytorch.org/blog/trace-analysis-for-masses/">PyTorch Trace Analysis for the Masses | PyTorch</A>
									<DT><A HREF="https://pytorch.org/blog/trace-analysis-for-masses/">PyTorch Trace Analysis for the Masses</A>
									<DT><A HREF="https://github.com/facebookincubator/dynolog">facebookincubator/dynolog: Dynolog is a telemetry daemon for performance monitoring and tracing. It exports metrics from different components in the system like the linux kernel, CPU, disks, Intel PT, GPUs etc. Dynolog also integrates with pytorch and can trigger traces for distributed training applications.</A>
									<DT><A HREF="https://pytorch-dev-podcast.simplecast.com/episodes/torch-trace-and-tlparse">TORCH_TRACE and tlparse | PyTorch Developer Podcast</A>
									<DT><A HREF="https://twitter.com/ezyang/status/1777475405642907887">(1) Edward Z. Yang en X: "I spent my airplane ride home polishing up torchdbg on a trace of maskrcnn. Here's the result (no need to collect a trace yourself, this downloads one I pre-baked for you): https://t.co/1E3pVnxO9g" / X</A>
								</DL><p>
								<DT><H3 FOLDED>torch-debug-memory</H3>
								<DL><p>
									<DT><H3 FOLDED>torch.utils.bottleneck</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/docs/main/bottleneck.html">torch.utils.bottleneck — PyTorch main documentation</A>
										<DT><A HREF="https://medium.com/biased-algorithms/mastering-memory-profiling-in-pytorch-40007ced2e46">Mastering Memory Profiling in PyTorch | by Hey Amit | Biased-Algorithms | Nov, 2024 | Medium</A>
									</DL><p>
									<DT><A HREF="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles | PyTorch</A>
									<DT><A HREF="https://zdevito.github.io/2022/12/09/memory-traces.html">Visualizing PyTorch memory usage over time | Zach’s Blog</A>
									<DT><A HREF="https://pytorch.org/docs/stable/torch_cuda_memory.html">Understanding CUDA Memory Usage — PyTorch 2.2 documentation</A>
									<DT><A HREF="https://pytorch.org/memory_viz">https://pytorch.org/memory_viz</A>
									<DT><A HREF="https://zdevito.github.io/2022/08/16/memory-snapshots.html">Debugging PyTorch memory use with snapshots | Zach’s Blog</A>
									<DT><A HREF="https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html">A guide to PyTorch’s CUDA Caching Allocator | Zach’s Blog</A>
									<DT><A HREF="https://github.com/google/zerocopy">google/zerocopy</A>
									<DT><A HREF="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">1. Overview — cuda-binary-utilities (SaSS)</A>
								</DL><p>
								<DT><H3 FOLDED>torch-metrics</H3>
								<DL><p>
									<DT><A HREF="https://github.com/Lightning-AI/torchmetrics">Lightning-AI/torchmetrics: Torchmetrics - Machine learning metrics for distributed, scalable PyTorch applications.</A>
								</DL><p>
								<DT><H3 FOLDED>torch-debu-cuda-kernels</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=tNe-18qVRXg&list=LL&index=163&t=569s">Demystify CUDA Debugging and Performance with Powerful Developer Tools NVIDIA On Demand - YouTube</A>
									<DT><A HREF="https://christianjmills.com/posts/cuda-mode-notes/lecture-001/">Christian Mills - CUDA MODE Lecture 1: How to profile CUDA kernels in PyTorch</A>
								</DL><p>
								<DT><H3 FOLDED>torch-graph</H3>
								<DL><p>
									<DT><A HREF="https://fkong.tech/posts/2023-05-31-torch-graph/">50 lines of code to capture PyTorch forward and reverse computation graphs</A>
									<DT><A HREF="https://github.com/alpha0422/torch-graph/tree/main">alpha0422/torch-graph: Simple PyTorch graph capturing.</A>
								</DL><p>
								<DT><A HREF="https://github.com/zasdfgbnm/TorchSnooper">zasdfgbnm/TorchSnooper: Debug PyTorch code using PySnooper</A>
								<DT><A HREF="https://github.com/xl0/lovely-tensors">xl0/lovely-tensors: Tensors, for human consumption</A>
								<DT><A HREF="https://github.com/graphcore-research/pytorch-tensor-tracker">graphcore-research/pytorch-tensor-tracker: Flexibly track outputs and grad-outputs of torch.nn.Module.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=iSCF-VOJtEw">PyTorch: Debugging session - reference cycle - YouTube</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/torch-compile-can-be-debugged-now/1595">Torch.compile can be debugged now! - compiler - PyTorch Dev Discussions</A>
								<DT><A HREF="https://pytorch.org/blog/understanding-gpu-memory-2/">Understanding GPU Memory 2: Finding and Removing Reference Cycles | PyTorch</A>
								<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch - YouTube</A>
								<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes — Good Coding Style | by Noam Shazeer | Feb, 2024 | Medium</A>
								<DT><A HREF="https://github.com/facebookincubator/dynolog">facebookincubator/dynolog: Dynolog is a telemetry daemon for performance monitoring and tracing. It exports metrics from different components in the system like the linux kernel, CPU, disks, Intel PT, GPUs etc. Dynolog also integrates with pytorch and can trigger traces for distributed training applications.</A>
								<DT><A HREF="https://github.com/ezyang/torchdbg">ezyang/torchdbg: PyTorch centric eager mode debugger</A>
								<DT><A HREF="https://github.com/ezyang/tlparse">ezyang/tlparse: TORCH_LOGS parser for PT2</A>
								<DT><A HREF="https://github.com/google-ai-edge/model-explorer/tree/main">google-ai-edge/model-explorer: A modern model graph visualizer and debugger</A>
								<DT><A HREF="https://github.com/cloneofsimo/reverse_eng_deepspeed_study/blob/main/day3/multiprocessing_pdb/multiprocessing_pdb/multiprocessing_pdb.py">nvoke-pdb-on-a-specific-rank-in-multi-node-training</A>
								<DT><A HREF="https://leimao.github.io/article/How-To-Debug-Deep-Learning-Inference-Applications/">How To Debug Deep Learning Inference Applications - Lei Mao's Log Book</A>
								<DT><A HREF="https://fkong.tech/posts/2023-05-24-torch-source/">Some thoughts on reading PyTorch source code</A>
							</DL><p>
							<DT><H3 FOLDED>torch-inference</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/blog/accelerating-generative-ai/">Accelerating Generative AI with PyTorch: Segment Anything, Fast | PyTorch</A>
								<DT><A HREF="https://twitter.com/PyTorch/status/1725242585667584453">(1) PyTorch en X: "New blog series: Accelerating Generative AI using native PyTorch. 🔥 In this post we talk through new PyTorch performance features from the conference and how they can be used to produce an 8x faster, entirely PyTorch implementation of Segment Anything. https://t.co/o4kc037DN8 https://t.co/QaOs0dIDB9" / X</A>
								<DT><A HREF="https://pytorch.org/blog/pytorch-compile-to-speed-up-inference/">PyTorch compile to speed up inference on Llama 2 | PyTorch</A>
								<DT><A HREF="https://pytorch.org/blog/high-performance-llama-2/?utm_content=270816312&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">High-Performance Llama 2 Training and Inference with PyTorch/XLA on Cloud TPUs | PyTorch</A>
								<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/pull/114001">Introduce 3 low-latency, intra-node allreduce algorithms for small messages to PyTorch by yifuwang · Pull Request #114001 · pytorch/pytorch</A>
							</DL><p>
							<DT><H3 FOLDED>torch-deployment</H3>
							<DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/the-future-of-c-model-deployment/1282">The future of C++ model deployment</A>
							</DL><p>
							<DT><H3 FOLDED>torch-distributed</H3>
							<DL><p>
								<DT><H3 FOLDED>torch-distributed-programming-model</H3>
								<DL><p>
									<DT><A HREF="https://github.com/cloneofsimo/min-fsdp/tree/main/journey/understanding_torch_distributed">min-fsdp/journey/understanding_torch_distributed at main · cloneofsimo/min-fsdp</A>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1861606582234030265">With the Nvidia GB200 NVL72, can a single process launch kernels on all GPUs, CUDA:0 to CUDA:71, or do you still have to start CPU processes on 18 different hosts?</A>
									<DT><A HREF="https://github.com/pytorch/rfcs/pull/71">RFC-0042-torch-distributed-redesign by youkaichao · Pull Request #71 · pytorch/rfcs</A>
									<DT><A HREF="https://github.com/youkaichao/rfcs/blob/master/RFC-0042-torch-distributed-redesign.md">rfcs/RFC-0042-torch-distributed-redesign.md at master · youkaichao/rfcs</A>
									<DT><A HREF="https://x.com/main_horse/status/1868667994995593534">Visualizing 6D Mesh Parallelism</A>
								</DL><p>
								<DT><H3 FOLDED>FSDP</H3>
								<DL><p>
									<DT><H3 FOLDED>fsdp-torch-compile</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/blog/maximizing-training-throughput/?utm_content=293931524&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Maximizing Training Throughput Using PyTorch FSDP and Torch.compile | PyTorch</A>
										<DT><A HREF="https://github.com/ByronHsu/torch-compile-fsdp-error/blob/master/training.py">torch-compile-fsdp-error/training.py at master · ByronHsu/torch-compile-fsdp-error</A>
									</DL><p>
									<DT><H3 FOLDED>fsdp-CUDACachingAllocator</H3>
									<DL><p>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486/1">FSDP &amp; CUDACachingAllocator: an outsider newb perspective - distributed - PyTorch Developer Mailing List</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2304.11277">[2304.11277] PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</A>
									<DT><A HREF="https://engineering.fb.com/2021/07/15/open-source/fsdp/">Fully Sharded Data Parallel: faster AI training with fewer GPUs Engineering at Meta -</A>
									<DT><A HREF="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/">Scaling services with Shard Manager - Engineering at Meta</A>
									<DT><A HREF="https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst">tutorials/intermediate_source/FSDP_tutorial.rst</A>
									<DT><A HREF="https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_adavnced_tutorial.rst">tutorials/intermediate_source/FSDP_adavnced_tutorial.rst</A>
									<DT><A HREF="https://scholar.google.co.uk/citations?view_op=view_citation&hl=en&user=_cHRq1kAAAAJ&citft=1&email_for_op=antonio.jfdominguez%40gmail.com&citation_for_view=_cHRq1kAAAAJ:mVmsd5A6BfQC">PyTorch FSDP: experiences on scaling fully sharded data parallel</A>
									<DT><A HREF="https://www.youtube.com/watch?v=a3iW6Cggccw">Part 4: FSDP Sharding Strategies</A>
									<DT><A HREF="https://www.youtube.com/watch?v=NiL7egqyJEI">[Long Review] Fully Sharded Data Parallel</A>
									<DT><A HREF="https://www.youtube.com/watch?v=By_O0k102PY">How Fully Sharded Data Parallel (FSDP) works</A>
									<DT><A HREF="https://github.com/facebookresearch/llama-recipes/blob/main/docs/multi_gpu.md">llama-recipes/docs/multi_gpu.md</A>
									<DT><A HREF="https://www.youtube.com/watch?v=3XUG7cjte2U">Invited Talk: PyTorch Distributed (DDP, RPC) - By Facebook Research Scientist Shen Li - YouTube</A>
									<DT><A HREF="https://github.com/foundation-model-stack/fms-fsdp">foundation-model-stack/fms-fsdp: 🚀 Efficiently (pre)training foundation models with native PyTorch features, including FSDP for training and SDPA implementation of Flash attention v2.</A>
									<DT><A HREF="https://github.com/pytorch/torchtitan">pytorch/torchtitan: A native PyTorch Library for large model training</A>
									<DT><A HREF="https://github.com/pytorch/torchsnapshot/blob/ce8d7b6d118816313b1da733cdb5a29ec0cd686a/benchmarks/fsdp/main.py#L38">torchsnapshot/benchmarks/fsdp/main.py</A>
									<DT><A HREF="https://sumanthrh.com/post/distributed-and-efficient-finetuning/">Everything about Distributed Training and Efficient Finetuning | Sumanth's Personal Website</A>
									<DT><A HREF="https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff">Training a 1 Trillion Parameter Model With PyTorch Fully Sharded Data Parallel on AWS | by PyTorch | PyTorch | Medium</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Hr2FWHBuNXs">L12b Parallelization -- Instructor: Wilson Yan - YouTube</A>
									<DT><A HREF="https://vitalflux.com/distributed-llm-training-explained-with-examples/">Distributed LLM Training &amp; DDP, FSDP Patterns: Examples</A>
									<DT><A HREF="https://github.com/Aleph-Alpha/scaling">Aleph-Alpha/scaling: Scaling is a distributed training library and installable dependency designed to scale up neural networks, with a dedicated module for training large language models.</A>
									<DT><A HREF="https://www.youtube.com/watch?v=pHFUUnXa6nU">PyTorch Composability Sync - AutoFSDP - YouTube</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/tree/main/torch/distributed/fsdp">pytorch/torch/distributed/fsdp at main · pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>torch-NCCL</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-distributed-multi-node</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html">Multinode Training — PyTorch Tutorials 2.4.0+cu121 documentation</A>
										<DT><A HREF="https://github.com/stas00/ml-engineering/blob/58bdecd9d245d4275b78f38a869631f9f08be168/network/benchmarks/all_reduce_bench.py#L49">ml-engineering/network/benchmarks/all_reduce_bench.py at 58bdecd9d245d4275b78f38a869631f9f08be168 · stas00/ml-engineering</A>
										<DT><A HREF="https://lambdalabs.com/blog/multi-node-pytorch-distributed-training-guide">Multi node PyTorch Distributed Training Guide For People In A Hurry</A>
										<DT><A HREF="https://docs.csc.fi/support/tutorials/ml-multi/">Multi-GPU and multi-node machine learning - Docs CSC</A>
										<DT><A HREF="https://aistudio.google.com/app/prompts/new_chat?pli=1">passwordless SSH setup to connect to the remote nodes and start the processes.</A>
										<DT><A HREF="https://www.ssh.com/academy/ssh/copy-id">What is ssh-copy-id? How ssh-copy-id works?</A>
									</DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/how-to-capture-nccl-communication-ops-in-faketensormode/1410">How to capture NCCL communication ops in FakeTensorMode</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/pull/114001">Introduce 3 low-latency, intra-node allreduce algorithms for small messages to PyTorch by yifuwang · Pull Request #114001 · pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>DTensor</H3>
								<DL><p>
									<DT><H3 FOLDED>Async Tensor Parallelism</H3>
									<DL><p>
										<DT><A HREF="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487">[Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch - distributed / torchtitan - PyTorch Forums</A>
									</DL><p>
									<DT><H3 FOLDED>SP</H3>
									<DL><p>
										<DT><A HREF="https://github.com/facebookresearch/xformers/blob/a99afbbd042ae2ab6cd63d67a7b26e6851da44ba/xformers/ops/sequence_parallel_fused_ops.py">xformers/xformers/ops/sequence_parallel_fused_ops.py: FusedSequenceParallel</A>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md">pytorch/torch/distributed/_tensor/README.md at main</A>
									<DT><A HREF="https://pytorch.org/blog/training-moes/?utm_content=298456196&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Training MoEs at Scale with PyTorch | PyTorch</A>
									<DT><A HREF="https://discuss.pytorch.org/t/distributed-w-torchtitan-training-with-zero-bubble-pipeline-parallelism/214420">[Distributed w/ TorchTitan] Training with Zero-Bubble Pipeline Parallelism - distributed / torchtitan - PyTorch Forums</A>
									<DT><A HREF="https://docs.google.com/document/d/1nFeJ8NSFNhNlCkNgWK31ZGRqm1L9rd0i_XN_RprphaI/edit?tab=t.0#heading=h.s66tp71b0w6g">[RFC] PyTorch DistributedTensor - Google Docs</A>
								</DL><p>
								<DT><H3 FOLDED>DeviceMesh</H3>
								<DL><p>
									<DT><A HREF="https://www.linkedin.com/feed/update/urn:li:ugcPost:7304979416915361792/?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7304979416915361792%2C7305031143198760960%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287305031143198760960%2Curn%3Ali%3AugcPost%3A7304979416915361792%29">Yi Wang: FSDP + TP</A>
									<DT><A HREF="https://www.linkedin.com/in/antferdom/recent-activity/reactions/">(3) Activity | Antonio J. Dominguez | LinkedIn</A>
									<DT><A HREF="https://www.linkedin.com/feed/update/urn:li:activity:7315771292253614080/">A Path to Learn PyTorch Device Mesh for LLM Training</A>
								</DL><p>
								<DT><H3 FOLDED>torch-ddp</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-ddp-multi-node</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-series/multinode.py">examples/distributed/ddp-tutorial-series/multinode.py at main · pytorch/examples</A>
									</DL><p>
									<DT><H3 FOLDED>torch-ddp-examples</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-series/multinode.py">examples/distributed/ddp-tutorial-series/multinode.py at main · pytorch/examples</A>
										<DT><A HREF="https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md">examples/distributed/ddp/README.md at main · pytorch/examples</A>
									</DL><p>
									<DT><A HREF="https://lambdalabs.com/blog/multi-node-pytorch-distributed-training-guide">Multi node PyTorch Distributed Training Guide For People In A Hurry</A>
								</DL><p>
								<DT><H3 FOLDED>torch-fsdp</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-fsdp-examples</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/examples/tree/main/distributed/FSDP">examples/distributed/FSDP at main · pytorch/examples</A>
									</DL><p>
									<DT><H3 FOLDED>FSDP2</H3>
									<DL><p>
										<DT><H3 FOLDED>overlap-compute-comm</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py">pytorch/torch/_inductor/config.py at main · pytorch/pytorch</A>
											<DT><A HREF="https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md">torchtitan/docs/fsdp.md at main · pytorch/torchtitan</A>
											<DT><A HREF="https://pytorch.org/docs/stable/fsdp.html">FullyShardedDataParallel — PyTorch 2.6 documentation</A>
											<DT><A HREF="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487">[Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch - distributed / torchtitan - PyTorch Forums</A>
											<DT><A HREF="https://www.youtube.com/watch?v=NAZdEzcGGJM">Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch - YouTube</A>
										</DL><p>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359">Enabling Float8 All-Gather in FSDP2 - distributed - PyTorch Developer Mailing List</A>
										<DT><A HREF="https://pytorch.org/blog/training-using-float8-fsdp2/?utm_content=317436495&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Supercharging Training using float8 and FSDP2 | PyTorch</A>
									</DL><p>
									<DT><H3 FOLDED>YaFSDP</H3>
									<DL><p>
										<DT><A HREF="https://github.com/yandex/YaFSDP">yandex/YaFSDP: YaFSDP: Yet another Fully Sharded Data Parallel</A>
									</DL><p>
									<DT><A HREF="https://engineering.fb.com/2021/07/15/open-source/fsdp/">Fully Sharded Data Parallel: faster AI training with fewer GPUs Engineering at Meta -</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/49f6ea6dd91c7d069cf6103eb00b1b849f1bdc03/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py#L59">pytorch/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py at 49f6ea6dd91c7d069cf6103eb00b1b849f1bdc03 · pytorch/pytorch</A>
									<DT><A HREF="https://www.youtube.com/watch?v=pHFUUnXa6nU">PyTorch Composability Sync - AutoFSDP - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>torch-distributed-utils</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/nanotron/blob/03d67f2103d5be0dc15ea6022a6cf16d6a633064/src/nanotron/distributed.py#L247">initialize_torch_distributed: torch.distributed utils (nanotron)</A>
								</DL><p>
								<DT><H3 FOLDED>torch-distributed-shampo</H3>
								<DL><p>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1824368214371188848">distributed low-precision shampoo + ZeRO1 only with torch.dist primitive</A>
									<DT><A HREF="https://github.com/cloneofsimo/zeroshampoo">cloneofsimo/zeroshampoo</A>
								</DL><p>
								<DT><H3 FOLDED>torch-distributed-debug</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/69d401d01054a739f6e50ee83e23c14e6de50352/docs/source/distributed.rst#L604">pytorch/docs/source/distributed.rst: torch.distributed.breakpoint(rank=0)</A>
									<DT><A HREF="https://github.com/stas00/ml-engineering/blob/master/debug/pytorch.md#invoke-pdb-on-a-specific-rank-in-multi-node-training">ml-engineering/debug/pytorch.md: torch.distributed.breakpoint() -&gt; up;;n back to normal code</A>
									<DT><A HREF="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/sglang/latency-accelerte-for-weight-updates/readme.md">Awesome-ML-SYS-Tutorial/sglang/latency-accelerte-for-weight-updates/readme.md at main · zhaochenyang20/Awesome-ML-SYS-Tutorial</A>
									<DT><A HREF="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/tree/main/torch-distributed">dist.barrier(device_ids=[0], group=pg)</A>
									<DT><A HREF="https://pytorch.org/tutorials/beginner/hta_intro_tutorial.html">Introduction to Holistic Trace Analysis — PyTorch Tutorials 2.6.0+cu124 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>torch-fault-tolerant</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/nvidia-resiliency-ext">NVIDIA/nvidia-resiliency-ext: NVIDIA Resiliency Extension is a python package for framework developers and users to implement fault-tolerant features. It improves the effective training time by minimizing the downtime due to failures and interruptions.</A>
									<DT><A HREF="https://github.com/pytorch/torchft">pytorch/torchft: PyTorch per step fault tolerance (actively under development)</A>
								</DL><p>
								<DT><H3 FOLDED>torch-distributed-experimental</H3>
								<DL><p>
									<DT><H3 FOLDED>_attention</H3>
									<DL><p>
										<DT><H3 FOLDED>_attention-test-ring-attention</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/ddd0ed1b430f3241490d9bc071036188466b0f22/test/distributed/_tensor/test_attention.py#L49">pytorch/test/distributed/_tensor/test_attention.py at ddd0ed1b430f3241490d9bc071036188466b0f22 · pytorch/pytorch</A>
										</DL><p>
										<DT><H3 FOLDED>_attention-ring-attention</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/3c63e76b03737085e2eb2e7fb7163d7ba16986ba/torch/distributed/tensor/experimental/_attention.py#L358C5-L358C30">pytorch/torch/distributed/tensor/experimental/_attention.py at 3c63e76b03737085e2eb2e7fb7163d7ba16986ba · pytorch/pytorch</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/commit/ddd0ed1b430f3241490d9bc071036188466b0f22">distributed: templated ring attention (#124215) · pytorch/pytorch@ddd0ed1</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/3c63e76b03737085e2eb2e7fb7163d7ba16986ba/torch/distributed/tensor/experimental/_attention.py#L358">pytorch/torch/distributed/tensor/experimental/_attention.py at 3c63e76b03737085e2eb2e7fb7163d7ba16986ba · pytorch/pytorch</A>
										</DL><p>
										<DT><A HREF="https://github.com/xdit-project/xDiT/blob/main/xfuser/model_executor/layers/usp.py">xDiT/xfuser/model_executor/layers/usp.py at main</A>
									</DL><p>
									<DT><H3 FOLDED>torch-communication-overlap</H3>
									<DL><p>
										<DT><A HREF="https://arxiv.org/html/2406.06858v1">Flux: Fast Software-based Communication Overlap on GPUs through Kernel Fusion</A>
										<DT><A HREF="https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359">Enabling Float8 All-Gather in FSDP2 - distributed - PyTorch Developer Mailing List</A>
									</DL><p>
									<DT><A HREF="https://github.com/yifuwang/symm-mem-recipes/blob/main/symm_mem_recipes/utils.py">symm-mem-recipes/symm_mem_recipes/utils.py at main · yifuwang/symm-mem-recipes</A>
								</DL><p>
								<DT><H3 FOLDED>_functional_collectives</H3>
								<DL><p>
									<DT><A HREF="https://github.com/chengzeyi/ParaAttention/blob/main/src/para_attn/primitives.py">ParaAttention/src/para_attn/primitives.py: ft_c</A>
								</DL><p>
								<DT><H3 FOLDED>_tools.runtime_estimator</H3>
								<DL><p>
									<DT><A HREF="https://x.com/typedfemale/status/1849716677560369201?s=12">AutoFSDP: grouping parameters to overlap communication with compute</A>
								</DL><p>
								<DT><H3 FOLDED>torchrun</H3>
								<DL><p>
									<DT><A HREF="https://x.com/apoorvkh/status/1899502812981350655">(2) Apoorv Khandelwal en X: "We made a library (torchrunx) to make multi-GPU / multi-node PyTorch easier, more robust, and more modular! 🧵 https://t.co/EhrHVfMKWQ Docs: https://t.co/ExOfQZDIa5 `(uv) pip install torchrunx` today! (w/ the very talented, Peter Curtin, Brown CS '25)" / X</A>
									<DT><A HREF="https://torchrun.xyz/">torchrunx documentation</A>
									<DT><A HREF="https://github.com/apoorvkh/torchrunx">apoorvkh/torchrunx: Easily run PyTorch on multiple GPUs &amp; machines</A>
								</DL><p>
								<DT><H3 FOLDED>PrimeIntellect</H3>
								<DL><p>
									<DT><A HREF="https://x.com/voooooogel/status/1863101056575832174">LLaMA 3 405B BF16 16xH100 over 100Gbe</A>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1862607165669900407">Releasing INTELLECT-1: We’re open-sourcing the first decentralized trained 10B model:</A>
									<DT><A HREF="https://github.com/PrimeIntellect-ai/prime/blob/main/INTELLECT_1_Technical_Report.pdf">prime/INTELLECT_1_Technical_Report.pdf at main · PrimeIntellect-ai/prime</A>
									<DT><A HREF="https://github.com/PrimeIntellect-ai/prime">PrimeIntellect-ai/prime: prime is a framework for efficient, globally distributed training of AI models over the internet.</A>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1862607175350460775">Technical innovations in PRIME framework:</A>
									<DT><A HREF="https://x.com/PrimeIntellect/status/1862607165669900407">Prime Intellect en X: "Releasing INTELLECT-1: We’re open-sourcing the first decentralized trained 10B model: - INTELLECT-1 base model &amp;amp; intermediate checkpoints - Pre-training dataset - Post-trained instruct models by @arcee_ai - PRIME training framework - Technical paper with all details https://t.co/kSstiU9ZPZ" / X</A>
									<DT><A HREF="https://cacm.acm.org/research/metas-hyperscale-infrastructure-overview-and-insights/">Meta’s Hyperscale Infrastructure: Overview and Insights – Communications of the ACM</A>
									<DT><A HREF="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/#:~:text=,open%20innovation%20across%20the%20industry">Building Meta’s GenAI Infrastructure - Engineering at Meta</A>
									<DT><A HREF="https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e">the world’s largest distributed LLM training job on TPU v5e | Google Cloud Blog</A>
									<DT><A HREF="https://www.usenix.org/system/files/osdi24-choudhury.pdf">MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale</A>
									<DT><A HREF="https://parsa.epfl.ch/course-info/cs723/papers/MSCCLang.pdf">MSCCLang: Microsoft Collective Communication Language</A>
									<DT><A HREF="https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/#:~:text=We%20tested%20NCCL%202,180x%20improvement%20at%2024k%20GPUs">Massively Scale Your Deep Learning Training with NCCL 2.4 | NVIDIA Technical Blog</A>
									<DT><A HREF="https://research.google/pubs/the-datacenter-as-a-computer-an-introduction-to-the-design-of-warehouse-scale-machines-second-edition/">The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, Second Edition</A>
									<DT><A HREF="https://www.youtube.com/watch?v=hobvps-H38o">Dylan Patel - Inference Math, Simulation, and AI Megaclusters - Stanford CS 229S - Autumn 2024 - YouTube</A>
								</DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019">Rethinking Fully Sharded Data Parallel (FSDP) from First Principles</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486">FSDP &amp; CUDACachingAllocator: an outsider newb perspective</A>
								<DT><A HREF="https://github.com/facebookresearch/fairring">facebookresearch/fairring: Fairring (FAIR + Herring) is a plug-in for PyTorch that provides a process group for distributed training that outperforms NCCL at large scales</A>
								<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html">DistributedDataParallel: gradient_as_bucket_view</A>
								<DT><A HREF="https://github.com/pkusys/TGS/blob/main/worker.py">TGS/worker.py at main · pkusys/TGS</A>
								<DT><A HREF="https://github.com/RulinShao/LightSeq/blob/main/lightseq/async_communication.py">LightSeq/lightseq/async_communication.py at main · RulinShao/LightSeq</A>
								<DT><A HREF="https://arxiv.org/pdf/2310.03294">DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training</A>
								<DT><A HREF="https://pytorch.org/docs/stable/elastic/run.html">torchrun (Elastic Launch) — PyTorch 2.4 documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=toUSzwR0EV8&list=LL&index=9&t=12s">Distributed Training with PyTorch: complete tutorial with cloud infrastructure and code - YouTube</A>
								<DT><A HREF="https://arxiv.org/abs/2006.15704">[2006.15704] PyTorch Distributed: Experiences on Accelerating Data Parallel Training</A>
								<DT><A HREF="https://github.com/bytedance/flux">bytedance/flux: A fast communication-overlapping library for tensor parallelism on GPUs.</A>
								<DT><A HREF="https://lambdalabs.com/blog/introduction-multi-gpu-multi-node-distributed-training-nccl-2-0">A Gentle Introduction to Multi GPU and Multi Node Distributed Training</A>
								<DT><A HREF="https://opus.nci.org.au/display/DAE/torchrun%3A+Multi-node+Distributed+Training">FSDP2</A>
								<DT><A HREF="https://www.geeksforgeeks.org/iotop-command-in-linux-with-examples/">iotop Command in Linux with Examples - GeeksforGeeks</A>
								<DT><A HREF="https://github.com/LambdaLabsML/distributed-training-guide">LambdaLabsML/distributed-training-guide: Best practices &amp; guides on how to write distributed pytorch training code</A>
								<DT><A HREF="https://github.com/pytorch-labs/torchft">pytorch-labs/torchft: PyTorch per step fault tolerance (actively under development)</A>
							</DL><p>
							<DT><H3 FOLDED>torch-jax</H3>
							<DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/help-pytorch-brain-understand-jax-flax-code/1554">Help PyTorch brain understand JAX/FLAX code - frontend API / autodiff - PyTorch Dev Discussions</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/help-pytorch-brain-understand-jax-flax-code/1554">Help PyTorch brain understand JAX/FLAX code</A>
								<DT><A HREF="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html">Tutorial 2 (JAX): Introduction to JAX+Flax</A>
							</DL><p>
							<DT><H3 FOLDED>torch-people</H3>
							<DL><p>
								<DT><A HREF="http://blog.ezyang.com/about/">Edward Z. Yang (BLOG)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=_qB2Ho1O3u4&t=7s">Edward Z. Yang (PyTorch Developer Podcast)</A>
								<DT><A HREF="https://pytorch.org/docs/stable/community/persons_of_interest.html">PyTorch Governance | Maintainers — PyTorch 2.1 documentation</A>
								<DT><A HREF="https://github.com/albanD">albanD</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/u/penguinwu/summary">penguinwu</A>
								<DT><A HREF="https://www.linkedin.com/in/peng--wu/">Peng Wu</A>
								<DT><A HREF="https://www.youtube.com/@edwardzyang/videos">Edward Z. Yang's PyTorch and PL - YouTube</A>
								<DT><A HREF="https://jasonansel.com/">jasonansel.com</A>
								<DT><A HREF="https://github.com/Chillee">Horace He (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/natalia-gimelshein-8347a480/">Natalia Gimelshein (OpenAI)</A>
								<DT><A HREF="https://www.linkedin.com/in/animesh-jain-39244417/">Animesh Jain (Meta)</A>
								<DT><A HREF="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiL3s_lt6iEAxXOU6QEHdc8D_gQFnoECBQQAQ&url=https%3A%2F%2Frocketreach.co%2Fmichael-voznesensky-email_1979109&usg=AOvVaw0-C9Hymj48aNDsG0DgWsG3&opi=89978449">Michael Voznesensky (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/bin-bao-9b095812/">Bin Bao (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/david-berard-2a0531176/">David Berard (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/geetachauhan/">Geeta Chauhan (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/anjali-chourdia/">Anjali Chourdia (Meta)</A>
								<DT><A HREF="https://github.com/wconstab">Will Constable (Meta)</A>
								<DT><A HREF="https://cs.stanford.edu/~zdevito/">Zachary DeVito (Meta)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Lg8F4F_qZxk">Elias Ellison (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/yfengus/">Will Feng (Meta)</A>
								<DT><A HREF="https://www.linkedin.com/in/jiong-gong-8504944/?originalSubdomain=cn">Jiong Gong (Intel)</A>
								<DT><A HREF="https://www.linkedin.com/in/philippe-tillet-809b5536/">Phil Tillet (OpenAI)</A>
								<DT><A HREF="https://www.jokeren.tech/">Keren Zhou (OpenAI)</A>
								<DT><A HREF="https://github.com/msaroufim">Mark Saroufim (PyTorch Labs)</A>
								<DT><A HREF="https://github.com/yzh119">Zihao Ye (flashinfer)</A>
								<DT><A HREF="https://github.com/efrantar">Elias Frantar (marlinn kernels)</A>
								<DT><A HREF="https://x.com/isidentical">isidentical: building the most efficient inference engine for diffusion models</A>
								<DT><A HREF="https://chengzeyi.github.io/markdown-cv/">Cheng Zeyi's CV | CV</A>
								<DT><A HREF="https://github.com/drisspg/driss_torch">drisspg/driss_torch: Cuda extensions for PyTorch</A>
								<DT><A HREF="https://github.com/drisspg">drisspg (Driss Guessous)</A>
								<DT><A HREF="https://github.com/zou3519">zou3519 (Richard Zou)</A>
								<DT><A HREF="https://github.com/cpuhrsch">cpuhrsch -&gt; compile graph breaks</A>
							</DL><p>
							<DT><H3 FOLDED>torch-rfcs</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch/rfcs/">pytorch/rfcs: PyTorch RFCs (experimental)</A>
								<DT><A HREF="https://github.com/pytorch/rfcs/pull/59">RFC-0033-GDS-checkpointing by antferdom · Pull Request #59 · pytorch/rfcs</A>
							</DL><p>
							<DT><H3 FOLDED>torch-cuda</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/docs/stable/notes/cuda.html#cuda-semantics">CUDA semantics — PyTorch 2.0 documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=LuhJEEJQgUM">Lecture 1 How to profile CUDA kernels in PyTorch - YouTube</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/custom_kernels/setup.py">text-generation-inference/server/custom_kernels/setup.py at main · huggingface/text-generation-inference</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/custom-cuda-extension-support-in-inductor/1924">Custom cuda extension support in Inductor - compiler - PyTorch Developer Mailing List</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/wiki/CUDA-basics">CUDA basics · pytorch/pytorch Wiki</A>
								<DT><A HREF="https://github.com/drisspg/driss_torch">drisspg/driss_torch: Cuda extensions for PyTorch</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/c3ee07c71cef9f085577c5e53dfe5faf8f2b3b4e/docs/source/notes/cuda.rst#L809">pytorch/docs/source/notes/cuda.rst</A>
								<DT><A HREF="https://pytorch.org/docs/stable/notes/cuda.html">CUDA semantics — PyTorch 2.4 documentation</A>
								<DT><A HREF="https://christianjmills.com/posts/cuda-mode-notes/lecture-004/">Christian Mills - CUDA MODE Lecture 4: Compute and Memory Basics</A>
								<DT><A HREF="https://pytorch.org/blog/cuda-free-inference-for-llms/?utm_content=306418723&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024">CUDA-Free Inference for LLMs | PyTorch</A>
							</DL><p>
							<DT><H3 FOLDED>pybind</H3>
							<DL><p>
								<DT><H3 FOLDED>cpp-cuda-extension</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/francoisfleuret/status/1741481952618676698">(1) François Fleuret en X: "Wow, inline C++ extensions for @pytorch are that simple?</A>
									<DT><A HREF="https://pytorch.org/tutorials/advanced/cpp_extension.html">Custom C++ and CUDA Extensions — PyTorch Tutorials 2.3.0+cu121 documentation</A>
									<DT><A HREF="https://research.colfax-intl.com/tutorial-python-binding-for-cuda-libraries-in-pytorch/">Tutorial: Python bindings for CUDA libraries in PyTorch – Colfax Research</A>
									<DT><A HREF="https://github.com/sophiawisdom/qr/blob/master/file.py">qr/file.py at master · sophiawisdom/qr</A>
									<DT><A HREF="https://github.com/pytorch/extension-cpp">pytorch/extension-cpp: C++ extensions in PyTorch</A>
									<DT><A HREF="https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#testing-an-operator">Custom C++ and CUDA Operators — PyTorch Tutorials 2.4.0+cu121 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>pybind-example</H3>
								<DL><p>
									<DT><A HREF="https://github.com/torstem/demo-cuda-pybind11">torstem/demo-cuda-pybind11: How to use CUDA with Python numpy</A>
									<DT><A HREF="https://github.com/hyhieu/easy_pybind">hyhieu/easy_pybind</A>
								</DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/custom-cuda-extension-support-in-inductor/1924">Custom cuda extension support in Inductor - compiler - PyTorch Developer Mailing List</A>
								<DT><A HREF="https://github.com/youkaichao/compare_pass_data/blob/main/example.cpp">compare_pass_data/example.cpp at main · youkaichao/compare_pass_data</A>
								<DT><A HREF="https://github.com/openai/triton/blob/main/third_party/amd/python/triton_amd.cc">triton/third_party/amd/python/triton_amd.cc at main · openai/triton</A>
								<DT><A HREF="https://github.com/rapidsai/cudf/tree/branch-0.7/python/cudf/bindings">cudf/python/cudf/bindings pxd &amp; pyx cython</A>
								<DT><A HREF="https://pybind11.readthedocs.io/en/stable/">pybind11 documentation</A>
								<DT><A HREF="https://twitter.com/SkyLi0n">Aaron Gokaslan (@SkyLi0n) / X</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-XLSyaJ6m3o&t=902s">WTF is Build.Zig? by Ed Yu - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MUISz2qA640&t=19s">Will Ada Replace C/C++? - YouTube</A>
								<DT><A HREF="https://research.colfax-intl.com/tutorial-python-binding-for-cuda-libraries-in-pytorch/">Tutorial: Python bindings for CUDA libraries in PyTorch – Colfax Research</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/custom_kernels/setup.py">text-generation-inference/server/custom_kernels/setup.py at main · huggingface/text-generation-inference</A>
								<DT><A HREF="https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary">flash-attention/csrc/rotary at main · Dao-AILab/flash-attention</A>
								<DT><A HREF="https://github.com/mlecauchois/micrograd-cuda/blob/main/micrograd_cuda/operations.py">micrograd-cuda/micrograd_cuda/operations.py at main · mlecauchois/micrograd-cuda</A>
								<DT><A HREF="https://github.com/neuralmagic/causal-conv1d/blob/main/setup.py">causal-conv1d/setup.py at main · neuralmagic/causal-conv1d</A>
								<DT><A HREF="https://github.com/FlagOpen/FlagGems">FlagOpen/FlagGems: FlagGems is an operator library for large language models implemented in Triton Language.</A>
								<DT><A HREF="https://github.com/ifromeast/cuda_learning/blob/main/01_cuda_op/setup.py">cuda_learning/01_cuda_op/setup.py at main · ifromeast/cuda_learning · GitHub</A>
								<DT><A HREF="https://github.com/vdesai2014/inference-optimization-blog-post">vdesai2014/inference-optimization-blog-post</A>
								<DT><A HREF="https://github.com/efeslab/Nanoflow/blob/main/pipeline/utils/pybindUtil.py">Nanoflow/pipeline/utils/pybindUtil.py at main · efeslab/Nanoflow</A>
								<DT><A HREF="https://github.com/wjakob/nanobind">wjakob/nanobind: nanobind: tiny and efficient C++/Python bindings</A>
								<DT><A HREF="https://github.com/KnowingNothing/MatmulTutorial/blob/main/cutlass.py/hw_info.py">MatmulTutorial/cutlass.py/hw_info.py at main · KnowingNothing/MatmulTutorial</A>
								<DT><A HREF="https://github.com/drisspg/driss_torch">drisspg/driss_torch: Cuda extensions for PyTorch</A>
								<DT><A HREF="https://github.com/search?q=repo%3ATiledTensor%2FTiledCUDA++language%3APython&type=code">tiledCuda torch bindings</A>
								<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/bdf733be55f0b323a8cf7cc6745a81c3f43cd7f0/hopper/setup.py#L163">flash-attention/hopper/setup.py at bdf733be55f0b323a8cf7cc6745a81c3f43cd7f0 · Dao-AILab/flash-attention</A>
								<DT><A HREF="https://github.com/TiledTensor/benchmarks/blob/master/gemm/cutlass/compile.py">benchmarks/gemm/cutlass/compile.py at master · TiledTensor/benchmarks</A>
								<DT><A HREF="https://github.com/HazyResearch/ThunderKittens/tree/main/kernels/example_bind">ThunderKittens/kernels/example_bind at main · HazyResearch/ThunderKittens</A>
							</DL><p>
							<DT><H3 FOLDED>torch-dataloader</H3>
							<DL><p>
								<DT><H3 FOLDED>spdl</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookresearch/spdl">facebookresearch/spdl: Scalable and Performant Data Loading</A>
									<DT><A HREF="https://facebookresearch.github.io/spdl/main/overview.html">Overview - SPDL 0.0.6 documentation</A>
									<DT><A HREF="https://ai.meta.com/blog/spdl-faster-ai-model-training-with-thread-based-data-loading-reality-labs/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=research">Introducing SPDL: Faster AI model training with thread-based data loading</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=Sk35MKtCXfQ&t=1748s">celeba_iterate</A>
								<DT><A HREF="https://www.youtube.com/watch?v=9Vfauv4ErwA&list=LL&index=7&pp=gAQBiAQB">🤗 Accelerate DataLoaders during Distributed Training: How Do They Work? - YouTube</A>
								<DT><A HREF="https://github.com/cloneofsimo/min-max-gpt/blob/master/run_trainer.py#L328">min-max-gpt/run_trainer.py at master · cloneofsimo/min-max-gpt</A>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1835586720424702387">Benjamin: NoGIL python torch.DataLoader context (Go data fetcher)</A>
								<DT><A HREF="https://developer.nvidia.com/blog/improved-data-loading-with-threads/">Improved Data Loading with Threads | NVIDIA Technical Blog</A>
								<DT><A HREF="https://static.sched.com/hosted_files/pytorch2024/2a/What%20could%20go%20wrong.pdf">Using Iterable Datasets What could go wrong?</A>
								<DT><A HREF="https://colab.research.google.com/github/NicolasHug/Iterable-Datasets-what-could-go-wrong/blob/main/whatcouldgowrong.ipynb">whatcouldgowrong.ipynb - Colab</A>
								<DT><A HREF="https://github.com/Photoroom/datago">Photoroom/datago: A golang-based data loader which can be used from Python. Useful to interact with a typical VectorDB stack through HTTP requests and then fetch payloads per sample, at GB/s speeds.</A>
							</DL><p>
							<DT><H3 FOLDED>torch-optim</H3>
							<DL><p>
								<DT><H3 FOLDED>cross-entropy</H3>
								<DL><p>
									<DT><A HREF="https://github.com/mgmalek/efficient_cross_entropy">mgmalek/efficient_cross_entropy</A>
								</DL><p>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/performance-comparison-between-torch-compile-and-apex-optimizers/2023">Performance Comparison between Torch.Compile and APEX optimizers - compiler - PyTorch Developer Mailing List</A>
								<DT><A HREF="https://github.com/lucidrains/adam-atan2-pytorch">lucidrains/adam-atan2-pytorch: Implementation of the proposed Adam-atan2 from Google Deepmind in Pytorch</A>
							</DL><p>
							<DT><H3 FOLDED>torch-tensorrt</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=eGDMJ3MY4zk&t=450s">Accelerated Inference in PyTorch 2.X with Torch-TensorRT</A>
								<DT><A HREF="https://pytorch.org/TensorRT/ts/getting_started_with_python_api.html#getting-started-with-python-api">Using Torch-TensorRT in Python — Torch-TensorRT v2.2.0.dev0+4da330d documentation</A>
								<DT><A HREF="https://pypi.org/project/torch-tensorrt/">torch-tensorrt · PyPI</A>
								<DT><A HREF="https://pytorch.org/TensorRT/">Torch-TensorRT — Torch-TensorRT v2.2.0.dev</A>
							</DL><p>
							<DT><H3 FOLDED>torch-testing</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch-labs/FACTO/blob/main/inputgen/overview.md">FACTO/inputgen/overview.md at main · pytorch-labs/FACTO</A>
								<DT><A HREF="https://github.com/facebookresearch/xformers/blob/d444815c02227b2eb9ab27ec09112a98e482f19c/xformers/benchmarks/benchmark_mem_eff_attention.py#L201">xformers/xformers/benchmarks/benchmark_mem_eff_attention.py at d444815c02227b2eb9ab27ec09112a98e482f19c · facebookresearch/xformers</A>
								<DT><A HREF="https://github.com/chengzeyi/ParaAttention/blob/16ab3b17c415b80bb757fe48fa6e95d0adda9430/tests/context_parallel/test_diffusers_adapters.py#L44">ParaAttention/tests/context_parallel/test_diffusers_adapters.py at 16ab3b17c415b80bb757fe48fa6e95d0adda9430 · chengzeyi/ParaAttention</A>
							</DL><p>
							<DT><A HREF="https://github.com/chu-tianxiang/llama-cpp-torch">chu-tianxiang/llama-cpp-torch: llama.cpp to PyTorch Converter</A>
							<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
							<DT><A HREF="https://pytorch.org/torchx/latest/">torchx: universal job launcher for PyTorch</A>
							<DT><A HREF="https://dev-discuss.pytorch.org/latest">Latest topics - PyTorch Dev Discussions</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/c22e77abfdb55f8248db852126737fbecfd52a7b/tinygrad/tensor.py">tinygrad/tinygrad/tensor.py at c22e77abfdb55f8248db852126737fbecfd52a7b · tinygrad/tinygrad</A>
							<DT><A HREF="https://gist.github.com/thecharlieblake/82f1b54bbf608d8d339043ed8852cf91">Given a numpy function, prints equivalent PyTorch code (as canonical ATen ops) and returns it as a new function.</A>
							<DT><A HREF="https://github.com/pytorch/torchtitan/issues/305">reload existing llama checkpoints</A>
							<DT><A HREF="https://github.com/SeoLabCornell/torch2chip">SeoLabCornell/torch2chip: Torch2Chip (MLSys, 2024)</A>
							<DT><A HREF="https://x.com/PyTorch/status/1831797748762607618">PyTorch en X: "Looking for new ways to get your PyTorch skills to the next level? Join us at the PyTorch Conference to learn about the latest features and components available in PyTorch. Here are some talks at the PyTorch Conference that will uplevel your PyTorch skills 🧵 (1/6)" / X</A>
						</DL><p>
						<DT><H3 FOLDED>Compilers Fundamentals</H3>
						<DL><p>
							<DT><H3 FOLDED>compiler-grammar</H3>
							<DL><p>
								<DT><H3 FOLDED>BNF</H3>
								<DL><p>
									<DT><A HREF="https://www.icosaedro.it/bnf_chk/">BNF Syntax Checker</A>
									<DT><A HREF="https://bnfplayground.pauliankline.com/">BNF Playground</A>
									<DT><A HREF="http://arran.fi.muni.cz/bnfparser2/bnfweb.php?sf=1&tf=">The BNF Verification Service</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form">Backus–Naur form - Wikipedia</A>
									<DT><A HREF="https://docs.python.org/3/reference/grammar.html">10. Full Grammar specification — Python 3.10.4 documentation</A>
									<DT><A HREF="https://inst.eecs.berkeley.edu/~cs164/sp18/python-grammar.html">Official Python Grammar (Python 2.5)</A>
									<DT><A HREF="https://github.com/python/cpython/blob/3.10/Grammar/python.gram">cpython/python.gram at 3.10 · python/cpython</A>
									<DT><A HREF="https://www.iso.org/standard/26153.html">ISO - ISO/IEC 14977:1996 - Extended BNF</A>
								</DL><p>
								<DT><H3 FOLDED>ANTLR</H3>
								<DL><p>
									<DT><A HREF="https://datacadamia.com/antlr/start">Antlr (ANother Tool for Language Recognition</A>
									<DT><A HREF="https://datacadamia.com/code/compiler/compiler-compiler">Language - Compiler compilers or (lexer|parser) generators</A>
									<DT><A HREF="https://datacadamia.com/antlr/grammar">Antlr - (Grammar|Lexicon) (g4)</A>
									<DT><A HREF="https://github.com/antlr/antlr4/blob/master/doc/grammars.md">Grammar Structure</A>
									<DT><A HREF="https://github.com/antlr/antlr4/blob/master/doc/lexicon.md">Grammar Lexicon</A>
									<DT><A HREF="http://bearcave.com/software/antlr/antlr_examples.html">ANTLR Examples</A>
									<DT><A HREF="https://github.com/antlr/grammars-v4">Grammars written for ANTLR4</A>
									<DT><H3 FOLDED>Indentation</H3>
									<DL><p>
										<DT><A HREF="https://github.com/yshavit/antlr-denter">Helper class for generating python-like INDENT/DEDENT tokens with antlr4.</A>
										<DT><A HREF="https://groups.google.com/g/antlr-discussion/c/Let2Q5gOvGo/m/-ieYBGv9CwAJ">Indentation/whitespace in actions with Python target</A>
										<DT><A HREF="https://github.com/wevre/wry/blob/master/grammars/DentLexer.g4">DentLexer.g4</A>
										<DT><A HREF="http://blog.yuvalshavit.com/2014/02/python-like-indentation-using-antlr4.html">Yuval Shavit: Python-like indentation using Antlr4</A>
										<DT><A HREF="https://docs.python.org/3/reference/lexical_analysis.html#indentation">2. Lexical analysis — Python 3.10.4 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>Functional parsing</H3>
									<DL><p>
										<DT><A HREF="https://github.com/cronburg/antlr-haskell">cronburg/antlr-haskell: A language parsing quasiquoter for Haskell based heavily on ANTLR4.</A>
									</DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/8642154/antlr-what-is-simpliest-way-to-realize-python-like-indent-depending-grammar">lexer - ANTLR What is simpliest way to realize python like indent-depending grammar? - Stack Overflow</A>
								</DL><p>
								<DT><H3 FOLDED>PEG</H3>
								<DL><p>
									<DT><A HREF="https://en.wikipedia.org/wiki/Parsing_expression_grammar">Parsing expression grammar (PEG)</A>
									<DT><A HREF="https://nim-lang.org/docs/pegs.html">PEG syntax and semantics (draft)</A>
								</DL><p>
								<DT><H3 FOLDED>grammar-conversions</H3>
								<DL><p>
									<DT><H3 FOLDED>EBNF to PEG</H3>
									<DL><p>
										<DT><A HREF="http://ceur-ws.org/Vol-928/0324.pdf">‎ceur-ws.org/Vol-928/0324.pdf</A>
										<DT><A HREF="https://stackoverflow.com/questions/53053899/convert-ebnf-grammar-to-peg">php - Convert EBNF grammar to PEG - Stack Overflow</A>
										<DT><A HREF="https://issueantenna.com/repo/dryruby/ebnf">gem ebnf</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://github.com/prql/prql/blob/main/prql-compiler/src/prql.pest">prql/prql.pest at main · prql/prql</A>
								<DT><A HREF="http://essay.utwente.nl/85728/1/vanderWal_BA_FMT.pdf">Rosetta ANTLR: Ultimate Grammar Extractor</A>
								<DT><A HREF="https://stackoverflow.com/questions/8816759/ll-versus-peg-parsers-what-is-the-difference">parsing - LL(*) versus PEG parsers : what is the difference?</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Left_recursion#Removing_left_recursion">Left recursion - Wikipedia</A>
								<DT><A HREF="https://github.com/rust-lang/wg-grammar/tree/master/testdata">test grammar</A>
								<DT><A HREF="https://www.libtrends.info/npm-compare/antlr4-vs-chevrotain-vs-peg-parser-vs-pegjs">antlr4 vs chevrotain vs peg parser vs pegjs comparison - LibTrends</A>
								<DT><A HREF="https://www.boost.org/doc/libs/1_47_0/libs/spirit/doc/html/spirit/abstracts/syntax_diagram.html">Terminal vs non-terminal</A>
								<DT><A HREF="https://news.ycombinator.com/item?id=2644458">Language.js - A fast PEG parser written in JavaScript | Hacker News</A>
								<DT><A HREF="https://www.w3.org/2000/10/swap/grammar/ebnf2turtle.py">Motivation</A>
								<DT><A HREF="https://www.reddit.com/r/rust/comments/cobadh/antlr_grammars_in_rust/">(1) Antlr grammars in Rust : rust</A>
								<DT><A HREF="https://github.com/kaby76/Domemtech.Trash">kaby76/Domemtech.Trash: Toolkit for grammars</A>
							</DL><p>
							<DT><H3 FOLDED>compilers-template-engine</H3>
							<DL><p>
								<DT><H3 FOLDED>Jinja2</H3>
								<DL><p>
									<DT><A HREF="https://sites.google.com/a/chromium.org/dev/developers/jinja">Jinja - The Chromium Projects</A>
									<DT><A HREF="https://jinja.palletsprojects.com/en/3.1.x/">Jinja — Jinja Documentation (3.1.x)</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Template_processor">Template processor - Wikipedia</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>compilers-courses</H3>
							<DL><p>
								<DT><A HREF="https://suif.stanford.edu/~courses/cs243/">CS243 - Advanced Compilers | Winter 2021</A>
								<DT><A HREF="https://ocw.mit.edu/courses/6-172-performance-engineering-of-software-systems-fall-2018/pages/lecture-slides/">Lecture Slides | Performance Engineering of Software Systems</A>
							</DL><p>
							<DT><H3 FOLDED>JIT</H3>
							<DL><p>
								<DT><A HREF="https://kipp.ly/jits-intro/">A Deep Introduction to JIT Compilers: JITs are not very Just-in-time | kipply's blog</A>
								<DT><A HREF="https://kipp.ly/jits-impls/">How JIT Compilers are Implemented and Fast: Pypy, LuaJIT, Graal and More | kipply's blog</A>
							</DL><p>
							<DT><A HREF="https://godbolt.org/">Compiler Explorer</A>
							<DT><A HREF="https://www.kirupa.com/hodgepodge/compiling_transpiling.htm">Compiling (and Transpiling) Explained</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Source-to-source_compiler">Source-to-source compiler - Wikipedia</A>
							<DT><A HREF="https://datacadamia.com/code/compiler/compiler">Translation</A>
							<DT><A HREF="https://datacadamia.com/code/compiler/lexer">Lexical Analysis - Lexer (Tokenizer)</A>
							<DT><A HREF="https://www.youtube.com/watch?v=k6ZsRiLSyvY">Compilation, Libraries and the pesky "unresolved external symbol" error - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=zX-kazAtX0c&t=47s">Calling Functions Across Languages — Richard Feldman - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ZI198eFghJk">Modernizing Compiler Design for Carbon Toolchain - Chandler Carruth - CppNow 2023 - YouTube</A>
							<DT><A HREF="https://twitter.com/chandlerc1024/status/1549411352657133568?lang=en">(1) Chandler Carruth on X: "Really excited we've been able to start sharing our experimental work on #CarbonLang with the wider C++ community. That said, I'd suggest folks read up on the docs and maaaybe wait for our announcement keynote and Q&amp;amp;A from #CppNorth before leaping to too many assumptions. =]" / X</A>
							<DT><A HREF="https://www.youtube.com/watch?v=L6f--pEHJMo">Writing a Programming Language (in Rust) 23: Laurel: Continuing a bash script port</A>
							<DT><A HREF="https://github.com/compiler-explorer/compiler-explorer">compiler-explorer/compiler-explorer: Run compilers interactively from your web browser and interact with the assembly</A>
							<DT><A HREF="https://www.youtube.com/watch?v=MC7qoiJ5uPc">Why is C is so important for compiler developers? - YouTube</A>
							<DT><A HREF="https://github.com/gergo-/missed-optimizations">gergo-/missed-optimizations: Missed optimizations in C compilers</A>
							<DT><A HREF="https://www.youtube.com/watch?v=M6NoMv69sgU">Writing a Compiler and Interpreter in Rust - Part 1 - YouTube</A>
							<DT><A HREF="https://kuterdinel.com/writing-a-very-simple-jit-compiler-in-about-1000-lines-of-c.html">Writing a very simple JIT Compiler in about 1000 lines of C • Kuter Dinel's blog</A>
							<DT><A HREF="https://github.com/davidhalter/parso">davidhalter/parso: A Python Parser</A>
							<DT><A HREF="https://github.com/KnowingNothing/compiler-and-arch">KnowingNothing/compiler-and-arch: A list of tutorials, paper, talks, and open-source projects for emerging compiler and architecture</A>
						</DL><p>
						<DT><H3 FOLDED>Triton</H3>
						<DL><p>
							<DT><H3 FOLDED>triton-examples</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ELS-RD/kernl/blob/main/test/debugger/test_debugger.py">Example: addition</A>
								<DT><A HREF="https://github.com/openai/triton/blob/main/python/triton/ops/blocksparse/matmul.py">Flash-Attention/blocksparse/matmul.py</A>
								<DT><A HREF="https://github.com/lucidrains/triton-transformer">lucidrains/triton-transformer: Implementation of a Transformer, but completely in Triton</A>
								<DT><A HREF="https://kushajveersingh.com/blog/writing-custom-cuda-kernels-with-triton">Writing custom CUDA kernels with Triton</A>
								<DT><A HREF="https://github.com/ELS-RD/kernl/blob/91e2cd92db44d503874d39a9f6dec42c9f481a8e/src/kernl/model_optimization.py#L27">kernl/src/kernl/model_optimization.py</A>
								<DT><A HREF="https://github.com/srush/Triton-Puzzles">srush/Triton-Puzzles: Puzzles for learning Triton</A>
								<DT><A HREF="https://github.com/nikitaved/Intro_to_Triton/blob/main/Intro%20to%20Triton.ipynb">Intro_to_Triton/Intro to Triton.ipynb at main · nikitaved/Intro_to_Triton</A>
								<DT><A HREF="https://x.com/pommedeterre33/status/1681935636129873920">(1) Michaël Benesty en X: "Boosted Llama V2 inference speed by 1.8x using @OpenAI's Triton (one key tech behind GPT-4) at batch=1 FP16 on a 3090 GPU, no quality compromises (-&amp;gt; no quant, etc.) Triton allows efficient custom GPU kernels, letting us merge operations together. Here's how we did + how it works" / X</A>
								<DT><A HREF="https://github.com/ELS-RD/kernl/tree/main/experimental/llama-v2">kernl/experimental/llama-v2 at main · ELS-RD/kernl</A>
								<DT><A HREF="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/OpenAITriton%20MLIR%20%E7%AC%AC%E4%BA%8C%E7%AB%A0%20Batch%20GEMM%20benchmark/#giantpandacv">OpenAITriton MLIR 第二章 Batch GEMM benchmark - GiantPandaCV</A>
								<DT><A HREF="http://giantpandacv.com/project/CUDA/%E3%80%90BBuf%E7%9A%84CUDA%E7%AC%94%E8%AE%B0%E3%80%91%E5%8D%81%E4%BA%94%EF%BC%8COpenAI%20Triton%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%E4%B8%89%20FusedAttention/">FlashAttention V2 (chinese blog)</A>
								<DT><A HREF="https://github.com/RulinShao/LightSeq/blob/main/lightseq/lightseq_async_attn.py#L436">LightSeq/lightseq/lightseq_async_attn.py</A>
								<DT><A HREF="https://github.com/FlagOpen/FlagAttention">FlagOpen/FlagAttention: A collection of memory efficient attention operators implemented in the Triton language.</A>
								<DT><A HREF="https://github.com/pytorch/ao/blob/main/torchao/kernel/intmm_triton.py">ao/torchao/kernel/intmm_triton.py</A>
								<DT><A HREF="https://gist.github.com/malfet/77ed58fdb34681ff094716ae7c085780">Test triton</A>
								<DT><A HREF="https://github.com/sgl-project/sglang/blob/5ff25cdf5b1310e83d9e595142b39ae4d7b561e9/python/sglang/srt/layers/layernorm.py">sglang/python/sglang/srt/layers/layernorm.py at 5ff25cdf5b1310e83d9e595142b39ae4d7b561e9 · sgl-project/sglang</A>
								<DT><A HREF="https://github.com/linkedin/Liger-Kernel/issues/119">[fun] llama.triton · Issue #119 · linkedin/Liger-Kernel</A>
								<DT><A HREF="https://pytorch.org/blog/cuda-free-inference-for-llms/?utm_content=306418723&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024">CUDA-Free Inference for LLMs | PyTorch</A>
								<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/fmha/triton_splitk.py">xformers/xformers/ops/fmha/triton_splitk.py at 0004c67c7e9ec3c9e7b3907db0e0b2957430b35b · facebookresearch/xformers</A>
								<DT><A HREF="https://github.com/lucidrains/triton-transformer/tree/main/triton_transformer">triton-transformer/triton_transformer</A>
								<DT><A HREF="https://isamu-website.medium.com/understanding-triton-tutorials-part-2-f6839ce50ae7">Understanding Triton Tutorials Part 2 | by Isamu Isozaki | Medium</A>
								<DT><A HREF="https://github.com/Jokeren/triton-samples/blob/main/sum-2d.py">triton-samples/sum-2d.py at main · Jokeren/triton-samples</A>
								<DT><A HREF="https://github.com/lessw2020/AdamW-Triton-PyTorch">lessw2020/AdamW-Triton-PyTorch: Can AdamW written in Triton be as performat as fused CUDA impl?</A>
								<DT><A HREF="https://github.com/lessw2020/triton_kernels_for_fun_and_profit">lessw2020/triton_kernels_for_fun_and_profit: Custom kernels in Triton language for accelerating LLMs</A>
							</DL><p>
							<DT><H3 FOLDED>triton-programming-model</H3>
							<DL><p>
								<DT><H3 FOLDED>triton-tutorials</H3>
								<DL><p>
									<DT><A HREF="https://triton-lang.org/main/getting-started/tutorials/index.html">Tutorials — Triton documentation</A>
									<DT><A HREF="https://isamu-website.medium.com/understanding-the-triton-tutorials-part-1-6191b59ba4c">Understanding the Triton Tutorials Part 1 | by Isamu Isozaki | Medium</A>
								</DL><p>
								<DT><A HREF="https://github.com/kshama-msft/triton/blob/f0cf3a2e7a35260a013e639aea558fe1b7befa7b/docs/programming-guide/chapter-1/introduction.rst">triton/docs/programming-guide/chapter-1/introduction.rst</A>
								<DT><A HREF="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8/OpenAITriton%20MLIR%20%E7%AC%AC%E4%BA%8C%E7%AB%A0%20Batch%20GEMM%20benchmark/#giantpandacv">OpenAITriton MLIR 第二章 Batch GEMM benchmark - GiantPandaCV</A>
								<DT><A HREF="https://github.com/kimbochen/md-blogs/tree/main/what-triton-does-in-a-matmul">md-blogs/what-triton-does-in-a-matmul at main</A>
								<DT><A HREF="https://github.com/cuda-mode/lectures/blob/main/lecture_014/A_Practitioners_Guide_to_Triton.ipynb">lectures/lecture_014/A_Practitioners_Guide_to_Triton.ipynb at main · cuda-mode/lectures</A>
								<DT><A HREF="https://github.com/gpu-mode/lectures/blob/main/lecture_014/A_Practitioners_Guide_to_Triton.ipynb">lectures/lecture_014/A_Practitioners_Guide_to_Triton.ipynb at main · gpu-mode/lectures</A>
								<DT><A HREF="https://chatgpt.com/c/66fd7308-78e4-800c-bdc1-6ffafff28651">pid &amp; groups &amp; grid</A>
								<DT><A HREF="https://github.com/alexzhang13/Triton-Puzzles-Solutions">alexzhang13/Triton-Puzzles-Solutions: Personal solutions to the Triton Puzzles</A>
								<DT><A HREF="https://github.com/S-LoRA/S-LoRA/blob/main/slora/models/peft/triton_kernel/lora/lora_prefill.py">x@W</A>
								<DT><A HREF="https://github.com/MekkCyber/TritonAcademy">MekkCyber/TritonAcademy: A repository to unravel the language of GPUs, making their kernel conversations easy to understand</A>
								<DT><A HREF="https://github.com/rkinas/triton-resources?tab=readme-ov-file">rkinas/triton-resources: A curated list of resources for learning and exploring Triton, OpenAI's programming language for writing efficient GPU code.</A>
								<DT><A HREF="https://arxiv.org/pdf/2503.14985">ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming</A>
							</DL><p>
							<DT><H3 FOLDED>triton-kernels</H3>
							<DL><p>
								<DT><H3 FOLDED>triton-layers</H3>
								<DL><p>
									<DT><H3 FOLDED>monkey-patches</H3>
									<DL><p>
										<DT><A HREF="https://github.com/linkedin/Liger-Kernel/blob/main/README.md">Liger-Kernel/README.md at main · linkedin/Liger-Kernel</A>
										<DT><A HREF="https://github.com/linkedin/Liger-Kernel/blob/ce71d59b0b0894f9f3e7512f5a3bf3780c5a1499/test/transformers/test_monkey_patch.py#L179">Liger-Kernel/test/transformers/test_monkey_patch.py at ce71d59b0b0894f9f3e7512f5a3bf3780c5a1499 · linkedin/Liger-Kernel</A>
										<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/bedf8774677315c5eb7e640eca6d7aa15e87775a/flash_attn/layers/patch_embed.py">flash-attention/flash_attn/layers/patch_embed.py at bedf8774677315c5eb7e640eca6d7aa15e87775a · Dao-AILab/flash-attention</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/commit/55842eb81a782da7e522ec0210c3fa1f3f74dc0a">feat: fused_moe fp8 monkey patch (#2174) · sgl-project/sglang@55842eb</A>
									</DL><p>
									<DT><H3 FOLDED>kernel-fallback</H3>
									<DL><p>
										<DT><A HREF="https://github.com/timudk/flux_triton/blob/40ba90d35b97891ff92e3df53effc51b6e71c582/src/flux/modules/layers.py#L17">TRITON_LN = os.getenv("TRITON_LN")</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/blob/4353acb469d46afe3b652928729803492873d0cd/python/sglang/srt/layers/layernorm.py#L25">sglang/python/sglang/srt/layers/layernorm.py: forward_native, forward_cuda</A>
									</DL><p>
									<DT><H3 FOLDED>triton-moe</H3>
									<DL><p>
										<DT><A HREF="https://github.com/triton-lang/triton/pull/6429">[BENCH] added production kernels and micro-benchmark for mixture-of-experts MLP by ptillet · Pull Request #6429 · triton-lang/triton</A>
										<DT><A HREF="https://x.com/typedfemale/status/1911925432120914002">Fp4 experts? GB200 benches? Basic switch transformer style routing?</A>
									</DL><p>
									<DT><A HREF="https://github.com/linkedin/Liger-Kernel">linkedin/Liger-Kernel: Efficient Triton Kernels for LLM Training</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/blob/a7c47e0f028c2a9e67cbc99ab67692ec765d3dd0/python/sglang/srt/layers/activation.py">sglang/python/sglang/srt/layers/activation.py</A>
									<DT><A HREF="https://github.com/KnowingNothing/MatmulTutorial/blob/main/examples/matmul/triton/triton_matmul_sm90.py">MatmulTutorial/examples/matmul/triton/triton_matmul_sm90.py at main · KnowingNothing/MatmulTutorial</A>
									<DT><A HREF="https://github.com/ModelTC/lightllm/blob/main/lightllm/models/llama/triton_kernel/flash_decoding.py">lightllm/lightllm/models/llama/triton_kernel/flash_decoding.py at main · ModelTC/lightllm</A>
									<DT><A HREF="https://github.com/ModelTC/lightllm/blob/main/lightllm/models/llama/triton_kernel/rmsnorm.py">lightllm/lightllm/models/llama/triton_kernel/rmsnorm.py at main · ModelTC/lightllm</A>
									<DT><A HREF="https://github.com/dtunai/triton-activations/blob/main/triton_activations/functions.py#L207">triton-activations/triton_activations/functions.py at main · dtunai/triton-activations</A>
									<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN/blob/528a9301587f5fb135b25d973a87ba0a40a703a7/flashnn/kernel_backend.py#L44">FLASHNN/flashnn/kernel_backend.py at 528a9301587f5fb135b25d973a87ba0a40a703a7 · AlibabaPAI/FLASHNN</A>
									<DT><A HREF="https://github.com/FlagOpen/FlagGems">FlagOpen/FlagGems: FlagGems is an operator library for large language models implemented in Triton Language.</A>
									<DT><A HREF="https://github.com/FlagOpen/FlagGems/blob/master/src/flag_gems/fused/gelu_and_mul.py">FlagGems/src/flag_gems/fused/gelu_and_mul.py at master · FlagOpen/FlagGems</A>
									<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/af314d400663fe895199b0586a9f1f718b1d7b79/flash_attn/ops/triton/linear.py#L131">flash-attention/flash_attn/ops/triton/linear.py at af314d400663fe895199b0586a9f1f718b1d7b79 · Dao-AILab/flash-attention</A>
									<DT><A HREF="https://github.com/Dao-AILab/flash-attention/blob/af314d400663fe895199b0586a9f1f718b1d7b79/flash_attn/ops/triton/mlp.py">flash-attention/flash_attn/ops/triton/mlp.py at af314d400663fe895199b0586a9f1f718b1d7b79 · Dao-AILab/flash-attention</A>
									<DT><A HREF="https://github.com/triton-lang/triton/blob/3ae95a858eac26088102075500e3860864432106/python/test/unit/hopper/test_flashattention.py#L294">triton/python/test/unit/hopper/test_flashattention.py: torch.autograd.Function</A>
									<DT><A HREF="https://github.com/BobMcDear/attorch">BobMcDear/attorch: A subset of PyTorch's neural network modules, written in Python using OpenAI's Triton.</A>
									<DT><A HREF="https://github.com/BobMcDear/attorch/blob/main/attorch/glu_kernels.py">attorch/attorch/glu_kernels.py</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/7efcb5e0ed6403f08965b999626c5807680b46ed/server/text_generation_server/layers/attention/flash_attn_triton.py#L811">text-generation-inference/server/text_generation_server/layers/attention/flash_attn_triton.py at 7efcb5e0ed6403f08965b999626c5807680b46ed · huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/7efcb5e0ed6403f08965b999626c5807680b46ed/server/text_generation_server/layers/gptq/quant_linear.py#L7">text-generation-inference/server/text_generation_server/layers/gptq/quant_linear.py at 7efcb5e0ed6403f08965b999626c5807680b46ed · huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/PygmalionAI/aphrodite-engine/blob/0e0bd02b52a9bd32c0709c47f501abd6c70453ad/aphrodite/_custom_ops.py#L228">aphrodite-engine/aphrodite/_custom_ops.py at 0e0bd02b52a9bd32c0709c47f501abd6c70453ad · PygmalionAI/aphrodite-engine</A>
									<DT><A HREF="https://github.com/ModelTC/lightllm/blob/0d418dcaca3c2090fcae2a87e5411e08e1ecbd42/lightllm/models/gemma_2b/triton_kernel/gelu_and_mul.py#L4">lightllm/lightllm/models/gemma_2b/triton_kernel/gelu_and_mul.py</A>
									<DT><A HREF="https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/rotary_embedding.py#L75">vllm/vllm/model_executor/layers/rotary_embedding.py</A>
									<DT><A HREF="https://github.com/ModelTC/lightllm/blob/452baa708559a1d09c0a8819b69f3fea3cd50d12/lightllm/models/bloom/triton_kernel/layernorm.py#L70">lightllm/lightllm/models/bloom/triton_kernel/layernorm.py</A>
									<DT><A HREF="https://github.com/ModelTC/lightllm/blob/452baa708559a1d09c0a8819b69f3fea3cd50d12/lightllm/models/gemma_2b/triton_kernel/gelu_and_mul.py#L9">lightllm/lightllm/models/gemma_2b/triton_kernel/gelu_and_mul.py</A>
									<DT><A HREF="https://github.com/ModelTC/lightllm/blob/452baa708559a1d09c0a8819b69f3fea3cd50d12/lightllm/models/llama/triton_kernel/silu_and_mul.py#L4">lightllm/lightllm/models/llama/triton_kernel/silu_and_mul.py</A>
									<DT><A HREF="https://arxiv.org/pdf/1910.07467">Root Mean Square Layer Normalization</A>
									<DT><A HREF="https://github.com/triton-lang/triton/blob/main/third_party/proton/tutorials/matmul.py#L189">triton/third_party/proton/tutorials/matmul.py at main · triton-lang/triton</A>
									<DT><A HREF="https://github.com/INT-FlashAttention2024/INT-FlashAttention/blob/main/flash_atten_fp.py">INT-FlashAttention/flash_atten_fp.py at main · INT-FlashAttention2024/INT-FlashAttention</A>
									<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN/blob/528a9301587f5fb135b25d973a87ba0a40a703a7/flashnn/triton_kernels/rotary_embedding.py#L157">FLASHNN/flashnn/triton_kernels/rotary_embedding.py at 528a9301587f5fb135b25d973a87ba0a40a703a7 · AlibabaPAI/FLASHNN</A>
									<DT><A HREF="https://github.com/lianakoleva/no-libtorch-compile/blob/master/triton-aoti.py">no-libtorch-compile/triton-aoti.py at master · lianakoleva/no-libtorch-compile</A>
									<DT><A HREF="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html">Linear/Fully-Connected Layers User's Guide - NVIDIA Docs</A>
									<DT><A HREF="https://github.com/dame-cell/Triformer">dame-cell/Triformer: Transformers components but in Triton</A>
									<DT><A HREF="https://github.com/PaliC/categorize_triton_functions">PaliC/categorize_triton_functions</A>
								</DL><p>
								<DT><H3 FOLDED>Liger-Kernel</H3>
								<DL><p>
									<DT><A HREF="https://github.com/linkedin/Liger-Kernel">linkedin/Liger-Kernel: Efficient Triton Kernels for LLM Training</A>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1827072737673982056">ntroducing Liger-Kernel: Efficient Triton Kernels for LLM Training.</A>
									<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN">AlibabaPAI/FLASHNN</A>
								</DL><p>
								<DT><H3 FOLDED>generative-recommenders</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookresearch/generative-recommenders/tree/main/generative_recommenders/ops/triton">generative-recommenders/generative_recommenders/ops/triton at main · facebookresearch/generative-recommenders</A>
								</DL><p>
								<DT><H3 FOLDED>mirage</H3>
								<DL><p>
									<DT><A HREF="https://github.com/mirage-project/mirage">mirage-project/mirage: Mirage: Automatically Generating Fast GPU Kernels without Programming in Triton/CUDA</A>
								</DL><p>
								<DT><A HREF="https://github.com/Dao-AILab/flash-attention/tree/main/flash_attn/ops/triton">flash-attention/flash_attn/ops/triton at main · Dao-AILab/flash-attention</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index">cuda-mode/triton-index: Cataloging released Triton kernels.</A>
								<DT><A HREF="https://github.com/cuda-mode/triton-index/blob/main/kernel_overview.md">triton-index/kernel_overview.md</A>
								<DT><A HREF="https://github.com/ELS-RD/kernl/blob/main/test/debugger/test_debugger.py">Example: addition</A>
								<DT><A HREF="https://github.com/openai/triton/blob/main/python/triton/ops/blocksparse/matmul.py">Flash-Attention/blocksparse/matmul.py</A>
								<DT><A HREF="https://github.com/ai-compiler-study/kernels/tree/main">ai-compiler-study/kernels</A>
								<DT><A HREF="https://github.com/zinccat/Awesome-Triton-Kernels?tab=readme-ov-file">zinccat/Awesome-Triton-Kernels: Collection of kernels written in Triton language</A>
								<DT><A HREF="https://github.com/mobiusml/gemlite/">mobiusml/gemlite: Simple and fast low-bit matmul kernels in CUDA / Triton</A>
								<DT><A HREF="https://github.com/ModelTC/lightllm/blob/0d418dcaca3c2090fcae2a87e5411e08e1ecbd42/lightllm/models/cohere/triton_kernels/rotary_emb.py#L4">lightllm/lightllm/models/cohere/triton_kernels/rotary_emb.py (rope)</A>
								<DT><A HREF="https://github.com/ModelTC/lightllm/blob/452baa708559a1d09c0a8819b69f3fea3cd50d12/lightllm/models/llama/triton_kernel/rmsnorm.py#L7">lightllm/lightllm/models/llama/triton_kernel/rmsnorm.py</A>
								<DT><A HREF="https://github.com/facebookresearch/xformers/blob/4a9dd7ec079e0c935db10daa2a1a89fd19cfa231/xformers/ops/_triton/tiled_matmul_kernels.py#L150">xformers/xformers/ops/_triton/tiled_matmul_kernels.py</A>
								<DT><A HREF="https://github.com/gpu-mode/triton-index/blob/main/kernel_overview.md">triton-index/kernel_overview.md at main · gpu-mode/triton-index</A>
								<DT><A HREF="https://github.com/Dao-AILab/flash-attention/tree/main/flash_attn/ops">flash-attention/flash_attn/ops at main · Dao-AILab/flash-attention</A>
								<DT><A HREF="https://github.com/facebookresearch/generative-recommenders/blob/main/ops/triton/triton_addmm.py">generative-recommenders/ops/triton/triton_addmm.py at main · facebookresearch/generative-recommenders</A>
							</DL><p>
							<DT><H3 FOLDED>triton-compiler</H3>
							<DL><p>
								<DT><H3 FOLDED>TTIR</H3>
								<DL><p>
									<DT><H3 FOLDED>triton-mlir</H3>
									<DL><p>
										<DT><H3 FOLDED>tritonc</H3>
										<DL><p>
											<DT><A HREF="https://x.com/i/bookmarks?post_id=1833331103131734071">autotuner constants search</A>
										</DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=NZz5sczZ_30&t=16s">Triton Conference 2024: Morning Session - YouTube</A>
										<DT><A HREF="https://github.com/triton-lang/triton/blob/f43badbb0e9810b79477cf4d1087cc8906cf3813/test/TritonGPU/pipeline-loop-nest.mlir">// RUN: triton-opt %s -pass-pipeline='builtin.module(convert-triton-to-tritongpu{num-warps=4 target=cuda:100},tritongpu-coalesce,tritongpu-accelerate-matmul,tritongpu-remove-layout-conversions,tritongpu-optimize-dot-operands,cse,tritongpu-fuse-nested-loops,canonicalize,tritongpu-optimize-accumulator-init,tritongpu-hoist-tmem-alloc,tritongpu-pipeline,canonicalize)' | FileCheck %s --check-prefix=BLACKWELL</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=etlFyqSsmL0">Compiler Tools: Writing an MLIR Pass - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>triton-dtypes</H3>
								<DL><p>
									<DT><H3 FOLDED>triton-fp8</H3>
									<DL><p>
										<DT><A HREF="https://github.com/triton-lang/triton/blob/10f59d8ce04052521c1bc0cb3a3f8b98918fc7e3/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp#L10">triton/lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp at 10f59d8ce04052521c1bc0cb3a3f8b98918fc7e3 · triton-lang/triton</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>triton-hopper</H3>
								<DL><p>
									<DT><H3 FOLDED>triton-warp-specialization</H3>
									<DL><p>
										<DT><A HREF="https://www.linkedin.com/posts/bertrand-maher_automatic-warp-specialization-optimization-activity-7285852014599655424-RV5M/?utm_source=share&utm_medium=member_ios">(2) Post | LinkedIn</A>
										<DT><A HREF="https://github.com/triton-lang/triton/pull/5622">Automatic Warp Specialization Optimization by htyu · Pull Request #5622 · triton-lang/triton</A>
										<DT><A HREF="https://x.com/andrew_n_carr/status/1880098533350732203">(1) Andrew Carr (e/🤸) en X: "Crazy new feature in Triton Basically, Warp specialization in Triton takes advantage of the GPU’s hardware-level concurrency by splitting your kernel into multiple asynchronous tasks (“warp groups”). Each warp group runs in parallel—communicating efficiently via H100’s fast https://t.co/GDORoUuKpP" / X</A>
										<DT><A HREF="https://pytorch.org/blog/warp-specialization/?utm_campaign=4079123-PyTorch%20Blog%20Post%20Promotion&utm_content=324019352&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Enabling advanced GPU features in PyTorch - Warp Specialization | PyTorch</A>
									</DL><p>
									<DT><H3 FOLDED>triton-tma</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/blog/hopper-tma-unit/">Deep Dive on the Hopper TMA Unit for FP8 GEMMs | PyTorch</A>
										<DT><A HREF="https://github.com/triton-lang/triton/blob/main/python/triton/tools/experimental_descriptor.py">triton/python/triton/tools/experimental_descriptor.py: TMA experimental API</A>
										<DT><A HREF="https://github.com/lcy-seso/DLFrameworkTest/pull/43/files#diff-6278ea2f3ba2c5944e0975de46068b9ef1f6451e2c54899d03172264dc3b3f6b">fix: refine and test tma copy. by lcy-seso · Pull Request #43 · lcy-seso/DLFrameworkTest</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>triton-blackwell</H3>
								<DL><p>
									<DT><H3 FOLDED>triton-tmem</H3>
									<DL><p>
										<DT><A HREF="https://github.com/triton-lang/triton/blob/40f3945c6970d372355440a6c635f42932fc6987/python/test/unit/blackwell/test_tmem.py#L8">triton/python/test/unit/blackwell/test_tmem.py</A>
									</DL><p>
									<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72876/">Blackwell Programming for the Masses With OpenAI Triton | GTC 25 2025 | NVIDIA On-Demand</A>
									<DT><A HREF="https://github.com/triton-lang/triton/pull/6429">[BENCH] added production kernels and micro-benchmark for mixture-of-experts MLP by ptillet · Pull Request #6429 · triton-lang/triton</A>
									<DT><A HREF="https://github.com/triton-lang/triton/blob/f43badbb0e9810b79477cf4d1087cc8906cf3813/README.md">triton/README.md: Enabling Blackwell support (pytorch-triton 3.3)</A>
									<DT><A HREF="https://webstorms.github.io/2025/02/06/5080-install.html">Running PyTorch and Triton on the RTX 5080 | Luke’s Blog</A>
									<DT><A HREF="https://www.youtube.com/watch?v=RW2-HtWaOS0">Accelerating the Future: Triton on Blackwell Architecture - YouTube</A>
								</DL><p>
								<DT><A HREF="https://github.com/triton-lang/triton/blob/main/python/triton/tools/compile.py">triton/python/triton/tools/compile.py</A>
								<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN/blob/528a9301587f5fb135b25d973a87ba0a40a703a7/flashnn/triton_kernels/triton_utils.py#L84">FLASHNN/flashnn/triton_kernels/triton_utils.py at 528a9301587f5fb135b25d973a87ba0a40a703a7 · AlibabaPAI/FLASHNN</A>
								<DT><A HREF="https://www.kapilsharma.dev/posts/deep-dive-into-triton-internals/">Deep Dive into Triton Internals (Part 1) | Kapil Sharma</A>
								<DT><A HREF="https://www.kapilsharma.dev/posts/deep-dive-into-triton-internals-2/">Deep Dive into Triton Internals (Part 2) | Kapil Sharma</A>
								<DT><A HREF="https://www.kapilsharma.dev/posts/deep-dive-into-triton-internals-3/">Deep Dive into Triton Internals (Part 3) | Kapil Sharma</A>
								<DT><A HREF="https://www.youtube.com/watch?v=njgow_zaJMw">Lecture 29: Triton Internals - YouTube</A>
								<DT><A HREF="https://pytorch.org/blog/triton-kernel-compilation-stages/?utm_content=314838032&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Triton Kernel Compilation Stages | PyTorch</A>
								<DT><A HREF="https://zhuanlan.zhihu.com/p/11901442836">CUDA-MODE 课程笔记 第29课 Triton内部机制 - 知乎</A>
								<DT><A HREF="https://github.com/triton-lang/triton/blob/40f3945c6970d372355440a6c635f42932fc6987/python/test/unit/blackwell/test_tmem.py#L8">kernel = triton.compile(f.name, target=GPUTarget("cuda", 100, 32))</A>
							</DL><p>
							<DT><H3 FOLDED>triton-backends</H3>
							<DL><p>
								<DT><H3 FOLDED>triton-backends-llvm</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ptillet/triton-llvm-releases">ptillet/triton-llvm-releases</A>
									<DT><A HREF="https://github.com/acollins3/triton-llvm-releases">acollins3/triton-llvm-releases</A>
								</DL><p>
								<DT><H3 FOLDED>triton-backends-ptx</H3>
								<DL><p>
									<DT><H3 FOLDED>triton-blackwell</H3>
									<DL><p>
										<DT><A HREF="https://github.com/triton-lang/triton/pull/5724">Add support for Nvidia Blackwell GPUs by ThomasRaoux · Pull Request #5724 · triton-lang/triton</A>
									</DL><p>
									<DT><A HREF="https://twitter.com/cHHillee/status/1779141387876962469">Triton kernels can be precompiled into .cubin files</A>
								</DL><p>
								<DT><H3 FOLDED>triton-backends-hip</H3>
								<DL><p>
									<DT><A HREF="https://github.com/openai/triton/pull/1983">[ROCM] Core Functionality for AMD by micmelesse · Pull Request #1983 · openai/triton</A>
									<DT><A HREF="https://gemini.google.com/app/ba25862900744324">HIP: clang2py</A>
								</DL><p>
								<DT><H3 FOLDED>triton-cpu</H3>
								<DL><p>
									<DT><A HREF="https://github.com/triton-lang/triton-cpu">triton-lang/triton-cpu: An experimental CPU backend for Triton</A>
									<DT><A HREF="https://github.com/kshama-msft/triton/blob/f0cf3a2e7a35260a013e639aea558fe1b7befa7b/docs/meetups/05-07-2024/notes.md">triton/docs/meetups/05-07-2024/notes</A>
									<DT><A HREF="https://www.youtube.com/watch?v=hgINpebZ7n0">Triton May Community meetup 20240507 - YouTube</A>
									<DT><A HREF="https://drive.google.com/drive/folders/1xPnRO5P59aMVJnXz_o9ASTUgTXK1lhHW">May 2024 meetup - Google Drive</A>
									<DT><A HREF="https://siboehm.com/articles/22/Fast-MMM-on-CPU">Fast Multidimensional Matrix Multiplication on CPU from Scratch</A>
									<DT><A HREF="https://github.com/pytorch-labs/triton-cpu">pytorch-labs/triton-cpu: An experimental CPU backend for Triton (https//github.com/openai/triton)</A>
								</DL><p>
								<DT><H3 FOLDED>triton-backends-xpu</H3>
								<DL><p>
									<DT><A HREF="https://github.com/intel/intel-xpu-backend-for-triton">intel/intel-xpu-backend-for-triton: OpenAI Triton backend for Intel® GPUs</A>
								</DL><p>
								<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/tree/main/src/fastertransformer/triton_backend">FasterTransformer/src/fastertransformer/triton_backend at main · NVIDIA/FasterTransformer</A>
							</DL><p>
							<DT><H3 FOLDED>triton-profiling</H3>
							<DL><p>
								<DT><H3 FOLDED>proton</H3>
								<DL><p>
									<DT><H3 FOLDED>Hatchet</H3>
									<DL><p>
										<DT><A HREF="https://hatchet.readthedocs.io/en/latest/">Hatchet — hatchet 1.4.0 documentation</A>
										<DT><A HREF="https://github.com/hatchet/hatchet">hatchet/hatchet: Analyze graph/hierarchical performance data using pandas dataframes</A>
									</DL><p>
									<DT><H3 FOLDED>proton-examples</H3>
									<DL><p>
										<DT><A HREF="https://github.com/KnowingNothing/MatmulTutorial/blob/4dcfbc970fb6184caf025d9f7c32e1dacf01078d/examples/matmul/triton/triton_matmul_sm90.py#L18">triton_matmul_sm90.py#L18</A>
										<DT><A HREF="https://github.com/ModelTC/lightllm/pull/783/commits/dfec3761cc8bf5ab1b82d55861b00ea9334cf552">DeepseekV3 support deepep, deepgemm, PD, DP TP SP Mix mode. by hiworldwzj · Pull Request #783 · ModelTC/lightllm</A>
									</DL><p>
									<DT><A HREF="https://github.com/kshama-msft/triton/blob/f0cf3a2e7a35260a013e639aea558fe1b7befa7b/docs/meetups/02-20-2024/Proton.pdf">triton/docs/meetups/02-20-2024/Proton.pdf</A>
									<DT><A HREF="https://github.com/openai/triton/tree/main/third_party/proton">triton/third_party/proton</A>
									<DT><A HREF="https://github.com/openai/triton/blob/main/third_party/proton/tutorials/matmul.py">triton/third_party/proton/tutorials/matmul.py at main · openai/triton</A>
									<DT><A HREF="https://www.youtube.com/watch?v=NZz5sczZ_30&t=16s">Triton Conference 2024: Morning Session - YouTube</A>
									<DT><A HREF="https://github.com/triton-lang/triton/tree/main/third_party/proton">triton/third_party/proton at main · triton-lang/triton</A>
									<DT><A HREF="https://drive.google.com/file/d/1a1FxzP9jcu1EZL6w6J0izwBjiVtirlLf/view">proton-interpreter-tutorial.pdf - Google Drive</A>
									<DT><A HREF="https://github.com/Jokeren/triton-samples/blob/main/Triton_Tools_Tutorial.ipynb">triton-samples/Triton_Tools_Tutorial.ipynb at main · Jokeren/triton-samples</A>
									<DT><A HREF="https://github.com/pytorch-labs/tritonbench/commit/87dffcc62bb45dee364a874f649b002f48939b5b">Add proton profiling (#102) · pytorch-labs/tritonbench@87dffcc</A>
								</DL><p>
								<DT><A HREF="https://github.com/pytorch-labs/tritonbench/blob/42da900e856473218e12f07e91ac176752ffa80a/tritonbench/utils/triton_op.py">tritonbench/tritonbench/utils/triton_op.py</A>
							</DL><p>
							<DT><H3 FOLDED>triton-jax</H3>
							<DL><p>
								<DT><A HREF="https://github.com/jax-ml/jax-triton">jax-ml/jax-triton: jax-triton contains integrations between JAX and OpenAI Triton</A>
							</DL><p>
							<DT><H3 FOLDED>triton-community-meetup</H3>
							<DL><p>
								<DT><H3 FOLDED>04-02-2024</H3>
								<DL><p>
									<DT><A HREF="https://drive.google.com/drive/folders/1bKpvz1NiBL_fHrGhMoZPvQfXCeetV2iY">March 2024 meetup - Google Drive</A>
									<DT><A HREF="https://github.com/triton-lang/triton/blob/7c42f6bf02c2583f73eccefb351f17b61bed1dfb/docs/meetups/04-02-2024/notes.md?plain=1#L16">triton/docs/meetups/04-02-2024/notes.md at 7c42f6bf02c2583f73eccefb351f17b61bed1dfb · triton-lang/triton</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=KZAzpKx1ebI">Triton October Community meetup 20231025 - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>triton-people</H3>
							<DL><p>
								<DT><H3 FOLDED>triton-people-openai</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pawelszczerbuk">pawelszczerbuk</A>
									<DT><A HREF="https://github.com/peterbell10">Peter Bell (@peterbell10)</A>
									<DT><A HREF="https://github.com/ptillet">Phil Tillet (@ptillet)</A>
									<DT><A HREF="https://github.com/jeffniu-openai">Jeff Niu (@jeffniu-openai)</A>
									<DT><A HREF="https://github.com/ThomasRaoux">Thomas Raoux (@ThomasRaoux)</A>
								</DL><p>
								<DT><A HREF="https://github.com/manbearian">ian Bearman (MS)</A>
								<DT><A HREF="https://twitter.com/Si_Boehm">(1) Simon Boehm (@Si_Boehm) / X</A>
								<DT><A HREF="https://x.com/nadavrot">(Nadav Rotem: Engineering director at Facebook. Interested in systems, compilers, ML, performance, and other stuff</A>
								<DT><A HREF="https://chengzeyi.github.io/markdown-cv/">Cheng Zeyi's CV | CV</A>
								<DT><A HREF="https://github.com/zhyncs?tab=stars">zhyncs (Yineng Zhang) / Starred (FlagGems, FLASHNN)</A>
								<DT><A HREF="https://github.com/daadaada?tab=stars">daadaada (Da Yan) / Starred</A>
								<DT><A HREF="https://github.com/KnowingNothing?tab=repositories">KnowingNothing (KnowingNothing) ByteDance Matmul</A>
								<DT><A HREF="https://github.com/KuangjuX">KuangjuX (ChengXiang Qi)</A>
								<DT><A HREF="https://github.com/lambda7xx?tab=stars">lambda7xx (Xiao)</A>
								<DT><A HREF="https://github.com/MARD1NO">MARD1NO (ZZK) (giantPandaCV)</A>
								<DT><A HREF="https://github.com/BBuf">BBuf (Xiaoyu Zhang) GiantPandaCV</A>
							</DL><p>
							<DT><H3 FOLDED>triton-debug</H3>
							<DL><p>
								<DT><H3 FOLDED>triton-debug-perf</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/main_horse/status/1742013125090795531">NVIDIA non-industrial: triton.ops.matmul</A>
									<DT><A HREF="https://twitter.com/cis_female/status/1660386176761724928">do_bench</A>
									<DT><A HREF="https://gist.github.com/Chillee/41baf11aac8036d25d637321c48dad20">You Could Have Invented Flash-Attention!</A>
									<DT><A HREF="https://pytorch.org/blog/accelerating-llama3/">Accelerating Llama3 FP8 Inference with Triton Kernels | PyTorch</A>
								</DL><p>
								<DT><H3 FOLDED>triton-interpreter</H3>
								<DL><p>
									<DT><A HREF="https://drive.google.com/drive/folders/1bKpvz1NiBL_fHrGhMoZPvQfXCeetV2iY">Triton Interpreter Update</A>
									<DT><A HREF="https://triton-lang.org/main/programming-guide/chapter-3/debugging.html">Debugging Triton — Triton documentation</A>
								</DL><p>
								<DT><H3 FOLDED>triton-debug-sass</H3>
								<DL><p>
									<DT><A HREF="https://github.com/compiler-explorer/compiler-explorer/pull/5531">Add Triton language by siboehm · Pull Request #5531 · compiler-explorer/compiler-explorer</A>
									<DT><A HREF="https://twitter.com/Si_Boehm/status/1708233662305910785">(1) Simon Boehm en X: "I got Triton working on (local) Godbolt instances, including Python &amp;lt;-&amp;gt; SASS line correspondence. Pretty nifty. Still got some infra issues to figure out, meanwhile PR is here if anyone wants to try it: https://t.co/FPJjgOo3tl https://t.co/U2ie1evR9o" / X</A>
								</DL><p>
								<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/de17730a993b1d2cce4fd09e3654b5f79fd23c96/kernels/triton/inference/gptq/a100_qlinear.py#L8">applied-ai/kernels/triton/inference/gptq/a100_qlinear.py: IR, TTGIR, PTX, registers users</A>
								<DT><A HREF="https://github.com/openai/triton/issues/517#issuecomment-2028547176">How to debug kernels · Issue #517 TRITON_INTERPRET=1</A>
								<DT><A HREF="https://github.com/openai/triton/blob/8e0c7b425ac149c43183de966ffa423fd46e4762/python/triton/testing.py#L441">triton/python/triton/testing.py (main)</A>
								<DT><A HREF="https://github.com/Deep-Learning-Profiling-Tools/triton-viz">Deep-Learning-Profiling-Tools/triton-viz</A>
								<DT><A HREF="https://chat.openai.com/c/74ad9a19-e6f0-4ba3-a0d7-0d7a0b6c5c61">Performance Logging &amp; Saving Kernel ASM &amp; IR</A>
								<DT><A HREF="https://triton-lang.org/main/programming-guide/chapter-3/debugging.html">Debugging Triton — Triton documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NZz5sczZ_30&t=16s">Triton Conference 2024: Morning Session - YouTube</A>
								<DT><A HREF="https://github.com/msaroufim/openaitritontutorial/blob/master/Down%20the%20openai%20triton%20rabbit%20hole.ipynb">openaitritontutorial/Down the openai triton rabbit hole.ipynb at master · msaroufim/openaitritontutorial</A>
							</DL><p>
							<DT><H3 FOLDED>triton-benchmark</H3>
							<DL><p>
								<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN/blob/main/benchmark/benchmark_flash_attn.py#L78">FLASHNN/benchmark/benchmark_flash_attn.py at main · AlibabaPAI/FLASHNN</A>
								<DT><A HREF="https://github.com/timudk/flux_triton/blob/40ba90d35b97891ff92e3df53effc51b6e71c582/src/profiling/profile_triton_comparison.ipynb">flux_triton/src/profiling/profile_triton_comparison.ipynb</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/7efcb5e0ed6403f08965b999626c5807680b46ed/server/text_generation_server/layers/gptq/custom_autotune.py#L5">text-generation-inference/server/text_generation_server/layers/gptq/custom_autotune.py at 7efcb5e0ed6403f08965b999626c5807680b46ed · huggingface/text-generation-inference</A>
								<DT><A HREF="https://github.com/linkedin/Liger-Kernel/issues/137">Benchmark against Flash attention repo · Issue #137 · linkedin/Liger-Kernel</A>
								<DT><A HREF="https://gist.github.com/cloneofsimo/af610ff8aa11a3f57956e7d7f578409c">FlashAttention comparison</A>
								<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/main/kernels/needs_perf_help/fp8_gemm_bench.py">applied-ai/kernels/needs_perf_help/fp8_gemm_bench.py at main · pytorch-labs/applied-ai</A>
								<DT><A HREF="https://github.com/INT-FlashAttention2024/INT-FlashAttention/blob/main/benchmark.py">INT-FlashAttention/benchmark.py at main · INT-FlashAttention2024/INT-FlashAttention</A>
								<DT><A HREF="https://github.com/pytorch-labs/applied-ai/blob/main/kernels/triton/inference/gptq/a100_qlinear.py">applied-ai/kernels/triton/inference/gptq/a100_qlinear.py: TTGIR, PTX</A>
								<DT><A HREF="https://github.com/lessw2020/triton_kernels_for_fun_and_profit/blob/main/h100/profile_kernels.py">triton_kernels_for_fun_and_profit/h100/profile_kernels.py at main · lessw2020/triton_kernels_for_fun_and_profit</A>
							</DL><p>
							<DT><H3 FOLDED>triton-torch-inductor</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch/pytorch/pull/111434">[Inductor] Support user defined triton kernels in inductor</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ACR1WnRScCc">Composability Sync - User defined Triton vs custom ops / C++</A>
								<DT><A HREF="https://github.com/BobMcDear/attorch">BobMcDear/attorch: A subset of PyTorch's neural network modules, written in Python using OpenAI's Triton.</A>
								<DT><A HREF="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html">Using User-Defined Triton Kernels with torch.compile — PyTorch Tutorials 2.3.0+cu121 documentation</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/issues/121367">`torch.compile` makes triton kernel slower in some cases · Issue #121367 · pytorch/pytorch</A>
							</DL><p>
							<DT><H3 FOLDED>triton-config</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/xformers/blob/main/xformers/ops/_triton/tiled_matmul_kernels.py">xformers/xformers/ops/_triton/tiled_matmul_kernels.py gen_config</A>
								<DT><A HREF="https://github.com/IBM/triton-dejavu/tree/main">IBM/triton-dejavu: Framework to reduce autotune overhead to zero for well known deployments.</A>
							</DL><p>
							<DT><H3 FOLDED>triton-utils</H3>
							<DL><p>
								<DT><A HREF="https://github.com/gpu-mode/lectures/blob/main/lecture_014/triton_util.py">lectures/lecture_014/triton_util.py at main · gpu-mode/lectures</A>
								<DT><A HREF="https://github.com/IBM/triton-dejavu">IBM/triton-dejavu: Framework to reduce autotune overhead to zero for well known deployments.</A>
								<DT><A HREF="https://github.com/UmerHA/triton_util">UmerHA/triton_util: Make triton easier</A>
							</DL><p>
							<DT><H3 FOLDED>triton-test</H3>
							<DL><p>
								<DT><A HREF="https://github.com/linkedin/Liger-Kernel/blob/63dd41b15e9f1c2957c817b771536d4ab7119322/test/transformers/test_rms_norm.py#L72">triton kernel correctness testing</A>
								<DT><A HREF="https://github.com/sgl-project/sglang/blob/main/test/srt/test_triton_attention_kernels.py">sglang/test/srt/test_triton_attention_kernels.py</A>
								<DT><A HREF="https://github.com/sgl-project/sglang/blob/main/python/sglang/test/test_activation.py">sglang/python/sglang/test/test_activation.py</A>
								<DT><A HREF="https://github.com/lancerts/Algo-ML-Kernels/blob/main/triton_load_from_uninitialized_data/test.py">Algo-ML-Kernels/triton_load_from_uninitialized_data/test.py at main · lancerts/Algo-ML-Kernels</A>
								<DT><A HREF="https://github.com/INT-FlashAttention2024/INT-FlashAttention/blob/main/benchmark.py">INT-FlashAttention/benchmark.py at main · INT-FlashAttention2024/INT-FlashAttention</A>
							</DL><p>
							<DT><H3 FOLDED>triton-collective-communication</H3>
							<DL><p>
								<DT><H3 FOLDED>triton-distributed</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ByteDance-Seed/Triton-distributed">ByteDance-Seed/Triton-distributed: Distributed Triton for Parallel Systems</A>
								</DL><p>
								<DT><A HREF="https://github.com/yifuwang/symm-mem-recipes">yifuwang/symm-mem-recipes</A>
								<DT><A HREF="https://github.com/cchan/tccl">cchan/tccl: extensible collectives library in triton</A>
								<DT><A HREF="https://github.com/ppl-ai/pplx-kernels">ppl-ai/pplx-kernels: Perplexity GPU Kernels</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=yCyZEJrlrfY&t=126s">Lightning Talk: Harnessing NVIDIA Tensor Cores: An Exploration of CUTLASS &amp; OpenAI</A>
							<DT><A HREF="https://openai.com/research/triton">Introducing Triton: Open-source GPU programming for neural networks</A>
							<DT><A HREF="https://github.com/microsoft/triton-shared">microsoft/triton-shared: Shared Middle-Layer for Triton Compilation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GHQ1M3VDOmU&t=8s">Intro to Triton: A Parallel Programming Compiler and Language (esp for AI acceleration) - YouTube</A>
							<DT><A HREF="https://github.com/ptillet/triton-llvm-releases">ptillet/triton-llvm-releases</A>
							<DT><A HREF="https://pytorch.org/blog/accelerating-triton/?utm_content=278887799&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">Accelerating Triton Dequantization Kernels for GPTQ | PyTorch</A>
							<DT><A HREF="https://github.com/pytorch/pytorch/pull/111434">[Inductor] Support user defined triton kernels in inductor by oulgen · Pull Request #111434 · pytorch/pytorch</A>
							<DT><A HREF="https://github.com/kakaobrain/trident">kakaobrain/trident: A performance library for machine learning applications.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=yCyZEJrlrfY&t=126s">NVIDIA Tensor Cores: An Exploration of CUTLASS &amp; OpenAI</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GHQ1M3VDOmU&t=8s">Intro to Triton: A Parallel Programming Compiler and Language</A>
							<DT><A HREF="https://github.com/kakaobrain/trident">kakaobrain/trident: A performance library for machine learning</A>
							<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/">GEMM kernels Hopper</A>
							<DT><A HREF="https://github.com/srush/Triton-Puzzles">srush/Triton-Puzzles: Puzzles for learning Triton</A>
							<DT><A HREF="https://www.youtube.com/watch?v=GHQ1M3VDOmU&t=8s">Intro to Triton: A Parallel Programming Compiler and Language (esp for AI acceleration)</A>
							<DT><A HREF="https://www.jokeren.tech/slides/triton_next.pdf">Towards Agile Development of Efficient Deep Learning Operators</A>
							<DT><A HREF="https://github.com/gfvvz/Triton-Compiler">gfvvz/Triton-Compiler: Triton Compiler related materials.</A>
							<DT><A HREF="https://x.com/cHHillee/status/1558668469335248896">(1) Horace He en X: "Some addendums: I couldn't figure out a nice way to fit this into the tweet thread, but it would be remiss of me to not mention CUTLASS (https://t.co/7wkLAL18Qi). Just like Triton, CUTLASS is another way to write highly performant matmuls in a white-box manner." / X</A>
							<DT><A HREF="https://fkong.tech/posts/2023-04-23-triton-cuda/">Demystify OpenAI Triton · fkong' tech blog</A>
							<DT><A HREF="https://www.youtube.com/watch?v=NZz5sczZ_30">Triton Conference 2024: Morning Session - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ONrKkI7KhU4">Triton Conference 2024: Afternoon Session - YouTube</A>
							<DT><A HREF="https://github.com/sgl-project/sglang/pull/2913">tl.gather</A>
						</DL><p>
						<DT><H3 FOLDED>JAX</H3>
						<DL><p>
							<DT><H3 FOLDED>jax-examples</H3>
							<DL><p>
								<DT><A HREF="https://github.com/yixiaoer/tpu-training-example">yixiaoer/tpu-training-example</A>
							</DL><p>
							<DT><H3 FOLDED>jax-profilling</H3>
							<DL><p>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/profiling.html">profiling Jax programs</A>
							</DL><p>
							<DT><H3 FOLDED>Pallas</H3>
							<DL><p>
								<DT><H3 FOLDED>pallas-people</H3>
								<DL><p>
									<DT><A HREF="https://sharadvikram.com/">Sharad Vikram</A>
									<DT><A HREF="https://github.com/sharadmv">sharadmv (Sharad Vikram)</A>
									<DT><A HREF="https://x.com/apaszke/status/1812897008031617493">(1) Adam Paszke en X: "Many of you are excited about H100 attention, so it’s a good time to show you Mosaic GPU: a Python DSL for H100s. The attention example matches FA3 performance, while being only ~200 lines of Python: https://t.co/12ecz3LftV It's easy to install too! Latest JAX packages have it." / X</A>
								</DL><p>
								<DT><H3 FOLDED>mosaic</H3>
								<DL><p>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1812897008031617493">Mosaic GPU: A Python DSL for H100</A>
									<DT><A HREF="https://www.youtube.com/watch?v=liKrhX2gm44">High Performance LLMs in Jax 2024 -- Session 10 - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>pallas-matmul</H3>
								<DL><p>
									<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html">Matrix Multiplication — JAX documentation</A>
								</DL><p>
								<DT><A HREF="https://github.com/google/jax/pull/17328">feat(pallas): Optimize Pallas Attention + Benchmark by jon-chuang · Pull Request #17328 · google/jax</A>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/_images/pallas_flow.png">pallas_flow.png 908×832 pixels</A>
								<DT><A HREF="https://bnikolic.co.uk/blog/python/jax/2020/10/20/jax-outputgraph.html">Jax: Visualising the computational graph of a jax program | B. Nikolic Software and Computing Blog</A>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/design.html">Pallas Design — JAX documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5ilr4gcenaA">(Google) JAX: Low-level control with shard_map and Pallas - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NFKubflDb1A">Pallas + Mosaic by Example (MAIN)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NFKubflDb1A">TPU Memory Model</A>
								<DT><A HREF="https://github.com/google/jax/pull/20462/">Add MegaBlox grouped matrix multiplication kernels for TPU. by copybara-service[bot] · Pull Request #20462 · google/jax</A>
								<DT><A HREF="https://github.com/LaurentMazare/jax-flash-attn3">LaurentMazare/jax-flash-attn3: JAX bindings for the flash-attention3 kernels</A>
								<DT><A HREF="https://x.com/sharadvikram/status/1822032380657594836">(1) Sharad Vikram en X: "We now have a guide to writing distributed communication on TPU using Pallas, written by @JustinFu769512! https://t.co/9TcqNizGV4 Overlapping comms + compute is a crucial performance optimization for large scale ML. Write your own custom overlapped kernels in Python! https://t.co/DABMQkI1Q5" / X</A>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/tpu/distributed.html">Distributed Computing in Pallas for TPUs — JAX documentation</A>
							</DL><p>
							<DT><H3 FOLDED>jax-triton</H3>
							<DL><p>
								<DT><A HREF="https://github.com/jax-ml/jax-triton">jax-ml/jax-triton: integrations between JAX and OpenAI Triton</A>
							</DL><p>
							<DT><H3 FOLDED>pjit</H3>
							<DL><p>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html?ref=blog.salesforceairesearch.com">Introduction to pjit — JAX documentation</A>
								<DT><A HREF="https://github.com/ayaka14732/einshard?tab=readme-ov-file">ayaka14732/einshard: High-level array sharding API for JAX</A>
							</DL><p>
							<DT><H3 FOLDED>Flax</H3>
							<DL><p>
								<DT><A HREF="https://github.com/skye/flax_bert">skye/flax_bert</A>
							</DL><p>
							<DT><H3 FOLDED>Haliax</H3>
							<DL><p>
								<DT><A HREF="https://github.com/stanford-crfm/haliax">stanford-crfm/haliax: Named Tensors for Legible Deep Learning in JAX</A>
							</DL><p>
							<DT><H3 FOLDED>jax-transformer</H3>
							<DL><p>
								<DT><A HREF="https://github.com/joschu/jax-exp/blob/master/jax_transformer.py#L96">John Schulman: jax-exp/jax_transformer.py at master</A>
								<DT><A HREF="https://github.com/xjdr-alt/simple_transformer/blob/main/simple_transformer.py">simple_transformer/simple_transformer.py at main · xjdr-alt/simple_transformer</A>
								<DT><A HREF="https://github.com/awf/functional-transformer">functional-transformer: A pure-functional implementation of a machine learning transformer model in Python/JAX</A>
								<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x</A>
								<DT><A HREF="https://twitter.com/LiamFedus/status/1536791574612303872">Switch Transformer models in T5X/JAX (1.6T param)</A>
								<DT><A HREF="https://github.com/yixiaoer/mistral-v0.2-jax">yixiaoer/mistral-v0.2-jax: JAX implementation of the Mistral 7b v0.2 model</A>
								<DT><A HREF="https://github.com/google-deepmind/nanodo/tree/main">google-deepmind/nanodo</A>
							</DL><p>
							<DT><H3 FOLDED>jax-graphs</H3>
							<DL><p>
								<DT><A HREF="https://github.com/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb">educational/intro_to_graph_nets_tutorial_with_jraph.ipynb at master · deepmind/educational</A>
								<DT><A HREF="https://github.com/deepmind/jraph">deepmind/jraph: A Graph Neural Network Library in Jax</A>
							</DL><p>
							<DT><H3 FOLDED>jax-torch</H3>
							<DL><p>
								<DT><A HREF="https://sjmielke.com/jax-purify.htm">From PyTorch to JAX: towards neural net frameworks that purify stateful code</A>
								<DT><A HREF="https://sjmielke.com/jax-purify.htm">From PyTorch to JAX</A>
							</DL><p>
							<DT><H3 FOLDED>jax-lectures</H3>
							<DL><p>
								<DT><A HREF="https://afmck.in/posts/2023-05-22-jax-post/">On Learning JAX</A>
							</DL><p>
							<DT><H3 FOLDED>jax-lax-scan</H3>
							<DL><p>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html">jax.lax.scan — JAX documentation</A>
							</DL><p>
							<DT><H3 FOLDED>jax-algorithms</H3>
							<DL><p>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/jax.lax.html">jax.lax module — JAX documentation</A>
								<DT><A HREF="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.iota.html">jax.lax.iota — JAX documentation</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=e85Ceq2g5z0&t=1s">(Day 1 - Breakout Session) StableHLO &amp; PJRT - YouTube</A>
							<DT><A HREF="https://github.com/jax-ml/jax-triton">jax-ml/jax-triton: jax-triton contains integrations between JAX and OpenAI Triton</A>
							<DT><A HREF="https://github.com/n2cholas/awesome-jax">n2cholas/awesome-jax: JAX - A curated list of resources</A>
							<DT><A HREF="https://github.com/skye/gpu_jaxlib_docker_image">skye/gpu_jaxlib_docker_image</A>
							<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox">NVIDIA/JAX-Toolbox: JAX-Toolbox</A>
							<DT><A HREF="https://www.youtube.com/watch?v=NlQ1N3W3Wms&t=40s">JAX.lax.scan tutorial (for autoregressive rollout) - YouTube</A>
							<DT><A HREF="https://jax.readthedocs.io/en/latest/autodidax.html">Autodidax: JAX core from scratch — JAX documentation</A>
							<DT><A HREF="https://research.google/pubs/pub46196/">A Computational Model for TensorFlow (An Introduction)</A>
							<DT><A HREF="https://github.com/google/jax/blob/967c38d53d632be713cde1f4caadb3e388b51f37/jax/_src/nn/functions.py#L559">jax/jax/_src/nn/functions.py OPS</A>
							<DT><A HREF="https://x.com/_xjdr/status/1801063628298412239">Megablox, splash attention, pallas and automatically sharded named axis come free and built in with Jax and y'all are still using pytorch in production?!?!" / X</A>
							<DT><A HREF="https://jax.readthedocs.io/en/latest/autodidax.html#part-2-jaxprs">Autodidax: JAX core from scratch — JAX documentation</A>
							<DT><A HREF="https://github.com/zhuzilin/aqt-pytorch/blob/main/aqt/int8_linear.py">aqt-pytorch/aqt/int8_linear.py at main · zhuzilin/aqt-pytorch</A>
							<DT><A HREF="https://x.com/_xjdr/status/1839391307648884834/photo/1">efficient TPU code</A>
						</DL><p>
						<DT><H3 FOLDED>MLIR</H3>
						<DL><p>
							<DT><H3 FOLDED>StableHLO</H3>
							<DL><p>
								<DT><A HREF="https://github.com/openxla/stablehlo/blob/main/docs/spec.md">stablehlo/docs/spec.md at main · openxla/stablehlo</A>
							</DL><p>
							<DT><H3 FOLDED>NVGPU Dialect</H3>
							<DL><p>
								<DT><A HREF="https://grypp.github.io/papers/nvdsl.pdf">Programming
Nvidia Hopper with MLIR’s NVGPU Dialect</A>
							</DL><p>
							<DT><A HREF="https://mlir.llvm.org/docs/LangRef/">MLIR Language Reference - MLIR</A>
							<DT><A HREF="https://www.youtube.com/watch?v=LPlRLt9w4b0">2023 EuroLLVM - What's new in MLIR? - YouTube</A>
							<DT><A HREF="https://github.com/openxla/iree">openxla/iree: A retargetable MLIR-based machine learning compiler and runtime toolkit.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=SEwTjZvy8vw">Mojo: A system programming language for heterogenous computing</A>
							<DT><A HREF="https://github.com/iree-org/iree-experimental">iree-org/iree-experimental: Experiments and prototypes associated with IREE or MLIR</A>
							<DT><A HREF="http://hy3na.com/posts/compilers.html">Towards high-performance AI compilers.</A>
						</DL><p>
						<DT><H3 FOLDED>AITemplate</H3>
						<DL><p>
							<DT><A HREF="https://github.com/facebookincubator/AITemplate">facebookincubator/AITemplate: AITemplate is a Python framework which renders neural network into high performance CUDA/HIP C++ code. Specialized for FP16 TensorCore (NVIDIA GPU) and MatrixCore (AMD GPU) inference.</A>
							<DT><A HREF="https://github.com/facebookincubator/AITemplate/blob/992e1a08e363f0d301bc269ca092f6d999abcca8/tests/unittest/ops/test_gemm_bias.py#L39">AITemplate/tests/unittest/ops/test_gemm_bias.py</A>
							<DT><A HREF="https://facebookincubator.github.io/AITemplate/tutorial/how_to_infer_pt.html">AIT module is a container to build a graph, while PyTorch module is a container to store parameters for eager</A>
							<DT><A HREF="https://facebookincubator.github.io/AITemplate/tutorial/how_to_visualize.html">How to visualize an AIT model — AITemplate 0.2 documentation</A>
						</DL><p>
						<DT><H3 FOLDED>DeepSpeed</H3>
						<DL><p>
							<DT><H3 FOLDED>deepspeed-kernels</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/csrc/transformer/inference/csrc/gelu.cu#L656">DeepSpeed/csrc/transformer/inference/csrc/gelu.cu at master · microsoft/DeepSpeed</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>XLA</H3>
						<DL><p>
							<DT><H3 FOLDED>torch-xla-auto-sharding</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=L1PSUhGtZVc">Optimizing PyTorch Auto sharding For Your Hardware 2024 04 25 09 40 GMT 7 - YouTube</A>
								<DT><A HREF="https://pytorch.org/blog/pytorch-xla-spmd/">PyTorch/XLA SPMD: Scale Up Model Training and Serving with Automatic Parallelization | PyTorch</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/pytorch-xla-2-3-dev-update/2124">PyTorch/XLA 2.3 dev update - compiler - PyTorch Developer Mailing List</A>
							</DL><p>
							<DT><H3 FOLDED>xla-videos</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=e85Ceq2g5z0&t=1s">(Day 1 - Breakout Session) StableHLO &amp; PJRT - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5ilr4gcenaA">(Google) JAX: Low-level control with shard_map and Pallas - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=NFKubflDb1A">(Day 1 - Breakout Session) JAX: Pallas and Shard Map - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=5i7xrBUCD38">OpenXLA Lightning Talks - April 27, 2023 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=zS7NTHYNiF4">OpenXLA Roadmap Presentations - April 27, 2023 - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>xla-primitives-operations</H3>
							<DL><p>
								<DT><A HREF="https://openxla.org/xla/operation_semantics">Operation semantics  |  OpenXLA Project</A>
							</DL><p>
							<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox">NVIDIA/JAX-Toolbox: JAX-Toolbox</A>
							<DT><A HREF="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">Mixed-input matrix multiplication performance optimizations – Google Research Blog</A>
							<DT><A HREF="https://github.com/google/jax/pull/20462/">Add MegaBlox grouped matrix multiplication kernels for TPU. by copybara-service[bot] · Pull Request #20462 · google/jax</A>
							<DT><A HREF="https://cloud.google.com/blog/products/ai-machine-learning/introducing-pytorch-xla-2-3">Introducing PyTorch/XLA 2.3 | Google Cloud Blog</A>
							<DT><A HREF="https://github.com/pytorch/xla">pytorch/xla: Enabling PyTorch on XLA Devices (e.g. Google TPU)</A>
						</DL><p>
						<DT><H3 FOLDED>ai-compilers-people</H3>
						<DL><p>
							<DT><A HREF="https://github.com/skye?tab=stars">skye (Skye Wanderman-Milne) / Starred</A>
							<DT><A HREF="https://pytorch.org/docs/stable/community/persons_of_interest.html">PyTorch Governance | Maintainers — PyTorch 2.1 documentation</A>
							<DT><A HREF="https://github.com/albanD">albanD</A>
							<DT><A HREF="https://github.com/ptillet">ptillet (Philippe Tillet) OpenAI Triton</A>
							<DT><A HREF="https://twitter.com/main_horse/status/1742013125090795531">main_horse</A>
							<DT><A HREF="https://github.com/philipturner/metal-flash-attention">Philip Turner: metal-flash-attention</A>
							<DT><A HREF="https://twitter.com/fluffykittnmeow">fluffy (Triton &amp; Tinygrad)</A>
							<DT><A HREF="https://github.com/Bruce-Lee-LY">Bruce-Lee-LY (Bruce-Lee-LY)</A>
							<DT><A HREF="https://twitter.com/MimeeXu/status/1736045781486846184">ML For Systems: Bill Dally (NVIDIA)</A>
							<DT><A HREF="https://github.com/scott-gray">scott-gray (Scott Gray) OpenAI</A>
							<DT><A HREF="https://github.com/sjfeng1999">sjfeng1999 (Feng Shijie)</A>
							<DT><A HREF="https://github.com/daadaada">daadaada (Da Yan)</A>
							<DT><A HREF="https://www.linkedin.com/in/rawn-henry/">Rawn Henry (NVIDIA)</A>
							<DT><A HREF="https://www.linkedin.com/in/quentincolombet/?originalSubdomain=ch">Quentin Colombet ML compiler (Google)</A>
							<DT><A HREF="https://www.linkedin.com/in/pradeep-ramani/">Pradeep Ramani (NVIDIA)</A>
							<DT><A HREF="https://github.com/thakkarV">Vijay Thakkar (NVIDIA)</A>
							<DT><A HREF="https://github.com/VictorTaelin">VictorTaelin (Victor Taelin)</A>
						</DL><p>
						<DT><H3 FOLDED>Tinygrad</H3>
						<DL><p>
							<DT><H3 FOLDED>tinygrad-backends</H3>
							<DL><p>
								<DT><H3 FOLDED>tinygrad-backends-triton</H3>
								<DL><p>
									<DT><A HREF="https://github.com/geohot/tinygrad/pull/470">A Triton backend for tinygrad by geohot</A>
								</DL><p>
								<DT><H3 FOLDED>tinygrad-backends-llvm</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=bMdzsQuQxKs">George Hotz | Programming | tinygrad: LLVM backend</A>
								</DL><p>
								<DT><H3 FOLDED>tinygrad-backends-hip</H3>
								<DL><p>
									<DT><A HREF="https://github.com/geohot/tinygrad/pull/750">HIP backend by nanamiwang · Pull Request #750 · geohot/tinygrad</A>
								</DL><p>
								<DT><H3 FOLDED>tinygrad-backends-llm.c</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/karpathy/status/1783527854741114981">Andrej Karpathy: 1) write a better compiler 2) write a better assembly-level program</A>
									<DT><A HREF="https://gist.github.com/geohot/7c9f10f5770f058a1de6ef0598e4c9d8">Outputted llm.c from tinygrad</A>
								</DL><p>
								<DT><A HREF="https://gemini.google.com/app/ba25862900744324">Gemini</A>
								<DT><A HREF="https://gist.github.com/fxkamd/ffd02d66a2863e444ec208ea4f3adc48">Observations about HSA and KFD backends in TinyGrad</A>
								<DT><A HREF="https://github.com/mesozoic-egg/tinygrad-notes/blob/main/backends.md">tinygrad-notes/backends.md at main · mesozoic-egg/tinygrad-notes</A>
								<DT><A HREF="https://mesozoic-egg.github.io/tinygrad-notes/backends.html">Kernel Fusion: the backends | tinygrad-notes</A>
							</DL><p>
							<DT><H3 FOLDED>tinygrad-people</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Qazalin?tab=repositories">Qazalin (Qazalin) / Repositories</A>
								<DT><A HREF="https://github.com/flammit">flammit (Francis Lam): fast Torch kernels extraction</A>
							</DL><p>
							<DT><H3 FOLDED>tinygrad-docs</H3>
							<DL><p>
								<DT><A HREF="https://mesozoic-egg.github.io/tinygrad-notes/">Tutorials on Tinygrad | tinygrad-notes</A>
								<DT><A HREF="https://www.youtube.com/watch?v=I_c9cdNAkH4&t=3146s">George Hotz (52:00) | Tinygrad layers of abstraction, Triton</A>
								<DT><A HREF="https://www.youtube.com/watch?v=fq__NqceKVs">George Hotz explains tinygrad's approach to winning at deep learning - YouTube</A>
								<DT><A HREF="https://mesozoic-egg.github.io/tinygrad-notes/shapetracker.html">How ShapeTracker works | tinygrad-notes</A>
								<DT><A HREF="https://docs.tinygrad.org/">tinygrad documentation - tinygrad docs</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/docs/tinygrad_intro.pdf">tinygrad/docs/tinygrad_intro.pdf at master · tinygrad/tinygrad</A>
							</DL><p>
							<DT><H3 FOLDED>tinygrad-kernels</H3>
							<DL><p>
								<DT><H3 FOLDED>tinygrad-hand-optimization</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/c4fdb9c725924fd1bc8a89ca07a1f405953b4d54/examples/handcode_resnet50_opt.py">tinygrad/examples/handcode_resnet50_opt.py</A>
								</DL><p>
								<DT><H3 FOLDED>tinygrad-kernel-fusion</H3>
								<DL><p>
									<DT><A HREF="https://x.com/__tinygrad__/status/1910574691284300167">(1) the tiny corp en X: ".fuse() is now supported on Tensors. Automatic fusing puts one reduce in a kernel, but if you want more, you can fuse back to the nearest contiguous with fuse. Try it, it's fun! Single kernel softmax works, and with a few tweaks, this is flash attention ($500 bounty). https://t.co/9iI4csuBKI" / X</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>tinygrad-checkpointing</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=2QO3vzwHXhg&t=3551s">converting to float16 slowing down</A>
							</DL><p>
							<DT><H3 FOLDED>tinygrad-lectures</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=Gm0t-kAsHuY">George Hotz | Programming | making tinygrad CLOUD=1 fast (even from Hong Kong!) | Good Vibes :) - YouTube</A>
							</DL><p>
							<DT><A HREF="https://docs.google.com/document/d/1q0VulPvi1awazH4EAsScXw3kqWHM-GAbBGY6IQzSV70/edit">tiny corp master plan - Google Docs</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/tinygrad/tensor.py">tinygrad/tinygrad/tensor.py at master · tinygrad/tinygrad</A>
							<DT><A HREF="https://github.com/tinygrad/gpuctypes">tinygrad/gpuctypes: ctypes wrappers for HIP, CUDA, and OpenCL</A>
							<DT><A HREF="https://github.com/geohot/ctypeslib">geohot/ctypeslib: Generate python ctypes classes from C headers. Requires LLVM clang</A>
							<DT><A HREF="https://github.com/tinygrad/teenygrad">tinygrad/teenygrad: If tinygrad wasn't small enough for you...</A>
							<DT><A HREF="https://twitter.com/__tinygrad__/status/1729596377028567137">Tinybox FP16 TFLOPs JAX</A>
							<DT><A HREF="https://www.youtube.com/watch?v=-iH5wvFnsKs">George Hotz | Programming | ripping out all of AMD's userspace, AMDGPU ioctls | GPU memory | HSA KFD - YouTube</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/examples/compile_efficientnet.py">tinygrad/examples/compile_efficientnet.py C codegen</A>
							<DT><A HREF="https://github.com/karpathy/llm.c">karpathy/llm.c: LLM training in simple, raw C/CUDA</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/discussions/3342">Do we have a Model Summary Feature in Tinygrad? · tinygrad/tinygrad · Discussion #3342</A>
							<DT><A HREF="https://developer.nvidia.com/blog/boosting-dynamic-programming-performance-using-nvidia-hopper-gpu-dpx-instructions/">Boosting Dynamic Programming Performance Using NVIDIA Hopper GPU DPX Instructions | NVIDIA Technical Blog</A>
							<DT><A HREF="https://mesozoic-egg.github.io/tinygrad-notes/">Tutorials on Tinygrad | tinygrad-notes</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/examples/llm.c/train_gpt2.py">tinygrad/examples/llm.c/train_gpt2.py</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/de832d26c64a9ec575e47aeb58efe27a0ccf4e0b/autogen_stubs.sh">tinygrad/autogen_stubs.sh (runtime.autogen) clang2py</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Sk35MKtCXfQ&t=4743s">matrix multiplication, a@b, cube</A>
							<DT><A HREF="https://x.com/__tinygrad__/status/1742365883048284421">(1) the tiny corp en X: "tinygrad's multiGPU tensor sharding is merged. experimental, but is that not the simplest API you have ever seen for data/model parallel? (hint: it's just what axis you shard) https://t.co/HDqtRVAHUy" / X</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/docs/tinygrad_intro.pdf">tinygrad/docs/tinygrad_intro.pdf at master · tinygrad/tinygrad</A>
							<DT><A HREF="https://github.com/geohot/cuda_ioctl_sniffer">geohot/cuda_ioctl_sniffer: Sniff CUDA ioctls</A>
							<DT><A HREF="https://github.com/tinygrad/toonygrad">tinygrad/toonygrad: Because tinygrad got out of hand with line count</A>
							<DT><A HREF="https://www.youtube.com/watch?v=h0jT60MBsvc">George Hotz | Programming | toonygrad! a rewrite of the tinygrad middleware (1/n) | Hong Kong - YouTube</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/397a2e6eb6cff0d956b22c46f63e109694695391/test/test_speed_v_torch.py#L4">tinygrad/test/test_speed_v_torch.py</A>
							<DT><A HREF="https://x.com/__tinygrad__/status/1910574691284300167?s=12">tinygrad fusion. Automatic fusing puts one reduce in a kernel</A>
						</DL><p>
						<DT><H3 FOLDED>Tensors</H3>
						<DL><p>
							<DT><H3 FOLDED>tensors-examples</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ezyang/data-dependent-shape-puzzles">ezyang/data-dependent-shape-puzzles: Puzzlers regarding data-dependent shapes in PT2</A>
								<DT><A HREF="https://github.com/srush/Tensor-Puzzles">srush/Tensor-Puzzles: Solve puzzles. Improve your pytorch.</A>
							</DL><p>
							<DT><H3 FOLDED>tensors-debug</H3>
							<DL><p>
								<DT><A HREF="https://github.com/xl0/lovely-tensors">xl0/lovely-tensors: Tensors, ready for human consumption</A>
								<DT><A HREF="https://github.com/xl0/lovely-grad">xl0/lovely-grad: 🫀 Lovely Grad - tiny tensors need some love</A>
								<DT><A HREF="https://github.com/ezyang/torchdbg">ezyang/torchdbg: PyTorch centric eager mode debugger</A>
							</DL><p>
							<DT><H3 FOLDED>named-tensors</H3>
							<DL><p>
								<DT><H3 FOLDED>Haliax</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/dlwh/status/1716900734120464834/photo/1">Named tensor library</A>
									<DT><A HREF="https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html">Stanford CRFM</A>
									<DT><A HREF="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</A>
								</DL><p>
								<DT><A HREF="https://nlp.seas.harvard.edu/NamedTensor">Tensor Considered Harmful</A>
								<DT><A HREF="https://pytorch.org/docs/stable/named_tensor.html">Named Tensors — PyTorch 2.3 documentation</A>
								<DT><A HREF="https://github.com/google-research/dex-lang">google-research/dex-lang: Research language for array processing in the Haskell/ML family</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md">Named Tensors using First-class Dimensions in PyTorch (pytorch/functorch/dim/README.md)</A>
								<DT><A HREF="https://github.com/google/maxtext/blob/53167aa550b49bfc867236790bc8065545f0d300/MaxText/layers/attentions.py#L110">MaxText attentions.py#L110</A>
								<DT><A HREF="https://github.com/google/maxtext/blob/53167aa550b49bfc867236790bc8065545f0d300/MaxText/common_types.py#L4">maxtext/MaxText/common_types.py</A>
								<DT><A HREF="https://github.com/google-deepmind/tensor_annotations">google-deepmind/tensor_annotations: Annotating tensor shapes using Python types</A>
							</DL><p>
							<DT><H3 FOLDED>Numpy</H3>
							<DL><p>
								<DT><A HREF="https://numpy.org/doc/stable/reference/generated/numpy.reshape.html">numpy.reshape — NumPy v1.22 Manual</A>
								<DT><A HREF="https://twitter.com/awnihannun/status/1779133564619284894">advance indexing</A>
								<DT><A HREF="https://arxiv.org/abs/1102.1523">[1102.1523] The NumPy array: a structure for efficient numerical computation</A>
								<DT><A HREF="https://github.com/joennlae/tensorli">joennlae/tensorli: Absolute minimalistic implementation of a GPT-like transformer using only numpy (&lt;650 lines).</A>
							</DL><p>
							<DT><H3 FOLDED>Shape Suffixes</H3>
							<DL><p>
								<DT><H3 FOLDED>tensor-typing</H3>
								<DL><p>
									<DT><A HREF="https://kidger.site/thoughts/jaxtyping/">No more shape errors! Type annotations for the shape+dtype of tensors/arrays</A>
									<DT><A HREF="https://github.com/patrick-kidger/jaxtyping">patrick-kidger/jaxtyping: Type annotations and runtime checking for shape and dtype of JAX/NumPy/PyTorch/etc. arrays. https://docs.kidger.site/jaxtyping/</A>
									<DT><A HREF="https://github.com/thecharlieblake/lovely-llama/blob/main/lovely_llama.py">lovely-llama/lovely_llama.py at main · thecharlieblake/lovely-llama</A>
									<DT><A HREF="https://x.com/srush_nlp/status/1830975083646722387">raw numpy / torch tensor type checking example: multihead_attention</A>
									<DT><A HREF="https://github.com/thecharlieblake/lovely-llama/blob/main/lovely_llama.py#L68">lovely-llama/lovely_llama.py at main · thecharlieblake/lovely-llama</A>
									<DT><A HREF="https://github.com/Narsil/zandle">Narsil/zandle: Testing zig comptime out for complex tensor typing thing.</A>
								</DL><p>
								<DT><H3 FOLDED>shape-suffixes-attention-heads</H3>
								<DL><p>
									<DT><A HREF="https://x.com/thecharlieblake/status/1830983614957527117">(1) Charlie Blake en X: "Yes! I was similarly frustrated, so did exactly what you suggest - a vmap-heavy jax implementation. Here's an attention head, I think it's much nicer this way: https://t.co/05JSiD2Dub" / X</A>
									<DT><A HREF="https://x.com/hwchung27/status/1831072828822827338">(1) Hyung Won Chung en X: "I'd like to clarify a few points on this slide from my previous talk to avoid potential confusion (https://t.co/irCiesi1Qo) 1) As cited in the slides, this function is adapted from Noam's multiquery paper, which I highly recommend. This is the best resource to learn about https://t.co/SQKGYCC5xi" / X</A>
									<DT><A HREF="https://x.com/srush_nlp/status/1830965798812422485">(1) Sasha Rush en X: "It's been 7 years of screaming into this abyss, but 🤷 This is ugly code. This function should *not* know there is a `batch` dimension. The inner part should not know there is a `head` dimension. We are in the assembly era of math coding." / X</A>
									<DT><A HREF="https://x.com/srush_nlp/status/1830975083646722387">raw numpy / torch tensor type checking example: multihead_attention</A>
								</DL><p>
								<DT><A HREF="https://github.com/patrick-kidger/jaxtyping">patrick-kidger/jaxtyping: Type annotations and runtime checking for shape and dtype of JAX/NumPy/PyTorch/etc. arrays. https://docs.kidger.site/jaxtyping/</A>
								<DT><A HREF="https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd">Shape Suffixes — Good Coding Style | by Noam Shazeer | Medium</A>
								<DT><A HREF="https://x.com/NoamShazeer/status/1762733550892401030">Noam Shazeer: Shape Suffixes — Good Coding Style</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md">Named Tensors using First-class Dimensions in PyTorch (pytorch/functorch/dim/README.md)</A>
								<DT><A HREF="https://github.com/joschu/jax-exp/blob/master/jax_transformer.py#L96">jax-exp/jax_transformer.py at master · joschu/jax-exp</A>
								<DT><A HREF="https://github.com/xjdr-alt/simple_transformer">xjdr-alt/simple_transformer: Simple Transformer in Jax</A>
								<DT><A HREF="https://github.com/zasdfgbnm/TorchSnooper">zasdfgbnm/TorchSnooper: Debug PyTorch code using PySnooper</A>
								<DT><A HREF="https://github.com/google-deepmind/nanodo/blob/main/nanodo/model.py">nanodo/nanodo/model.py at main · google-deepmind/nanodo</A>
								<DT><A HREF="https://x.com/_xjdr/status/1838340564766593519/photo/1">xjdr: MLA impl</A>
							</DL><p>
							<DT><H3 FOLDED>einsum</H3>
							<DL><p>
								<DT><H3 FOLDED>einops-examples</H3>
								<DL><p>
									<DT><A HREF="https://theaisummer.com/einsum-attention/">Einsum: Transformer example</A>
								</DL><p>
								<DT><A HREF="https://sankalp.bearblog.dev/einsum-new/">Shape Rotation 101: An Intro to Einsum and Jax Transformers | sankalp's blog</A>
								<DT><A HREF="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g27b2b69cf28_1_205">Language Language Models (in 2023) - Google Slides: Einsum: generalization of matmul</A>
								<DT><A HREF="https://x.com/dejavucoder/status/1804379391910383784">(1) sankalp en X: "new post is up. it's split up in two parts. in the first part, i talk about the einsum operation in detail. second part is all about understanding @_xjdr 's JAX transformer implementation (that has lots of einsums). https://t.co/5fJhorgoul https://t.co/JiZcg5cknC" / X</A>
								<DT><A HREF="https://jimypbr.github.io/2020/02/fast-ai-lesson-8-notes-backprop-from-the-foundations">go-seq | James Briggs' Blog</A>
								<DT><A HREF="https://nlp.seas.harvard.edu/NamedTensor">Tensor Considered Harmful</A>
								<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.einsum.html">torch.einsum — PyTorch 2.3 documentation</A>
								<DT><A HREF="https://zhuanlan.zhihu.com/p/361209187">一文学会 Pytorch 中的 einsum - 知乎</A>
								<DT><A HREF="https://mp.weixin.qq.com/s?__biz=MzA4MjY4NTk0NQ==&mid=2247493772&idx=1&sn=4eea0e68f2e813fb1e474bf74f3c3f6e&chksm=9f83521aa8f4db0c2d2221d259b1ff7b8fca53024fdd4be9def3d1f439b0eebd494501223079&token=650657988&lang=zh_CN#rd">A literature on einsum in Pytorch</A>
								<DT><A HREF="https://stackoverflow.com/questions/26089893/understanding-numpys-einsum/47966452#47966452">einsum</A>
								<DT><A HREF="https://rockt.ai/2018/04/30/einsum">Einsum is All you Need - Einstein Summation in Deep Learning</A>
							</DL><p>
							<DT><H3 FOLDED>tensors-dtypes</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/microxcaling">microsoft/microxcaling: PyTorch emulation library for Microscaling (MX)-compatible data formats</A>
							</DL><p>
							<DT><H3 FOLDED>tensor-theory</H3>
							<DL><p>
								<DT><A HREF="https://github.com/EurekaLabsAI/tensor">EurekaLabsAI/tensor: The Tensor (or Array)</A>
								<DT><A HREF="http://www.continuummechanics.org/tensornotationbasic.html">Tensor Notation (Basics)</A>
								<DT><A HREF="https://link.springer.com/chapter/10.1007/978-3-030-74386-4_1">Tensor Computation | SpringerLink</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Hafo7hIl8MU">Tensor Puzzles: Let's Play - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>operators</H3>
							<DL><p>
								<DT><A HREF="https://github.com/nadavrot/fast_log">nadavrot/fast_log: A fast implementation of log() and exp()</A>
							</DL><p>
							<DT><H3 FOLDED>tensor-compression</H3>
							<DL><p>
								<DT><A HREF="https://github.com/zipnn/zipnn">zipnn/zipnn: A lossless and near-lossless compression method optimized for numbers/tensors in the Foundation Models environment</A>
								<DT><A HREF="https://github.com/neuralmagic/compressed-tensors">neuralmagic/compressed-tensors: A safetensors extension to efficiently store sparse quantized tensors on disk</A>
							</DL><p>
							<DT><H3 FOLDED>zml</H3>
							<DL><p>
								<DT><A HREF="https://docs.zml.ai/learn/concepts/">ZML Concepts - ZML</A>
								<DT><A HREF="https://github.com/zml/zml?tab=readme-ov-file">zml/zml: High performance AI inference stack. Built for production. @ziglang / @openxla / MLIR / @bazelbuild</A>
							</DL><p>
							<DT><A HREF="https://developer.nvidia.com/blog/nvidia-research-tensors-are-the-future-of-deep-learning/">NVIDIA Research: Tensors Are the Future of Deep Learning</A>
							<DT><A HREF="https://twitter.com/ayaka14732/media">Tweets con contenido multimedia de Ayaka (@ayaka14732) / Twitter</A>
							<DT><A HREF="https://github.com/Bruce-Lee-LY/cuda_hgemm">Bruce-Lee-LY/cuda_hgemm: Several optimization methods of half-precision general matrix multiplication (HGEMM) using tensor core with WMMA API and MMA PTX instruction.</A>
							<DT><A HREF="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine: A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper and Ada GPUs, to provide better performance with lower memory utilization in both training and inference.</A>
							<DT><A HREF="https://github.com/snap-research/BitsFusion">snap-research/BitsFusion</A>
						</DL><p>
						<DT><H3 FOLDED>Assembler &amp; disassemblers</H3>
						<DL><p>
							<DT><A HREF="https://github.com/daadaada/turingas">daadaada/turingas: Assembler for NVIDIA Volta and Turing GPUs</A>
							<DT><A HREF="https://github.com/tinygrad/tinygrad/tree/957e9800f15bb3b8727f56b9298433432f703d9f/disassemblers">tinygrad/disassemblers/adreno/disasm-a3xx.c</A>
							<DT><A HREF="https://github.com/cloudcores/CuAssembler">cloudcores/CuAssembler: An unofficial cuda assembler, for all generations of SASS, hopefully ：）</A>
							<DT><A HREF="https://github.com/vosen/ZLUDA">vosen/ZLUDA: CUDA on AMD GPUs</A>
							<DT><A HREF="https://justine.lol/matmul/">disassembly for the C++ code I'm working on will pop up on the screen in a few milliseconds (llamafile)</A>
							<DT><A HREF="https://github.com/QianyanTech/NBAssembler">QianyanTech/NBAssembler: Assembler and Decompiler for NVIDIA (Maxwell Pascal Volta Turing Ampere) GPUs.</A>
						</DL><p>
						<DT><H3 FOLDED>xformers</H3>
						<DL><p>
							<DT><H3 FOLDED>xformers-triton</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/_triton/tiled_matmul_kernels.py#L152">xformers/xformers/ops/_triton/tiled_matmul_kernels.py at 0004c67c7e9ec3c9e7b3907db0e0b2957430b35b · facebookresearch/xformers</A>
							</DL><p>
							<DT><H3 FOLDED>xformers-custom-op</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/ops/fmha/flash3.py">xformers/xformers/ops/fmha/flash3.py at 0004c67c7e9ec3c9e7b3907db0e0b2957430b35b · facebookresearch/xformers</A>
							</DL><p>
							<DT><H3 FOLDED>xformers-flash3</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/xformers/pull/1157">[FA3] Link to cuda library to fix the FA3 extension build by xuzhao9 · Pull Request #1157 · facebookresearch/xformers</A>
								<DT><A HREF="https://github.com/ohwi/xformers/commit/c6528114fecb5927d11689ec78eba444b1f78741">Update flash3 arguments · ohwi/xformers@c652811</A>
							</DL><p>
							<DT><A HREF="https://github.com/facebookresearch/xformers/blob/ad986981b141a218bf07bf968e920051ff2c7b41/xformers/benchmarks/benchmark_mem_eff_attention.py#L85">xformers/xformers/benchmarks/benchmark_mem_eff_attention.py at ad986981b141a218bf07bf968e920051ff2c7b41 · facebookresearch/xformers</A>
							<DT><A HREF="https://x.com/fvsmassa/status/1580229170629849089">(1) Francisco Massa en X: "Do you need fast and memory-efficient transformers which are easy to install? I'm happy to share that xFormers now ships precompiled conda packages for PyTorch 1.12.1 and CUDA 11.3/11.6 (Linux-only for now). https://t.co/djPV1dKxVT https://t.co/zCHcg4DLZl" / X</A>
							<DT><A HREF="https://github.com/facebookresearch/xformers/blob/0004c67c7e9ec3c9e7b3907db0e0b2957430b35b/xformers/profiler/device_limits.py#L21">xformers/xformers/profiler/device_limits.py  at facebookresearch/xformers: NVIDIA datashet reports with sparsity</A>
							<DT><A HREF="https://github.com/facebookresearch/xformers/blob/4a9dd7ec079e0c935db10daa2a1a89fd19cfa231/xformers/ops/_triton/tiled_matmul_kernels.py#L150">xformers/xformers/ops/_triton/tiled_matmul_kernels.py at 4a9dd7ec079e0c935db10daa2a1a89fd19cfa231 · facebookresearch/xformers</A>
							<DT><A HREF="https://facebookresearch.github.io/xformers/components/ops.html">xFormers optimized operators | xFormers 0.0.29 documentation</A>
						</DL><p>
						<DT><H3 FOLDED>ONNX</H3>
						<DL><p>
							<DT><H3 FOLDED>onnx-optimum</H3>
							<DL><p>
								<DT><H3 FOLDED>optimum-docker</H3>
								<DL><p>
									<DT><A HREF="https://github.com/awslabs/llm-hosting-container/tree/main/huggingface/pytorch/optimum/docker">llm-hosting-container/huggingface/pytorch/optimum/docker at main · awslabs/llm-hosting-container</A>
								</DL><p>
								<DT><A HREF="https://github.com/philschmid/optimum-static-quantization">philschmid/optimum-static-quantization</A>
								<DT><A HREF="https://github.com/huggingface/optimum">huggingface/optimum: 🚀 Accelerate training and inference of 🤗 Transformers and 🤗 Diffusers with easy to use hardware optimization tools</A>
								<DT><A HREF="https://github.com/huggingface/optimum-nvidia">huggingface/optimum-nvidia</A>
							</DL><p>
							<DT><H3 FOLDED>onnxscript</H3>
							<DL><p>
								<DT><A HREF="https://github.com/microsoft/onnxscript">microsoft/onnxscript: ONNX Script enables author ONNX functions and models using a subset of Python.</A>
								<DT><A HREF="https://github.com/microsoft/onnxscript/blob/main/examples/pattern_rewriting.py">onnxscript/examples/pattern_rewriting.py</A>
							</DL><p>
							<DT><H3 FOLDED>onnx-olive</H3>
							<DL><p>
								<DT><H3 FOLDED>olive-convert</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/tlwu/sdxl-turbo-onnxruntime">tlwu/sdxl-turbo-onnxruntime -&gt; olive model generation cmdline</A>
									<DT><A HREF="https://github.com/microsoft/Olive/blob/6b93df3a493ac40c914fcae1155ac24659cba750/examples/stable_diffusion/stable_diffusion.py#L196">stable_diffusion.py#L196: optimize-model</A>
								</DL><p>
								<DT><A HREF="https://github.com/microsoft/Olive">microsoft/Olive: Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation.</A>
								<DT><A HREF="https://github.com/microsoft/Olive/tree/main/examples/stable_diffusion">Olive/examples/stable_diffusion at main · microsoft/Olive</A>
							</DL><p>
							<DT><A HREF="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization">ONNX Runtime</A>
							<DT><A HREF="https://onnx.ai/">ONNX | Home</A>
							<DT><A HREF="https://onnxruntime.ai/">ONNX Runtime | Home</A>
							<DT><A HREF="https://github.com/lutzroeder/netron">lutzroeder/netron: Visualizer for neural networks</A>
							<DT><A HREF="https://onnxruntime.ai/docs/performance/olive.html">End to end optimization with Olive | onnxruntime</A>
							<DT><A HREF="https://github.com/microsoft/onnxscript/blob/main/docs/examples/04_plot_eager_mode_evaluation.py">onnxscript/docs/examples/04_plot_eager_mode_evaluation.py at main · microsoft/onnxscript</A>
							<DT><A HREF="https://github.com/onnx/onnx/blob/main/docs/Operators.md">Operator Schemas</A>
							<DT><A HREF="https://github.com/onnx/onnx/blob/main/docs/Syntax.md">onnx/docs/Syntax.md at main · onnx/onnx</A>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html">Developer Guide :: NVIDIA Deep Learning TensorRT Documentation</A>
							<DT><A HREF="https://github.com/Ki6an/fastT5">Ki6an/fastT5: ⚡ boost inference speed of T5 models by 5x &amp; reduce the model size by 3x.</A>
							<DT><A HREF="https://github.com/stars/pommedeterresautee/lists/quantization">pommedeterresautee's list / quantization</A>
							<DT><A HREF="https://github.com/microsoft/nnfusion/tree/main/models/pytorch2onnx">nnfusion/models/pytorch2onnx at main · microsoft/nnfusion</A>
							<DT><A HREF="https://github.com/mlcommons/inference_results_v4.0/blob/main/closed/NVIDIA/code/stable-diffusion-xl/tensorrt/sdxl_graphsurgeon.py">inference_results_v4.0/closed/NVIDIA/code/stable-diffusion-xl/tensorrt/sdxl_graphsurgeon.py at main · mlcommons/inference_results_v4.0</A>
							<DT><A HREF="https://github.com/daquexian/onnx-simplifier">daquexian/onnx-simplifier: Simplify your onnx model</A>
						</DL><p>
						<DT><H3 FOLDED>TensorRT</H3>
						<DL><p>
							<DT><H3 FOLDED>tensorrt-diffusion</H3>
							<DL><p>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT/blob/release/10.2/demo/Diffusion/demo_txt2img_sd3.py">TensorRT/demo/Diffusion/demo_txt2img_sd3.py at release/10.2 · NVIDIA/TensorRT</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT/blob/release/10.2/demo/Diffusion/README.md">TensorRT/demo/Diffusion/README.md at release/10.2 · NVIDIA/TensorRT</A>
								<DT><A HREF="https://github.com/search?q=repo%3ANVIDIA%2FTensorRT-Model-Optimizer%20FLUX&type=code">DiT FLUX</A>
							</DL><p>
							<DT><A HREF="https://segmentfault.com/a/1190000039977778">Introduction</A>
							<DT><A HREF="https://github.com/pytorch/TensorRT">pytorch/TensorRT: PyTorch/TorchScript/FX compiler for NVIDIA GPUs using TensorRT</A>
							<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51714/">Exploring TensorRT</A>
							<DT><A HREF="https://developer.nvidia.com/tensorrt">TensorRT SDK | NVIDIA Developer</A>
							<DT><A HREF="https://els-rd.github.io/transformer-deploy/python/">Direct use TensorRT in Python script (no server)</A>
							<DT><A HREF="https://www.photoroom.com/inside-photoroom/stable-diffusion-25-percent-faster-and-save-seconds">Stable Diffusion</A>
							<DT><A HREF="https://github.com/wangzyon/trt_learn">wangzyon/trt_learn: TensorRT encapsulation, learn, rewrite, practice.</A>
							<DT><A HREF="https://leimao.github.io/blog/Docker-TensorRT/">TensorRT In Docker - Lei Mao's Log Book</A>
							<DT><A HREF="https://github.com/stars/pommedeterresautee/lists/quantization">pommedeterresautee's list / quantization</A>
							<DT><A HREF="https://github.com/NVIDIA/TensorRT/tree/main/tools/experimental/trt-engine-explorer">TensorRT/tools/experimental/trt-engine-explorer at main · NVIDIA/TensorRT</A>
							<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo">neuralmagic/tensorrt-demo</A>
						</DL><p>
						<DT><H3 FOLDED>LLVM</H3>
						<DL><p>
							<DT><H3 FOLDED>llvm-compiler-optimizations</H3>
							<DL><p>
								<DT><A HREF="https://github.com/google/ml-compiler-opt">google/ml-compiler-opt: Infrastructure for Machine Learning Guided Optimization (MLGO) in LLVM.</A>
								<DT><A HREF="https://lists.llvm.org/pipermail/llvm-dev/2020-April/140763.html">[llvm-dev] RFC: a practical mechanism for applying Machine Learning for optimization policies in LLVM</A>
							</DL><p>
							<DT><H3 FOLDED>NVPTX LLVM</H3>
							<DL><p>
								<DT><A HREF="https://github.com/llvm/llvm-project/pull/87065">[mlir][nvgpu] NVGPU Tutorials by grypp · Pull Request #87065 · llvm/llvm-project</A>
								<DT><A HREF="https://grypp.github.io/papers/nvdsl.pdf">Zero to Hero: Programming Nvidia Hopper Tensor Core with MLIR's NVGPU Dialect)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=xNe9fPvU7-U">Open MLIR Meeting 11-16-2023: Targeting H100 with NVGPU and NVVM Dialects - YouTube</A>
							</DL><p>
							<DT><A HREF="https://github.com/banach-space/llvm-tutor">banach-space/llvm-tutor: A collection of out-of-tree LLVM passes for teaching and learning</A>
							<DT><A HREF="https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/?utm_source=twitter&utm_medium=organic_social&utm_content=link&utm_campaign=fair">Meta Large Language Model Compiler: Foundation Models of Compiler Optimization | Research - AI at Meta</A>
							<DT><A HREF="https://compilergym.com/">Indices and tables — CompilerGym 0.2.5 documentation</A>
							<DT><A HREF="https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/?utm_source=linkedin&utm_medium=organic_social&utm_content=image&utm_campaign=fair">Meta Large Language Model Compiler: Foundation Models of Compiler Optimization | Research - AI at Meta</A>
						</DL><p>
						<DT><H3 FOLDED>WASM</H3>
						<DL><p>
							<DT><H3 FOLDED>wasm-internals</H3>
							<DL><p>
								<DT><A HREF="https://developer.mozilla.org/en-US/docs/WebAssembly/Understanding_the_text_format">Understanding WebAssembly text format - WebAssembly | MDN</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ojYEfRye6aE&t=13s">HELLO WEBASSEMBLY - wat</A>
								<DT><A HREF="https://developer.mozilla.org/en-US/docs/WebAssembly/Text_format_to_wasm">Converting WebAssembly text format to wasm - WebAssembly | MDN</A>
								<DT><A HREF="https://rustwasm.github.io/docs.html">Rust and WebAssembly Documentation | Rust and WebAssembly</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-C/C++</H3>
							<DL><p>
								<DT><H3 FOLDED>Emscripten</H3>
								<DL><p>
									<DT><A HREF="https://emscripten.org/">Main — Emscripten 3.0.1-git (dev) documentation</A>
									<DT><H3 FOLDED>installation</H3>
									<DL><p>
										<DT><A HREF="https://emscripten.org/docs/building_from_source/toolchain_what_is_needed.html#toolchain-what-you-need">Emscripten Toolchain Requirements — Emscripten 3.1.9</A>
										<DT><A HREF="https://formulae.brew.sh/formula/emscripten">emscripten — Homebrew Formulae</A>
										<DT><A HREF="https://formulae.brew.sh/formula/llvm#default">llvm — Homebrew Formulae</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://medium.com/@tdeniffel/pragmatic-compiling-from-c-to-webassembly-a-guide-a496cc5954b8">Pragmatic compiling of C++ to WebAssembly. A Guide. | by Thomas Deniffel | Medium</A>
								<DT><A HREF="https://web.dev/loading-wasm/">Loading WebAssembly modules efficiently</A>
								<DT><A HREF="https://nodejs.dev/learn/nodejs-with-webassembly">Node.js with WebAssembly</A>
								<DT><A HREF="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/Interacting-with-code.html">Interacting with code — Emscripten 3.1.9-git (dev) documentation</A>
								<DT><A HREF="https://web.dev/emscripten-npm/">Emscripten and npm</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-wabt</H3>
							<DL><p>
								<DT><A HREF="https://github.com/WebAssembly/wabt">WebAssembly/wabt: The WebAssembly Binary Toolkit</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-wasi</H3>
							<DL><p>
								<DT><A HREF="https://github.com/WebAssembly/WASI">WebAssembly/WASI: WebAssembly System Interface</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-containers</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=ulZGjeFZirU&t=21s">WASM + Kubernetes: Beyond Containers - YouTube</A>
							</DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=VhCgep06-I8">Browserless app runtime in Rust - Demo app in Zig - Wasm/WebGPU - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=b5HHyb1d4Ys">Evolution of Wasm: Past, Present, Future - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ulZGjeFZirU&t=21s">WASM + Kubernetes: Beyond Containers - YouTube</A>
							<DT><A HREF="https://wasmcloud.dev/">wasmCloud Documentation</A>
							<DT><A HREF="https://pragprog.com/titles/khrust/programming-webassembly-with-rust/">Programming WebAssembly with Rust</A>
							<DT><A HREF="https://pspdfkit.com/blog/2017/webassembly-a-new-hope/">WebAssembly: A New Hope | PSPDFKit</A>
							<DT><A HREF="https://www.youtube.com/watch?v=fh9WXPu0hw8">Bringing WebAssembly outside the web with WASI by Lin Clark - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>GGML</H3>
						<DL><p>
							<DT><H3 FOLDED>llamafile</H3>
							<DL><p>
								<DT><A HREF="https://justine.lol/matmul/">LLaMA Now Goes Faster on CPUs</A>
								<DT><A HREF="https://github.com/Mozilla-Ocho/llamafile">Mozilla-Ocho/llamafile: Distribute and run LLMs with a single file.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-mRi-B3t6fA&t=593s">Llamafile: bringing AI to the masses with fast CPU inference: Stephen Hood and Justine Tunney - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>llama.cpp</H3>
							<DL><p>
								<DT><H3 FOLDED>llama.cpp-convert</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/c8a0090922bad576623de4aae227717085249262/convert_hf_to_gguf_update.py#L234">llama.cpp/convert_hf_to_gguf_update.py</A>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/c8a0090922bad576623de4aae227717085249262/scripts/hf.sh#L6">llama.cpp/scripts/hf.sh</A>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/c8a0090922bad576623de4aae227717085249262/convert_hf_to_gguf.py">llama.cpp/convert_hf_to_gguf.py</A>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/c8a0090922bad576623de4aae227717085249262/ci/run.sh#L562">run.sh#L562</A>
								</DL><p>
								<DT><H3 FOLDED>gguf</H3>
								<DL><p>
									<DT><H3 FOLDED>gguf-models</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/TheBloke/CodeLlama-70B-Instruct-GPTQ">TheBloke/CodeLlama-70B-Instruct-GPTQ · Hugging Face</A>
									</DL><p>
									<DT><H3 FOLDED>gguf-py</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ggerganov/llama.cpp/tree/c8a0090922bad576623de4aae227717085249262/gguf-py">llama.cpp/gguf-py</A>
									</DL><p>
									<DT><A HREF="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">ggml/docs/gguf.md at master · ggerganov/ggml</A>
									<DT><A HREF="https://github.com/antirez/gguf-tools">antirez/gguf-tools: GGUF implementation in C as a library and a tools CLI program</A>
								</DL><p>
								<DT><H3 FOLDED>llama.cpp-apple</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/discussions/4508">Performance of llama.cpp on Apple Silicon A-series · ggerganov/llama.cpp · Discussion #4508</A>
								</DL><p>
								<DT><H3 FOLDED>llama.cpp-python</H3>
								<DL><p>
									<DT><A HREF="https://github.com/abetlen/llama-cpp-python">abetlen/llama-cpp-python: Python bindings for llama.cpp</A>
								</DL><p>
								<DT><H3 FOLDED>llama.cpp-torch</H3>
								<DL><p>
									<DT><A HREF="https://github.com/chu-tianxiang/llama-cpp-torch">chu-tianxiang/llama-cpp-torch: llama.cpp to PyTorch Converter</A>
								</DL><p>
								<DT><A HREF="https://steelph0enix.github.io/posts/llama-cpp-guide/">llama.cpp guide - Running LLMs locally, on any hardware, from scratch ::</A>
							</DL><p>
							<DT><A HREF="https://github.com/marella/ctransformers">marella/ctransformers: Python bindings for the Transformer models implemented in C/C++ using GGML library.</A>
							<DT><A HREF="https://huggingface.co/crusoeai">crusoeai (Crusoe AI)</A>
							<DT><A HREF="https://github.com/ggerganov/ggml/blob/master/src/ggml-quants.h">ggml/src/ggml-quants.h at master · ggerganov/ggml</A>
						</DL><p>
						<DT><H3 FOLDED>Higher-order Virtual Machine 2</H3>
						<DL><p>
							<DT><A HREF="https://github.com/VictorTaelin">VictorTaelin (Victor Taelin)</A>
							<DT><A HREF="https://github.com/HigherOrderCO/HVM">HigherOrderCO/HVM: A massively parallel, optimal functional runtime in Rust</A>
							<DT><A HREF="https://github.com/HigherOrderCO/bend">HigherOrderCO/Bend</A>
						</DL><p>
						<DT><H3 FOLDED>Metal</H3>
						<DL><p>
							<DT><H3 FOLDED>metal-llama.cpp</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/discussions/4167">Performance of llama.cpp on Apple Silicon M-series · ggerganov/llama.cpp · Discussion #4167</A>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/discussions/4508">Performance of llama.cpp on Apple Silicon A-series · ggerganov/llama.cpp · Discussion #4508</A>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/discussions/4508#user-content-fn-4-533433ec2a70d995c2039ce1939985be">Performance of llama.cpp on Apple Silicon A-series · ggerganov/llama.cpp · Discussion #4508</A>
								<DT><A HREF="https://github.com/ggerganov/llama.cpp/tree/0e18b2e7d0b5c0a509ea40098def234b8d4a938a/examples/llama.swiftui">llama.cpp/examples/llama.swiftui</A>
							</DL><p>
							<DT><H3 FOLDED>metal-perf</H3>
							<DL><p>
								<DT><A HREF="https://github.com/tlkh/asitop">tlkh/asitop: Perf monitoring CLI tool for Apple Silicon</A>
							</DL><p>
							<DT><H3 FOLDED>metal-mlx</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ml-explore/mlx">ml-explore/mlx: MLX: An array framework for Apple silicon</A>
								<DT><A HREF="https://github.com/ml-explore/mlx-examples">ml-explore/mlx-examples: Examples in the MLX framework</A>
								<DT><A HREF="https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e">GPT from Scratch with MLX. Define and train GPT-2 on your MacBook | by Pranav Jadhav | Jun, 2024 | Towards Data Science</A>
							</DL><p>
							<DT><H3 FOLDED>metal-corenet</H3>
							<DL><p>
								<DT><A HREF="https://github.com/apple/corenet">apple/corenet: CoreNet: A library for training deep neural networks</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/atiorh/status/1737912777153609918">(Apple) Atila en X: "My takeaways from Apple's “LLM in a flash" (1/n)" / X</A>
						</DL><p>
						<DT><H3 FOLDED>Tenstorrent</H3>
						<DL><p>
							<DT><A HREF="https://github.com/tenstorrent-metal/tt-metal">tenstorrent-metal/tt-metal: ttnn - a python API and OP library. TT-Metalium - a low level kernel programming model</A>
						</DL><p>
						<DT><H3 FOLDED>ai-compilers-tracing</H3>
						<DL><p>
							<DT><A HREF="https://github.com/facebookresearch/xformers/blob/ad986981b141a218bf07bf968e920051ff2c7b41/xformers/benchmarks/benchmark_mem_eff_attention.py#L85">xformers/xformers/benchmarks/benchmark_mem_eff_attention.py at ad986981b141a218bf07bf968e920051ff2c7b41 · facebookresearch/xformers</A>
							<DT><A HREF="https://github.com/facebookresearch/xformers/blob/ad986981b141a218bf07bf968e920051ff2c7b41/xformers/benchmarks/benchmark_mem_eff_attention.py#L85">xformers/xformers/benchmarks/benchmark_mem_eff_attention.py</A>
						</DL><p>
						<DT><H3 FOLDED>Exo</H3>
						<DL><p>
							<DT><A HREF="https://exo-lang.dev/">The Exo Language | Exo is a low-level language (and exocompiler) designed to help performance engineers write, optimize, and target high-performance computing kernels onto new hardware accelerators.</A>
							<DT><A HREF="https://github.com/exo-lang/ExoBLAS">exo-lang/ExoBLAS: BLAS implementation using Exo</A>
							<DT><A HREF="https://github.com/exo-lang">exo-lang</A>
						</DL><p>
						<DT><H3 FOLDED>GEMMINI</H3>
						<DL><p>
							<DT><A HREF="https://github.com/ucb-bar/gemmini">ucb-bar/gemmini: Berkeley's Spatial Array Generator</A>
							<DT><A HREF="https://people.eecs.berkeley.edu/~ysshao/assets/papers/genc2021-dac.pdf">Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration</A>
							<DT><A HREF="https://github.com/ucb-bar/chipyard">ucb-bar/chipyard: An Agile RISC-V SoC Design Framework with in-order cores, out-of-order cores, accelerators, and more</A>
							<DT><A HREF="https://www.chisel-lang.org/">Home | Chisel</A>
							<DT><A HREF="https://blog.research.google/2024/01/mixed-input-matrix-multiplication.html">Mixed-input matrix multiplication performance optimizations – Google Research Blog</A>
							<DT><A HREF="https://research.colfax-intl.com/adding-fp8-to-flashattention/">Delivering 1 PFLOP/s of Performance with FP8 FlashAttention-2 – Colfax Research</A>
						</DL><p>
						<DT><H3 FOLDED>TVM</H3>
						<DL><p>
							<DT><A HREF="https://mlc.ai/chapter_graph_optimization/index.html#prelude">7. Computational Graph Optimization — Machine Learing Compilation 0.0.1 documentation</A>
						</DL><p>
						<DT><H3 FOLDED>Mojo</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=3FKSlhZNdL0">Mojo Community Meeting #2 - YouTube</A>
							<DT><A HREF="https://www.modular.com/blog/mojo-vs-rust-is-mojo-faster-than-rust">Modular: Mojo vs. Rust: is Mojo 🔥 faster than Rust 🦀 ?</A>
						</DL><p>
						<DT><H3 FOLDED>stable-fast</H3>
						<DL><p>
							<DT><H3 FOLDED>stable-fast-installation</H3>
							<DL><p>
								<DT><A HREF="https://github.com/DataCrunch-io/inferno/blob/5b0633c8b5a9d6c84b7215b660e5e510ef00315c/Dockerfiles/Dockerfile.lcm#L24">Download the wheel corresponding to your system: pip3 install &lt;wheel file&gt;</A>
							</DL><p>
							<DT><H3 FOLDED>siliconflow</H3>
							<DL><p>
								<DT><A HREF="https://siliconflow.cn/">SiliconFlow, Accelerate AGI to Benefit Humanity</A>
								<DT><A HREF="https://chengzeyi.github.io/markdown-cv/">Cheng Zeyi's CV | CV</A>
							</DL><p>
							<DT><A HREF="https://github.com/chengzeyi/stable-fast">chengzeyi/stable-fast: Best inference performance optimization framework for HuggingFace Diffusers on NVIDIA GPUs.</A>
							<DT><A HREF="https://www.vrushankdes.ai/diffusion-inference-optimization">Diffusion Inference Optimization</A>
						</DL><p>
						<DT><H3 FOLDED>OneDiff</H3>
						<DL><p>
							<DT><H3 FOLDED>onediff-compiler</H3>
							<DL><p>
								<DT><H3 FOLDED>nexfort</H3>
								<DL><p>
									<DT><H3 FOLDED>nextfort-fp8</H3>
									<DL><p>
										<DT><A HREF="https://github.com/siliconflow/onediff/blob/6b53a83bd72952c11102c30b6d739bfcb3b0f7da/onediff_diffusers_extensions/onediffx/compilers/diffusion_pipeline_compiler.py#L192">onediff/onediff_diffusers_extensions/onediffx/compilers/diffusion_pipeline_compiler.py at 6b53a83bd72952c11102c30b6d739bfcb3b0f7da · siliconflow/onediff</A>
										<DT><A HREF="https://github.com/lukiod/T2I-and-I2I-Report/blob/main/testi2i.py">fp8_e4m3_e4m3_dynamic</A>
									</DL><p>
									<DT><H3 FOLDED>nexfort-benchmarking</H3>
									<DL><p>
										<DT><A HREF="https://github.com/lukiod/T2I-and-I2I-Report/blob/main/testi2i.py">fp8_e4m3_e4m3_dynamic</A>
									</DL><p>
									<DT><H3 FOLDED>nexfort-installation</H3>
									<DL><p>
										<DT><A HREF="https://github.com/siliconflow/onediff/blob/main/README.md#nexfort">onediff/README.md at main · siliconflow/onediff</A>
										<DT><A HREF="https://github.com/siliconflow/onediff/blob/6fe6fddec747b0f1f64b4160b94ec7a0a99e246b/onediff_comfy_nodes/docs/sd3/README.md">onediff/onediff_comfy_nodes/docs/sd3/README.md at 6fe6fddec747b0f1f64b4160b94ec7a0a99e246b · siliconflow/onediff</A>
									</DL><p>
									<DT><A HREF="https://github.com/siliconflow/onediff/blob/6fe6fddec747b0f1f64b4160b94ec7a0a99e246b/onediff_comfy_nodes/extras_nodes/nodes_nexfort_booster.py#L12">compiler_modes</A>
									<DT><A HREF="https://github.com/siliconflow/onediff/tree/main/src/onediff/infer_compiler/backends/nexfort#readme">onediff/src/onediff/infer_compiler/backends/nexfort at main · siliconflow/onediff</A>
									<DT><A HREF="https://github.com/siliconflow/onediff/tree/main/src/onediff/infer_compiler">onediff/src/onediff/infer_compiler at main · siliconflow/onediff</A>
									<DT><A HREF="https://github.com/lukiod/T2I-and-I2I-Report">lukiod/T2I-and-I2I-Report</A>
									<DT><A HREF="https://github.com/siliconflow/onediff/blob/6fe6fddec747b0f1f64b4160b94ec7a0a99e246b/onediff_comfy_nodes/docs/sd3/README.md">onediff/onediff_comfy_nodes/docs/sd3/README.md at 6fe6fddec747b0f1f64b4160b94ec7a0a99e246b · siliconflow/onediff</A>
									<DT><A HREF="https://github.com/kijai/ComfyUI-CogVideoXWrapper/blob/750deb391813bfc822896599f24e740e4c14a300/nodes.py#L430">ComfyUI-CogVideoXWrapper/nodes.py: DiT cogVideoX inductor config</A>
								</DL><p>
								<DT><H3 FOLDED>onediff-diffusers-extensions</H3>
								<DL><p>
									<DT><H3 FOLDED>onediff-diffusers-extensions-installation</H3>
									<DL><p>
									</DL><p>
									<DT><A HREF="https://github.com/pytorch/ao">pytorch/ao: The missing pytorch dtype and layout library for training and inference</A>
									<DT><A HREF="https://github.com/siliconflow/onediff/blob/6b53a83bd72952c11102c30b6d739bfcb3b0f7da/onediff_diffusers_extensions/README.md#install-and-setup">onediff/onediff_diffusers_extensions/README.md at 6b53a83bd72952c11102c30b6d739bfcb3b0f7da · siliconflow/onediff</A>
									<DT><A HREF="https://github.com/siliconflow/onediff/tree/7c325253d4e280e470613be43fa3e582a476923e/onediff_diffusers_extensions">onediff/onediff_diffusers_extensions at 7c325253d4e280e470613be43fa3e582a476923e · siliconflow/onediff</A>
								</DL><p>
								<DT><A HREF="https://github.com/siliconflow/onediff/blob/main/onediff_diffusers_extensions/examples/sd3/text_to_image_sd3.py">onediff/onediff_diffusers_extensions/examples/sd3/text_to_image_sd3.py at main · siliconflow/onediff</A>
							</DL><p>
							<DT><H3 FOLDED>onediff-dit</H3>
							<DL><p>
								<DT><H3 FOLDED>onediff-flux</H3>
								<DL><p>
									<DT><A HREF="https://github.com/siliconflow/onediff/issues/1066">Add acceleration support for FLUX models · Issue #1066 · siliconflow/onediff</A>
									<DT><A HREF="https://github.com/siliconflow/onediff/blob/main/src/onediff/infer_compiler/backends/oneflow/param_utils.py#L130">onediff/src/onediff/infer_compiler/backends/oneflow/param_utils.py: get_sub_module</A>
								</DL><p>
								<DT><H3 FOLDED>onediff-diffusers-extensions</H3>
								<DL><p>
									<DT><A HREF="https://github.com/siliconflow/onediff/blob/main/onediff_diffusers_extensions/examples/sd3/text_to_image_sd3.py">onediff/onediff_diffusers_extensions/examples/sd3/text_to_image_sd3.py at main · siliconflow/onediff</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>onediff-people</H3>
							<DL><p>
								<DT><A HREF="https://chengzeyi.github.io/markdown-cv/">Cheng Zeyi's CV | CV</A>
							</DL><p>
							<DT><A HREF="https://medium.com/@SiliconFlowAI">SiliconFlow – Medium</A>
							<DT><A HREF="https://www.felixsanz.dev/articles/ultimate-guide-to-optimizing-stable-diffusion-xl">Ultimate guide to optimizing Stable Diffusion XL - Félix Sanz</A>
							<DT><A HREF="https://github.com/ccssu/how_to_write_user_op">ccssu/how_to_write_user_op: OneFlow User op 开发笔记</A>
							<DT><A HREF="https://github.com/siliconflow/onediff/wiki">Home · siliconflow/onediff Wiki</A>
						</DL><p>
						<DT><H3 FOLDED>IREE</H3>
						<DL><p>
							<DT><A HREF="https://github.com/iree-org">IREE</A>
							<DT><A HREF="https://github.com/iree-org/iree-turbine">iree-org/iree-turbine: IREE's PyTorch Frontend, based on Torch Dynamo.</A>
						</DL><p>
						<DT><H3 FOLDED>Paddle</H3>
						<DL><p>
							<DT><A HREF="https://github.com/PaddlePaddle/Paddle">PaddlePaddle/Paddle: PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice （『飞桨』核心框架，深度学习&amp;机器学习高性能单机、分布式训练和跨平台部署）</A>
						</DL><p>
						<DT><H3 FOLDED>AI Compiler Study</H3>
						<DL><p>
							<DT><H3 FOLDED>compiler-study-slides</H3>
							<DL><p>
								<DT><A HREF="https://docs.google.com/presentation/d/1QokvjA2zwqpvIIE5p4xvKl-41Ss6eqVefu_XlWNqipM/edit#slide=id.g2ef2341fd89_0_311">week8_presentation - Google Slides</A>
							</DL><p>
							<DT><H3 FOLDED>compiler-study-meeting-notes</H3>
							<DL><p>
								<DT><A HREF="https://carpedm30.notion.site/ebd2c4ec3ab9479e8a1563e900a710af?v=9a7d402493c043078dbc172b457aab13">Diffusion opt meeting notes</A>
							</DL><p>
							<DT><A HREF="https://carpedm30.notion.site/AI-Compiler-Study-aaf4cff2c8734e50ad95ac6230dbd80b">♟️ AI Compiler Study</A>
							<DT><A HREF="https://github.com/ai-compiler-study">AI Compiler Study</A>
							<DT><A HREF="https://pytorch.org/blog/cuda-free-inference-for-llms/?utm_content=306418723&utm_medium=social&utm_source=twitter&hss_channel=tw-776585502606721024">CUDA-Free Inference for LLMs | PyTorch</A>
							<DT><A HREF="https://research.colfax-intl.com/cutlass-tutorial-design-of-a-gemm-kernel/">CUTLASS Tutorial: Efficient GEMM kernel designs with Pipelining – Colfax Research</A>
						</DL><p>
						<DT><H3 FOLDED>PL</H3>
						<DL><p>
							<DT><H3 FOLDED>Python</H3>
							<DL><p>
								<DT><H3 FOLDED>py-virtual-environment</H3>
								<DL><p>
									<DT><H3 FOLDED>pipenv</H3>
									<DL><p>
										<DT><A HREF="https://pipenv.pypa.io/en/stable/">Pipenv</A>
										<DT><A HREF="https://pipenv-fork.readthedocs.io/en/latest/basics.html">Basic Usage of Pipenv</A>
										<DT><A HREF="https://stackoverflow.com/questions/50161551/set-python-version-when-creating-virtualenv-using-pipenv">Set python version when creating virtualenv</A>
									</DL><p>
									<DT><H3 FOLDED>py-virtual-environment-vanilla</H3>
									<DL><p>
										<DT><A HREF="https://docs.python.org/3/tutorial/venv.html">12. Virtual Environments and Packages — Python 3.9.6 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>conda</H3>
									<DL><p>
										<DT><H3 FOLDED>conda-linux</H3>
										<DL><p>
											<DT><A HREF="https://docs.anaconda.com/anaconda/install/linux/">Installing on Linux — Anaconda documentation</A>
											<DT><A HREF="https://repo.anaconda.com/archive/">Index of conda</A>
											<DT><A HREF="https://stackoverflow.com/questions/28852841/install-anaconda-on-ubuntu-or-linux-via-command-line">python - Install Anaconda on Ubuntu (or Linux) via command line - Stack Overflow</A>
											<DT><A HREF="https://medium.com/data-professor/how-to-install-conda-on-google-colab-e7bbf9036f76">How to install conda on Google Colab</A>
											<DT><A HREF="https://askubuntu.com/questions/1268833/error-command-path-to-env-bin-python3-7-im-ensurepip-upgrade">python - Error: Command '['/path/to/env/bin/python3.7', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1 - Ask Ubuntu</A>
										</DL><p>
										<DT><H3 FOLDED>conda-scripts</H3>
										<DL><p>
											<DT><A HREF="https://github.com/fastai/fastsetup/blob/master/setup-conda.sh">fastsetup/setup-conda.sh at master · fastai/fastsetup</A>
										</DL><p>
										<DT><A HREF="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">Managing environments — conda 4.12.0</A>
										<DT><A HREF="https://stackoverflow.com/questions/41060382/using-pip-to-install-packages-to-anaconda-environment">python - Using Pip to install packages to conda env</A>
										<DT><A HREF="https://stackoverflow.com/questions/51042589/conda-version-pip-install-r-requirements-txt-target-lib">environment.yml</A>
										<DT><A HREF="https://datumorphism.leima.is/til/programming/python/python-anaconda-install-requirements/">Installing a specific Python pip version within the virtual environment</A>
										<DT><A HREF="https://repo.anaconda.com/archive/">Index of conda</A>
										<DT><A HREF="https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#viewing-a-list-of-your-environments">Managing environments — conda 4.14.0.post16+8b846957c documentation</A>
										<DT><A HREF="https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html">Managing Python — conda 4.14.0</A>
										<DT><A HREF="https://whiteboxml.com/blog/the-definitive-guide-to-python-virtual-environments-with-conda">5.2 Packaging a conda environment with conda-pack</A>
										<DT><A HREF="https://stackoverflow.com/questions/50005949/python-3-2-in-anaconda">Python 3.2 in anaconda - Stack Overflow</A>
										<DT><A HREF="https://stackoverflow.com/questions/66607225/adding-python-3-7-to-anaconda">pip - Adding Python 3.7 to Anaconda - Stack Overflow</A>
										<DT><A HREF="https://stackoverflow.com/questions/70205633/cannot-install-python-3-7-on-osx-arm64">conda - Cannot install Python 3.7 on osx-arm64 - Stack Overflow</A>
									</DL><p>
									<DT><H3 FOLDED>virtualenv</H3>
									<DL><p>
										<DT><A HREF="https://gist.github.com/Geoyi/d9fab4f609e9f75941946be45000632b">virtualenv man</A>
									</DL><p>
									<DT><A HREF="https://towardsdatascience.com/virtual-environments-104c62d48c54">A Guide to Python’s Virtual Environments</A>
									<DT><A HREF="https://towardsdatascience.com/python-and-the-module-search-path-e71ae7a7e65f">Python and the Module Search Path</A>
									<DT><A HREF="https://towardsdatascience.com/python-the-system-path-and-how-conda-and-pyenv-manipulate-it-234f8e8bbc3e">PATH and conda</A>
								</DL><p>
								<DT><H3 FOLDED>py-package-manager</H3>
								<DL><p>
									<DT><H3 FOLDED>py-brew</H3>
									<DL><p>
										<DT><A HREF="https://docs.brew.sh/Homebrew-and-Python">Python — Homebrew Documentation</A>
									</DL><p>
									<DT><H3 FOLDED>pip</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=gSKTfG1GXYQ">(1) uv: An Extremely Fast Python Package Manager - YouTube</A>
										<DT><A HREF="https://github.com/triton-lang/triton/issues/4310">Latest nightly triton causes my custom fused attention kernel to output incorrect results. · Issue #4310 · triton-lang/triton</A>
									</DL><p>
									<DT><H3 FOLDED>package-manager-uv</H3>
									<DL><p>
										<DT><A HREF="https://pypi.org/project/uv/">uv · PyPI</A>
									</DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/46375576/get-the-list-of-packages-installed-in-anaconda">Get the list of packages installed in Anaconda</A>
									<DT><A HREF="https://stackoverflow.com/questions/41060382/using-pip-to-install-packages-to-anaconda-environment">python - Using Pip to install packages to Anaconda Environment</A>
									<DT><A HREF="https://github.com/pdm-project/pdm">pdm-project/pdm: A modern Python package and dependency manager supporting the latest PEP standards</A>
								</DL><p>
								<DT><H3 FOLDED>py-visualization</H3>
								<DL><p>
									<DT><A HREF="https://matplotlib.org/stable/tutorials/introductory/pyplot.html#">Pyplot tutorial — Matplotlib 3.5.2 documentation</A>
									<DT><A HREF="https://twitter.com/andfanilo/status/1530505914981179392/photo/2">Official cheatsheet</A>
								</DL><p>
								<DT><H3 FOLDED>py-idioms</H3>
								<DL><p>
									<DT><A HREF="https://note.nkmk.me/en/python-tuple-list-unpack/">Unpack a tuple / list in Python</A>
									<DT><A HREF="https://www.freecodecamp.org/news/list-comprehension-in-python/">List Comprehension in Python Explained for Beginners</A>
									<DT><A HREF="https://towardsdatascience.com/elegant-and-efficient-usage-of-if-else-clauses-d41d3e88fe07">Elegant And Efficient Usage of If-Else Clauses</A>
									<DT><A HREF="https://towardsdatascience.com/8-more-python-best-practices-for-writing-industry-standard-code-64d97f42da5e">8 More Python Best Practices for Writing Industry-Standard Code</A>
									<DT><A HREF="https://www.w3schools.com/python/ref_dictionary_items.asp">Python Dictionary items() Method</A>
									<DT><A HREF="https://realpython.com/python-type-checking/">Python Type Checking (Guide)</A>
									<DT><A HREF="https://github.com/openai/openai-quickstart-python/blob/master/app.py">String template and formatting</A>
									<DT><A HREF="https://docs.python.org/3/library/functools.html">functools — Higher-order functions and operations on callable objects</A>
									<DT><A HREF="https://pyneng.readthedocs.io/en/latest/book/additional_info/naming_conventions/underscore_names.html">Underscore in names - Python for network engineers</A>
									<DT><A HREF="https://docs.python.org/3/library/pprint.html">pprint — Data pretty printer — Python 3.10.6 documentation</A>
									<DT><A HREF="https://stackoverflow.com/questions/65214482/make-list-of-dictionaries-overwriting-one-key-entry-from-a-list-using-iterators">make list of dictionaries overwriting one key entry from a list using iterators</A>
									<DT><A HREF="https://github.com/lovasoa/marshmallow_dataclass">Dataclasses</A>
									<DT><A HREF="https://stackoverflow.com/questions/6578986/how-to-convert-json-data-into-a-python-object">How to convert JSON data into a Python object? - Stack Overflow</A>
									<DT><A HREF="https://towardsdatascience.com/how-to-use-variable-number-of-arguments-in-python-functions-d3a49a9b7db6">*args &amp; **kwargs</A>
									<DT><A HREF="https://www.programiz.com/python-programming/property">@property</A>
									<DT><A HREF="https://stackoverflow.com/questions/6981717/pythonic-way-to-combine-for-loop-and-if-statement">Combine for-loop and if-statement</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/model/wrapper.py#L501">model_class (dynamic import)</A>
								</DL><p>
								<DT><H3 FOLDED>py-build-system</H3>
								<DL><p>
									<DT><A HREF="https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/">pyproject.toml - pip documentation v22.2</A>
									<DT><A HREF="https://stackoverflow.com/questions/62983756/what-is-pyproject-toml-file-for">What is pyproject.toml file for?</A>
									<DT><A HREF="https://docs.python.org/3/library/functions.html#compile">Built-in Functions — Python 3.10.8 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>py-fmt</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/yapf">google/yapf: A formatter for Python files</A>
									<DT><A HREF="https://realpython.com/python-f-strings/#multiline-f-strings">f-strings</A>
									<DT><A HREF="https://google.github.io/styleguide/pyguide.html">styleguide | Style guides for Google-originated open-source projects</A>
								</DL><p>
								<DT><H3 FOLDED>py-functional-programming</H3>
								<DL><p>
									<DT><A HREF="https://towardsdatascience.com/if-you-can-write-functions-you-can-use-dask-bbb6d8b3a248">If You Can Write Functions, You Can Use Dask</A>
									<DT><A HREF="https://github.com/kykosic/pycats">kykosic/pycats: Functional Python with Typeclasses and Categories</A>
								</DL><p>
								<DT><H3 FOLDED>py-profiling</H3>
								<DL><p>
									<DT><H3 FOLDED>py-spy</H3>
									<DL><p>
										<DT><A HREF="https://www.benfrederickson.com/profiling-native-python-extensions-with-py-spy/">Profiling Native Python Extensions</A>
										<DT><A HREF="https://github.com/benfred/py-spy">benfred/py-spy: Sampling profiler for Python programs</A>
										<DT><A HREF="https://github.com/jlfwong/speedscope">jlfwong/speedscope: 🔬 A fast, interactive web-based viewer for performance profiles.</A>
									</DL><p>
									<DT><A HREF="https://blog.codingconfessions.com/p/python-profilers-intro">Everything You Wanted to Know About Profilers in Python (main)</A>
									<DT><A HREF="https://github.com/jrfonseca/gprof2dot">jrfonseca/gprof2dot: Converts profiling output to a dot graph.</A>
									<DT><A HREF="https://github.com/bloomberg/memray">bloomberg/memray: Memray is a memory profiler for Python</A>
									<DT><A HREF="https://github.com/vpelletier/pprofile">vpelletier/pprofile: Line-granularity, thread-aware deterministic and statistic pure-python profiler</A>
								</DL><p>
								<DT><H3 FOLDED>py-configuration</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/gin-config">Gin provides a lightweight configuration framework for Python</A>
									<DT><A HREF="https://www.bitecode.dev/p/python-as-a-configuration-language">Python as a configuration language - Bite code!</A>
									<DT><A HREF="https://www.bitecode.dev/">Bite code! | Substack</A>
								</DL><p>
								<DT><H3 FOLDED>py-debug</H3>
								<DL><p>
									<DT><H3 FOLDED>pdb</H3>
									<DL><p>
										<DT><A HREF="https://docs.python.org/3/library/pdb.html">pdb — The Python Debugger — Python 3.10.5 documentation</A>
										<DT><A HREF="https://www.uni-muenster.de/AMM/num/Vorlesungen/Pythonkurs_SS15/pdb_cheatsheet.pdf">pdb cheatsheet</A>
									</DL><p>
									<DT><A HREF="https://docs.python.org/3/library/pdb.html">pdb — The Python Debugger — Python 3.10.5 documentation</A>
									<DT><A HREF="https://github.com/xdit-project/xDiT/issues/324">FLUX Hopper benchmarking · Issue #324 · xdit-project/xDiT</A>
								</DL><p>
								<DT><H3 FOLDED>py-utils</H3>
								<DL><p>
									<DT><A HREF="https://docs.python.org/3/library/contextlib.html">contextlib — Utilities for with-statement contexts — Python 3.10.6 documentation</A>
									<DT><A HREF="https://book.pythontips.com/en/latest/context_managers.html">27. Context Managers — Python Tips 0.1 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>py-built-in</H3>
								<DL><p>
									<DT><H3 FOLDED>py-iterators-generators</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=yadfyn6-TzE">20240104 Iterators, Generators</A>
										<DT><A HREF="https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do">Iterable &amp; Generators &amp; Yield</A>
									</DL><p>
									<DT><H3 FOLDED>hasattr</H3>
									<DL><p>
										<DT><A HREF="https://www.w3schools.com/python/ref_func_hasattr.asp">Python hasattr() Function</A>
										<DT><A HREF="https://www.geeksforgeeks.org/python-hasattr-method/">Python hasattr() method - GeeksforGeeks</A>
									</DL><p>
									<DT><H3 FOLDED>magic methods</H3>
									<DL><p>
										<DT><A HREF="https://rszalski.github.io/magicmethods/">A Guide to Python's Magic Methods</A>
									</DL><p>
									<DT><H3 FOLDED>py-string-formattng</H3>
									<DL><p>
										<DT><A HREF="https://docs.python.org/3/tutorial/inputoutput.html">String Formattng: 7. Input and Output — Python 3.10.6 documentation</A>
										<DT><A HREF="https://www.bitecode.dev/p/string-manipulations-python-beginners">String manipulations Python beginners should know</A>
									</DL><p>
									<DT><H3 FOLDED>built-in-pattern-matching</H3>
									<DL><p>
										<DT><A HREF="https://peps.python.org/pep-0636/">PEP 636 – Structural Pattern Matching: Tutorial | peps.python.org</A>
									</DL><p>
									<DT><A HREF="https://docs.python.org/3/library/functions.html#dir">dir(): With an argument, attempt to return a list of valid attributes for that object.</A>
									<DT><A HREF="https://docs.python.org/3/library/os.html#os.system">os.system()</A>
									<DT><A HREF="https://docs.python.org/3/whatsnew/3.8.html">Walrus Operator</A>
									<DT><A HREF="https://docs.python.org/3/tutorial/inputoutput.html">String Formattng: 7. Input and Output — Python 3.10.6 documentation</A>
									<DT><A HREF="https://docs.python.org/3/tutorial/classes.html">9. Classes — Python 3.10.6 documentation</A>
									<DT><A HREF="https://www.geeksforgeeks.org/classmethod-in-python/">classmethod() in Python - GeeksforGeeks</A>
									<DT><A HREF="https://devtut.github.io/python/property-objects.html">Python - Property Objects</A>
									<DT><A HREF="https://stackoverflow.com/questions/34439/finding-what-methods-a-python-object-has">dir() - Finding what methods a Python object</A>
									<DT><A HREF="https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do">Iterable &amp; Generators &amp; Yield</A>
									<DT><A HREF="https://rszalski.github.io/magicmethods/">A Guide to Python's Magic Methods</A>
									<DT><A HREF="https://docs.python.org/3/library/shelve.html">shelve — Python object persistence — Python 3.12.0 documentation</A>
									<DT><A HREF="https://wiki.python.org/moin/UsingSlots">UsingSlots - Python Wiki</A>
									<DT><A HREF="https://www.bitecode.dev/p/python-variables-references-and-mutability">Python variables, references and mutability - Bite code!</A>
									<DT><A HREF="https://www.browserstack.com/guide/assert-in-python">Assert in Python: What is it and How to use it | BrowserStack</A>
								</DL><p>
								<DT><H3 FOLDED>py-async</H3>
								<DL><p>
									<DT><A HREF="https://superfastpython.com/asyncio-gather/">asyncio.gather()</A>
								</DL><p>
								<DT><H3 FOLDED>py-compiler</H3>
								<DL><p>
									<DT><H3 FOLDED>cython</H3>
									<DL><p>
										<DT><H3 FOLDED>ctypes</H3>
										<DL><p>
											<DT><A HREF="https://gist.github.com/fxkamd/ffd02d66a2863e444ec208ea4f3adc48">Observations about HSA and KFD backends in TinyGrad  (fast prototyping)</A>
											<DT><A HREF="https://pybind11.readthedocs.io/en/stable/">pybind11 documentation</A>
										</DL><p>
										<DT><H3 FOLDED>pxd &amp; pyx</H3>
										<DL><p>
											<DT><A HREF="https://cython.readthedocs.io/en/latest/src/tutorial/pxd_files.html">pxd files — Cython 3.1.0a0 documentation</A>
											<DT><A HREF="https://github.com/rapidsai/cudf/tree/branch-0.7/python/cudf/bindings">cudf/python/cudf/bindings</A>
										</DL><p>
										<DT><A HREF="https://github.com/explosion/cython-blis?tab=readme-ov-file">cython-blis?tab=readme-ov-file</A>
										<DT><A HREF="https://cython.org/">Cython: C-Extensions for Python</A>
										<DT><A HREF="https://github.com/facebookincubator/cinder">facebookincubator/cinder: Cinder is Meta's internal performance-oriented production version of CPython.</A>
										<DT><A HREF="https://cython.readthedocs.io/en/latest/src/tutorial/pxd_files.html">pxd files — Cython 3.1.0a0 documentation</A>
										<DT><A HREF="https://www.youtube.com/watch?v=GXwYjI9cJd0">Lightning Talk: Write Valid C++ and Python in One File - Roth Michaels - CppCon 2023 - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=MUISz2qA640&t=19s">Will Ada Replace C/C++? - YouTube</A>
										<DT><A HREF="https://trycinder.com/">Cinder Explorer</A>
										<DT><A HREF="https://github.com/mypyc/mypyc">mypyc/mypyc: Compile type annotated Python to fast C extensions</A>
										<DT><A HREF="https://blog.codingconfessions.com/p/cpython-memory-management-internals?utm_source=profile&utm_medium=reader2">CPython Memory Management Internals - by Abhinav Upadhyay</A>
										<DT><A HREF="https://github.com/google/pycnite">google/pycnite: A collection of utilities for working with compiled Python bytecode.</A>
										<DT><A HREF="https://x.com/abhi9u/status/1839631899876032540">CPython runtime internals</A>
									</DL><p>
									<DT><H3 FOLDED>clang2py</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/extra/nv_gpu_driver/codegen.sh">tinygrad/extra/nv_gpu_driver/codegen.sh at master · tinygrad/tinygrad</A>
										<DT><A HREF="https://github.com/trolldbois/ctypeslib/blob/master/ctypeslib/clang2py.py">ctypeslib/ctypeslib/clang2py.py at master · trolldbois/ctypeslib</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/de832d26c64a9ec575e47aeb58efe27a0ccf4e0b/autogen_stubs.sh">tinygrad/autogen_stubs.sh  (runtimes)</A>
									</DL><p>
									<DT><H3 FOLDED>LPython</H3>
									<DL><p>
										<DT><A HREF="https://lpython.org/blog/2023/07/lpython-novel-fast-retargetable-python-compiler/">LPython: Novel, Fast, Retargetable Python Compiler -</A>
										<DT><A HREF="https://github.com/lcompilers/lpython">lcompilers/lpython: Python compiler</A>
										<DT><A HREF="https://dev.lpython.org/">LPython</A>
									</DL><p>
									<DT><A HREF="https://lpython.org/blog/2023/07/lpython-novel-fast-retargetable-python-compiler/">LPython: Novel, Fast, Retargetable Python Compiler -</A>
									<DT><A HREF="https://pybind11.readthedocs.io/en/stable/">pybind11 documentation</A>
								</DL><p>
								<DT><H3 FOLDED>py-PEP</H3>
								<DL><p>
									<DT><A HREF="https://peps.python.org/pep-0604/">PEP 604 – Allow writing union types as X | Y | peps.python.org</A>
									<DT><A HREF="https://www.youtube.com/watch?v=YaDYUQ5mD5Q">NEW generic / alias syntax for python 3.12 (PEP 695) (intermediate) anthony explains #561 - YouTube</A>
									<DT><A HREF="https://peps.python.org/pep-0523/">PEP 523 – Adding a frame evaluation API to CPython | peps.python.org</A>
								</DL><p>
								<DT><H3 FOLDED>py-type-system</H3>
								<DL><p>
									<DT><H3 FOLDED>py-types</H3>
									<DL><p>
										<DT><H3 FOLDED>TypeVar</H3>
										<DL><p>
											<DT><A HREF="https://discuss.python.org/t/differences-in-bound-object-vs-bound-any/27052">Differences in bound=object vs bound=Any - Python Help - Discussions on Python.org</A>
											<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/bb2b2959a222594dd8a41b8bb18d7b6fe280730a/server/text_generation_server/models/model.py#L12">HF-TGI: model.py#L12 TypeVar("B", bound=Batch)</A>
											<DT><A HREF="https://stackoverflow.com/questions/59933946/difference-between-typevart-a-b-and-typevart-bound-uniona-b">python - Difference between TypeVar('T', A, B) and TypeVar('T', bound=Union[A, B]) - Stack Overflow</A>
										</DL><p>
										<DT><A HREF="http://mypy-lang.org/">mypy - Optional Static Typing for Python</A>
										<DT><A HREF="https://stackoverflow.com/questions/60459641/how-do-you-get-mypy-to-recognize-a-newer-version-of-python">How do you get mypy to recognize a newer version of python? - Stack Overflow</A>
										<DT><A HREF="https://kobzol.github.io/rust/python/2023/05/20/writing-python-like-its-rust.html">Writing Python like it’s Rust | Kobzol’s blog</A>
										<DT><A HREF="https://github.com/kykosic/pycats">kykosic/pycats: Functional Python with Typeclasses and Categories</A>
										<DT><A HREF="https://github.com/twtrubiks/python-notes/blob/master/MappingProxyType_tutorial.py">python-notes/MappingProxyType_tutorial.py</A>
									</DL><p>
									<DT><H3 FOLDED>py-interfaces-classes</H3>
									<DL><p>
										<DT><A HREF="https://github.com/google/seqio/blob/1e3a46e690f4c867e7acca7e836c425a7b0f32a7/seqio/dataset_providers.py#L244">typing.Protocol: dataset_providers.py#L244 (seqio)</A>
									</DL><p>
									<DT><H3 FOLDED>static analyzer</H3>
									<DL><p>
										<DT><H3 FOLDED>mypy</H3>
										<DL><p>
											<DT><A HREF="https://mypy-lang.org/">mypy - Optional Static Typing for Python</A>
											<DT><A HREF="http://mypy-lang.org/">mypy - Optional Static Typing for Python</A>
											<DT><A HREF="https://stackoverflow.com/questions/60459641/how-do-you-get-mypy-to-recognize-a-newer-version-of-python">How do you get mypy to recognize a newer version of python? - Stack Overflow</A>
											<DT><A HREF="https://github.com/python/mypy/issues/4440">Proper way to type circular dependency</A>
											<DT><A HREF="https://vickiboykis.com/2023/12/11/why-if-type_checking/">Why if TYPE_CHECKING?</A>
											<DT><A HREF="https://www.stefaanlippens.net/circular-imports-type-hints-python.html">Yet another solution to dig you out of a circular import hole in Python - Stefaan Lippens inserts content here</A>
											<DT><A HREF="https://peps.python.org/pep-0484/#forward-references">PEP 484 – Type Hints | peps.python.org</A>
											<DT><A HREF="https://mypy.readthedocs.io/en/stable/getting_started.html#strict-mode-and-configuration">Getting started - mypy 1.10.0 documentation</A>
											<DT><A HREF="https://www.youtube.com/watch?v=tH3Nul6jDQM">typing the untype-able with mypy plugins (advanced) anthony explains #574 - YouTube</A>
										</DL><p>
										<DT><A HREF="https://github.com/google/pytype">google/pytype: A static type analyzer for Python code</A>
									</DL><p>
									<DT><H3 FOLDED>dataclasses</H3>
									<DL><p>
										<DT><A HREF="https://docs.python.org/3/library/dataclasses.html">dataclasses — Data Classes — Python 3.12.3 documentation</A>
									</DL><p>
									<DT><A HREF="https://docs.python.org/3/library/functions.html#float">Types — Python 3.10.5 documentation</A>
									<DT><A HREF="https://peps.python.org/pep-0484/">PEP 484 – Type Hints | peps.python.org</A>
									<DT><A HREF="https://www.youtube.com/watch?v=YaDYUQ5mD5Q&t=308s">NEW generic / alias syntax for python 3.12 (PEP 695) (intermediate) anthony explains #561 - YouTube</A>
									<DT><A HREF="https://mypy.readthedocs.io/en/stable/protocols.html">Protocols and structural subtyping - mypy 1.9.0 documentation</A>
									<DT><A HREF="https://vickiboykis.com/2023/12/11/why-if-type_checking/">Why if TYPE_CHECKING?</A>
									<DT><A HREF="https://docs.python.org/3/library/typing.html#constant">typing — Support for type hints — Python 3.12.3 documentation</A>
									<DT><A HREF="https://stackoverflow.com/questions/61545580/how-does-mypy-use-typing-type-checking-to-resolve-the-circular-import-annotation">python - How does mypy use typing.TYPE_CHECKING to resolve the circular import annotation problem? - Stack Overflow</A>
									<DT><A HREF="https://www.youtube.com/watch?v=tH3Nul6jDQM">typing the untype-able with mypy plugins (advanced) anthony explains #574 - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>py-packaging</H3>
								<DL><p>
									<DT><H3 FOLDED>pyproject.toml</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/pull/2187/files#diff-50c86b7ed8ac2cf95bd48334961bf0530cdc77b5a56f852c5c61b89d735fd711">Modernize setup.py by Eliulm · Pull Request #2187 · tinygrad/tinygrad</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/d8a50d96f943c656672dc0cdb3f5fd3ed53abe57/setup.py">tinygrad/setup.py: keep for backwards compability</A>
										<DT><A HREF="https://gregoryszorc.com/blog/2023/10/30/my-user-experience-porting-off-setup.py/">Porting Off setup.py</A>
										<DT><A HREF="https://github.com/pypa/setuptools/issues/2088">Please do not remove `setup.py install` as it is needed for distribution packagers · Issue #2088 · pypa/setuptools</A>
										<DT><A HREF="https://setuptools.pypa.io/en/latest/userguide/dependency_management.html">optional dependencies: setuptools extra_require</A>
										<DT><A HREF="https://packaging.python.org/en/latest/guides/writing-pyproject-toml/">Writing your pyproject.toml - Python Packaging User Guide</A>
										<DT><A HREF="https://packaging.python.org/en/latest/specifications/dependency-specifiers/#dependency-specifiers">Dependency specifiers - Python Packaging User Guide</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/pull/2277">George Hotz suggestion: modernize setup.py</A>
									</DL><p>
									<DT><H3 FOLDED>setup.py</H3>
									<DL><p>
										<DT><A HREF="https://github.com/python-poetry/poetry">python-poetry/poetry: Python packaging and dependency management made easy</A>
										<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/setup.py">DeepSpeed/setup.py at master · microsoft/DeepSpeed</A>
										<DT><A HREF="https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#flat-layout">Package Discovery and Namespace Packages - setuptools 69.2.0.post20240313 documentation</A>
										<DT><A HREF="https://xebia.com/blog/a-practical-guide-to-using-setup-py/">A Practical Guide to Using Setup.py - Xebia</A>
										<DT><A HREF="https://github.com/beeware/briefcase/issues/1270">Install a python requirement with an --extra-index-url argument</A>
										<DT><A HREF="https://stackoverflow.com/questions/1594827/cleaning-build-directory-in-setup-py">python - Cleaning build directory in setup.py - Stack Overflow</A>
									</DL><p>
									<DT><H3 FOLDED>py-build-system</H3>
									<DL><p>
										<DT><A HREF="https://build.pypa.io/en/latest/index.html">build 1.2.1</A>
										<DT><A HREF="https://pip.pypa.io/en/stable/reference/build-system/pyproject-toml/">pyproject.toml - pip documentation v22.2</A>
										<DT><A HREF="https://stackoverflow.com/questions/62983756/what-is-pyproject-toml-file-for">What is pyproject.toml file for?</A>
										<DT><A HREF="https://docs.python.org/3/library/functions.html#compile">Built-in Functions — Python 3.10.8 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>wheel</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pypa/wheel">pypa/wheel: The official binary distribution format for Python</A>
										<DT><A HREF="https://wheel.readthedocs.io/en/stable/user_guide.html">installing wheels — wheel 0.43.0 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>requirements.txt</H3>
									<DL><p>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/integration-tests/requirements.txt">text-generation-inference/integration-tests/requirements.txt</A>
									</DL><p>
									<DT><A HREF="https://github.com/python-poetry/poetry">python-poetry/poetry: Python packaging and dependency management made easy</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/setup.py">DeepSpeed/setup.py at master · microsoft/DeepSpeed</A>
									<DT><A HREF="https://setuptools.pypa.io/en/latest/userguide/package_discovery.html#flat-layout">Package Discovery and Namespace Packages - setuptools 69.2.0.post20240313 documentation</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/setup.py">DeepSpeed/setup.py at master</A>
									<DT><A HREF="https://xebia.com/blog/a-practical-guide-to-setuptools-and-pyproject-toml/">A Practical Guide to Setuptools and Pyproject.toml - Xebia</A>
									<DT><A HREF="https://peps.python.org/pep-0517/">PEP 517 – A build-system independent format for source trees | peps.python.org</A>
									<DT><A HREF="https://peps.python.org/pep-0518/">PEP 518 – Specifying Minimum Build System Requirements for Python Projects | peps.python.org</A>
								</DL><p>
								<DT><H3 FOLDED>py-cli</H3>
								<DL><p>
									<DT><H3 FOLDED>py-cli-typer</H3>
									<DL><p>
										<DT><A HREF="https://typer.tiangolo.com/">Typer</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/pyproject.toml">text-generation-server = 'text_generation_server.cli:app'</A>
										<DT><A HREF="https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#a-full-example">Writing your pyproject.toml - Python Packaging User Guide</A>
										<DT><A HREF="https://typer.tiangolo.com/tutorial/typer-command/">typer command - Typer</A>
									</DL><p>
									<DT><H3 FOLDED>py-cli-click</H3>
									<DL><p>
										<DT><A HREF="https://click.palletsprojects.com/en/8.1.x/">Welcome to Click — Click Documentation (8.1.x)</A>
									</DL><p>
									<DT><H3 FOLDED>shlex</H3>
									<DL><p>
										<DT><A HREF="https://github.com/xdit-project/xDiT/issues/324">FLUX Hopper benchmarking · Issue #324 · xdit-project/xDiT</A>
										<DT><A HREF="https://docs.python.org/3/library/shlex.html">shlex — Simple lexical analysis — Python 3.13.1 documentation</A>
									</DL><p>
									<DT><A HREF="https://github.com/chengzeyi/piflux/blob/main/src/piflux/config.py">optional_bool_from_env: None &amp; 1 to False and True</A>
									<DT><A HREF="https://x.com/i/bookmarks?post_id=1835575326258401733">(Simo Ryu) why bother with argparse when you can use torchrun with click?</A>
								</DL><p>
								<DT><H3 FOLDED>py-subprocess</H3>
								<DL><p>
									<DT><A HREF="https://github.com/xdit-project/xDiT/issues/324">FLUX Hopper benchmarking · Issue #324 · xdit-project/xDiT</A>
								</DL><p>
								<DT><H3 FOLDED>py-serialization-deserialization</H3>
								<DL><p>
									<DT><A HREF="https://github.com/ijl/orjson">ijl/orjson: Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy</A>
								</DL><p>
								<DT><H3 FOLDED>py-internals</H3>
								<DL><p>
									<DT><H3 FOLDED>py-bytecode</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>py-logging</H3>
									<DL><p>
										<DT><H3 FOLDED>loguru</H3>
										<DL><p>
											<DT><A HREF="https://github.com/Delgan/loguru">Delgan/loguru: Python logging made (stupidly) simple</A>
											<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/cli.py">text-generation-inference/server/text_generation_server/cli.py</A>
										</DL><p>
										<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/9b6ef9e1f0d8acaefd989440b27da9069aa69207/deepspeed/utils/logging.py">DeepSpeed/deepspeed/utils/logging.py</A>
										<DT><A HREF="https://docs.python.org/3/howto/logging.html#">Logging HOWTO — Python 3.12.3 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>py-imports</H3>
									<DL><p>
										<DT><H3 FOLDED>importlib</H3>
										<DL><p>
											<DT><A HREF="https://docs.python.org/3/library/importlib.html">importlib — The implementation of import — Python 3.12.3 documentation</A>
											<DT><A HREF="https://github.com/wangzyon/pyInfer/blob/bff1d9800ffd773ab6745f2ea98d4a83dfdb032a/pyinfer/utils/common/config.py#L40">import_modules_from_strings</A>
											<DT><A HREF="https://github.com/huggingface/diffusers/blob/3511a9623f5beabf360df44cc7cb78e33d13ff4e/src/diffusers/utils/import_utils.py#L764">diffusers/src/diffusers/utils/import_utils.py</A>
										</DL><p>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=e6zFlbEU76I">Python 3 Gets TONS of New Features | Prime News - YouTube</A>
									<DT><A HREF="https://www.bitecode.dev/">Bite code! | Substack</A>
									<DT><A HREF="https://www.python.org/">Welcome to Python.org</A>
									<DT><A HREF="https://www.bitecode.dev/p/whats-up-python-the-gil-removed-a">What's up, Python? The GIL removed, a new compiler, optparse deprecated...</A>
									<DT><A HREF="https://github.com/LaurentMazare/hojo">LaurentMazare/hojo: A small python library to run iterators in a separate process</A>
									<DT><A HREF="https://www.geeksforgeeks.org/how-to-get-list-of-parameters-name-from-a-function-in-python/">How to get list of parameters name from a function in Python? - GeeksforGeeks</A>
									<DT><A HREF="https://www.bitecode.dev/p/python-variables-references-and-mutability">Python variables, references and mutability - Bite code!</A>
									<DT><A HREF="https://www.bitecode.dev/p/whats-up-python-the-gil-removed-a">What's up, Python? The GIL removed, a new compiler</A>
									<DT><A HREF="https://www.geeksforgeeks.org/how-to-get-list-of-parameters-name-from-a-function-in-python/">How to get list of parameters name from a function</A>
									<DT><A HREF="https://github.com/twtrubiks/python-notes/blob/master/MappingProxyType_tutorial.py">python-notes/MappingProxyType_tutorial.py</A>
									<DT><A HREF="https://www.teach.cs.toronto.edu/~csc110y/fall/notes/06-memory-model/04-python-memory-model-1.html">6.4 The Python Memory Model: Introduction</A>
									<DT><A HREF="https://www.youtube.com/@MichaelFoord/videos">py core object model, closures, decorators, references, iterators</A>
								</DL><p>
								<DT><H3 FOLDED>py-context-managers</H3>
								<DL><p>
									<DT><A HREF="https://docs.python.org/3/library/contextlib.html">contextlib — Utilities for with-statement contexts — Python 3.10.6 documentation</A>
									<DT><A HREF="https://book.pythontips.com/en/latest/context_managers.html">27. Context Managers — Python Tips 0.1 documentation</A>
									<DT><A HREF="https://lwn.net/Articles/706424/">Python context managers [LWN.net]</A>
								</DL><p>
								<DT><H3 FOLDED>py-testing</H3>
								<DL><p>
									<DT><H3 FOLDED>pytest</H3>
									<DL><p>
										<DT><H3 FOLDED>pytest-examples</H3>
										<DL><p>
											<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/pytest.ini">TensorRT-LLM/tests/pytest.ini at main · NVIDIA/TensorRT-LLM</A>
											<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/tests/unit/ops/adam/test_adamw.py">DeepSpeed/tests/unit/ops/adam/test_adamw.py</A>
											<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/integration-tests/conftest.py#L284">text-generation-inference/integration-tests/conftest.py</A>
										</DL><p>
										<DT><H3 FOLDED>pytest-debug</H3>
										<DL><p>
											<DT><A HREF="https://chatgpt.com/c/07ff34de-b541-4a89-9242-bb2db5aff30f">Pytest Debugging Options</A>
										</DL><p>
										<DT><H3 FOLDED>syrupy</H3>
										<DL><p>
										</DL><p>
										<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-2-moving">Testing with Python (part 2): moving to pytest</A>
										<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-3-pytest">Testing with Python (part 3): pytest setup - Bite code!</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/dc514df2afad386739bf8471ab351a86d5c5ffc7/test/conftest.py#L4">pytorch/test/conftest.py</A>
										<DT><A HREF="https://docs.pytest.org/en/7.1.x/how-to/parametrize.html">How to parametrize fixtures and test functions — pytest documentation</A>
										<DT><A HREF="https://github.com/tophat/syrupy">tophat/syrupy: :pancakes: The sweeter pytest snapshot plugin</A>
										<DT><A HREF="https://claude.ai/chat/3e157484-1ce9-4dda-91b7-1e6e7072e387">pytest: print statements and logger outputs</A>
									</DL><p>
									<DT><H3 FOLDED>unittest</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/test/test_tensor.py">tinygrad/test/test_tensor.py at master · tinygrad/tinygrad</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/tests/test_layer.py">TensorRT-LLM/tests/test_layer.py</A>
										<DT><A HREF="https://docs.python.org/3/library/unittest.html#classes-and-functions">unittest — Unit testing framework — Python 3.12.3 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>property-based testing</H3>
									<DL><p>
										<DT><H3 FOLDED>hypothesis</H3>
										<DL><p>
											<DT><A HREF="https://www.cs.toronto.edu/~david/course-notes/csc110-111/04-function-specification-and-correctness/04-testing-functions-2.html">4.4 Testing Functions II: hypothesis</A>
											<DT><A HREF="https://www.inspiredpython.com/course/testing-with-hypothesis/testing-your-python-code-with-hypothesis">Testing your Python Code with Hypothesis • Inspired Python</A>
											<DT><A HREF="https://hypothesis.readthedocs.io/en/latest/quickstart.html">Quick start guide — Hypothesis 6.100.2 documentation</A>
											<DT><A HREF="https://hypothesis.readthedocs.io/en/latest/data.html">What you can generate and how — Hypothesis 6.100.2 documentation</A>
											<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/test/test_tensor.py">tinygrad/test/test_tensor.py at master</A>
										</DL><p>
									</DL><p>
									<DT><A HREF="https://abseil.io/docs/python/guides/testing">abseil / Testing</A>
									<DT><A HREF="https://github.com/ezyang/expecttest">ezyang/expecttest ("golden" tests)</A>
									<DT><A HREF="https://github.com/abseil/abseil-py/tree/main">abseil/abseil-py: Abseil Common Libraries (Python)</A>
									<DT><A HREF="https://www.bitecode.dev/p/xmas-decoration-part-1">Xmas decoration, part 1 - Bite code!</A>
									<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-1-the-basics">Testing with Python (part 1): the basics - Bite code!</A>
									<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-2-moving">Testing with Python (part 2): moving to pytest</A>
									<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-4-why-and?utm_source=post-email-title&publication_id=1516188&post_id=144370735&utm_campaign=email-post-title&isFreemail=true&r=1tutvb&triedRedirect=true&utm_medium=email">Testing with Python (part 4): why and what to test?</A>
									<DT><A HREF="https://www.bitecode.dev/p/testing-with-python-part-5-the-different">Testing with Python (part 5): the different types of tests</A>
									<DT><A HREF="https://github.com/george0st/qgate-perf">george0st/qgate-perf: Performance tests under quality gate solution.</A>
								</DL><p>
								<DT><H3 FOLDED>py-refactor</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookincubator/Bowler">facebookincubator/Bowler: Safe code refactoring for modern Python.</A>
								</DL><p>
								<DT><H3 FOLDED>python-linter-code-formater</H3>
								<DL><p>
									<DT><H3 FOLDED>ruff</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/master/ruff.toml">tinygrad/ruff.toml at master · tinygrad/tinygrad</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>py-imports</H3>
								<DL><p>
									<DT><H3 FOLDED>py-lazy-imports</H3>
									<DL><p>
										<DT><A HREF="https://github.com/chengzeyi/stable-fast/blob/fffe290680ec2ddc01f511e8e7fc62357ed901d8/src/sfast/dynamo/backends/registry.py#L4">stable-fast/src/sfast/dynamo/backends/registry.py</A>
										<DT><A HREF="https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py">optuna/optuna/integration/__init__.py at master</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/42cae93b942ec904ead46c26c42be24422adc92c/src/diffusers/utils/import_utils.py#L760">diffusers/src/diffusers/utils/import_utils.py</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/67bef2027cc461af5bbe73b3c0f35bb1350f5aa8/src/diffusers/pipelines/consistency_models/__init__.py">diffusers/src/diffusers/pipelines/consistency_models/__init__.py</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>py-inheritance</H3>
								<DL><p>
									<DT><H3 FOLDED>py-mixin</H3>
									<DL><p>
										<DT><A HREF="https://www.residentmar.io/2019/07/07/python-mixins.html">Aleksey Bilogur— Blog</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://towardsdatascience.com/if-you-can-write-functions-you-can-use-dask-bbb6d8b3a248">If You Can Write Functions, You Can Use Dask</A>
								<DT><A HREF="https://github.com/abseil/abseil-py/tree/main">abseil/abseil-py: Abseil Common Libraries (Python)</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/9b6ef9e1f0d8acaefd989440b27da9069aa69207/deepspeed/utils/logging.py">DeepSpeed/deepspeed/utils/logging.py</A>
								<DT><A HREF="https://www.youtube.com/watch?v=yadfyn6-TzE">20240104 Iterators, Generators - YouTube</A>
								<DT><A HREF="https://towardsdatascience.com/virtual-environments-104c62d48c54">A Guide to Python’s Virtual Environments</A>
								<DT><A HREF="https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/convert.py#L4">errors as values: safetensors/bindings/python/convert.py</A>
								<DT><A HREF="https://www.youtube.com/watch?v=SzL2Oo3RktU">Breaking up long lines of code in Python - YouTube</A>
								<DT><A HREF="https://github.com/igrek51/wat">igrek51/wat: Deep inspection of Python objects</A>
								<DT><A HREF="https://x.com/srush_nlp/status/1820910844827357518">Frozen Dataclasses+Optree+Jaxtyping - the rest of Python is a really good programming language.</A>
								<DT><A HREF="https://github.com/astanin/python-tabulate">astanin/python-tabulate: Pretty-print tabular data in Python, a library and a command-line utility. Repository migrated from bitbucket.org/astanin/python-tabulate.</A>
							</DL><p>
							<DT><H3 FOLDED>Rust</H3>
							<DL><p>
								<DT><H3 FOLDED>rust-installation</H3>
								<DL><p>
									<DT><A HREF="https://www.rust-lang.org/tools/install">Install Rust - Rust Programming Language</A>
									<DT><A HREF="https://forge.rust-lang.org/infra/other-installation-methods.html">Other Installation Methods - Rust Forge</A>
									<DT><A HREF="https://sourabhbajaj.com/mac-setup/Rust/">Rust · macOS Setup Guide</A>
								</DL><p>
								<DT><H3 FOLDED>rust-procedural-macros</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=geovSK3wMB8">Procedural Macros in Rust (part 1) - YouTube</A>
									<DT><A HREF="https://github.com/dtolnay/cargo-expand">dtolnay/cargo-expand: Subcommand to show result of macro expansion</A>
									<DT><A HREF="https://github.com/dtolnay/proc-macro-workshop">dtolnay/proc-macro-workshop: Learn to write Rust procedural macros  [Rust Latam conference, Montevideo Uruguay, March 2019]</A>
								</DL><p>
								<DT><H3 FOLDED>rust-python-bindings</H3>
								<DL><p>
									<DT><A HREF="https://github.com/prql/prql/tree/main/prql-python">prql/prql-python at main · prql/prql</A>
									<DT><A HREF="https://github.com/PyO3/pyo3">PyO3/pyo3: Rust bindings for the Python interpreter</A>
									<DT><A HREF="https://pyo3.rs/v0.14.5/index.html">Introduction - PyO3 user guide</A>
									<DT><A HREF="https://github.com/google/autocxx">google/autocxx: Tool for safe ergonomic Rust/C++ interop driven from existing C++ headers</A>
									<DT><A HREF="https://github.com/LaurentMazare/hojo">LaurentMazare/hojo: A small python library to run iterators in a separate process</A>
								</DL><p>
								<DT><H3 FOLDED>rust-logging</H3>
								<DL><p>
									<DT><A HREF="https://users.rust-lang.org/t/unable-to-enable-anything-above-info-logging/27470">Unable to enable anything above INFO logging - help - The Rust Programming Language Forum</A>
									<DT><A HREF="https://rust-lang-nursery.github.io/rust-cookbook/development_tools/debugging/log.html">Log Messages - Rust Cookbook</A>
								</DL><p>
								<DT><H3 FOLDED>rust-async</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=kSQ9-JSl0z4">Rust Live | Asynchronous Rust - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=gkU4NGSe21I">Zed Decoded: Async Rust</A>
								</DL><p>
								<DT><H3 FOLDED>rust-debug</H3>
								<DL><p>
									<DT><A HREF="https://lldb.llvm.org/">LLDB Homepage — The LLDB Debugger</A>
									<DT><A HREF="https://www.youtube.com/watch?v=8D74GaBIYI4">Rust: GDB debugging - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>rust-people</H3>
								<DL><p>
									<DT><A HREF="https://github.com/oovm?tab=stars">oovm (SasakiSaki) / Starred</A>
									<DT><A HREF="https://github.com/kykosic">kykosic (Kyle Kosic) (xAI &amp; OpenAI)</A>
									<DT><A HREF="https://github.com/LaurentMazare">LaurentMazare (Laurent Mazare) (Huggingface)</A>
								</DL><p>
								<DT><H3 FOLDED>rust-crates</H3>
								<DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/66915951/rust-use-vs-mod">import - Rust use vs mod?</A>
									<DT><A HREF="https://github.com/rust-itertools/itertools">rust-itertools/itertools: Extra iterator adaptors, iterator methods, free functions, and macros.</A>
								</DL><p>
								<DT><H3 FOLDED>rust-parallel</H3>
								<DL><p>
									<DT><A HREF="https://docs.rs/rayon/1.8.0/rayon/index.html">rayon: Data-parallelism to convert sequential computations into parallel</A>
								</DL><p>
								<DT><H3 FOLDED>rust-internals</H3>
								<DL><p>
									<DT><H3 FOLDED>rustc</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=q2vJ8Faundw">Parallel</A>
									</DL><p>
									<DT><H3 FOLDED>Borrow Checker</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=KVwP6nY4xgA">Rust's borrow rules: but why, really?</A>
										<DT><A HREF="https://doc.rust-lang.org/1.8.0/book/references-and-borrowing.html">References and Borrowing</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>cgo</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=KYdlqhb267c&list=LL&index=4">RustConf 2023 - Integrating Rust and Go: Lessons from Github Code Search - YouTube</A>
									<DT><A HREF="https://pkg.go.dev/cmd/cgo">cgo command - cmd/cgo - Go Packages</A>
									<DT><A HREF="https://doc.rust-lang.org/nomicon/ffi.html">FFI - The Rustonomicon</A>
									<DT><A HREF="https://github.com/ollama/ollama/blob/main/llm/ext_server/ext_server.h">ollama/llm/ext_server/ext_server.h extern "C"</A>
								</DL><p>
								<DT><H3 FOLDED>cargo</H3>
								<DL><p>
									<DT><A HREF="https://crates.io/crates/cargo-update">cargo-update - crates.io: Rust Package Registry</A>
								</DL><p>
								<DT><A HREF="https://doc.rust-lang.org/stable/rust-by-example/mod/visibility.html">Visibility - Rust By Example</A>
								<DT><A HREF="https://replit.com/@antferdom/rustlings#.replit">rustlings - Replit</A>
								<DT><A HREF="https://doc.rust-lang.org/cargo/commands/cargo-tree.html">cargo-tree - Display a tree visualization of a dependency graph</A>
								<DT><A HREF="https://doc.rust-lang.org/book/ch02-00-guessing-game-tutorial.html">The Rust Programming Language (main)</A>
								<DT><A HREF="https://www.thecodeteacher.com/question/96947/How-do-I-%22use%22-or-import-a-local-Rust-file?">mod: Include internal code</A>
								<DT><A HREF="https://learning-rust.github.io/docs/a4.cargo,crates_and_basic_project_structure.html">Cargo, Crates and Basic Project Structure | Learning Rust</A>
								<DT><A HREF="https://github.com/clap-rs/clap/blob/master/examples/cargo-example.md">clap: command line argparser</A>
								<DT><A HREF="https://www.youtube.com/watch?v=D1NAREuicNs">Flame Graphs and ASM optimizations</A>
								<DT><A HREF="https://dev.to/tangram/writing-the-fastest-gbdt-libary-in-rust-197k">Writing the fastest GBDT libary in Rust - DEV Community</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-h1oa6GYvV8">Rust Memory Ordering (Atomics and Locks Chapter 3) - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=QHjSJC6yrs8&t=20s">Programming in Rust: Enums, Tagged Unions, Memory Layout and Pattern Matching - YouTube</A>
								<DT><A HREF="https://github.com/slawlor/ractor">slawlor/ractor: Rust actor framework</A>
								<DT><A HREF="https://github.com/LaurentMazare/tch-rs">LaurentMazare/tch-rs: Rust bindings for the C++ api of PyTorch.</A>
								<DT><A HREF="https://github.com/kykosic/actix-pytorch-example/tree/master">kykosic/actix-pytorch-example: An example of using Torch rust bindings to serve trained machine learning models via Actix Web</A>
								<DT><A HREF="https://github.com/sxyazi/yazi">sxyazi/yazi: 💥 Blazing fast terminal file manager written in Rust, based on async I/O.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=VJsPd24gByY">Episode 006: Zig and Rust - YouTube</A>
								<DT><A HREF="https://github.com/matklad/xshell">matklad/xshell</A>
								<DT><A HREF="https://github.com/dsherret/dax">dsherret/dax: Cross platform shell tools for Deno inspired by zx.</A>
								<DT><A HREF="https://github.com/google/zx">google/zx: A tool for writing better scripts</A>
								<DT><A HREF="https://google.github.io/comprehensive-rust/">Welcome to Comprehensive Rust 🦀 - Comprehensive Rust 🦀</A>
								<DT><A HREF="https://github.com/google/zerocopy">google/zerocopy</A>
								<DT><A HREF="https://github.com/google/autocxx">google/autocxx: Tool for safe ergonomic Rust/C++ interop driven from existing C++ headers</A>
								<DT><A HREF="https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html">Recoverable Errors with Result - The Rust Programming Language</A>
								<DT><A HREF="https://dev.to/chaudharypraveen98/form-validation-in-rust-404l">Form Validation in Rust (Actix-Web) - DEV Community</A>
								<DT><A HREF="https://www.youtube.com/watch?v=KVwP6nY4xgA">Rust's borrow rules: but why, really?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=g6mUtBVESb0">Rust's trait system is a proof engine, let's make it prove us an ABI!</A>
								<DT><A HREF="https://github.com/LukeMathWalker/pavex/tree/main">LukeMathWalker/pavex: An easy-to-use Rust framework for building robust and performant APIs</A>
								<DT><A HREF="https://github.com/nadavrot/compressor">nadavrot/compressor: An educational implementation of a modern compressor in Rust</A>
								<DT><A HREF="https://www.youtube.com/watch?v=fpkjmE-56Gw">Rust Allocators and Memory Management - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Kdpfhj3VM04">Compiler-Driven Development in Rust - YouTube</A>
								<DT><A HREF="https://github.com/google/comprehensive-rust">google/comprehensive-rust: This is the Rust course used by the Android team at Google. It provides you the material to quickly teach Rust.</A>
							</DL><p>
							<DT><H3 FOLDED>cpp</H3>
							<DL><p>
								<DT><H3 FOLDED>cpp-building</H3>
								<DL><p>
									<DT><H3 FOLDED>cpp-installation</H3>
									<DL><p>
										<DT><A HREF="http://www-scf.usc.edu/~csci104/20142/installation/gccmac.html">CSCI 104 – Installing G++ on a Mac</A>
										<DT><A HREF="https://www.moncefbelyamani.com/how-to-install-xcode-homebrew-git-rvm-ruby-on-mac/">including M1 Apple Silicon</A>
										<DT><A HREF="https://stackoverflow.com/questions/68880134/gdb-no-bottle-available-gdb-install">macos - gdb: no bottle available-gdb install (ARM not supported)</A>
									</DL><p>
									<DT><H3 FOLDED>bazel-cpp</H3>
									<DL><p>
										<DT><A HREF="https://github.com/run-ai/runai-model-streamer/blob/master/cpp/WORKSPACE">runai-model-streamer/cpp/WORKSPACE at master · run-ai/runai-model-streamer</A>
										<DT><A HREF="https://github.com/run-ai/runai-model-streamer/blob/master/cpp/rules.bzl">runai-model-streamer/cpp/rules.bzl at master · run-ai/runai-model-streamer</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>cpp-compilation</H3>
								<DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/3178342/compiling-a-c-program-with-gcc">Compiling a C++ program with gcc - Stack Overflow</A>
									<DT><A HREF="https://www.youtube.com/watch?v=iYHVR5NrOYQ&list=LL&index=3&t=1062s">Static Library and Shared Library In C using GCC, Linux, Ubuntu - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-comptime</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=5eneQ9mFbpA">The Superpower of C++ - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-memory-model</H3>
								<DL><p>
									<DT><H3 FOLDED>RAII</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>memory-arenas</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=K1heXXn324Q">Pointers | Arena Allocator in C - YouTube</A>
									</DL><p>
									<DT><A HREF="https://medium.com/swlh/writing-c-when-youre-a-java-developer-memory-management-7c42e222645e">Memory Management</A>
									<DT><A HREF="https://isocpp.org/wiki/faq/freestore-mgmt">Memory Management</A>
									<DT><A HREF="https://thenumb.at/rpp/">Oxidizing C++</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-learning</H3>
								<DL><p>
									<DT><A HREF="https://github.com/federico-busato/Modern-CPP-Programming">federico-busato/Modern-CPP-Programming: Modern C++ Programming Course (C++03/11/14/17/20/23/26)</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-libc</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>cpp-standard</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>stl-algorithms</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=zlJg9mCNfkQ">Thrust and the C++ Standard Algorithms - Conor Hoekstra - GTC 2021 - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=W2tWOdzgXHA">GoingNative 2013 C++ Seasoning</A>
									<DT><A HREF="https://www.youtube.com/watch?v=h4Jl1fk3MkQ">CppCon 2016: Marshall Clow “STL Algorithms - why you should use them, and how to write your own" - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=2olsGf6JIkU">CppCon 2018: Jonathan Boccara “105 STL Algorithms in Less Than an Hour” - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=48gV1SNm3WA">C++Now 2019: Conor Hoekstra “Algorithm Intuition”</A>
									<DT><A HREF="https://github.com/codereport/Content/tree/main/Talks">Content/Talks at main · codereport/Content</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-error-handling</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=uj9ozuzZy6g">C++23's std::expected - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>cpp-file</H3>
								<DL><p>
									<DT><A HREF="https://github.com/run-ai/runai-model-streamer/blob/master/cpp/utils/temp/file/file.h">runai-model-streamer/cpp/utils/temp/file/file.h at master · run-ai/runai-model-streamer</A>
								</DL><p>
								<DT><H3 FOLDED>carbon</H3>
								<DL><p>
									<DT><A HREF="https://github.com/carbon-language/carbon-lang">carbon-language/carbon-lang: Carbon Language's main repository: documents, design, implementation, and related tools. (NOTE: Carbon Language is experimental; see README)</A>
								</DL><p>
								<DT><H3 FOLDED>cosmopolitan</H3>
								<DL><p>
									<DT><A HREF="https://github.com/jart/cosmopolitan">jart/cosmopolitan: build-once run-anywhere c library</A>
								</DL><p>
								<DT><A HREF="https://medium.com/swlh/writing-c-when-youre-a-java-developer-memory-management-7c42e222645e">Memory Management</A>
								<DT><A HREF="https://isocpp.org/wiki/faq/freestore-mgmt">Memory Management</A>
								<DT><A HREF="https://www.youtube.com/watch?v=BP6NxVxDQIs&t=37s">CppCon 2016: Timur Doumler “Want fast C++? Know your hardware!" - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Qv1Yn-3lvtU">Refactoring to C++23 with GCC 13 - YouTube</A>
								<DT><A HREF="https://github.com/carbon-language/carbon-lang">carbon-language/carbon-lang: Carbon Language's main repository: documents, design, implementation, and related tools. (NOTE: Carbon Language is experimental; see README)</A>
								<DT><A HREF="https://github.com/harrism/cpp11-range/tree/70f844968c5f669ce85f8ce4cbd24a3584c57f4b">harrism/cpp11-range</A>
								<DT><A HREF="https://omairmajid.com/posts/2020-07-08-what-is-glibcxx-error/">2020-07-08-what-is-glibcxx-error</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-erXR6k9TeE">C++Now 2018: Rong Lu “C++ Development with Visual Studio Code” - YouTube</A>
								<DT><A HREF="https://learn.microsoft.com/en-us/cpp/cpp/object-lifetime-and-resource-management-modern-cpp?view=msvc-170">Object lifetime and resource management (RAII) | Microsoft Learn</A>
								<DT><A HREF="https://learn.microsoft.com/en-us/cpp/cpp/templates-cpp?view=msvc-170&source=recommendations">Templates (C++) | Microsoft Learn</A>
								<DT><A HREF="https://github.com/google/autocxx">google/autocxx: Tool for safe ergonomic Rust/C++ interop driven from existing C++ headers</A>
								<DT><A HREF="https://github.com/google/mosaic">google/mosaic: A C++ bindings generator for Rust.</A>
								<DT><A HREF="https://thenumb.at/rpp/">Oxidizing C++</A>
								<DT><A HREF="https://www.youtube.com/watch?v=FnMfhWiSweo">Low-Latency Trading Systems in C++: Templated Meta-State Machines in HFT - Jason McGuiness - ACCU 23 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=fsIfQqRjc1U">1 Problem, 4 C++'s - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=YBtnqaMTfHg">Glean: Code Indexing at Meta - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=50sQUgBZCIA">{fmt}: The Cool Parts - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MUISz2qA640&t=19s">Will Ada Replace C/C++? - YouTube</A>
								<DT><A HREF="https://gist.github.com/Chillee/fbd504a65312893df1b402624042a965">Educational implementations</A>
								<DT><A HREF="https://gist.github.com/Chillee/c729ac9d1995665ea9426226c4203ca5">elapsed_time</A>
								<DT><A HREF="https://github.com/mcinglis/c-style">mcinglis/c-style: My favorite C programming practices.</A>
								<DT><A HREF="https://www.geeksforgeeks.org/exit0-vs-exit1-in-c-c-with-examples/">exit(0) vs exit(1) in C/C++ with Examples - GeeksforGeeks</A>
								<DT><A HREF="https://www.youtube.com/watch?v=g7CCaRwRVBQ&list=PL71Y0EmrppR0KyZvQWj63040UEzKQU7n8">Advanced C #1: Function Pointers - YouTube</A>
								<DT><A HREF="https://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html">What Every C Programmer Should Know About Undefined Behavior #1/3 - The LLVM Project Blog</A>
								<DT><A HREF="https://www.youtube.com/watch?v=n7Tl1qJxTew">Uninitialized Uses in Systems C++ Programming: The Bytes Before the C++ Types - JF Bastien - YouTube</A>
								<DT><A HREF="https://github.com/CppOnlineConference/CppOnline2024">CppOnlineConference/CppOnline2024: Slide Repository For CppOnline 2024</A>
								<DT><A HREF="https://github.com/jarro2783/cxxopts/tree/eb787304d67ec22f7c3a184ee8b4c481d04357fd">jarro2783/cxxopts: Lightweight C++ command line option parser</A>
								<DT><A HREF="https://github.com/nlohmann/json/tree/bc889afb4c5bf1c0d8ee29ef35eaaf4c8bef8a5d">nlohmann/json: JSON for Modern C++</A>
								<DT><A HREF="https://github.com/drisspg/simple_cpp/tree/main">drisspg/simple_cpp</A>
								<DT><A HREF="https://github.com/federico-busato/Modern-CPP-Programming">federico-busato/Modern-CPP-Programming: Modern C++ Programming Course (C++03/11/14/17/20/23/26)</A>
								<DT><A HREF="https://github.com/p-ranav/argparse">p-ranav/argparse: Argument Parser for Modern C++</A>
								<DT><A HREF="https://leimao.github.io/blog/Static-Library-VS-Shared-Library/">Static Library VS Shared Library - Lei Mao's Log Book</A>
								<DT><A HREF="https://github.com/filipdutescu/modern-cpp-template">filipdutescu/modern-cpp-template: A template for modern C++ projects using CMake, Clang-Format, CI, unit testing and more, with support for downstream inclusion.</A>
							</DL><p>
							<DT><H3 FOLDED>Zig</H3>
							<DL><p>
								<DT><H3 FOLDED>zig-build-system</H3>
								<DL><p>
									<DT><A HREF="https://ziglearn.org/chapter-3/">Chapter 3 - Build system | ziglearn.org</A>
									<DT><A HREF="https://www.youtube.com/watch?v=-XLSyaJ6m3o&t=902s">WTF is Build.Zig? by Ed Yu - YouTube</A>
									<DT><A HREF="https://github.com/kimmolinna/duckdb-zig-build/">kimmolinna/duckdb-zig-build: DuckDB is an in-process SQL OLAP Database Management System</A>
									<DT><A HREF="https://github.com/akhildevelops/cudaz">akhildevelops/cudaz: A Zig Cuda wrapper</A>
								</DL><p>
								<DT><H3 FOLDED>zig-people</H3>
								<DL><p>
									<DT><A HREF="https://github.com/fengb">fengb (Benjamin Feng)</A>
								</DL><p>
								<DT><H3 FOLDED>zig-http-web-server</H3>
								<DL><p>
									<DT><A HREF="https://github.com/zigzap/zap">zigzap/zap: blazingly fast backends in zig</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=_WccWcx0p4k">Zig overview</A>
								<DT><A HREF="https://www.youtube.com/watch?v=iZFXAN8kpPo">Advanced Hello World in Zig - Loris Cro - YouTube</A>
								<DT><A HREF="https://log2base2.com/c-with-dsa?utm_src=youtube&utm_target=ycwdeug1&gclid=CjwKCAjwt52mBhB5EiwA05YKo4S_xtW1JucfNad8OMbuQj0b_tg5eej5VfMOx5M8PdeFGvIrlEiHeRoCxHkQAvD_BwE">Learn C Programming | Pointers Visualization | Log2Base2</A>
								<DT><A HREF="https://www.youtube.com/watch?v=vHWiDx_l4V0">What's a Memory Allocator Anyway? - Benjamin Feng - YouTube</A>
								<DT><A HREF="https://github.com/fengb/zee_alloc">fengb/zee_alloc: tiny Zig allocator primarily targeting WebAssembly</A>
								<DT><A HREF="https://www.youtube.com/watch?v=CwXixVcliP0">How to Build Software From Source - Andrew Kelley - Software You Can Love Vancouver 2023 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=x1N9JPPPC18&t=4s">Proficient Parallel Programming - King Butcher - Software You Can Love VC 2023 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=8MbREuiLQrM">Zig Compiler Internals - Andrew Kelley - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=1N85yU6RMcY">Ziglibc: Sweeping out the rug from underneath C - Jonathan Marler - Software You Can Love 2022 - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=BiYPrMjPU60">Zig Lexer : Finished it!!! - YouTube</A>
								<DT><A HREF="https://github.com/unum-cloud/ucall">unum-cloud/ucall: Remote Procedure Calls - 50x lower latency and 70x higher bandwidth than FastAPI, implementing REST &amp; JSON-RPC over io_uring and SIMDJSON ☎️</A>
								<DT><A HREF="https://www.youtube.com/watch?v=VJsPd24gByY">Episode 006: Zig and Rust - YouTube</A>
								<DT><A HREF="https://github.com/allyourcodebase/zlib">allyourcodebase/zlib: https://www.zlib.net/</A>
								<DT><A HREF="https://www.youtube.com/watch?v=xIPrwrBAU2c">Zig Data Structure Katas - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=D5XTnYAgIp0">Episode 013: Prepare Repair - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=a--v9mt8ep0">Episode 21: Grid.Write - YouTube</A>
								<DT><A HREF="https://zig.news/edyu/zig-package-manager-wtf-is-zon-2-0110-update-1jo3">Zig Package Manager 2 - WTF is Build.Zig.Zon and Build.Zig (0.11.0 Update) - Zig NEWS</A>
								<DT><A HREF="https://www.youtube.com/watch?v=pnnx1bkFXng">i changed my mind about zig - YouTube</A>
								<DT><A HREF="https://github.com/Meenachinmay/5-million-Txns-in-Tigerbeetle">Meenachinmay/5-million-Txns-in-Tigerbeetle: Trying Tigerbeetle transactional database.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=MWy_mrmE4gs">Episode 034: Prefetching From Disk - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=a--v9mt8ep0&t=247s">Episode 21: Grid.Write - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>Go</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ollama/ollama/blob/main/llm/ext_server/ext_server.h">ollama/llm/ext_server/ext_server.h extern "C"</A>
								<DT><A HREF="https://github.com/tliron/py4go">tliron/py4go: Tight bidirectional integration between Go and Python</A>
								<DT><A HREF="https://github.com/tliron/py4go/tree/main/examples/hello-world">py4go/examples/hello-world at main · tliron/py4go</A>
								<DT><A HREF="https://github.com/bytedance/gopkg?tab=readme-ov-file">bytedance/gopkg: Universal Utilities for Go</A>
								<DT><A HREF="https://github.com/Meenachinmay">Meenachinmay (Chinmay Anand)</A>
								<DT><A HREF="https://www.slideshare.net/slideshow/golang-mobile-app-golang-introduction-of-golang/257575240">[Golang] 以 Mobile App 工程師視角，帶你進入 Golang 的世界 (Introduction of GoLang) | PPT</A>
								<DT><A HREF="https://github.com/bytedance/gopkg">bytedance/gopkg: Universal Utilities for Go</A>
							</DL><p>
							<DT><H3 FOLDED>Java</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=BaUrpq_7KMk">How Netflix Really Uses Java - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>Gleam</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=D88S_RdagP8">Introduction to Gleam's Concurrency Model</A>
								<DT><A HREF="https://www.youtube.com/@lpil">Louis Pilfold - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>array programming</H3>
							<DL><p>
								<DT><A HREF="https://www.brainstobytes.com/hands-on-numpy-universal-functions-and-array-oriented-programming/">Hands-on NumPy(IV): Universal Functions and Array-oriented Programming</A>
								<DT><A HREF="https://link.springer.com/chapter/10.1007/978-3-030-74386-4_1">Tensor Computation | SpringerLink</A>
								<DT><A HREF="https://www.youtube.com/watch?v=aFal9-SJjGY&list=PL_lsbAsL_o2BivkGLiDfHY9VqWlaNoZ2O&index=40">Keynote: The Promise of PyTorch as a General-Purpose Array-Oriented Computational..- Travis Oliphant - YouTube</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>machine learning compilation</H3>
						<DL><p>
							<DT><A HREF="https://mlc.ai/">Machine Learning Compiler — Machine Learing Compiler 0.0.1 documentation</A>
						</DL><p>
						<DT><H3 FOLDED>compilers-tuning</H3>
						<DL><p>
							<DT><A HREF="https://research.google/blog/advancements-in-machine-learning-for-machine-learning/">Advancements in machine learning for machine learning (Google AI)</A>
							<DT><A HREF="https://www.youtube.com/watch?v=esD_zvAf49I">Autotuning Production Machine Learning Compilers | SAMPL Talk 2021/11/04 - YouTube</A>
						</DL><p>
						<DT><A HREF="https://research.google/blog/advancements-in-machine-learning-for-machine-learning/">Advancements in machine learning for machine learning (Google AI)</A>
						<DT><A HREF="https://carpedm30.notion.site/AI-Compiler-Study-aaf4cff2c8734e50ad95ac6230dbd80b">♟️ AI Compiler Study</A>
						<DT><A HREF="https://gist.github.com/sophiawisdom/ccdff5b7ebcd782393dbc5be3f0866f9">shittytransformer.py</A>
						<DT><A HREF="https://www.thonking.ai/p/strangely-matrix-multiplications">Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data! [short]</A>
						<DT><A HREF="https://github.com/daadaada/turingas">daadaada/turingas: Assembler for NVIDIA Volta and Turing GPUs</A>
						<DT><A HREF="https://github.com/banach-space/llvm-tutor">banach-space/llvm-tutor: A collection of out-of-tree LLVM passes for teaching and learning</A>
						<DT><A HREF="https://github.com/yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs">yzhaiustc/Optimizing-SGEMM-on-NVIDIA-Turing-GPUs: Optimizing SGEMM kernel functions on NVIDIA GPUs to a close-to-cuBLAS performance.</A>
						<DT><A HREF="https://twitter.com/cis_female/status/1737448740620013751/photo/1">Sophia: Quantitative Analsysis</A>
						<DT><A HREF="https://startup.jobs/graph-compiler-engineer-openai-4658227">Graph Compiler Engineer at OpenAI</A>
						<DT><A HREF="https://github.com/IST-DASLab">IST Austria Distributed Algorithms and Systems Lab</A>
						<DT><A HREF="https://www.zhihu.com/people/liang-de-peng">GiantPandaCV (chinese high-proffesional discussions)</A>
						<DT><A HREF="https://github.com/microsoft/Olive">microsoft/Olive: Olive is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation.</A>
						<DT><A HREF="https://pytorchtoatoms.substack.com/p/nvidia-quantum-x800-next-generation">NVIDIA Quantum-X800: Next Generation Infiniband 800Gbit/s Network Topology</A>
						<DT><A HREF="https://x.com/PytorchToAtoms">(1) Pytorch To Atoms (@PytorchToAtoms) / X</A>
						<DT><A HREF="https://www.youtube.com/watch?v=XD9DqcZOB3A">Cornell ECE 5545: Guest Lecture: CentML Gennady Pekhimenko - YouTube</A>
						<DT><A HREF="https://github.com/dorjeduck/llm.mojo">dorjeduck/llm.mojo: port of Andrjey Karpathy's llm.c to Mojo</A>
						<DT><A HREF="https://research.colfax-intl.com/tutorial-python-binding-for-cuda-libraries-in-pytorch/">Tutorial: Python bindings for CUDA libraries in PyTorch – Colfax Research</A>
						<DT><A HREF="https://twitter.com/cis_female/status/1782576009239560336/photo/1">(1) sophia (chrysanthemum princess) (@cis_female) / X</A>
						<DT><A HREF="https://twitter.com/cis_female/status/1660386176761724928">(1) sophia (chrysanthemum princess) en X: "I implemented the transformer in 30 minutes and 30 lines of python which compiles to 6670 instructions of gpu microcode (lower-level assembly), or ~100kb total (twitter js is ~1000kb). This shitty implementation achieves 40% of the A100's *THEORETICAL MAXIMUM* performance" / X</A>
						<DT><A HREF="https://gist.github.com/sophiawisdom/4b3886a251d0728625dd8d1f76e9eb60">cublas fp16 matmul</A>
						<DT><A HREF="https://gist.github.com/sophiawisdom/b8e63dc8ce6037b6032eeb010b93e446">layernorm</A>
						<DT><A HREF="https://github.com/facebookincubator/dynolog/tree/main?tab=readme-ov-file#gpu-monitoring">facebookincubator/dynolog: Dynolog is a telemetry daemon for performance monitoring and tracing. It exports metrics from different components in the system like the linux kernel, CPU, disks, Intel PT, GPUs etc. Dynolog also integrates with pytorch and can trigger traces for distributed training applications.</A>
						<DT><A HREF="https://www.youtube.com/watch?v=6BiNzPdy6YA">[REFAI Seminar 04/16/24] ML for ML Compilers at Google - YouTube</A>
						<DT><A HREF="https://www.modular.com/blog/how-to-be-confident-in-your-performance-benchmarking">Modular: How to Be Confident in Your Performance Benchmarking</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/IEEE_754">IEEE 754 - Wikipedia</A>
						<DT><A HREF="https://github.com/chengzeyi/yatc">chengzeyi/yatc: A Pure Python Deep Learning Compiler</A>
						<DT><A HREF="https://chsasank.com/sycl-portable-cuda-alternative.html">SYCL: A Portable Alternative to CUDA - Sasank's Blog</A>
						<DT><A HREF="https://github.com/merrymercy/awesome-tensor-compilers">merrymercy/awesome-tensor-compilers: A list of awesome compiler projects and papers for tensor computation and deep learning.</A>
						<DT><A HREF="https://github.com/tensor-fusion/compiler-explorer">tensor-fusion/compiler-explorer: Run compilers interactively from your web browser and interact with the assembly</A>
						<DT><A HREF="https://github.com/run-ai">run:ai</A>
						<DT><A HREF="https://2024resumedropco-design.splashthat.com/">Meta Research AI and Systems Co-Design Resume Drop (PhD New Grad/Intern)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=4HgShra-KnY&t=1022s">ASPLOS Keynote: The Golden Age of Compiler Design in an Era of HW/SW Co-design by Dr. Chris Lattner - YouTube</A>
						<DT><A HREF="https://github.com/hikettei/Caten">hikettei/Caten: [wip] Deep Learning Compiler based on Polyhedral Compiler, Light-weight IRs, and Optimizing Pattern Matcher.</A>
					</DL><p>
					<DT><H3 FOLDED>sw-large-transformer-model-inference-optimization</H3>
					<DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-quantization</H3>
						<DL><p>
							<DT><H3 FOLDED>sw-post-training quantization</H3>
							<DL><p>
								<DT><H3 FOLDED>LLM.int8()</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/joaoalvarenga/bloom-8bit">bloom-8bit</A>
									<DT><A HREF="https://huggingface.co/blog/hf-bitsandbytes-integration">A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes</A>
									<DT><A HREF="https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es">GPT-J 8-bit compression</A>
								</DL><p>
								<DT><H3 FOLDED>int4</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/NolanoOrg/status/1635409631530057728">LLaMa int4</A>
									<DT><A HREF="https://github.com/openai/triton/issues/675">int4 support · Issue #675 · openai/triton</A>
								</DL><p>
								<DT><A HREF="https://pytorch.org/docs/stable/quantization.html">Quantization — PyTorch 2.0 documentation</A>
								<DT><A HREF="https://www.philschmid.de/static-quantization-optimum">Static Quantization with HF Optimum</A>
								<DT><A HREF="https://www.philschmid.de/bert-deepspeed-inference">Accelerate BERT inference with DeepSpeed-Inference on GPUs</A>
								<DT><A HREF="https://blog.speechmatics.com/gpu-quantisation">Fast and Accurate GPU Quantization for Transformers</A>
							</DL><p>
							<DT><H3 FOLDED>sw-Transformer Engine (FP8)</H3>
							<DL><p>
								<DT><A HREF="https://github.com/cchan/nanoGPT-fp8">cchan/nanoGPT-fp8</A>
								<DT><A HREF="https://twitter.com/itsclivetime/status/1655515089506820097">(1) Clive Chan en Twitter: "WIP FP8 training on consumer graphics cards - 🧵/4 I hacked nanoGPT to use TransformerEngine on RTX 4090 and ran a few iterations of GPT-2 training: - nanoGPT Block (+flashattn) =&amp;gt; TE TransformerLayer (both BF16): 15% faster - BF16 =&amp;gt; FP8: additional +18% https://t.co/cJNWehoGeu" / Twitter</A>
								<DT><A HREF="https://blogs.bing.com/Engineering-Blog/october-2021/Bing-delivers-more-contextualized-search-using-quantized-transformer-inference-on-NVIDIA-GPUs-in-Azu">Microsoft's Bing example</A>
							</DL><p>
							<DT><H3 FOLDED>sw-Activation-aware Weight Quantization</H3>
							<DL><p>
								<DT><A HREF="https://github.com/mit-han-lab/llm-awq">mit-han-lab/llm-awq: AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</A>
								<DT><A HREF="https://twitter.com/jilin_14/status/1683377972840124417">TinyChat (RTX4090)</A>
							</DL><p>
							<DT><A HREF="https://github.com/mit-han-lab/smoothquant">SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</A>
							<DT><A HREF="https://arxiv.org/pdf/2004.09602.pdf">Integer Quantization For Deep Learing Inference: Principles and Evaluation</A>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html">Transformer Engine documentation — Transformer Engine 0.7.0 documentation</A>
							<DT><A HREF="https://github.com/IST-DASLab/gptq">IST-DASLab/gptq: Code for the ICLR 2023 paper "GPTQ: Accurate Post-training Quantization of Generative Pretrained Transformers".</A>
							<DT><A HREF="https://huggingface.co/docs/transformers/main_classes/quantization#fp4-quantization">Quantize 🤗 Transformers models</A>
							<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
							<DT><A HREF="https://github.com/IST-DASLab/marlin">IST-DASLab/marlin: FP16xINT4 LLM inference kernel that can achieve near-ideal ~4x speedups up to medium batchsizes of 16-32 tokens.</A>
							<DT><A HREF="https://github.com/IST-DASLab/QUIK">IST-DASLab/QUIK: Repository for the QUIK project, enabling the use of 4bit kernels for generative inference</A>
							<DT><A HREF="https://github.com/mit-han-lab/qserve">mit-han-lab/qserve: QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-sparsity</H3>
						<DL><p>
							<DT><H3 FOLDED>sw-transformer-inference-sparsity-DeepSpeed</H3>
							<DL><p>
								<DT><A HREF="https://www.deepspeed.ai/tutorials/sparse-attention/#how-to-config-sparsity-structures">DeepSpeed Sparse Attention</A>
							</DL><p>
							<DT><A HREF="https://github.com/EleutherAI/gpt-neox/blob/main/configs/sparse.yml">gpt-neox/sparse.yml at main · EleutherAI/gpt-neox · GitHub</A>
							<DT><A HREF="https://docs.cerebras.net/en/latest/wsc/how_to_guides/sparsity.html#id1">Train a model with weight sparsity (Beta) — Cerebras Developer Documentation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=4gKYE9-YtP0">MICRO'23 TorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs - YouTube</A>
							<DT><A HREF="https://github.com/AlibabaResearch/flash-llm">AlibabaResearch/flash-llm: Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity</A>
							<DT><A HREF="https://github.com/ptillet/torch-blocksparse">ptillet/torch-blocksparse: Block-sparse primitives for PyTorch</A>
							<DT><A HREF="https://github.com/IST-DASLab/SparseFinetuning">IST-DASLab/SparseFinetuning: Repository for Sparse Finetuning of LLMs via modified version of the MosaicML llmfoundry</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-pruning</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-architectural-optimization</H3>
						<DL><p>
							<DT><H3 FOLDED>Adaptive Computation</H3>
							<DL><p>
								<DT><A HREF="https://arxiv.org/pdf/2207.07061.pdf">Confident Adaptive Language Modeling (CALM)</A>
								<DT><A HREF="https://github.com/hao-ai-lab/LookaheadDecoding">hao-ai-lab/LookaheadDecoding</A>
								<DT><A HREF="https://arxiv.org/pdf/2305.10427.pdf">Accelerating Transformer Inference for Translation via Parallel Decoding</A>
							</DL><p>
							<DT><H3 FOLDED>FasterTransformer</H3>
							<DL><p>
								<DT><A HREF="https://fast-transformers.github.io/">Fast Transformers for PyTorch</A>
								<DT><A HREF="https://fast-transformers.github.io/#research">Fast Transformers for PyTorch</A>
								<DT><A HREF="https://github.com/NVIDIA/FasterTransformer">NVIDIA/FasterTransformer: Transformer related optimization, including BERT, GPT</A>
								<DT><A HREF="https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566">How to convert the SalesForce CodeGen models to GPT-J</A>
								<DT><A HREF="https://developer.nvidia.com/blog/increasing-inference-acceleration-of-kogpt-with-fastertransformer/">Increasing Inference Acceleration of KoGPT with NVIDIA FasterTransformer | NVIDIA Technical Blog</A>
								<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-CWES52119/?ncid=em-even-124008-vt33">Connect with the Experts: GPU Performance Analysis and Optimization | NVIDIA On-Demand</A>
								<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-S51196/?ncid=em-even-124008-vt33">FP8</A>
								<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/tree/6ea1c77c7fabf1a046463eceddce1839efc63e60">NVIDIA/FasterTransformer at 6ea1c77c7fabf1a046463eceddce1839efc63e60</A>
								<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/blob/6ea1c77c7fabf1a046463eceddce1839efc63e60/examples/pytorch/gpt/gpt_example.py#L237">FasterTransformer/examples/pytorch/gpt/gpt_example.py at 6ea1c77c7fabf1a046463eceddce1839efc63e60 · NVIDIA/FasterTransformer</A>
							</DL><p>
							<DT><A HREF="https://github.com/google/automl/tree/master/lion#language-modeling">Google AutoML: Lion Optimizer over Adam</A>
							<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT/triton">NVIDIA/DeepLearningExamples · GitHub</A>
							<DT><A HREF="https://gist.github.com/moyix/0f37da9c21c4ddfa0ab39ddad1639db4">Convert a SalesForce CodeGen model's weights to plain GPT-J</A>
							<DT><A HREF="https://huggingface.co/docs/optimum/bettertransformer/overview">Better Transformer</A>
							<DT><A HREF="https://github.com/triton-inference-server/model_analyzer">triton-inference-server/model_analyzer: Triton Model Analyzer is a CLI tool to help with better understanding of the compute and memory requirements of the Triton Inference Server models.</A>
							<DT><A HREF="https://developer.nvidia.com/blog/increasing-inference-acceleration-of-kogpt-with-fastertransformer/">General Optimizations List</A>
							<DT><A HREF="https://github.com/feifeibear/LLMSpeculativeSampling">Speculative Decoding</A>
							<DT><A HREF="https://github.com/tomaarsen/attention_sinks">tomaarsen/attention_sinks: Extend existing LLMs way beyond the original training length with constant memory usage, and without retraining</A>
							<DT><A HREF="https://github.com/stas00/tinypar">TP/PP/DP implementation of llama using apex blocks</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-distillation</H3>
						<DL><p>
						</DL><p>
						<DT><H3 FOLDED>runtime optimizations (cpp or rust)</H3>
						<DL><p>
							<DT><A HREF="https://github.com/ggerganov/llama.cpp">ggerganov/llama.cpp: LLM inference in C/C++</A>
							<DT><A HREF="https://github.com/huggingface/candle">huggingface/candle: Minimalist ML framework for Rust</A>
							<DT><A HREF="https://github.com/kykosic/actix-pytorch-example">kykosic/actix-pytorch-example (xAI prototype)</A>
							<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md">llama.cpp/examples/server/README.md at master · ggerganov/llama.cpp</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-profiling</H3>
						<DL><p>
							<DT><A HREF="https://github.com/DataCrunch-io/large_transformer_training_playbook/blob/main/transformers.ipynb">Transformers.ipynb: Megatron-LM &amp; DeepSpeed &amp; HF Transformers</A>
							<DT><A HREF="https://github.com/pythonprofilers/memory_profiler">pythonprofilers/memory_profiler: Monitor Memory usage of Python code</A>
							<DT><A HREF="https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras">TensorFlow Profiler: Profile model performance  |  TensorBoard</A>
							<DT><A HREF="https://unix.stackexchange.com/questions/125429/tracking-down-where-disk-space-has-gone-on-linux">du</A>
							<DT><A HREF="https://docs.contrastsecurity.com/en/python-middleware.html">Configure middleware</A>
							<DT><A HREF="https://www.cyberciti.biz/open-source/install-ncdu-on-linux-unix-ncurses-disk-usage/">ncdu</A>
							<DT><A HREF="https://github.com/Syllo/nvtop">Syllo/nvtop: GPUs process monitoring for AMD, Intel and NVIDIA</A>
							<DT><A HREF="https://github.com/gperftools/gperftools">gperftools/gperftools: Main gperftools repository</A>
						</DL><p>
						<DT><H3 FOLDED>serving</H3>
						<DL><p>
							<DT><H3 FOLDED>serving-research</H3>
							<DL><p>
								<DT><H3 FOLDED>serving-research-web-server</H3>
								<DL><p>
									<DT><H3 FOLDED>Robyn</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sparckles/Robyn">sparckles/Robyn: Robyn is a Super Fast Async Python Web Framework with a Rust runtime.</A>
									</DL><p>
									<DT><H3 FOLDED>uvicorn</H3>
									<DL><p>
										<DT><H3 FOLDED>uvicorn-logger</H3>
										<DL><p>
											<DT><A HREF="https://github.com/roy-pstr/fastapi-custom-exception-handlers-and-logs/blob/master/logger.py">logger.py</A>
											<DT><A HREF="https://github.com/encode/uvicorn/blob/0efd3835da6dcc713f74aadf7b52779d0d1fa17d/uvicorn/config.py#L357">uvicorn: config.py#L357 (log_config=None)</A>
											<DT><A HREF="https://docs.python.org/3/library/logging.config.html">logging.config — Logging configuration — Python 3.12.3 documentation</A>
											<DT><A HREF="https://github.com/encode/uvicorn/blob/0efd3835da6dcc713f74aadf7b52779d0d1fa17d/uvicorn/config.py#L32">uviconr: LOG_LEVELS config.py#L32</A>
											<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/9b6ef9e1f0d8acaefd989440b27da9069aa69207/deepspeed/utils/logging.py">DeepSpeed/deepspeed/utils/logging.py</A>
										</DL><p>
										<DT><A HREF="https://github.com/encode/uvicorn">encode/uvicorn: An ASGI web server, for Python. 🦄</A>
										<DT><A HREF="https://github.com/Lightning-AI/LitServe/blob/ee1a7b53772332a937bdb548277b28cc54ba16e0/src/litserve/server.py#L25">LitServe/src/litserve/server.py</A>
									</DL><p>
									<DT><H3 FOLDED>zap</H3>
									<DL><p>
										<DT><A HREF="https://github.com/zigzap/zap">zigzap/zap: blazingly fast backends in zig</A>
									</DL><p>
									<DT><A HREF="https://github.com/facebook/wangle">facebook/wangle: Wangle is a framework providing a set of common client/server abstractions for building services in a consistent, modular, and composable way.</A>
									<DT><A HREF="https://github.com/Lightning-AI/LitServe?tab=readme-ov-file#implement-a-server">Lightning-AI/LitServe: Deploy AI models at scale. High-throughput serving engine for AI/ML models that uses the latest state-of-the-art model deployment techniques.</A>
									<DT><A HREF="https://github.com/awslabs/aws-c-http">awslabs/aws-c-http: C99 implementation of the HTTP/1.1 and HTTP/2 specifications</A>
									<DT><A HREF="https://github.com/emmett-framework/granian">emmett-framework/granian: A Rust HTTP server for Python applications</A>
								</DL><p>
								<DT><H3 FOLDED>serving-research-rust</H3>
								<DL><p>
									<DT><H3 FOLDED>axum</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=Wnb_n5YktO8&t=30s">Decrusting the axum crate - YouTube</A>
										<DT><A HREF="https://crates.io/crates/axum">axum - crates.io: Rust Package Registry</A>
										<DT><A HREF="https://github.com/tokio-rs/axum">tokio-rs/axum: Ergonomic and modular web framework built with Tokio, Tower, and Hyper</A>
										<DT><A HREF="https://github.com/serde-rs/serde">serde-rs/serde: Serialization framework for Rust</A>
										<DT><A HREF="https://github.com/tokio-rs/tokio">tokio-rs/tokio: A runtime for writing reliable asynchronous applications with Rust. Provides I/O, networking, scheduling, timers, ...</A>
										<DT><A HREF="https://github.com/tokio-rs/tracing">tokio-rs/tracing: Application level tracing for Rust.</A>
										<DT><A HREF="https://crates.io/crates/tracing-subscriber">tracing-subscriber - crates.io: Rust Package Registry</A>
										<DT><A HREF="https://github.com/hyperium/hyper">hyperium/hyper: An HTTP library for Rust (HTTP parser)</A>
										<DT><A HREF="https://github.com/tower-rs/tower">tower-rs/tower: async fn(Request) (Middleware)</A>
										<DT><A HREF="https://docs.rs/matchit/latest/matchit/">matchit (route matching)</A>
										<DT><A HREF="https://docs.rs/hyper/1.2.0/hyper/">hyper - Rust</A>
										<DT><A HREF="https://docs.rs/hyper/0.14.20/hyper/server/struct.Builder.html">Builder in hyper::server - Rust</A>
										<DT><A HREF="https://tokio.rs/blog/2021-05-14-inventing-the-service-trait">Inventing the Service trait | Tokio - An asynchronous Rust runtime</A>
										<DT><A HREF="https://docs.rs/axum/0.6.20/axum/attr.debug_handler.html">debug_handler in axum (FromRequestParts) MAIN</A>
										<DT><A HREF="https://github.com/tokio-rs/axum/blob/2ec68d6c4dab10b83b9195c3acd4ccc7c26d0e8a/axum/src/handler/mod.rs#L206-248">FromRequest -&gt; Handler (macro)</A>
										<DT><A HREF="https://github.com/tokio-rs/axum/blob/2ec68d6c4dab10b83b9195c3acd4ccc7c26d0e8a/axum-core/src/response/into_response.rs#L395">IntoResponse (last arg special)</A>
										<DT><A HREF="https://docs.rs/axum/0.6.20/axum/extract/struct.State.html">extactors::State (impl FromRequestParts)</A>
										<DT><A HREF="https://docs.rs/axum/0.6.20/axum/handler/struct.HandlerService.html">HandlerService in axum::handler</A>
										<DT><A HREF="https://docs.rs/tower/0.4.13/tower/trait.Service.html">tower::Service (Future -&gt; self.state.clone())</A>
										<DT><A HREF="https://crates.io/crates/tokio-blocking">tokio-blocking (inference step)</A>
										<DT><A HREF="https://docs.rs/axum-extra/0.9.2/axum_extra/">axum_extra - (utilities)</A>
									</DL><p>
									<DT><A HREF="https://github.com/kykosic/actix-pytorch-example">kykosic/actix-pytorch-example (xAI prototype)</A>
									<DT><A HREF="https://github.com/kykosic/actix-tensorflow-example">kykosic/actix-tensorflow-example</A>
									<DT><A HREF="https://github.com/hyperium/hyper">hyperium/hyper: An HTTP library for Rust</A>
									<DT><A HREF="https://github.com/hyperium/h2">hyperium/h2: HTTP 2.0 client &amp; server implementation for Rust.</A>
									<DT><A HREF="https://www.youtube.com/watch?v=P-v8xRhpquM">Rust Programming Part 2 HTTP Server Using Result Type &amp; Responding To Client - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Wnb_n5YktO8&t=30s">Decrusting the axum crate - YouTube</A>
									<DT><A HREF="https://blog.hippoml.com/unified-datacenter-local-foundation-model-serving-beyond-docker-way-a929003fa07c">Unified DataCenter &amp; Local Foundation Model Serving: Beyond Docker Way | by HippoML Blog | Jan, 2024 | Medium</A>
								</DL><p>
								<DT><H3 FOLDED>network services</H3>
								<DL><p>
									<DT><A HREF="https://github.com/cloudflare/pingora">cloudflare/pingora: A library for building fast, reliable and evolvable network services.</A>
									<DT><A HREF="https://blog.cloudflare.com/pingora-open-source">Open sourcing Pingora: our Rust framework for building programmable network services</A>
									<DT><A HREF="https://github.com/cloudflare/pingora/blob/main/docs/quick_start.md">pingora/docs/quick_start.md at main · cloudflare/pingora</A>
									<DT><A HREF="https://github.com/cloudflare/pingora/blob/main/pingora-proxy/examples/load_balancer.rs">pingora/pingora-proxy/examples/load_balancer.rs at main · cloudflare/pingora</A>
									<DT><A HREF="https://github.com/bytedance/g3">bytedance/g3: Enterprise-oriented Generic Proxy Solutions</A>
								</DL><p>
								<DT><H3 FOLDED>Network Messaging Protocol</H3>
								<DL><p>
									<DT><H3 FOLDED>NATS</H3>
									<DL><p>
										<DT><H3 FOLDED>nats-protobuf</H3>
										<DL><p>
											<DT><A HREF="https://natsbyexample.com/examples/messaging/protobuf/go">NATS by Example - Protobuf for Message Payloads (Go)</A>
											<DT><A HREF="https://github.com/savaki/nats-protobuf">savaki/nats-protobuf: write protobuf services with NATS as the transport; service discovery simplified</A>
										</DL><p>
										<DT><H3 FOLDED>nats-client</H3>
										<DL><p>
											<DT><H3 FOLDED>Client Protocol</H3>
											<DL><p>
												<DT><A HREF="https://docs.nats.io/reference/reference-protocols/nats-protocol">Client Protocol | NATS Docs</A>
											</DL><p>
											<DT><A HREF="https://github.com/3kwa/goingnats">Python minimal NATS client (no asyncio)</A>
											<DT><A HREF="https://github.com/nats-io/nats.py">nats-io/nats.py: Python3 client for NATS (asycio)</A>
											<DT><A HREF="https://github.com/nats-io/nats.net.v2">nats-io/nats.net.v2: Full Async C# / .NET client for NATS</A>
											<DT><A HREF="https://github.com/nats-io/nats.zig">nats-io/nats.zig: Zig Client for NATS</A>
											<DT><A HREF="https://github.com/rutgerbrf/zig-nats/blob/master/example/main.zig">zig-nats/example/main.zig at master · rutgerbrf/zig-nats</A>
										</DL><p>
										<DT><A HREF="https://github.com/nats-io/nats-server">nats-io/nats-server: High-Performance server for NATS.io, the cloud and edge native messaging system.</A>
										<DT><A HREF="https://nats.io/download/">NATS.io – Cloud Native, Open Source, High-performance Messaging</A>
										<DT><A HREF="https://gcoolinfo.medium.com/comparing-nats-nats-streaming-and-nats-jetstream-ec2d9f426dc8">Comparing NATS, NATS Streaming and NATS JetStream | by George Koulouris | Medium</A>
										<DT><A HREF="https://github.com/nats-io/nats.c">nats-io/nats.c: A C client for NATS</A>
										<DT><A HREF="https://github.com/ConnectEverything/nats-by-example/#getting-started">ConnectEverything/nats-by-example: Collection of runnable, reference examples using NATS (https://nats.io)</A>
										<DT><A HREF="https://docs.nats.io/nats-concepts/what-is-nats">What is NATS | NATS Docs</A>
										<DT><A HREF="https://docs.nats.io/nats-concepts/jetstream">JetStream | NATS Docs</A>
										<DT><A HREF="https://www.youtube.com/watch?v=JNQM_aq9pd4">aggregator (multiple backends e.g. /rest/* &amp; /nats/*)</A>
										<DT><A HREF="https://github.com/gcool-info/nats-playground">gcool-info/nats-playground: Playground for a secure, Highly Available NATS Cluster with message persistence (using JetStream)</A>
										<DT><A HREF="https://www.youtube.com/watch?v=SLb4rdI5lIM">NATS for Modern Messaging and Microservices - YouTube</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=kH7P1ZX44DQ&list=LL&index=29">How to Design a Network Messaging Protocol!</A>
								</DL><p>
								<DT><H3 FOLDED>jetstream</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/JetStream/tree/main">google/JetStream: A throughput and memory optimized engine for LLM inference on TPUs!</A>
									<DT><A HREF="https://www.pjm.com/-/media/etools/jetstream/introduction-to-jetstream.ashx">ntroduction-to-jetstream</A>
									<DT><A HREF="https://nats-io.github.io/nats.net.v2/documentation/jetstream/intro.html#:~:text=JetStream%20is%20the%20built%2Din,functionalities%20and%20qualities%20of%20service.">JetStream</A>
									<DT><A HREF="https://nats-io.github.io/nats.net.v2/documentation/serialization.html">Serialization</A>
									<DT><A HREF="https://gcoolinfo.medium.com/comparing-nats-nats-streaming-and-nats-jetstream-ec2d9f426dc8">Comparing NATS, NATS Streaming and NATS JetStream | by George Koulouris | Medium</A>
								</DL><p>
								<DT><H3 FOLDED>websockets</H3>
								<DL><p>
									<DT><A HREF="https://github.com/kykosic/python-websocket-exmple/blob/master/server.py">python-websocket-exmple/server.py at master · kykosic/python-websocket-exmple</A>
									<DT><A HREF="https://www.youtube.com/watch?v=yrzv6o9Pnao">WebSocket Server from Zero by Specs - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>gRPC</H3>
								<DL><p>
									<DT><H3 FOLDED>Protocol Buffers</H3>
									<DL><p>
										<DT><A HREF="https://github.com/protocolbuffers">Protocol Buffers</A>
										<DT><A HREF="https://github.com/protocolbuffers/protobuf">protocolbuffers/protobuf: Protocol Buffers - Google's data interchange format</A>
										<DT><A HREF="https://flatbuffers.dev/">FlatBuffers: FlatBuffers</A>
										<DT><A HREF="https://github.com/google/flatbuffers">google/flatbuffers: FlatBuffers: Memory Efficient Serialization Library</A>
										<DT><A HREF="https://www.infoq.com/news/2023/07/linkedin-protocol-buffers-restli/">LinkedIn Adopts Protocol Buffers for Microservices Integration and Reduces Latency by up to 60%</A>
										<DT><A HREF="https://www.youtube.com/watch?v=9IxE2UQqJCw&t=7s">Reduce Latency By 60% With ProtoBufs!!! | Prime Reacts - YouTube</A>
										<DT><A HREF="https://protobuf.dev/overview/">Overview | Protocol Buffers Documentation</A>
										<DT><A HREF="https://gist.github.com/jambonn/1f5fffc23f97f8413372a438739c1bff">How to Install Protobuf on Ubuntu 20.04</A>
										<DT><A HREF="https://protobuf.dev/overview/#updating-defs">Overview | Protocol Buffers Documentation</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/proto/generate.proto">text-generation-inference/proto/generate.proto at main · huggingface/text-generation-inference</A>
									</DL><p>
									<DT><H3 FOLDED>Rust</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=kerKXChDmsE">Tonic makes gRPC in Rust stupidly simple</A>
										<DT><A HREF="https://github.com/hyperium/tonic">hyperium/tonic: A native gRPC client &amp; server implementation with async/await support.</A>
										<DT><A HREF="https://github.com/tokio-rs/prost">tokio-rs/prost: PROST! a Protocol Buffers implementation for the Rust Language</A>
										<DT><A HREF="https://github.com/hyperium/tonic/blob/master/tonic-build/README.md">tonic/tonic-build/README.md at master · hyperium/tonic</A>
									</DL><p>
									<DT><H3 FOLDED>client</H3>
									<DL><p>
										<DT><A HREF="https://github.com/fullstorydev/grpcurl">fullstorydev/grpcurl: Like cURL, but for gRPC: Command-line tool for interacting with gRPC servers</A>
									</DL><p>
									<DT><H3 FOLDED>uds: Unix Domain Socket</H3>
									<DL><p>
										<DT><A HREF="https://github.com/grpc/grpc/tree/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/python/uds">uds: Unix Domain Socket Example in gRPC Python</A>
										<DT><A HREF="https://grpc.io/docs/languages/python/quickstart/">Quick start | Python | gRPC</A>
									</DL><p>
									<DT><A HREF="https://github.com/grpc/grpc/blob/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/python/uds/async_greeter_server.py">(EXAMPLE) grpc/examples/python/uds/async_greeter_server.py at d68161a64f191b8d8d5afe0507e7a2291f91ff1a · grpc/grpc</A>
									<DT><A HREF="https://www.youtube.com/watch?v=SDnPul2-N9w">Big picture</A>
									<DT><A HREF="https://www.youtube.com/watch?v=uGYZn6xk-hA">Serialization formats: JSON and Protobuf</A>
									<DT><A HREF="https://janhendrikewers.uk/pydantic_vs_protobuf_vs_namedtuple_vs_dataclasses.html">Pydantic vs Protobuf vs Namedtuples vs Dataclasses</A>
									<DT><A HREF="https://github.com/bazelbuild/starlark">bazelbuild/starlark: Starlark Language</A>
									<DT><A HREF="https://github.com/stripe/skycfg">stripe/skycfg: Skycfg is an extension library for the Starlark language that adds support for constructing Protocol Buffer messages.</A>
									<DT><A HREF="https://www.youtube.com/watch?v=kerKXChDmsE">Tonic makes gRPC in Rust stupidly simple</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/7dbaf9e9013060af52024ea1a8b361b107b50a69/router/src/server.rs#L115">text-generation-inference/router/src/server.rs at 7dbaf9e9013060af52024ea1a8b361b107b50a69 · huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/fullstorydev/grpcurl">fullstorydev/grpcurl: Like cURL, but for gRPC: Command-line tool for interacting with gRPC servers</A>
									<DT><A HREF="https://github.com/grpc/grpc/tree/master/src/python/grpcio">grpc/src/python/grpcio at master</A>
									<DT><A HREF="https://grpc.github.io/grpc/python/grpc.html">gRPC — gRPC Python 1.62.0 documentation</A>
									<DT><A HREF="https://github.com/grpc/grpc/blob/master/doc/python/server_reflection.md">grpc/doc/python/server_reflection.md (reflection)</A>
									<DT><A HREF="https://github.com/d5h-foss/grpc-interceptor">d5h-foss/grpc-interceptor: Simplified Python gRPC interceptors</A>
									<DT><A HREF="https://www.youtube.com/watch?v=M1qt83N3JWg">¿Qué es gRPC? - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>WSGI</H3>
								<DL><p>
									<DT><A HREF="https://www.fullstackpython.com/wsgi-servers.html">WSGI Servers - Full Stack Python</A>
								</DL><p>
								<DT><H3 FOLDED>environ</H3>
								<DL><p>
									<DT><A HREF="https://github.com/triton-inference-server/server/blob/ddd6c4b4a286970e0b6c18dcd5c90c7a121d3e48/qa/L0_backend_fastertransformer/test.sh">Triton Inference Server: $TRITON_DIR (fastertransformer backend)</A>
									<DT><A HREF="https://twitter.com/wangzhr4/status/1783772055294329159/photo/1">sensitive information as local env vars</A>
									<DT><A HREF="https://chat.openai.com/c/607f5eb0-ab64-494d-acdb-b1a3f7319695">Exposure of Sensitive Information &amp; Security practices</A>
									<DT><A HREF="https://github.com/vllm-project/vllm/blob/8674f9880e2d8574c2adc759027e0f27dc9b95de/setup.py#L31">setup.py#L31 # cannot import envs directly because it depends on vllm which is not installed yet</A>
									<DT><A HREF="https://github.com/vllm-project/vllm/blob/8674f9880e2d8574c2adc759027e0f27dc9b95de/setup.py#L18">setup.py#L18</A>
								</DL><p>
								<DT><H3 FOLDED>asycio</H3>
								<DL><p>
									<DT><H3 FOLDED>pyInfer</H3>
									<DL><p>
										<DT><A HREF="https://github.com/wangzyon/pyInfer">wangzyon/pyInfer: async inference for machine learning model</A>
										<DT><A HREF="https://github.com/wangzyon/pyInfer/blob/master/pyinfer/utils/common/registry.py">pyInfer/pyinfer/utils/common/registry.py REGISTER ENGINE</A>
									</DL><p>
									<DT><A HREF="https://github.com/facebookincubator/later">facebookincubator/later: A framework for python asyncio with batteries included for people writing services in python asyncio</A>
									<DT><A HREF="https://github.com/wangzyon/pyInfer/blob/master/pyinfer/core/engine/engine.py">pyInfer/pyinfer/core/engine/engine.py at master · wangzyon/pyInfer</A>
								</DL><p>
								<DT><H3 FOLDED>serving-research-web-server-max_tokens</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-best-practices.md">max_batch_size * max_input_len * alpha + max_batch_size * max_beam_width * (1 - alpha)</A>
								</DL><p>
								<DT><H3 FOLDED>serving-research-torch.compile</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/commit/78f87d5a0c7d82911a639c397577284868a53c42">Temporary implem of torch.compile on our stuff. · huggingface/text-generation-inference@78f87d5</A>
								</DL><p>
								<DT><H3 FOLDED>image-compression</H3>
								<DL><p>
									<DT><H3 FOLDED>PNG-network-transfer</H3>
									<DL><p>
										<DT><H3 FOLDED>cuda-encoding-png</H3>
										<DL><p>
											<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51286/">CUDA-Based Acceleration of PNG Image Encoder and Decoder | GTC Digital Spring 2023 | NVIDIA On-Demand</A>
											<DT><A HREF="https://github.com/NVIDIA/DALI">NVIDIA/DALI: A GPU-accelerated library containing highly optimized building blocks and an execution engine for data processing to accelerate deep learning training and inference applications.</A>
											<DT><A HREF="https://github.com/NVIDIA/nvcomp">NVIDIA/nvcomp: Repository for nvCOMP docs and examples. nvCOMP is a library for fast lossless compression/decompression on the GPU that can be downloaded from https://developer.nvidia.com/nvcomp.</A>
											<DT><A HREF="https://github.com/NVIDIA/CUDALibrarySamples/tree/master/nvCOMP">CUDALibrarySamples/nvCOMP at master · NVIDIA/CUDALibrarySamples</A>
										</DL><p>
										<DT><H3 FOLDED>fpng</H3>
										<DL><p>
											<DT><H3 FOLDED>fpng-python</H3>
											<DL><p>
												<DT><A HREF="https://github.com/qrmt/fpng-python">qrmt/fpng-python: Python bindings for fpng</A>
											</DL><p>
											<DT><A HREF="https://github.com/richgel999/fpng">richgel999/fpng: Super fast C++ .PNG writer/reader</A>
										</DL><p>
										<DT><H3 FOLDED>opencv-libpng</H3>
										<DL><p>
										</DL><p>
										<DT><H3 FOLDED>mtpng</H3>
										<DL><p>
											<DT><A HREF="https://github.com/bvibber/mtpng?tab=readme-ov-file">bvibber/mtpng: A parallelized PNG encoder in Rust</A>
											<DT><A HREF="https://github.com/pwuertz/pymtpng">pwuertz/pymtpng: Python bindings for MTPNG library.</A>
											<DT><A HREF="https://crates.io/crates/png">png - crates.io: Rust Package Registry</A>
											<DT><A HREF="https://github.com/image-rs/image-png">image-rs/image-png: PNG decoding and encoding library in pure Rust</A>
											<DT><A HREF="https://docs.rs/png/0.17.16/png/">png - Rust</A>
										</DL><p>
										<DT><A HREF="https://grok.com/chat/d47cbdc5-f753-48f1-ad22-1efe35b78179">Optimizing PNG Serialization for Image Generation - Grok</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://github.com/geohot/minikeyvalue/blob/master/src/server.go">minikeyvalue/src/server.go at master · geohot/minikeyvalue</A>
								<DT><A HREF="https://github.com/theroyallab/tabbyAPI/blob/main/endpoints/OAI/router.py">tabbyAPI/endpoints/OAI/router.py (good &amp; minimal FastAPI ref impl)</A>
								<DT><A HREF="https://github.com/triton-inference-server/server/blob/ddd6c4b4a286970e0b6c18dcd5c90c7a121d3e48/qa/L0_http/http_test.py">server/qa/L0_http/http_test.py</A>
								<DT><A HREF="https://github.com/triton-inference-server/server/blob/ddd6c4b4a286970e0b6c18dcd5c90c7a121d3e48/qa/L0_http/python_http_aio_test.py">server/qa/L0_http/python_http_aio_test.py (readiness)</A>
								<DT><A HREF="https://github.com/triton-inference-server/client">triton-inference-server/client: Triton Python, C++ and Java client libraries</A>
								<DT><A HREF="https://github.com/triton-inference-server/client/blob/a00b97131ffd46c814ae3c2d4eca98266d19f804/src/python/library/tritonclient/http/_client.py#L434">triton client: model_version API structure _client.py#L434</A>
								<DT><A HREF="https://www.youtube.com/watch?v=BhtxEDwgylU&list=LL&index=10">Use AppMap with VS Code Dev Containers - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=6BiNzPdy6YA">[REFAI Seminar 04/16/24] ML for ML Compilers at Google (AutoFDO)</A>
								<DT><A HREF="https://github.com/facebook/proxygen">facebook/proxygen: A collection of C++ HTTP libraries including an easy to use HTTP server.</A>
								<DT><A HREF="https://engineering.fb.com/2014/11/05/production-engineering/introducing-proxygen-facebook-s-c-http-framework/">Introducing Proxygen, Facebook's C++ HTTP framework - Engineering at Meta</A>
								<DT><A HREF="https://www.youtube.com/watch?v=t08OA4CfRFs">Using Rust to write scalable Python APIs - YouTube</A>
								<DT><A HREF="https://github.com/PaddlePaddle/Serving/blob/v0.9.0/doc/Python_Pipeline/Pipeline_Design_CN.md">Serving/doc/Python_Pipeline/Pipeline_Design_CN.md at v0.9.0 · PaddlePaddle/Serving</A>
								<DT><A HREF="https://github.com/pytorch/pytorch/blob/c3ee07c71cef9f085577c5e53dfe5faf8f2b3b4e/benchmarks/inference/server.py#L239">pytorch/benchmarks/inference/server.py</A>
								<DT><A HREF="https://jonathanc.net/blogs/maximizing_pytorch_throughput">Maximizing PyTorch Throughput with FastAPI – Jonathan Chang’s Blog</A>
							</DL><p>
							<DT><H3 FOLDED>serving-inference-engine</H3>
							<DL><p>
								<DT><A HREF="https://github.com/wangzyon/pyInfer/blob/master/pyinfer/core/engine/engine.py">pyInfer/pyinfer/core/engine/engine.py</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/legacy/models/providers/diffusers.py">DeepSpeed-MII/mii/legacy/models/providers/diffusers.py</A>
								<DT><A HREF="https://github.com/zhaochenyang20/ModelServer">zhaochenyang20/ModelServer: Efficient, Flexible, and Highly Fault-Tolerant Model Service Management Based on SGLang</A>
							</DL><p>
							<DT><H3 FOLDED>serving-FastAPI</H3>
							<DL><p>
								<DT><H3 FOLDED>Starlette</H3>
								<DL><p>
									<DT><A HREF="https://github.com/encode/starlette/blob/9f16bf5c25e126200701f6e04330864f4a91a898/docs/server-push.md?plain=1#L30">routes without decorators, but constructors and funcs</A>
									<DT><A HREF="https://github.com/tiangolo/fastapi/blob/38929aae1b6d42848652705e5ca618a675dba0e1/fastapi/routing.py#L651">fastapi/fastapi/routing.py</A>
								</DL><p>
								<DT><H3 FOLDED>FastAPI-lifespan</H3>
								<DL><p>
									<DT><A HREF="https://medium.com/@life-is-short-so-enjoy-it/fastapi-experiment-lifespan-feature-7f87de5601db">FastAPI: experiment lifespan feature | by Life-is-short--so--enjoy-it | Medium</A>
								</DL><p>
								<DT><H3 FOLDED>fastapi-examples</H3>
								<DL><p>
									<DT><H3 FOLDED>llama-agentic-system</H3>
									<DL><p>
										<DT><A HREF="https://github.com/meta-llama/llama-agentic-system">meta-llama/llama-agentic-system: Agentic components of the Llama Stack APIs</A>
										<DT><A HREF="https://github.com/meta-llama/llama-agentic-system/blob/b266c1c62bea3c184a93b1bb295468a41d7b555b/llama_agentic_system/server.py#L44">server.py#L44</A>
									</DL><p>
									<DT><A HREF="https://github.com/Lightning-AI/LitServe/blob/ee1a7b53772332a937bdb548277b28cc54ba16e0/src/litserve/server.py#L25">LitServe/src/litserve/server.py</A>
								</DL><p>
								<DT><A HREF="https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&hw=ph&test=db">HTTP Servers Benchmarks</A>
								<DT><A HREF="https://github.com/roy-pstr/fastapi-custom-exception-handlers-and-logs/blob/master/main.py">fastapi-custom-exception-handlers-and-logs/main.py at master · roy-pstr/fastapi-custom-exception-handlers-and-logs</A>
								<DT><A HREF="https://github.com/tiangolo/fastapi/blob/38929aae1b6d42848652705e5ca618a675dba0e1/fastapi/routing.py#L655C13-L655C23">FastAPI: routes as list is deprecated routing.py#L655C13-L655C23</A>
								<DT><A HREF="https://github.com/tiangolo/fastapi/blob/38929aae1b6d42848652705e5ca618a675dba0e1/tests/test_extra_routes.py#L22">FastAPI: router no decorated test_extra_routes.py#L22</A>
								<DT><A HREF="https://github.com/tiangolo/fastapi/blob/38929aae1b6d42848652705e5ca618a675dba0e1/tests/test_include_route.py">fastapi/tests/test_include_route.py</A>
								<DT><A HREF="https://www.youtube.com/watch?v=t08OA4CfRFs">Using Rust to write scalable Python APIs - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=row-SdNdHFE">Best Practice to Make HTTP Request in FastAPI Application - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=tGD3653BrZ8">How FastAPI Handles Requests Behind the Scenes - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=vkQZe8Idbtg">Fastapi endpoints testing with pytest | Tutorial 1 - YouTube</A>
								<DT><A HREF="https://viktorsapozhok.github.io/fastapi-oauth2-postgres/">Structuring FastAPI application with multiple services using 3-tier design pattern. | Vanilla Ninja</A>
								<DT><A HREF="https://github.com/Lightning-AI/LitServe?tab=readme-ov-file#implement-a-server">Lightning-AI/LitServe: Deploy AI models at scale. High-throughput serving engine for AI/ML models that uses the latest state-of-the-art model deployment techniques.</A>
								<DT><A HREF="https://github.com/Lightning-AI/LitServe/blob/ee1a7b53772332a937bdb548277b28cc54ba16e0/src/litserve/server.py#L25">LitServe/src/litserve/server.py</A>
								<DT><A HREF="https://github.com/meta-llama/llama-agentic-system">meta-llama/llama-agentic-system: Agentic components of the Llama Stack APIs</A>
								<DT><A HREF="https://github.com/pytorch/ao/blob/6b529961bd0b41953d26cdde5851f460839d5cf6/examples/sam2_amg_server/server.py">ao/examples/sam2_amg_server/server.py: sam2</A>
							</DL><p>
							<DT><H3 FOLDED>serving-test</H3>
							<DL><p>
								<DT><H3 FOLDED>serving-test-device</H3>
								<DL><p>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/dc514df2afad386739bf8471ab351a86d5c5ffc7/torch/testing/_internal/common_cuda.py">pytorch/torch/testing/_internal/common_cuda.py</A>
								</DL><p>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_throughput.py#L126">vllm/benchmarks/benchmark_throughput.py</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_serving.py">vllm/benchmarks/benchmark_serving.py</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/integration-tests/conftest.py#L284">text-generation-inference/integration-tests/conftest.py</A>
								<DT><A HREF="https://github.com/tinygrad/tinygrad/discussions?discussions_q=is%3Aopen+test">tinygrad/tinygrad · Discussions · GitHub</A>
								<DT><A HREF="https://github.com/sgl-project/sglang/blob/main/python/sglang/test/test_utils.py#L396">sglang/python/sglang/test/test_utils.py: popen_launch_server</A>
							</DL><p>
							<DT><H3 FOLDED>serving-benchmark</H3>
							<DL><p>
								<DT><H3 FOLDED>serving-test-regression</H3>
								<DL><p>
								</DL><p>
								<DT><H3 FOLDED>HTTP-benchmarking</H3>
								<DL><p>
									<DT><A HREF="https://github.com/hatoo/oha">hatoo/oha: Ohayou(おはよう), HTTP load generator, inspired by rakyll/hey with tui animation.</A>
									<DT><A HREF="https://github.com/wg/wrk">wg/wrk: Modern HTTP benchmarking tool</A>
									<DT><A HREF="https://github.com/grafana/k6">grafana/k6: A modern load testing tool, using Go and JavaScript - https://k6.io</A>
									<DT><A HREF="https://github.com/sharkdp/hyperfine">sharkdp/hyperfine: A command-line benchmarking tool</A>
									<DT><A HREF="https://httpd.apache.org/docs/current/programs/ab.html">ab - Apache HTTP server benchmarking tool - Apache HTTP Server Version 2.4</A>
									<DT><A HREF="https://gist.github.com/aliesbelik/840eff7c5bc78a141eab8e36f2f4edf2">Benchmarking &amp; load testing tools</A>
								</DL><p>
								<DT><H3 FOLDED>lmsys</H3>
								<DL><p>
									<DT><A HREF="https://lmsys.org/blog/2024-07-25-sglang-llama3/">Achieving Faster Open-Source Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM) | LMSYS Org</A>
								</DL><p>
								<DT><A HREF="https://bentoml.com/blog/benchmarking-llm-inference-backends">Benchmarking LLM Inference Backends</A>
								<DT><A HREF="https://github.com/sgl-project/sglang/blob/main/python/sglang/bench_serving.py">sglang/python/sglang/bench_serving.py</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_throughput.py#L126">vllm/benchmarks/benchmark_throughput.py</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/launch_tgi_server.sh">vllm/benchmarks/launch_tgi_server.sh</A>
								<DT><A HREF="https://www.youtube.com/watch?v=7njmta3SlxE">Hao Zhang - Chatbot Arena (UCSD / LMSys) - YouTube</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_serving.py">vllm/benchmarks/benchmark_serving.py</A>
								<DT><A HREF="https://twitter.com/dzhulgakov/status/1737917306565697990?s=46&t=yqOem5ktaowo8FyJ-ilbzQ">(1) Dmytro Dzhulgakov en X: "I’m all pro open benchmarks, but comparing **public** LLM endpoints just doesn’t work unless there’s a confirmed huge user base like OpenAI. In fact, performance may be even anti-correlated with popularity😉 Here’s why 🧵" / X</A>
								<DT><A HREF="https://twitter.com/anyscalecompute/status/1737883193720922413">(1) Anyscale en X: "📈We’re excited to introduce the LLMPerf leaderboard: the first public and open source leaderboard for benchmarking performance of various LLM inference providers in the market. Our goal with this leaderboard is to equip users and developers with a clear understanding of the... https://t.co/XGF4fhkaWG" / X</A>
								<DT><A HREF="https://github.com/fw-ai/benchmark">fw-ai/benchmark: Benchmark suite for LLMs from Fireworks.ai</A>
								<DT><A HREF="https://github.com/ray-project/llmperf">ray-project/llmperf: LLMPerf is a library for validating and benchmarking LLMs</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/tree/main/benchmarks">vllm/benchmarks at main</A>
							</DL><p>
							<DT><H3 FOLDED>serving-types</H3>
							<DL><p>
								<DT><H3 FOLDED>InferenceRequest</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NVIDIA/Megatron-LM/blob/ccfeda47cb5ca10ee3c4efd9b78c6bb15c2cd3d2/megatron/core/inference_params.py#L1">Megatron-LM: inference_params.py#L1</A>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/74a1be88f589bdd53e6b110528ae65dba5a1e9af/examples/mamba.py#L297">Tinygrad: inference_params (mamba.py)#L297</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/inference-request.md">TensorRT-LLM/docs/source/advanced/inference-request.md</A>
								</DL><p>
								<DT><H3 FOLDED>pydantic</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/clients/python/text_generation/types.py">text-generation-inference/clients/python/text_generation/types.py (pydantic v2)</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>SGLang</H3>
							<DL><p>
								<DT><H3 FOLDED>sglang-release-notes</H3>
								<DL><p>
									<DT><H3 FOLDED>sglang-v0.4.3</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sgl-project/sglang/releases/tag/v0.4.3">Release v0.4.3 · sgl-project/sglang</A>
									</DL><p>
									<DT><A HREF="https://lmsys.org/blog/2024-07-25-sglang-llama3/">Achieving Faster Open-Source Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM) | LMSYS Org</A>
									<DT><A HREF="https://lmsys.org/blog/2024-12-04-sglang-v0-4/">SGLang v0.4: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs | LMSYS Org</A>
									<DT><A HREF="https://x.com/lmsysorg/status/1872251875070021831">sglang v0.4.1</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-dev</H3>
								<DL><p>
									<DT><H3 FOLDED>sglang-roadmap</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sgl-project/sglang/issues/4042">Development Roadmap (2025 H1) · Issue #4042 · sgl-project/sglang</A>
									</DL><p>
									<DT><A HREF="https://github.com/sgl-project/sglang/blob/main/docs/developer/development_guide_using_docker.md#h200">sglang/docs/developer/development_guide_using_docker.md at main · sgl-project/sglang</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-docs</H3>
								<DL><p>
									<DT><A HREF="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/tree/main/sglang/code-walk-through">Awesome-ML-SYS-Tutorial/sglang/code-walk-through at main · zhaochenyang20/Awesome-ML-SYS-Tutorial</A>
									<DT><A HREF="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial">zhaochenyang20/Awesome-ML-SYS-Tutorial: My learning notes/codes for ML SYS.</A>
									<DT><A HREF="https://github.com/weishengying/Notes/tree/main/sglang">Notes/sglang at main · weishengying/Notes</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-lectures</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=XQylGyG7yp8&t=2162s">Lecture 35: SGLang - YouTube</A>
									<DT><A HREF="https://github.com/sgl-project/sgl-learning-materials/blob/main/slides/amd_dev_day_v2.pdf">sgl-learning-materials/slides/amd_dev_day_v2.pdf at main · sgl-project/sgl-learning-materials</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Ny4xxErgFgQ">Efficient LLM Inference with SGLang, Lianmin Zheng, xAI - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=XQylGyG7yp8">Lecture 35: SGLang - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=KjH7Gl0_pq0">DeepSeek V3, SGLang, and the state of Open Model Inference in 2025 (Quantization, MoEs, Pricing) - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-benchmarking</H3>
								<DL><p>
									<DT><H3 FOLDED>sglang-benchmarking-deepseek</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3">sglang/benchmark/deepseek_v3 at main · sgl-project/sglang</A>
										<DT><A HREF="https://x.com/i/bookmarks?post_id=1881267587603890338">DeepSeek R1 one batch decoding benchmark</A>
										<DT><A HREF="https://x.com/lmsysorg/status/1881267587603890338">(1) lmsys.org en X: "Under this command: python3 -m sglang.bench_one_batch --batch-size 1 --input 128 --output 256 --model deepseek-ai/DeepSeek-R1 --tp 8 --trust-remote-code you can check performance on 8 * H200. And here's the serving command: python3 -m sglang.launch_server --model https://t.co/orjYuKSz7D" / X</A>
									</DL><p>
									<DT><A HREF="https://github.com/sgl-project/sglang/blob/main/python/sglang/bench_serving.py">sglang/python/sglang/bench_serving.py at main · sgl-project/sglang</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/tree/main/benchmark/blog_v0_2">sglang/benchmark/blog_v0_2 at main · sgl-project/sglang</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/blob/2b340adfb1ebf6dee420885479ee92296694078c/docker/compose.yaml#L22">sglang/docker/compose.yaml: model weights loading container</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/blob/main/test/srt/configs/random_config.yaml">sglang/test/srt/configs/random_config.yaml at main · sgl-project/sglang</A>
									<DT><A HREF="https://x.com/lmsysorg/status/1890235303270740454">(1) lmsys.org en X: "🚀 Big update from the sglang team! We've made significant progress on the highly anticipated deepseek model: 🚀 FlashInfer MLA Attention integration for 4x faster long-context performance ⚡ torch.compile support, hitting 50 tokens/s for online inference 🔥 CUTLASS block-wise" / X</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-profiling</H3>
								<DL><p>
									<DT><A HREF="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/sglang/latency-accelerte-for-weight-updates/readme.md">Awesome-ML-SYS-Tutorial/sglang/latency-accelerte-for-weight-updates/readme.md at main · zhaochenyang20/Awesome-ML-SYS-Tutorial</A>
								</DL><p>
								<DT><H3 FOLDED>srt</H3>
								<DL><p>
									<DT><H3 FOLDED>multiple-token-generation</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sgl-project/sglang/pull/3634">add mtp unit test by zhyncs · Pull Request #3634 · sgl-project/sglang</A>
									</DL><p>
									<DT><A HREF="https://github.com/sgl-project/sglang/tree/main/python/sglang/srt/layers">sglang/python/sglang/srt/layers at main · sgl-project/sglang</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-turbomind</H3>
								<DL><p>
									<DT><A HREF="https://github.com/sgl-project/sglang">sgl-project/sglang: SGLang is yet another fast serving framework for large language models and vision language models.</A>
									<DT><A HREF="https://github.com/InternLM/turbomind">InternLM/turbomind</A>
									<DT><A HREF="https://www.youtube.com/watch?v=XQylGyG7yp8&t=2162s">Lecture 35: SGLang - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-kernels</H3>
								<DL><p>
									<DT><H3 FOLDED>sglang-moe</H3>
									<DL><p>
										<DT><H3 FOLDED>deepseek-moe</H3>
										<DL><p>
											<DT><A HREF="https://mp.weixin.qq.com/s?__biz=MzUxNzQ5MTExNw==&mid=2247493157&idx=1&sn=51c0e27a347dd3fe1ed868d87f667897&chksm=f995f6e7cee27ff1a95d59aefe6bcf4117115343b301dddcc3ea7646dfeb61b582a90d507a16&cur_album_id=3396415517161095170&scene=189&poc_token=HFCN_mejAGWeYB52g6Vft2lx3uMgK8i_wzNqNGnJ">详细谈谈DeepSeek MoE相关的技术发展</A>
										</DL><p>
										<DT><A HREF="https://arxiv.org/abs/2501.08313">[2501.08313] MiniMax-01: Scaling Foundation Models with Lightning Attention</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/pull/2920">[kernel] MiniMax-Text-01 decode lightning_attn with triton by BBuf · Pull Request #2920 · sgl-project/sglang</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/issues/2471#event-15791112196">[Feature] FusedMoE H200 tuning · Issue #2471 · sgl-project/sglang</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/pull/2767">add benchmark_moe_align_blocks by BBuf · Pull Request #2767 · sgl-project/sglang</A>
										<DT><A HREF="https://mp.weixin.qq.com/s/WFJxnTF9fGIIXPA7GQ5V2w">详细谈谈DeepSeek MoE相关的技术发展</A>
										<DT><A HREF="https://github.com/triton-lang/triton/pull/6429">[BENCH] added production kernels and micro-benchmark for mixture-of-experts MLP by ptillet · Pull Request #6429 · triton-lang/triton</A>
									</DL><p>
									<DT><H3 FOLDED>sglang-deepseek</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sgl-project/sglang/issues/2591">[Feature] DeepSeek V3 optimization · Issue #2591 · sgl-project/sglang</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/pull/2719">DeepSeek V3 FP8 Support by yingcanw · Pull Request #2719 · NVIDIA/TensorRT-LLM</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/issues/2965">[Feature] remove vllm _custom_ops · Issue #2965 · sgl-project/sglang</A>
										<DT><A HREF="https://x.com/lmsysorg/status/1890235303270740454">(1) lmsys.org en X: "🚀 Big update from the sglang team! We've made significant progress on the highly anticipated deepseek model: 🚀 FlashInfer MLA Attention integration for 4x faster long-context performance ⚡ torch.compile support, hitting 50 tokens/s for online inference 🔥 CUTLASS block-wise" / X</A>
										<DT><A HREF="https://github.com/sgl-project/sglang/releases/tag/v0.4.3">Release v0.4.3 · sgl-project/sglang</A>
									</DL><p>
									<DT><H3 FOLDED>sglang-inductor</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sgl-project/sglang/issues/4748">[Feature] beat torch compile · Issue #4748 · sgl-project/sglang</A>
									</DL><p>
									<DT><A HREF="https://github.com/sgl-project/sglang/tree/main/sgl-kernel">sglang/sgl-kernel at main · sgl-project/sglang</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/issues/2965">[Feature] remove vllm _custom_ops · Issue #2965 · sgl-project/sglang</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/pull/3178">chore: bump 0.0.3 for sgl-kernel by zhyncs · Pull Request #3178 · sgl-project/sglang</A>
									<DT><A HREF="https://github.com/BBuf/tensorrt-llm-moe">BBuf/tensorrt-llm-moe</A>
									<DT><A HREF="https://github.com/zhihu/ZhiLight">zhihu/ZhiLight: A highly optimized LLM inference acceleration engine for Llama and its variants.</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/pull/3602">Expert Parallelism (EP) Support for DeepSeek V3 by sleepcoo · Pull Request #3602 · sgl-project/sglang</A>
									<DT><A HREF="https://zhuanlan.zhihu.com/p/23574608727">SGLang DP MLA 特性解读 - 知乎</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-inference-server</H3>
								<DL><p>
									<DT><A HREF="https://github.com/lm-sys/FastChat/tree/main">lm-sys/FastChat: An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena.</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/tree/main/python/sglang/srt/managers">sglang/python/sglang/srt/managers at main · sgl-project/sglang</A>
								</DL><p>
								<DT><H3 FOLDED>disaggregating-prefill-decoding</H3>
								<DL><p>
									<DT><H3 FOLDED>sglang-dynamo</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ai-dynamo/dynamo">ai-dynamo/dynamo: A Datacenter Scale Distributed Inference Serving Framework</A>
									</DL><p>
									<DT><A HREF="https://arxiv.org/abs/2401.09670">[2401.09670] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</A>
									<DT><A HREF="https://arxiv.org/abs/2504.02263">[2504.02263] MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism</A>
									<DT><A HREF="https://github.com/ppl-ai/pplx-kernels">ppl-ai/pplx-kernels: Perplexity GPU Kernels</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-router</H3>
								<DL><p>
									<DT><A HREF="https://github.com/sgl-project/sglang/tree/main/rust">sglang/rust at main · sgl-project/sglang</A>
									<DT><A HREF="https://x.com/hsu_byron/status/1864449841239347341">(1) Byron Hsu en X: "(1/n) Introducing SGLang Router: a cache-aware router for LLM Inference in SGLang v0.4! Prefix Caching has been an effective technique to speed up LLM inference. However, when it comes to data parallelism, the load balancer substantially degrades cache hit rates because naive https://t.co/TjxeKyoUMe" / X</A>
									<DT><A HREF="https://arxiv.org/abs/2407.00023">[2407.00023] Preble: Efficient Distributed Prompt Scheduling for LLM Serving</A>
								</DL><p>
								<DT><H3 FOLDED>sgl-tensorrt</H3>
								<DL><p>
									<DT><A HREF="https://github.com/sgl-project/tensorrt-demo">sgl-project/tensorrt-demo</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-constrained-decoding</H3>
								<DL><p>
									<DT><A HREF="https://arxiv.org/abs/2312.07104">[2312.07104] SGLang: Efficient Execution of Structured Language Model Programs</A>
									<DT><A HREF="https://arxiv.org/html/2312.07104v2">SGLang: Efficient Execution of Structured Language Model Programs</A>
									<DT><A HREF="https://www.aidancooper.co.uk/constrained-decoding/">A Guide to Structured Outputs Using Constrained Decoding</A>
								</DL><p>
								<DT><H3 FOLDED>lightllm</H3>
								<DL><p>
									<DT><H3 FOLDED>lightllm-deepseek</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ModelTC/lightllm/pull/783">DeepseekV3 support deepep, deepgemm, PD, DP TP SP Mix mode. by hiworldwzj · Pull Request #783 · ModelTC/lightllm</A>
									</DL><p>
									<DT><A HREF="https://github.com/ModelTC/lightllm">ModelTC/lightllm: LightLLM is a Python-based LLM (Large Language Model) inference and serving framework, notable for its lightweight design, easy scalability, and high-speed performance.</A>
								</DL><p>
								<DT><H3 FOLDED>sglang-people</H3>
								<DL><p>
									<DT><A HREF="https://zhyncs.com/">Yineng Zhang</A>
								</DL><p>
								<DT><A HREF="https://docs.google.com/document/d/1xEow4eIM152xNcRxqZz9VEcOiTQo8-CEuuQ5qTmkt-E/edit?tab=t.0#heading=h.kyti36ufipru">SGL community meeting - Google Docs</A>
								<DT><A HREF="https://github.com/sgl-project/sglang">sgl-project/sglang: SGLang is yet another fast serving framework for large language models and vision language models.</A>
								<DT><A HREF="https://github.com/sgl-project/sglang/tree/main/benchmark/blog_v0_2">sglang/benchmark/blog_v0_2 at main · sgl-project/sglang</A>
								<DT><A HREF="https://github.com/microsoft/sarathi-serve">microsoft/sarathi-serve: A low-latency &amp; high-throughput serving engine for LLMs</A>
								<DT><A HREF="https://github.com/microsoft/vattention?tab=readme-ov-file">microsoft/vattention: Dynamic Memory Management for Serving LLMs without PagedAttention</A>
								<DT><A HREF="https://github.com/AlibabaPAI/FLASHNN/blob/528a9301587f5fb135b25d973a87ba0a40a703a7/flashnn/kernel_backend.py#L44">FLASHNN/flashnn/kernel_backend.py at 528a9301587f5fb135b25d973a87ba0a40a703a7 · AlibabaPAI/FLASHNN</A>
								<DT><A HREF="https://github.com/sgl-project/sgl-learning-materials/tree/main">sgl-project/sgl-learning-materials: Materials for learning SGLang</A>
								<DT><A HREF="https://github.com/zhaochenyang20/ModelServer">zhaochenyang20/ModelServer: Efficient, Flexible, and Highly Fault-Tolerant Model Service Management Based on SGLang</A>
								<DT><A HREF="https://x.com/ibab/status/1827047684714463603">(1) ibab en X: "Grok 2 mini is now 2x faster than it was yesterday. In the last three days @lm_zheng and @MalekiSaeed rewrote our inference stack from scratch using SGLang (https://t.co/M1M8BlXosH). This has also allowed us to serve the big Grok 2 model, which requires multi-host inference, at a https://t.co/G9iXTV8o0z" / X</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Ny4xxErgFgQ">Efficient LLM Inference with SGLang, Lianmin Zheng, xAI - YouTube</A>
								<DT><A HREF="https://github.com/interestingLSY/swiftLLM">interestingLSY/swiftLLM: A tiny yet powerful LLM inference system tailored for researching purpose. vLLM-equivalent performance with only 2k lines of code (2% of vLLM).</A>
								<DT><A HREF="https://aflah02.substack.com/p/multi-node-llm-inference-with-sglang">Multi-Node LLM Inference with SGLang on SLURM-Enabled Clusters</A>
							</DL><p>
							<DT><H3 FOLDED>MLCEngine</H3>
							<DL><p>
								<DT><A HREF="https://blog.mlc.ai/2024/10/10/optimizing-and-characterizing-high-throughput-low-latency-llm-inference">MLC | Optimizing and Characterizing High-Throughput Low-Latency LLM Inference in MLCEngine</A>
								<DT><A HREF="https://blog.mlc.ai/">MLC | Home</A>
								<DT><A HREF="https://github.com/mlc-ai/mlc-llm">mlc-ai/mlc-llm: Universal LLM Deployment Engine with ML Compilation</A>
							</DL><p>
							<DT><H3 FOLDED>lmdeploy</H3>
							<DL><p>
								<DT><H3 FOLDED>lmdeploy-docs</H3>
								<DL><p>
									<DT><A HREF="https://lmdeploy.readthedocs.io/en/latest/">Welcome to LMDeploy’s tutorials! — lmdeploy</A>
									<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/main/docs/en/installation.md">lmdeploy/docs/en/installation.md at main</A>
								</DL><p>
								<DT><H3 FOLDED>lmdeploy-multi-modal</H3>
								<DL><p>
									<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/main/docs/en/multi_modal/cogvlm.md">lmdeploy/docs/en/multi_modal/cogvlm.md at main · InternLM/lmdeploy</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5ddb6bf218ed16a2dcf0058f20c59a247e180fd2/examples/multimodal/run.py#L920">run.py#L920 -&gt; image_path</A>
									<DT><A HREF="https://github.com/THUDM/CogVLM2/blob/main/basic_demo/requirements.txt">CogVLM2/basic_demo/requirements.txt at main · THUDM/CogVLM2</A>
								</DL><p>
								<DT><H3 FOLDED>lmdeploy-container</H3>
								<DL><p>
									<DT><H3 FOLDED>lmdeploy-container-example</H3>
									<DL><p>
										<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo/tree/main">neuralmagic/tensorrt-demo</A>
										<DT><A HREF="https://hub.docker.com/layers/openmmlab/lmdeploy/latest/images/sha256-77e3080cdc32839a39dc245770c42183162f9d787ef4fc4b3d4f11ddfaacb092?context=explore">Image Layer Details - openmmlab/lmdeploy:latest | Docker Hub</A>
										<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/02077a7d03b6bdaf905ba32e8bdd755d41d77401/docs/en/multi_modal/vl_pipeline.md">lmdeploy/docs/en/multi_modal/vl_pipeline.md at 02077a7d03b6bdaf905ba32e8bdd755d41d77401 · InternLM/lmdeploy</A>
										<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/40fdc777688d4830b92ae8b5c5380686dc0de97f/docs/en/serving/api_server_vl.md">lmdeploy/docs/en/serving/api_server_vl.md at 40fdc777688d4830b92ae8b5c5380686dc0de97f · InternLM/lmdeploy</A>
										<DT><A HREF="https://github.com/InternLM/lmdeploy/tree/40fdc777688d4830b92ae8b5c5380686dc0de97f">InternLM/lmdeploy at 40fdc777688d4830b92ae8b5c5380686dc0de97f</A>
										<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/02077a7d03b6bdaf905ba32e8bdd755d41d77401/docs/en/supported_models/supported_models.md#L4">lmdeploy/docs/en/supported_models/supported_models.md at 02077a7d03b6bdaf905ba32e8bdd755d41d77401 · InternLM/lmdeploy</A>
										<DT><A HREF="https://aistudio.google.com/app/prompts/1KnqPc-9ZNiYGbWD-sO3I0ro5touxoTU0?pli=1">Docker build | Google AI Studio</A>
										<DT><A HREF="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B">THUDM/cogvlm2-llama3-chat-19B · Hugging Face</A>
										<DT><A HREF="https://github.com/THUDM/CogVLM2/blob/main/basic_demo/requirements.txt">CogVLM2/basic_demo/requirements.txt at main · THUDM/CogVLM2</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/f565d16acbeea9c6c222349bdd32e4c6e0854d24/Dockerfile#L4">pytorch/Dockerfile at f565d16acbeea9c6c222349bdd32e4c6e0854d24 · pytorch/pytorch</A>
										<DT><A HREF="https://x.com/home">(1) Inicio / X</A>
									</DL><p>
									<DT><A HREF="https://hub.docker.com/r/openmmlab/lmdeploy">openmmlab/lmdeploy - Docker Image | Docker Hub</A>
									<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/40fdc777688d4830b92ae8b5c5380686dc0de97f/docs/en/serving/api_server_vl.md">lmdeploy/docs/en/serving/api_server_vl.md: Each model may require specific dependencies not included in the Docker image.</A>
								</DL><p>
								<DT><H3 FOLDED>lmdeploy-serving</H3>
								<DL><p>
									<DT><H3 FOLDED>lmdeploy-benchmarking</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sgl-project/sglang/blob/c8e9fed87a85241180cb83230c8407d5d96c5f85/python/sglang/benchmarks/bench_serving.py#L243">sglang: bench_serving.py#L243 "lmdeploy": async_request_openai_completions,</A>
										<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo/tree/main">neuralmagic/tensorrt-demo</A>
									</DL><p>
									<DT><H3 FOLDED>openai-protocol-vision</H3>
									<DL><p>
										<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/ebae7d28d9dcf2b62dab926770d97089764e69da/docs/en/get_started.md">lmdeploy/docs/en/get_started.md at ebae7d28d9dcf2b62dab926770d97089764e69da · InternLM/lmdeploy</A>
										<DT><A HREF="https://platform.openai.com/docs/guides/vision?lang=curl">Vision - OpenAI API: type: image_url | image_url: url</A>
									</DL><p>
									<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/40fdc777688d4830b92ae8b5c5380686dc0de97f/docs/en/serving/api_server_vl.md">lmdeploy/docs/en/serving/api_server_vl.md: Each model may require specific dependencies not included in the Docker image.</A>
									<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/85daad96ec4832e47678066ff07f6d6cc69697d0/lmdeploy/serve/openai/api_server.py#L371">lmdeploy/lmdeploy/serve/openai/api_server.py: /v1/chat/completions -&gt; image interaction</A>
									<DT><A HREF="https://github.com/InternLM/lmdeploy/blob/ebae7d28d9dcf2b62dab926770d97089764e69da/lmdeploy/serve/openai/protocol.py#L104">lmdeploy/lmdeploy/serve/openai/protocol.py: ignore_eos</A>
								</DL><p>
								<DT><H3 FOLDED>turbomind</H3>
								<DL><p>
									<DT><A HREF="https://github.com/InternLM/turbomind">InternLM/turbomind</A>
									<DT><A HREF="https://www.youtube.com/watch?v=XQylGyG7yp8&t=2162s">Lecture 35: SGLang - YouTube</A>
									<DT><A HREF="https://github.com/mobiusml/gemlite">mobiusml/gemlite: Simple and fast low-bit matmul kernels in CUDA / Triton</A>
									<DT><A HREF="https://x.com/Mobius_Labs/status/1907450136696856910">GemLite</A>
								</DL><p>
								<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo/tree/main">neuralmagic/tensorrt-demo</A>
								<DT><A HREF="https://github.com/InternLM/lmdeploy">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.</A>
								<DT><A HREF="https://github.com/ModelTC/lightllm/tree/main">ModelTC/lightllm: LightLLM is a Python-based LLM (Large Language Model) inference and serving framework, notable for its lightweight design, easy scalability, and high-speed performance.</A>
								<DT><A HREF="https://github.com/InternLM/turbomind">InternLM/turbomind</A>
								<DT><A HREF="https://github.com/InternLM/InternLM">InternLM/InternLM: Official release of InternLM2.5 base and chat models. 1M context support</A>
								<DT><A HREF="https://github.com/zhihu/ZhiLight">zhihu/ZhiLight: A highly optimized LLM inference acceleration engine for Llama and its variants.</A>
							</DL><p>
							<DT><H3 FOLDED>Triton Inference Server</H3>
							<DL><p>
								<DT><H3 FOLDED>tis-releases</H3>
								<DL><p>
									<DT><H3 FOLDED>tis-release-24.05</H3>
									<DL><p>
										<DT><A HREF="https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel-24-05.html">Release Notes :: NVIDIA Deep Learning Triton Inference Server Documentation</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/releases/tag/v0.11.0">Release TensorRT-LLM 0.11.0 Release · NVIDIA/TensorRT-LLM</A>
										<DT><A HREF="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver">Triton Inference Server | NVIDIA NGC</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>triton-remote-caching</H3>
								<DL><p>
									<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62531/">Deploying, Optimizing, and Benchmarking Large Language Models With Triton Inference Server | NVIDIA On-Demand</A>
								</DL><p>
								<DT><H3 FOLDED>tis-client</H3>
								<DL><p>
									<DT><A HREF="https://github.com/triton-inference-server/client">triton-inference-server/client</A>
								</DL><p>
								<DT><H3 FOLDED>tis-server</H3>
								<DL><p>
									<DT><H3 FOLDED>dynamic batching</H3>
									<DL><p>
										<DT><A HREF="https://github.com/sgl-project/sglang/tree/main/python/sglang/srt/managers">sglang/python/sglang/srt/managers at main · sgl-project/sglang</A>
									</DL><p>
									<DT><H3 FOLDED>concurrent model execution</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>tis-examples</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=JgP2WgNIq_w">How to Deploy HuggingFace’s Stable Diffusion Pipeline with Triton Inference Server</A>
										<DT><A HREF="https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer">Triton Inference Server - FasterTransformer GPT-J and GPT-NeoX 20B - CoreWeave</A>
										<DT><A HREF="https://docs.coreweave.com/machine-learning-and-ai/inference/examples/triton-inference/triton-inference-server-fastertransformer#gpt-neox-20b">FasterTransformer GPT-J and GPT: NeoX 20B - CoreWeave</A>
										<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-S52370/?ncid=em-even-124008-vt33">Inference of Large Language Models with NVIDIA Triton Inference Server (Presented by CoreWeave) | NVIDIA On-Demand</A>
										<DT><A HREF="https://developer.nvidia.com/blog/accelerated-inference-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inference-server/?nvid=nv-int-txtad-664399-vt27#cid=an01_nv-int-txtad_en-us">Accelerated Inference for Large Transformer Models Using NVIDIA Triton Inference Server | NVIDIA Technical Blog</A>
										<DT><A HREF="https://developer.nvidia.com/blog/deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server/">Deploying GPT-J and T5 with NVIDIA Triton Inference Server | NVIDIA Technical Blog</A>
										<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT/triton">Deploying the BERT model on Triton Inference Server</A>
										<DT><A HREF="https://www.youtube.com/watch?v=NR_iUl2Ooc0">NVIDIA Triton Inference Server and its use in Netflix's Model Scoring Service: model ensemble (pipeline)</A>
									</DL><p>
									<DT><H3 FOLDED>tis-dtypes</H3>
									<DL><p>
										<DT><A HREF="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html">TYPE_BF16</A>
										<DT><A HREF="https://github.com/triton-inference-server/server/blob/e1816629dd8bc1b05748450739420639543bcb11/qa/python_models/identity_bf16/config.pbtxt#L33">server/qa/python_models/identity_bf16/config.pbtxt</A>
									</DL><p>
									<DT><H3 FOLDED>tis-ensemble</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=NR_iUl2Ooc0">NVIDIA Triton Inference Server and its use in Netflix's Model Scoring Service: model ensemble (pipeline)</A>
									</DL><p>
									<DT><A HREF="https://github.com/triton-inference-server/server">server</A>
									<DT><A HREF="https://github.com/triton-inference-server/server">triton-inference-server/server: The Triton Inference Server provides an optimized cloud and edge inferencing solution.</A>
									<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/scripts/launch_triton_server.py">tensorrtllm_backend/scripts/launch_triton_server.py at main · triton-inference-server/tensorrtllm_backend</A>
								</DL><p>
								<DT><H3 FOLDED>TensorRT-LLM</H3>
								<DL><p>
									<DT><H3 FOLDED>tensorrt-llm-build</H3>
									<DL><p>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5d8ca2faf74c494f220c8f71130340b513eea9a9/docs/source/installation/build-from-source-linux.md">TensorRT-LLM/docs/source/installation/build-from-source-linux.md at 5d8ca2faf74c494f220c8f71130340b513eea9a9 · NVIDIA/TensorRT-LLM</A>
									</DL><p>
									<DT><H3 FOLDED>tensorrt-llm-lectures</H3>
									<DL><p>
										<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62031/">Speeding up LLM Inference With TensorRT-LLM | NVIDIA On-Demand</A>
										<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62531/">Deploying, Optimizing, and Benchmarking Large Language Models With Triton Inference Server | NVIDIA On-Demand</A>
									</DL><p>
									<DT><H3 FOLDED>tensorrt-llm-docs</H3>
									<DL><p>
										<DT><A HREF="https://nvidia.github.io/TensorRT-LLM/">Welcome to TensorRT-LLM’s Documentation! — tensorrt_llm documentation</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/main/docs/source">TensorRT-LLM/docs/source at main · NVIDIA/TensorRT-LLM</A>
										<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo/tree/main">neuralmagic/tensorrt-demo</A>
									</DL><p>
									<DT><H3 FOLDED>tensorrt-llm-triton-kernels</H3>
									<DL><p>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/openai_triton/README.md">TensorRT-LLM/examples/openai_triton/README.md</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/openai_triton/manual_plugin">TensorRT-LLM/examples/openai_triton/manual_plugin</A>
										<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo">neuralmagic/tensorrt-demo</A>
										<DT><A HREF="https://github.com/BBuf/tensorrt-llm-moe">BBuf/tensorrt-llm-moe</A>
									</DL><p>
									<DT><H3 FOLDED>tensorrt-llm-multimodal</H3>
									<DL><p>
										<DT><H3 FOLDED>multimodal-session</H3>
										<DL><p>
											<DT><A HREF="https://zhuanlan.zhihu.com/p/700214123?utm_psn=1779287628619632640">https://zhuanlan.zhihu.com/p/700214123?utm_psn=1779287628619632640</A>
											<DT><A HREF="https://madsys.cs.tsinghua.edu.cn/">https://madsys.cs.tsinghua.edu.cn/</A>
											<DT><A HREF="https://claude.ai/chat/21a90660-5bf8-4d48-87a7-da590e138e30">https://claude.ai/chat/21a90660-5bf8-4d48-87a7-da590e138e30</A>
											<DT><A HREF="https://lmsys.org/blog/2024-07-25-sglang-llama3/">https://lmsys.org/blog/2024-07-25-sglang-llama3/</A>
											<DT><A HREF="https://github.com/sgl-project/sglang/blob/c8e9fed87a85241180cb83230c8407d5d96c5f85/python/sglang/benchmarks/bench_serving.py#L4">https://github.com/sgl-project/sglang/blob/c8e9fed87a85241180cb83230c8407d5d96c5f85/python/sglang/benchmarks/bench_serving.py#L4</A>
											<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#llama-v3-updates">https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#llama-v3-updates</A>
											<DT><A HREF="https://docs.google.com/document/d/1QYl7Yvy5nGvXlzWFenxIpovGTkgxjvfRKzt1ecMd1GM/edit">triton server using tensorrt-llm llama 3 - Google Docs</A>
											<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo/tree/main">https://github.com/neuralmagic/tensorrt-demo/tree/main</A>
											<DT><A HREF="https://github.com/datacrunch-research/tensorrt-demo/blob/main/docs/multimodal.md">https://github.com/datacrunch-research/tensorrt-demo/blob/main/docs/multimodal.md</A>
											<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend/blob/b25d578a48422db3b2d5bd89b16c235dd85c4300/inflight_batcher_llm/src/utils.cc#L92">https://github.com/triton-inference-server/tensorrtllm_backend/blob/b25d578a48422db3b2d5bd89b16c235dd85c4300/inflight_batcher_llm/src/utils.cc#L92</A>
											<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file">https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file</A>
											<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/a681853d3803ee5893307e812530b5e7004bb6e1/examples/multimodal#cogvlm">https://github.com/NVIDIA/TensorRT-LLM/tree/a681853d3803ee5893307e812530b5e7004bb6e1/examples/multimodal#cogvlm</A>
											<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/releases/tag/v0.11.0">Release TensorRT-LLM 0.11.0 Release · NVIDIA/TensorRT-LLM</A>
											<DT><A HREF="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html</A>
											<DT><A HREF="https://github.com/triton-inference-server/server/blob/e1816629dd8bc1b05748450739420639543bcb11/qa/python_models/identity_bf16/config.pbtxt#L33">server/qa/python_models/identity_bf16/config.pbtxt at e1816629dd8bc1b05748450739420639543bcb11 · triton-inference-server/server</A>
											<DT><A HREF="https://jax.readthedocs.io/en/latest/autodidax.html#part-2-jaxprs">Autodidax: JAX core from scratch — JAX documentation</A>
											<DT><A HREF="https://outlook.office.com/mail/inbox/id/AAQkADNiMWRmMjMwLTQ0MzItNDhjOS1iOGFjLTk3NzQyNjY1Y2RhMQAQACFINjNdKRpGiAD0qfgsu54%3D">Email - Antonio Dominguez - Outlook</A>
											<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/a681853d3803ee5893307e812530b5e7004bb6e1/examples/dit/README.md">TensorRT-LLM/examples/dit/README.md at a681853d3803ee5893307e812530b5e7004bb6e1 · NVIDIA/TensorRT-LLM</A>
											<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/a681853d3803ee5893307e812530b5e7004bb6e1/tensorrt_llm/models/dit/model.py">TensorRT-LLM/tensorrt_llm/models/dit/model.py at a681853d3803ee5893307e812530b5e7004bb6e1 · NVIDIA/TensorRT-LLM</A>
											<DT><A HREF="https://github.com/black-forest-labs/flux/blob/main/src/flux/modules/layers.py#L11">flux/src/flux/modules/layers.py at main · black-forest-labs/flux</A>
										</DL><p>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5ddb6bf218ed16a2dcf0058f20c59a247e180fd2/examples/multimodal/README.md#cogvlm">TensorRT-LLM/examples/multimodal/README.md</A>
									</DL><p>
									<DT><H3 FOLDED>tensorrt-llm-llama-3</H3>
									<DL><p>
										<DT><A HREF="https://docs.google.com/document/d/1QYl7Yvy5nGvXlzWFenxIpovGTkgxjvfRKzt1ecMd1GM/edit">Untitled document - Google Docs</A>
										<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo/blob/main/README.md?plain=1">tensorrt-demo/README.md at main · neuralmagic/tensorrt-demo</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#llama-v3-updates">TensorRT-LLM/examples/llama at main · NVIDIA/TensorRT-LLM</A>
										<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/docs/llama.md?plain=1">tensorrtllm_backend/docs/llama.md at main · triton-inference-server/tensorrtllm_backend</A>
										<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/scripts/launch_triton_server.py">tensorrtllm_backend/scripts/launch_triton_server.py at main · triton-inference-server/tensorrtllm_backend</A>
										<DT><A HREF="https://huggingface.co/docs/transformers/main/en/model_doc/llama">LLaMA</A>
										<DT><A HREF="https://www.notion.so/dl-compiler-26c51f0aaa4842c381f710a9b5496116">dl-compiler</A>
										<DT><A HREF="https://chatgpt.com/c/c01100b8-e432-437c-b167-c7d9598c3922">ChatGPT</A>
										<DT><A HREF="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">meta-llama/Meta-Llama-3-8B-Instruct · Hugging Face</A>
									</DL><p>
									<DT><H3 FOLDED>tensorrt-llm-dit</H3>
									<DL><p>
										<DT><A HREF="https://github.com/chengzeyi/piflux/blob/main/src/piflux/patch.py#L6">piflux/src/piflux/patch.py at main · chengzeyi/piflux</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/dit">TensorRT-LLM/examples/dit</A>
										<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/tensorrt_llm/models/dit/model.py">TensorRT-LLM/tensorrt_llm/models/dit/model.py</A>
										<DT><A HREF="https://github.com/chengzeyi/piflux">chengzeyi/piflux: (WIP) Parallel inference for black-forest-labs' FLUX model.</A>
									</DL><p>
									<DT><H3 FOLDED>tensorrt-llm-custom-model</H3>
									<DL><p>
										<DT><A HREF="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62031/">Speeding up LLM Inference With TensorRT-LLM | NVIDIA On-Demand</A>
									</DL><p>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0">TensorRT-LLM</A>
									<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend">triton-inference-server/tensorrtllm_backend: The Triton TensorRT-LLM Backend</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/issues/2014">failed to use TensorRT-LLM/examples/apps/fastapi_server.py · Issue #2014 · NVIDIA/TensorRT-LLM</A>
									<DT><A HREF="https://github.com/triton-inference-server/tensorrtllm_backend/blob/main/docs/llama.md">tensorrtllm_backend/docs/llama.md at main</A>
									<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo/tree/main">neuralmagic/tensorrt-demo</A>
									<DT><A HREF="https://github.com/zhyncs/TensorRT-LLM-Hacks">zhyncs/TensorRT-LLM-Hacks: TensorRT LLM Hacks</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/40274aac39f2542483906d92ec3b8014faf62912/benchmarks/Suite.md?plain=1#L6">TensorRT-LLM/benchmarks/Suite.md: trt-bench</A>
								</DL><p>
								<DT><H3 FOLDED>FasterTransformer</H3>
								<DL><p>
									<DT><A HREF="https://github.com/triton-inference-server/fastertransformer_backend">triton-inference-server/fastertransformer_backend</A>
									<DT><A HREF="https://github.com/NVIDIA/FasterTransformer/blob/main/docs/t5_guide.md">FasterTransformer/docs/t5_guide.md at main · NVIDIA/FasterTransformer</A>
									<DT><A HREF="https://github.com/triton-inference-server/fastertransformer_backend#run-inter-node-t-x-p--gpus-per-node-models">triton-inference-server/fastertransformer_backend</A>
									<DT><A HREF="https://www.youtube.com/watch?v=MDqNwSTLimU">[NVIDIA] Faster Transformer</A>
									<DT><A HREF="https://github.com/OpenNMT/OpenNMT-tf">DecoderTransformer: OpenNMT/OpenNMT-tf</A>
									<DT><A HREF="https://github.com/NVIDIA/FasterTransformer">NVIDIA/FasterTransformer: Transformer related optimization, including BERT, GPT</A>
									<DT><A HREF="https://carper.ai/diff-models-a-new-way-to-edit-code/">Diff Models – A New Way to Edit Code | CarperAI</A>
									<DT><A HREF="https://github.com/pytorch/serve/tree/master/examples/FasterTransformer_HuggingFace_Bert">serve/examples/FasterTransformer_HuggingFace_Bert</A>
								</DL><p>
								<DT><A HREF="https://github.com/triton-inference-server/fastertransformer_backend">triton-inference-server/fastertransformer_backend</A>
								<DT><A HREF="https://github.com/triton-inference-server/backend/blob/main/docs/backend_platform_support_matrix.md">backend/backend_platform_support_matrix.md</A>
								<DT><A HREF="https://developer.nvidia.com/blog/power-your-ai-inference-with-new-nvidia-triton-and-nvidia-tensorrt-features/?ncid=so-link-726143&=&linkId=100000196119468#cid=dl05_so-link_en-us">PyTriton</A>
								<DT><A HREF="https://medium.com/nvidia-ai/how-to-deploy-almost-any-hugging-face-model-on-nvidia-triton-inference-server-with-an-8ee7ec0e6fc4">Big Picture: Example showcase demo</A>
								<DT><A HREF="https://github.com/ELS-RD/transformer-deploy">Transformer Deployment</A>
								<DT><A HREF="https://github.com/triton-inference-server/python_backend?tab=readme-ov-file#decoupled-mode">triton-inference-server/python_backend: Triton backend that enables pre-process, post-processing and other logic to be implemented in Python.</A>
							</DL><p>
							<DT><H3 FOLDED>vLLM</H3>
							<DL><p>
								<DT><H3 FOLDED>AIBrix</H3>
								<DL><p>
									<DT><A HREF="https://x.com/mcraddock/status/1893337828693586318?s=12">(1) Mark 🇺🇳 en X: "ByteDance released their full AI stack: AIBrix, an open-source initiative designed to provide essential building blocks to construct scalable GenAI inference infrastructure. AIBrix delivers a cloud-native solution optimised for deploying, managing, and scaling LLMs https://t.co/JLYXy1EGtM" / X</A>
									<DT><A HREF="https://github.com/vllm-project/aibrix">vllm-project/aibrix: Cost-efficient and pluggable Infrastructure components for GenAI inference</A>
								</DL><p>
								<DT><H3 FOLDED>vllm-meetings</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit#slide=id.g2650ce3df47_0_0">vLLM @ Fourth Meetup (Public) - Google Slides</A>
									<DT><A HREF="https://docs.google.com/presentation/d/1OF6GBbxDNwlgwmyYiCyN98W3t2HNfFlhpFJlqJgp_aI/mobilepresent?slide=id.g273594aa4df_1_6">vLLM @ Fourth Meetup (Public) - Google Slides (Neural Magic)</A>
								</DL><p>
								<DT><H3 FOLDED>nm-vllm</H3>
								<DL><p>
									<DT><H3 FOLDED>vllm-fp8</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/collections/neuralmagic/fp8-llms-for-vllm-666742ed2b78b7ac8df13127">FP8 LLMs for vLLM - a neuralmagic Collection</A>
									</DL><p>
									<DT><A HREF="https://github.com/neuralmagic/nm-vllm">neuralmagic/nm-vllm (fork)</A>
									<DT><A HREF="https://github.com/neuralmagic/nm-vllm/blob/788b4e526d379aa6b910cb1932a756d17cbcc997/vllm/model_executor/weight_utils.py#L81">weight_utils.py#L81 convert_bin_to_safetensor_file (checkpoint format conversion procedure)</A>
									<DT><A HREF="https://github.com/datacrunch-research/transmogrifier/blob/main/transmogrifier/convert.py#L156">transmogrifier/transmogrifier/convert.py at main · datacrunch-research/transmogrifier</A>
									<DT><A HREF="https://github.com/neuralmagic/AutoFP8">neuralmagic/AutoFP8</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024">DeepSpeed/blogs/deepspeed-fp6/03-05-2024 at master · microsoft/DeepSpeed</A>
									<DT><A HREF="https://github.com/neuralmagic/compressed-tensors">neuralmagic/compressed-tensors: A safetensors extension to efficiently store sparse quantized tensors on disk</A>
									<DT><A HREF="https://github.com/neuralmagic/guidellm/tree/main">neuralmagic/guidellm</A>
								</DL><p>
								<DT><H3 FOLDED>vllm-splitwise</H3>
								<DL><p>
									<DT><A HREF="https://github.com/vllm-project/vllm/pull/2809">Add Splitwise implementation to vLLM</A>
									<DT><A HREF="https://www.microsoft.com/en-us/research/blog/splitwise-improves-gpu-usage-by-splitting-llm-inference-phases/">Splitwise improves GPU usage by splitting LLM inference phases - Microsoft Research</A>
									<DT><A HREF="https://arxiv.org/abs/2311.18677">[2311.18677] Splitwise: Efficient generative LLM inference using phase splitting</A>
									<DT><A HREF="https://github.com/Mutinifni/splitwise-sim">Mutinifni/splitwise-sim: LLM serving cluster simulator</A>
									<DT><A HREF="https://www.youtube.com/watch?v=WwJvecXOeUA">OSDI '24 - DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language... - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>vllm-benchmark</H3>
								<DL><p>
									<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/launch_tgi_server.sh#L4">vllm/benchmarks/launch_tgi_server.sh</A>
									<DT><A HREF="https://github.com/vllm-project/vllm/blob/fa32207842f1ed5a966372ed0513914bff8426c4/benchmarks/benchmark_throughput.py#L200">vllm/benchmarks/benchmark_throughput.py</A>
								</DL><p>
								<DT><H3 FOLDED>vllm-tensorize</H3>
								<DL><p>
									<DT><A HREF="https://docs.vllm.ai/en/latest/getting_started/examples/tensorize_vllm_model.html">Tensorize vLLM Model — vLLM</A>
								</DL><p>
								<DT><H3 FOLDED>vllm-models</H3>
								<DL><p>
									<DT><A HREF="https://github.com/vllm-project/vllm/issues/6265">[New Model]: CogVlm2 - SOTA Vision Large Language Model for Document Understanding · Issue #6265 · vllm-project/vllm</A>
								</DL><p>
								<DT><A HREF="https://github.com/vllm-project/vllm">vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs</A>
								<DT><A HREF="https://vllm.ai/">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</A>
								<DT><A HREF="https://twitter.com/edacih/status/1671885000030191616">Continuous batching &amp; PagedAttention (TW thread)</A>
								<DT><A HREF="https://www.anyscale.com/blog/continuous-batching-llm-inference">Continuous batching &amp; PagedAttention</A>
								<DT><A HREF="https://github.com/EmbeddedLLM/vllm-rocm">EmbeddedLLM/vllm-rocm: ROCm-enabled vLLM: A high-throughput and memory-efficient inference and serving engine for LLMs</A>
								<DT><A HREF="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda/src/llm/paged/cache_engine.rs">aici/rllm/rllm-cuda/src/llm/paged/cache_engine.rs</A>
								<DT><A HREF="https://github.com/triton-inference-server/tutorials/blob/main/Quick_Deploy/vLLM/README.md#deploying-a-vllm-model-in-triton">tutorials/Quick_Deploy/vLLM/README.md</A>
								<DT><A HREF="https://www.youtube.com/watch?v=xPvjwqX1m_I">vLLM and Neural Magic Office Hours - June 5, 2024 - YouTube</A>
								<DT><A HREF="https://github.com/vllm-project/vllm/blob/4a6769053ab2616f7f490e6ec5b8241e76ef0c2a/vllm/envs.py#L4">vllm/vllm/envs.py</A>
							</DL><p>
							<DT><H3 FOLDED>Torch Serve</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch/serve">pytorch/serve: Serve, optimize and scale PyTorch models in production</A>
								<DT><A HREF="https://dev-discuss.pytorch.org/t/the-future-of-c-model-deployment/1282">The future of C++ model deployment</A>
								<DT><A HREF="https://github.com/BBuf/how-to-optim-algorithm-in-cuda/commit/fec7d0015b9c3b0d4ca5ca77072aec57d59c8d38">add Deploying LLMs with TorchServe + vLLM.md · BBuf/how-to-optim-algorithm-in-cuda@fec7d00</A>
							</DL><p>
							<DT><H3 FOLDED>jax-serving</H3>
							<DL><p>
								<DT><H3 FOLDED>Saxml</H3>
								<DL><p>
									<DT><H3 FOLDED>serving-alpa</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/zhuohan123/status/1629007867834695681">AlpaServe: model parallelism</A>
										<DT><A HREF="https://www.youtube.com/watch?v=qzYoMldlyoA&t=13s">Trends Driving Big Models</A>
										<DT><A HREF="https://www.youtube.com/watch?v=LGYYRRKxCjE">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning - YouTube</A>
									</DL><p>
									<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
									<DT><A HREF="https://cloud.google.com/kubernetes-engine/docs/tutorials/tpu-multihost-saxml#load_the_model">Serve an LLM using multi-host TPUs on GKE with Saxml  |  Kubernetes Engine  |  Google Cloud</A>
									<DT><A HREF="https://github.com/google/saxml/blob/e060b11a7c2de1cca75370a8cddd8f89d6f4664c/saxml/server/pax/lm/servable_lm_model_test.py#L279">saxml/saxml/server/pax/lm/servable_lm_model_test.py</A>
									<DT><A HREF="https://github.com/google/JetStream/tree/main">google/JetStream: A throughput and memory optimized engine for LLM inference on TPUs!</A>
								</DL><p>
								<DT><A HREF="https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/serving/README.md">jax2tf: serving examples</A>
								<DT><A HREF="https://alpa.ai/tutorials/opt_serving.html#launch-a-web-server-to-serve-the-opt-models">Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa — Alpa 0.2.3.dev17 documentation</A>
								<DT><A HREF="https://github.com/alpa-projects/alpa">alpa-projects/alpa: Training and serving large-scale neural networks with auto parallelization.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=e85Ceq2g5z0&t=1s">(Day 1 - Breakout Session) StableHLO &amp; PJRT - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-XyysbKrShk">PyTorch 💙 XLA - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>Deepspeed-MII</H3>
							<DL><p>
								<DT><H3 FOLDED>Deepspeed-MII-engine</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md">DeepSpeed/inference-tutorial.md at master · microsoft/DeepSpeed · GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeedExamples">microsoft/DeepSpeedExamples: Example models using DeepSpeed</A>
									<DT><A HREF="https://github.com/huggingface/accelerate/blob/55691b14c21748319bc7004f09d2ea6019e71a25/benchmarks/big_model_inference.py#L100">accelerate/big_model_inference.py at 55691b14c21748319bc7004f09d2ea6019e71a25 · huggingface/accelerate</A>
									<DT><A HREF="https://lightning.ai/pages/community/serve-stable-diffusion-three-times-faster/">Stable Diffusion kernel injection</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/pull/4604">DeepSpeed-FastGen by cmikeh2 · Pull Request #4604 · microsoft/DeepSpeed</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/4828d71d076b9a5cbe7ef48007cc5b51907c3319/blogs/deepspeed-fastgen/README.md">DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</A>
								</DL><p>
								<DT><H3 FOLDED>Deepspeed-MII-architecture</H3>
								<DL><p>
									<DT><A HREF="https://github.com/antferdom/infrastructure/blob/master/rest_server/server.py">infrastructure/server.py at master · antferdom/infrastructure · GitHub</A>
									<DT><A HREF="https://github.com/pythonprofilers/memory_profiler">pythonprofilers/memory_profiler: Monitor Memory usage of Python code</A>
									<DT><A HREF="https://github.com/huggingface/accelerate/blob/3cb9d5fd9c78c1da9fbc3127d6e63679a2475c6a/src/accelerate/utils/modeling.py#L443">accelerate/modeling.py at 3cb9d5fd9c78c1da9fbc3127d6e63679a2475c6a · huggingface/accelerate</A>
									<DT><A HREF="https://github.com/huggingface/accelerate/blob/5e6351502aa2117d9f73da4c001e9baa87c65b67/tests/test_big_modeling.py#L600">accelerate/test_big_modeling.py at 5e6351502aa2117d9f73da4c001e9baa87c65b67 · huggingface/accelerate</A>
									<DT><A HREF="https://github.com/huggingface/accelerate/blob/55691b14c21748319bc7004f09d2ea6019e71a25/benchmarks/big_model_inference.py#L100">accelerate/big_model_inference.py at 55691b14c21748319bc7004f09d2ea6019e71a25 · huggingface/accelerate</A>
									<DT><A HREF="https://github.com/huggingface/transformers/blob/370f0ca18c8e4577357df59936e790acdecef4ac/tests/models/t5/test_modeling_tf_t5.py#L551">transformers/test_modeling_tf_t5.py at 370f0ca18c8e4577357df59936e790acdecef4ac · huggingface/transformers</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TableQuestionAnsweringPipeline">Pipelines</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/pipelines#transformers.Text2TextGenerationPipeline">Pipelines</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeedExamples">microsoft/DeepSpeedExamples: Example models using DeepSpeed</A>
									<DT><A HREF="https://arxiv.org/pdf/2207.00032.pdf">https://arxiv.org/pdf/2207.00032.pdf</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md">DeepSpeed/inference-tutorial.md at master · microsoft/DeepSpeed · GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/server.py">DeepSpeed-MII/server.py at main · microsoft/DeepSpeed-MII · GitHub</A>
									<DT><A HREF="https://github.com/mallorbc/Finetune_LLMs/blob/main/inference/query.py">Finetune_LLMs/query.py at main · mallorbc/Finetune_LLMs · GitHub</A>
								</DL><p>
								<DT><H3 FOLDED>Deepspeed-MII-deployment-stack-trace</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/deployment.py#LL128C1-L129C1">DeepSpeed-MII/deployment.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 · microsoft/DeepSpeed-MII · GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/models/score/generate.py">DeepSpeed-MII/generate.py at main · microsoft/DeepSpeed-MII · GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/models/score/generate.py#L12">DeepSpeed-MII/generate.py at main · microsoft/DeepSpeed-MII</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/mii/models/score/score_template.py">DeepSpeed-MII/score_template.py at main · microsoft/DeepSpeed-MII · GitHub</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/deployment.py#L147">DeepSpeed-MII/deployment.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 · microsoft/DeepSpeed-MII</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/utils.py#L155">DeepSpeed-MII/utils.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 · microsoft/DeepSpeed-MII</A>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/server.py">DeepSpeed-MII/server.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 · microsoft/DeepSpeed-MII · GitHub</A>
								</DL><p>
								<DT><H3 FOLDED>Deepspeed-MII-benchmarking</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/issues/204">Issue #204: Benchmarking MII performance</A>
								</DL><p>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII#getting-started-with-mii">microsoft/DeepSpeed-MII: MII makes low-latency and high-throughput inference possible, powered by DeepSpeed.</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/examples/local/text-generation-zero-example.py#L24">DeepSpeed-MII/text-generation-zero-example.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 · microsoft/DeepSpeed-MII</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/9527eb5d39fc93ede11c4761abbc13fe7e644795/mii/models/load_models.py">DeepSpeed-MII/load_models.py at 9527eb5d39fc93ede11c4761abbc13fe7e644795 · microsoft/DeepSpeed-MII · GitHub</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed-MII/blob/main/examples/benchmark/txt2img/baseline-sd.py">DeepSpeed-MII/baseline-sd.py at main · microsoft/DeepSpeed-MII · GitHub</A>
								<DT><A HREF="https://github.com/huggingface/diffusers/blob/v0.7.1/examples/text_to_image/train_text_to_image.py#L603">diffusers/train_text_to_image.py at v0.7.1 · huggingface/diffusers · GitHub</A>
								<DT><A HREF="https://github.com/huggingface/diffusers/blob/d2285f51589bbee18673272611b709d306e7f911/examples/research_projects/intel_opts/textual_inversion_dfq/text2images.py#L68">diffusers/text2images.py at d2285f51589bbee18673272611b709d306e7f911 · huggingface/diffusers</A>
								<DT><A HREF="https://github.com/pytorch/serve">pytorch/serve: Serve, optimize and scale PyTorch models in production</A>
								<DT><A HREF="https://docs.python.org/3/library/inspect.html">inspect — Inspect live objects — Python 3.11.3 documentation</A>
								<DT><A HREF="https://www.deepspeed.ai/2022/09/09/zero-inference.html">ZeRO-Inference: Democratizing massive model inference - DeepSpeed</A>
								<DT><A HREF="https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/welcoming-mistral-phi-jais-code-llama-nvidia-nemotron-and-more/ba-p/3982699#:~:text=Today,%20we%20are%20also%20pleased,API%20endpoint%20to%20their%20applications.">Azure AI: Model as a Service</A>
								<DT><A HREF="https://github.com/Azure/azure-sdk-for-python">Azure/azure-sdk-for-python: This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://docs.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python.</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen">FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference</A>
								<DT><A HREF="https://www.youtube.com/watch?v=fkHYRWMmTmM">[short] DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference - YouTube</A>
								<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen">DeepSpeed/blogs/deepspeed-fastgen at master · microsoft/DeepSpeed</A>
							</DL><p>
							<DT><H3 FOLDED>Text Generation Inference (TGI)</H3>
							<DL><p>
								<DT><H3 FOLDED>tgi-release-notes</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/docs/text-generation-inference/conceptual/chunking">TGI v3 overview</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-lectures</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=6OozhhI6U4g&t=3483s">Open Assistant Inference Backend Development (Hands-On Coding) - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=jlMAX2Oaht0">Text-generation-inference (TGI) deployment optimization and benchmarking - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=jKbbvy-xB4w&t=1894s">HuggingFace: Text Generation Inference: Part 1 - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-client</H3>
								<DL><p>
									<DT><A HREF="https://superfastpython.com/asyncio-gather/">How to Use asyncio.gather() in Python</A>
									<DT><A HREF="https://stackoverflow.com/questions/2785954/creating-a-list-in-python-with-multiple-copies-of-a-given-object-in-a-single-lin">Creating a list in Python with multiple copies of a given object in a single line - Stack Overflow</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-launcher</H3>
								<DL><p>
									<DT><H3 FOLDED>PyTorch checkpointing</H3>
									<DL><p>
										<DT><A HREF="https://mail.google.com/mail/u/0/#inbox?compose=CllgCKCFShQFztGvfWXCdvvFKVPMZgRSNlMlmGFlhSNBPphbqHsjpFVWTxZZBRhfsPrlVtnVXWL">Inbox - antonio.jfdominguez@gmail.com - Gmail</A>
										<DT><A HREF="https://github.com/huggingface/safetensors/blob/96061e97bb7fc4ea6cdd1f79f58701efc4710d22/docs/source/index.mdx#L41">safetensors/docs/source/index.mdx at 96061e97bb7fc4ea6cdd1f79f58701efc4710d22 · huggingface/safetensors</A>
										<DT><A HREF="https://github.com/huggingface/safetensors/blob/96061e97bb7fc4ea6cdd1f79f58701efc4710d22/bindings/python/tests/test_pt_comparison.py#L194">safetensors/bindings/python/tests/test_pt_comparison.py at 96061e97bb7fc4ea6cdd1f79f58701efc4710d22 · huggingface/safetensors</A>
										<DT><A HREF="https://chat.openai.com/c/ac65640c-ae35-4308-bc9e-208baf30a139">Optimizing PyTorch Distributed Launch</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/228b31047858eda15d58a8bc03831f197e4c2120/extra/utils.py#L37">tinygrad/extra/utils.py at 228b31047858eda15d58a8bc03831f197e4c2120 · tinygrad/tinygrad</A>
										<DT><A HREF="https://github.com/facebookresearch/llama-recipes/blob/373000b2ac13f3e52b5df11cec79ed5f2e5b9cbe/src/llama_recipes/inference/model_utils.py#L8">llama-recipes/src/llama_recipes/inference/model_utils.py at 373000b2ac13f3e52b5df11cec79ed5f2e5b9cbe · facebookresearch/llama-recipes</A>
										<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/c8d6a1f34ab6f1b6bd468d256e535a61f98f114c/convert.py#L1016">llama.cpp/convert.py at c8d6a1f34ab6f1b6bd468d256e535a61f98f114c · ggerganov/llama.cpp</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/serialization.py#L866">pytorch/torch/serialization.py at main · pytorch/pytorch</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/96a982ad8fc232479384476b1596a880697cc1d0/Makefile">text-generation-inference/Makefile at 96a982ad8fc232479384476b1596a880697cc1d0 · huggingface/text-generation-inference</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/96a982ad8fc232479384476b1596a880697cc1d0/server/text_generation_server/utils/weights.py">text-generation-inference/server/text_generation_server/utils/weights.py at 96a982ad8fc232479384476b1596a880697cc1d0 · huggingface/text-generation-inference</A>
										<DT><A HREF="https://pytorch.org/docs/stable/elastic/run.html">torchrun (Elastic Launch) — PyTorch 2.1 documentation</A>
										<DT><A HREF="https://huggingface.co/bigscience/bloom-560m/blob/main/config.json">config.json · bigscience/bloom-560m at main</A>
										<DT><A HREF="https://pytorch.org/torchx/latest/cli.html">CLI — PyTorch/TorchX main documentation</A>
										<DT><A HREF="https://github.githistory.xyz/huggingface/text-generation-inference/blob/main/Makefile">Git History - Makefile</A>
										<DT><A HREF="https://github.com/grpc/grpc/tree/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/python/uds">grpc/examples/python/uds at d68161a64f191b8d8d5afe0507e7a2291f91ff1a · grpc/grpc</A>
										<DT><A HREF="https://github.com/facebookexperimental/protoquant">facebookexperimental/protoquant: Prototype routines for GPU quantization written using PyTorch.</A>
										<DT><A HREF="https://github.com/datacrunch-research/text-generation-inference/commit/fbf28546706038db51870bd53684ceb528affe6b">(COMPLETE): minimal working PyTorch checkpointing in launcher · datacrunch-research/text-generation-inference@fbf2854</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>tgi-shardmanager</H3>
								<DL><p>
									<DT><A HREF="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/">Scaling services with Shard Manager - Engineering at Meta</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-router (serving system)</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/tree/main/router">README</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/469">FastTokenizer: heuristics for the scheduler</A>
									<DT><A HREF="https://www.usenix.org/conference/osdi22/presentation/yu">Orca: A Distributed Serving System for Transformer-Based Generative Models | USENIX</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/pull/210">feat(router): Dynamic batch sizing</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/pull/210">Dynamic batch sizing</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/advanced/batch-manager.md">TensorRT-LLM/docs/source/advanced/batch-manager.md</A>
									<DT><A HREF="https://github.com/IBM/text-generation-router">IBM/text-generation-router: Routing proxy for TGIS</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-inference-engine</H3>
								<DL><p>
									<DT><H3 FOLDED>tgi-quantization</H3>
									<DL><p>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/pull/438">Inference support for GPTQ (llama + falcon tested) + Quantization script by Narsil · Pull Request #438 · huggingface/text-generation-inference</A>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/455">SPQR discussion (Meta AI)</A>
									</DL><p>
									<DT><H3 FOLDED>tgi-vlm</H3>
									<DL><p>
										<DT><A HREF="https://github.com/leizhao1234/cogvlm2">leizhao1234/cogvlm2: Add cogVLM v2 to TGI (MAIN)</A>
									</DL><p>
									<DT><H3 FOLDED>tgi-model-loading</H3>
									<DL><p>
										<DT><A HREF="https://github.com/search?q=repo%3Ahuggingface%2Ftext-generation-inference%20language_model&type=code">language_model</A>
										<DT><A HREF="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B-tgi">THUDM/cogvlm2-llama3-chat-19B-tgi · Hugging Face</A>
									</DL><p>
									<DT><H3 FOLDED>tgi-format-model</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B-tgi">THUDM/cogvlm2-llama3-chat-19B-tgi · Hugging Face</A>
										<DT><A HREF="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B/blob/main/model.safetensors.index.json">model.safetensors.index.json · THUDM/cogvlm2-llama3-chat-19B at main</A>
										<DT><A HREF="https://huggingface.co/THUDM/cogvlm2-llama3-chat-19B-tgi/blob/main/pytorch_model.bin.index.json">pytorch_model.bin.index.json · THUDM/cogvlm2-llama3-chat-19B-tgi at main</A>
									</DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/376">Improve inference speed of Santacoder and Starcoder (and others) · Issue #376 · huggingface/text-generation-inference</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-inference-container</H3>
								<DL><p>
									<DT><H3 FOLDED>tgi-dockerfile</H3>
									<DL><p>
										<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/Dockerfile">text-generation-inference/Dockerfile at main · huggingface/text-generation-inference · GitHub</A>
										<DT><A HREF="https://github.com/awslabs/llm-hosting-container/blob/main/docs/huggingface/tgi-0.9.3.md">HuggingFace TGI Container Environemnt Variables</A>
										<DT><A HREF="https://github.com/awslabs/llm-hosting-container/blob/main/huggingface/pytorch/tgi/docker/2.2.0/Dockerfile">llm-hosting-container/huggingface/pytorch/tgi/docker/2.2.0/Dockerfile at main · awslabs/llm-hosting-container</A>
										<DT><A HREF="https://github.com/danieldk/tgi-nix">danieldk/tgi-nix</A>
										<DT><A HREF="https://github.com/datacrunch-research/hosting-container">datacrunch-research/hosting-container: Large Language Model Hosting Container</A>
									</DL><p>
									<DT><A HREF="https://github.com/awslabs/llm-hosting-container">awslabs/llm-hosting-container: Large Language Model Hosting Container</A>
									<DT><A HREF="https://github.com/aws/sagemaker-python-sdk">aws/sagemaker-python-sdk: A library for training and deploying machine learning models on Amazon SageMaker</A>
									<DT><A HREF="https://github.com/aws/deep-learning-containers/blob/master/pytorch/inference/buildspec.yml">deep-learning-containers/buildspec.yml at master · aws/deep-learning-containers · GitHub</A>
									<DT><A HREF="https://github.com/sgl-project/sglang/blob/5d0d40d0eb8c347d8b3598f0a375696728df66c4/scripts/playground/launch_tgi.sh#L4">sglang/scripts/playground/launch_tgi.sh at 5d0d40d0eb8c347d8b3598f0a375696728df66c4 · sgl-project/sglang</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-benchmark</H3>
								<DL><p>
									<DT><A HREF="https://github.com/bigcode-project/bigcode-inference-benchmark/blob/main/scripts/run_grid.sh">bigcode-inference-benchmark: run_grid.sh</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-profiling</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/1014">Profile TGI with nsys launch/start/stop lead to Error: ShardCannotStart · Issue #1014 · huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/1129">multi-gpu</A>
									<DT><A HREF="https://developer.nvidia.com/nsight-systems">Nsight Systems | NVIDIA Developer | NVIDIA Developer</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/863">NCCL nsight-systems</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-fork</H3>
								<DL><p>
									<DT><H3 FOLDED>IBM</H3>
									<DL><p>
										<DT><A HREF="https://github.com/IBM/text-generation-inference/blob/main/integration_tests/text_generation_tests/test_server.py">integration_tests: test_server.py</A>
										<DT><A HREF="https://twitter.com/YiTayML/status/1714315484357857766">The Effiency Misnomer</A>
										<DT><A HREF="https://github.com/foundation-model-stack/foundation-model-stack">foundation-model-stack/foundation-model-stack</A>
										<DT><A HREF="https://github.com/IBM/text-generation-inference">IBM/text-generation-inference: IBM development fork</A>
										<DT><A HREF="https://github.com/caikit/caikit-nlp">caikit/caikit-nlp</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>tgi-embeddings</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/199">[Feature] Return embeddings · Issue #199 · huggingface/text-generation-inference</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-billing</H3>
								<DL><p>
									<DT><A HREF="https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#example-response-2">OpenAI GPT Response: usage</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/pull/578">enh: Adding additional response header X-Total-Tokens by brightsparc · Pull Request #578 · huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/637">Return total tokens generated as an http response header · Issue #637 · huggingface/text-generation-inference</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/435">Request a new api endpoint to check and retrieve token length for given text/prompt · Issue #435 · huggingface/text-generation-inference</A>
								</DL><p>
								<DT><H3 FOLDED>tgi-discussion</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/1819">Planned/Potential of significant work #1819</A>
								</DL><p>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference">huggingface/text-generation-inference</A>
								<DT><A HREF="https://huggingface.co/inference-endpoints">Inference Endpoints - Hugging Face</A>
								<DT><A HREF="https://huggingface.co/docs/transformers/v4.28.1/en/pipeline_webserver">Using pipelines for a webserver</A>
								<DT><A HREF="https://huggingface.co/bigscience/mt0-xxl-mt">bigscience/mt0-xxl-mt · Hugging Face</A>
								<DT><A HREF="https://github.com/google-research/multilingual-t5">google-research/multilingual-t5</A>
								<DT><A HREF="https://huggingface.github.io/text-generation-inference/">Text Generation Inference API</A>
								<DT><A HREF="https://twitter.com/yacineMTB/status/1691208981698498560">HF Transformers Software Complexity</A>
								<DT><A HREF="https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c">Hugging Face Transformer Inference Under 1 Millisecond Latency</A>
							</DL><p>
							<DT><H3 FOLDED>ZhiLight</H3>
							<DL><p>
								<DT><A HREF="https://github.com/zhihu/ZhiLight">zhihu/ZhiLight: A highly optimized LLM inference acceleration engine for Llama and its variants.</A>
							</DL><p>
							<DT><H3 FOLDED>gpt-fast</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/blog/accelerating-generative-ai-2/">Accelerating Generative AI with PyTorch II: GPT, Fast | PyTorch</A>
								<DT><A HREF="https://github.com/pytorch-labs/gpt-fast/blob/main/tp.py">gpt-fast/tp.py at main · pytorch-labs/gpt-fast</A>
								<DT><A HREF="https://github.com/MDK8888/GPTFast">MDK8888/GPTFast: Accelerate your Hugging Face Transformers 7.6-9x. Native to Hugging Face and PyTorch.</A>
							</DL><p>
							<DT><H3 FOLDED>nanoflow</H3>
							<DL><p>
								<DT><A HREF="https://github.com/efeslab/Nanoflow">efeslab/Nanoflow: A throughput-oriented high-performance serving framework for LLMs</A>
								<DT><A HREF="https://x.com/bariskasikci/status/1828167529585672504">(1) Baris Kasikci en X: "We present Nanoflow, an LLM serving framework with close-to-optimal throughput. Nanoflow aggressively overlaps compute, memory, network operations yielding 1.91x more throughput on Splitwise, Lmsys and ShareGPT datasets wrt the best baseline, Tensor RT LLM (~69% of optimal tput) https://t.co/zqYcs9bMay" / X</A>
							</DL><p>
							<DT><H3 FOLDED>fal</H3>
							<DL><p>
								<DT><H3 FOLDED>fal-people</H3>
								<DL><p>
									<DT><A HREF="https://x.com/gorkemyurt">(1) Gorkem Yurtseven (@gorkemyurt) / X</A>
									<DT><A HREF="https://x.com/isidentical">batuhan taskaya (@isidentical) / X</A>
									<DT><A HREF="https://x.com/cloneofsimo">Simo Ryu (@cloneofsimo) / X</A>
									<DT><A HREF="https://x.com/drochetti">Daniel Rochetti (@drochetti) / X</A>
									<DT><A HREF="https://x.com/jfischoff">Jonathan Fischoff (@jfischoff) / X</A>
									<DT><A HREF="https://x.com/chamini2">Matteo Ferrando (@chamini2) / X</A>
									<DT><A HREF="https://x.com/gokayfem">gokaygokay (@gokayfem) / X</A>
									<DT><A HREF="https://x.com/aykutkardas">Aykut (@aykutkardas) / X</A>
									<DT><A HREF="https://x.com/rkuprieiev">Ruslan Kuprieiev 🇺🇦 (@rkuprieiev) / X</A>
									<DT><A HREF="https://x.com/fp8e4m3fn">Vedat Baday (@fp8e4m3fn) / X</A>
									<DT><A HREF="https://x.com/_yatharthg">Yatharth Gupta (@_yatharthg) / X</A>
									<DT><A HREF="https://x.com/Gothos03">(1) Vishnu Jaddipal (@Gothos03) / X</A>
									<DT><A HREF="https://x.com/chengzeyi">Cheng (@chengzeyi) -&gt;  stable-fast.</A>
								</DL><p>
								<DT><A HREF="https://github.com/fal-ai/fal">fal-ai/fal: ⚡ Fastest way to serve open source ML models to millions</A>
							</DL><p>
							<DT><H3 FOLDED>awslabs-LISA</H3>
							<DL><p>
								<DT><H3 FOLDED>AWS DJL</H3>
								<DL><p>
									<DT><A HREF="https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-fastertransformer.html">Large model inference with FasterTransformer and DJL Serving - Amazon SageMaker</A>
									<DT><A HREF="https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/llm-workshop/lab5-flan-t5-xxl/flan-xxl-sagemaker-fastertransformer-s5cmd.ipynb">amazon-sagemaker-examples/inference/generativeai/llm-workshop/lab5-flan-t5-xxl/flan-xxl-sagemaker-fastertransformer-s5cmd.ipynb at main · aws/amazon-sagemaker-examples</A>
									<DT><A HREF="https://github.com/deepjavalibrary/djl-serving">deepjavalibrary/djl-serving: A universal scalable machine learning model deployment solution</A>
									<DT><A HREF="https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/optimizations/aitemplate/lmi-aitemplate-stablediff.ipynb">amazon-sagemaker-examples/inference/generativeai/optimizations/aitemplate/lmi-aitemplate-stablediff.ipynb at main · aws/amazon-sagemaker-examples</A>
								</DL><p>
								<DT><A HREF="https://github.com/awslabs/LISA">awslabs/LISA: LLM inference solution for Amazon Dedicated Cloud (LISA).</A>
							</DL><p>
							<DT><H3 FOLDED>serving-coreweave</H3>
							<DL><p>
								<DT><A HREF="https://docs.coreweave.com/coreweave-machine-learning-and-ai/inference">Inference | CoreWeave</A>
								<DT><A HREF="https://docs.coreweave.com/">CoreWeave Cloud - CoreWeave</A>
								<DT><A HREF="https://github.com/kserve/kserve">kserve/kserve: Standardized Serverless ML Inference Platform on Kubernetes</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference">huggingface/text-generation-inference: Large Language Model Text Generation Inference</A>
								<DT><A HREF="https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer">https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer</A>
							</DL><p>
							<DT><H3 FOLDED>serving-modal</H3>
							<DL><p>
								<DT><A HREF="https://modal.com/docs/examples/trtllm_llama">Serverless TensorRT-LLM (LLaMA 3 8B) | Modal Docs</A>
								<DT><A HREF="https://github.com/modal-labs">Modal Labs</A>
								<DT><A HREF="https://github.com/modal-labs/serverless-queuing-theory-model">modal-labs/serverless-queuing-theory-model: Implementation of a serverless queuing theory model to explore optimal policies and tradeoffs between latency &amp; utilization</A>
								<DT><A HREF="https://github.com/modal-labs/mountpoint-s3">modal-labs/mountpoint-s3: A simple, high-throughput file client for mounting an Amazon S3 bucket as a local file system.</A>
								<DT><A HREF="https://github.com/modal-labs/modal-examples/blob/main/07_web_endpoints/basic_web.py">modal-examples/07_web_endpoints/basic_web.py at main · modal-labs/modal-examples</A>
							</DL><p>
							<DT><H3 FOLDED>serving-checkpointing</H3>
							<DL><p>
								<DT><H3 FOLDED>nm-compressed-tensors</H3>
								<DL><p>
									<DT><A HREF="https://github.com/neuralmagic/compressed-tensors">neuralmagic/compressed-tensors: A safetensors extension to efficiently store sparse quantized tensors on disk</A>
								</DL><p>
								<DT><H3 FOLDED>safetensors</H3>
								<DL><p>
									<DT><H3 FOLDED>model-streamer</H3>
									<DL><p>
										<DT><A HREF="https://github.com/run-ai/runai-model-streamer">run-ai/runai-model-streamer</A>
										<DT><A HREF="https://www.run.ai/blog/accelerating-model-loading-with-run-ai-model-streamer">From Storage to GPU—but Faster: Accelerating Model Loading with Run:ai Model Streamer</A>
										<DT><A HREF="https://pages.run.ai/hubfs/PDFs/White%20Papers/Model-Streamer-Performance-Benchmarks.pdf">Ruan:ai research model streamer performance benchmarks</A>
										<DT><A HREF="https://github.com/vllm-project/vllm/pull/9941">[Core]Add New Run:ai Streamer Load format. by pandyamarut · Pull Request #9941 · vllm-project/vllm</A>
									</DL><p>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/200">equivalent of torch.load from io stream to gpu</A>
									<DT><A HREF="https://github.com/vladmandic/sd-loader/blob/main/bench.py">sd-loader/bench.py at main · vladmandic/sd-loader</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/200">using fp16 or fp32 (load times)</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/18">Some clarification about PyTorch format</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/models/flash_llama.py#L216-L311">(TGI): Safetensors lazy_loading (flash_llama.py)</A>
									<DT><A HREF="https://huggingface.co/docs/safetensors/speed#gpu-benchmark">Speed Comparison</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/67">Support for model streaming (disk -&gt; VRAM)? · Issue #67</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/pull/189/files">Adding memmap example to read from file</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/242">tensorstore</A>
								</DL><p>
								<DT><H3 FOLDED>torch-checkpointing</H3>
								<DL><p>
									<DT><H3 FOLDED>torch-checkpointing-distributed</H3>
									<DL><p>
										<DT><A HREF="https://pytorch.org/blog/performant-distributed-checkpointing/?utm_content=258825622&utm_medium=social&utm_source=linkedin&hss_channel=lcp-78618366">PyTorch Distributed checkpointing in Production</A>
										<DT><A HREF="https://www.youtube.com/watch?v=ldBmHNva_Fw&t=31s">Distributed Checkpoint - Iris Zhang &amp; Chien-Chin Huang, Meta - YouTube</A>
										<DT><A HREF="https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html">Getting Started with Distributed Checkpoint (DCP)</A>
									</DL><p>
									<DT><H3 FOLDED>skipping initialization of module parameters</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/29523">Option to skip random weight initialization at module instance creation</A>
										<DT><A HREF="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to_empty">meta device + added a to_empty function 2 from the meta device</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/nn/utils/init.py#L5">nn/utils/init.py at main Added device/dtype kwargs to all nn.Modules</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/96161">[torchdistx] Future of the large model initialization META device</A>
										<DT><A HREF="https://pytorch.org/torchdistx/latest/fake_tensor.html">Fake Tensor — torchdistX 0.2.0 documentation</A>
										<DT><H3 FOLDED>problems: not solve 2x model parameters being in memory</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/90465">[FSDP] Revisit meta device initialization · Issue #90465 · pytorch/pytorch</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>loading parameters</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/module.py#L1941">pytorch/torch/nn/modules/module.py at main · pytorch/pytorch</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/103405">(PROFILING) load model on infer so much memory on GPU and CPU · Issue #103405</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/64601">Reuse tensors from state_dict in load_state_dict · Issue #64601</A>
										<DT><A HREF="https://github.com/stas00/transformers/blob/main/src/transformers/modeling_utils.py#L2968">transformers/src/transformers/modeling_utils.py (reuse_tensor)</A>
										<DT><H3 FOLDED>state_dict</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/97196">Sequential/Partial unpickling and loading of models · Issue #97196</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/75242">(stass) torch.load: that loads one submodule at a time</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/issues/64327">RFC: multi-part `torch.load`/`torch.save` to support huge models and/or low CPU memory</A>
										</DL><p>
										<DT><H3 FOLDED>spliting</H3>
										<DL><p>
											<DT><A HREF="https://github.com/huggingface/transformers/issues/13548">RFC: split checkpoint load/save for huge models · Issue #13548 · huggingface/transformers</A>
											<DT><A HREF="https://github.com/finetunej/transformers/blob/ca5d90ac1965982db122a649c2c9c902bde74a03/src/transformers/modeling_utils.py#L417-L443">transformers/src/transformers/modeling_utils.py</A>
											<DT><A HREF="https://github.com/huggingface/transformers/blob/c030fc891395d11249046e36b9e0219685b33399/src/transformers/modeling_utils.py#L1045">floating_point_ops</A>
										</DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/issues/87033">Saving and loading from physical storage · Issue #87033</A>
										<DT><H3 FOLDED>mmap</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/619ae87a1d1ae086f59a64d3b71dbfe4af8b804a/torch/serialization.py#L1010">serialization.py: mmap</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/619ae87a1d1ae086f59a64d3b71dbfe4af8b804a/test/test_serialization.py">test_serialization_mmap_loading</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/pull/101446">[WIP] Let torch.load load memory-mapped tensors</A>
											<DT><A HREF="https://github.com/nadavrot/memset_benchmark?tab=readme-ov-file">nadavrot/memset_benchmark: This repository contains high-performance implementations of memset and memcpy in assembly.</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>ByteCheckpoint</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ByteDance-Seed/ByteCheckpoint">ByteDance-Seed/ByteCheckpoint: ByteCheckpoint: An Unified Checkpointing Library for LFMs</A>
										<DT><A HREF="https://arxiv.org/abs/2407.20143">[2407.20143] ByteCheckpoint: A Unified Checkpointing System for Large Foundation Model Development</A>
									</DL><p>
									<DT><A HREF="https://dev-discuss.pytorch.org/t/state-of-model-creation-initialization-seralization-in-pytorch-core/1240">State of model creation/initialization/seralization in PyTorch Core</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md">TorchScript serialization</A>
									<DT><A HREF="https://pytorch-dev-podcast.simplecast.com/episodes/serialization">Serialization | PyTorch Developer Podcast</A>
									<DT><A HREF="https://github.com/msaroufim/cpuoffload/tree/main">msaroufim/cpuoffload</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/compare/main...mikaylagawarecki:pytorch:gds#diff-6c848cacbf0ee6cb3ace89d0e3a8b407b61f31543e5bf4ba70f149bc4b6585c7">Comparing pytorch:main...mikaylagawarecki:gds · pytorch/pytorch: Torch load/save gds</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/db53e9d5ae262d15d8c8cfb9e5c67d620f66f54c/generate_new_checkpoint.py">pytorch/generate_new_checkpoint.py at db53e9d5ae262d15d8c8cfb9e5c67d620f66f54c · pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>serving-checkpointing-DeepSpeed</H3>
								<DL><p>
									<DT><A HREF="https://github.com/microsoft/DeepSpeed/issues/2379">DeepSpeed: Pre-sharding</A>
								</DL><p>
								<DT><H3 FOLDED>transmogrifier</H3>
								<DL><p>
									<DT><A HREF="https://mail.google.com/mail/u/0/#inbox?compose=CllgCKCFShQFztGvfWXCdvvFKVPMZgRSNlMlmGFlhSNBPphbqHsjpFVWTxZZBRhfsPrlVtnVXWL">Inbox - antonio.jfdominguez@gmail.com - Gmail</A>
									<DT><A HREF="https://github.com/huggingface/safetensors/blob/main/bindings/python/convert.py#L158">safetensors/bindings/python/convert.py at main · huggingface/safetensors</A>
									<DT><A HREF="https://github.com/huggingface/huggingface_hub/blob/ef48c7f7311c0717db98f1b555c8e61d76fc1870/src/huggingface_hub/hf_api.py#L4225">https://github.com/huggingface/huggingface_hub/blob/ef48c7f7311c0717db98f1b555c8e61d76fc1870/src/huggingface_hub/hf_api.py#L4225</A>
									<DT><A HREF="https://chat.openai.com/c/3e8059da-4190-4d44-996e-38c5f08b0fb6">Argparse Args Programmatically</A>
									<DT><A HREF="https://github.com/datacrunch-research/transmogrifier/blob/main/transmogrifier/convert.py">transmogrifier/transmogrifier/convert.py at main · datacrunch-research/transmogrifier</A>
									<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/228b31047858eda15d58a8bc03831f197e4c2120/extra/utils.py#L37">https://github.com/tinygrad/tinygrad/blob/228b31047858eda15d58a8bc03831f197e4c2120/extra/utils.py#L37</A>
									<DT><A HREF="https://github.com/openai/triton/blob/96b04493f11a72bf4f2fcc746259ce84b10cc730/python/triton/testing.py#L160">https://github.com/openai/triton/blob/96b04493f11a72bf4f2fcc746259ce84b10cc730/python/triton/testing.py#L160</A>
									<DT><A HREF="https://github.com/facebookresearch/llama-recipes/blob/373000b2ac13f3e52b5df11cec79ed5f2e5b9cbe/src/llama_recipes/inference/model_utils.py#L8">llama-recipes/src/llama_recipes/inference/model_utils.py at 373000b2ac13f3e52b5df11cec79ed5f2e5b9cbe · facebookresearch/llama-recipes</A>
									<DT><A HREF="https://www.notion.so/datacrunchio/model-transmogrifier-27445d5b11134f7d9745039fd66dbda8">https://www.notion.so/datacrunchio/model-transmogrifier-27445d5b11134f7d9745039fd66dbda8</A>
									<DT><A HREF="https://github.com/huggingface/hf_transfer">huggingface/hf_transfer</A>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/blob/c8d6a1f34ab6f1b6bd468d256e535a61f98f114c/convert.py#L1016">llama.cpp/convert.py at c8d6a1f34ab6f1b6bd468d256e535a61f98f114c · ggerganov/llama.cpp</A>
									<DT><A HREF="https://github.com/ggerganov/llama.cpp/tree/34b2a5e1ee4fe6295fb4420eb91131d743694c65">ggerganov/llama.cpp at 34b2a5e1ee4fe6295fb4420eb91131d743694c65</A>
									<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/utils/gptq/exllama.py#L40">text-generation-inference/server/text_generation_server/utils/gptq/exllama.py at main · huggingface/text-generation-inference</A>
									<DT><A HREF="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/tree/main">stabilityai/stable-diffusion-xl-base-1.0 at main</A>
									<DT><A HREF="https://huggingface.co/meta-llama/Llama-2-13b-hf/tree/main">meta-llama/Llama-2-13b-hf at main</A>
									<DT><A HREF="https://github.com/huggingface/huggingface_hub/blob/ef48c7f7311c0717db98f1b555c8e61d76fc1870/src/huggingface_hub/hf_api.py#L2956">huggingface_hub/src/huggingface_hub/hf_api.py at ef48c7f7311c0717db98f1b555c8e61d76fc1870 · huggingface/huggingface_hub</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/torch/serialization.py#L866">pytorch/torch/serialization.py at main · pytorch/pytorch</A>
									<DT><A HREF="https://github.com/allenai/cached_path">allenai/cached_path: A file utility for accessing both local and remote files through a unified interface.</A>
								</DL><p>
								<DT><H3 FOLDED>buffers</H3>
								<DL><p>
									<DT><A HREF="https://www.tutorialspoint.com/How-an-entire-file-is-read-into-buffer-and-returned-as-a-string-in-Python">How an entire file is read into buffer and returned as a string in Python?</A>
									<DT><A HREF="https://stackoverflow.com/questions/59026110/python-read-data-from-buffer">Python read data from Buffer - Stack Overflow</A>
									<DT><A HREF="https://docs.python.org/3/library/os.html">os.read</A>
									<DT><A HREF="https://docs.python.org/3/library/io.html">io: readinto</A>
									<DT><A HREF="https://www.youtube.com/watch?v=THWDx_RyZ6A">Episode 011: Let's Go to the Disk! - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>checkpointing-gds</H3>
								<DL><p>
									<DT><H3 FOLDED>cuFile</H3>
									<DL><p>
										<DT><H3 FOLDED>kvikio</H3>
										<DL><p>
											<DT><A HREF="https://github.com/rapidsai/kvikio/blob/branch-24.04/python/tests/test_defaults.py">kvikio/python/tests/test_defaults.py (compat_mode)</A>
											<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/topics/cufile-compatibility.html">cuFile Compatibility Mode - NVIDIA Docs</A>
											<DT><A HREF="https://docs.rapids.ai/api/libkvikio/nightly/">Compatibility Mode (KVIKIO_COMPAT_MODE)</A>
										</DL><p>
										<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html">cuFile API Reference Guide - NVIDIA Docs</A>
										<DT><A HREF="https://github.com/rapidsai/kvikio">rapidsai/kvikio</A>
										<DT><A HREF="https://github.com/rapidsai/kvikio/pull/135">Overload `numpy.fromfile()` and `cupy.fromfile()` by madsbk · Pull Request #135 · rapidsai/kvikio</A>
										<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py at main · alpa-projects/alpa</A>
										<DT><A HREF="https://github.com/pfnet/pytorch-pfn-extras/blob/f6b127063ec910b71788db2ae6ef96a3d89832b1/tests/pytorch_pfn_extras_tests/cuda_tests/test_allocator.py">pytorch-pfn-extras/tests/pytorch_pfn_extras_tests/cuda_tests/test_allocator.py at f6b127063ec910b71788db2ae6ef96a3d89832b1 · pfnet/pytorch-pfn-extras</A>
										<DT><A HREF="https://pytorch-pfn-extras.readthedocs.io/en/latest/user_guide/cuda.html">CUDA (CuPy Interoperability) — pytorch-pfn-extras documentation</A>
										<DT><A HREF="https://github.com/NVIDIA/apex/blob/810ffae374a2b9cb4b5c5e28eaeca7d7998fca0c/apex/contrib/csrc/gpu_direct_storage/gds.cpp">apex/apex/contrib/csrc/gpu_direct_storage/gds.cpp</A>
										<DT><A HREF="https://github.com/NVIDIA/MagnumIO/blob/main/gds/samples/cufile_sample_022.cc">cuFile Batch APIs</A>
										<DT><A HREF="https://chat.openai.com/c/61bb588b-35a7-42a6-90e9-c2355e5646a9">Identify Disk Storage Type</A>
									</DL><p>
									<DT><A HREF="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">NVIDIA GPUDirect Storage Overview Guide - NVIDIA Docs</A>
									<DT><A HREF="https://ieeexplore.ieee.org/document/7973709">Offloading Communication Control Logic in GPU</A>
									<DT><A HREF="https://github.com/Mellanox/gpu_direct_rdma_access">Mellanox/gpu_direct_rdma_access: example code for using DC QP for providing RDMA READ and WRITE operations to remote GPU memory</A>
									<DT><A HREF="https://github.com/NVIDIA/gdrcopy">NVIDIA/gdrcopy: A fast GPU memory copy library based on NVIDIA GPUDirect RDMA technology</A>
									<DT><A HREF="https://github.com/NVIDIA/gds-nvidia-fs">NVIDIA/gds-nvidia-fs: NVIDIA GPUDirect Storage Driver</A>
									<DT><A HREF="https://github.com/lw?tab=stars">lw (Luca Wehrstedt)</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">alpa/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py</A>
									<DT><A HREF="https://github.com/pytorch/pytorch/compare/main...mikaylagawarecki:pytorch:gds">Comparing pytorch:main...mikaylagawarecki:gds · pytorch/pytorch</A>
								</DL><p>
								<DT><H3 FOLDED>serving-checkpointing-serialization</H3>
								<DL><p>
									<DT><H3 FOLDED>json</H3>
									<DL><p>
										<DT><A HREF="https://github.com/ijl/orjson">ijl/orjson: Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy</A>
										<DT><A HREF="https://github.com/bytedance/sonic">bytedance/sonic: A blazingly fast JSON serializing &amp; deserializing library</A>
									</DL><p>
									<DT><A HREF="https://github.com/huggingface/safetensors/issues/358">checkpoint quantized weights (bitsandbyres.nf4, SpQR etc)</A>
									<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/checkpoints">ml-engineering/checkpoint</A>
									<DT><A HREF="https://github.com/alpa-projects/alpa/blob/824f2ffd5124d24935811bc738ed903796ab13ac/alpa/serialization.py#L54">(JAX) Alpa: serialization.py</A>
								</DL><p>
								<DT><H3 FOLDED>checkpointing-tinygrad</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=2QO3vzwHXhg&t=3551s">converting to float16 slowing down</A>
								</DL><p>
								<DT><A HREF="https://github.com/xai-org/grok-1/blob/main/checkpoint.py">grok-1/checkpoint.py at main</A>
								<DT><A HREF="https://scale.com/blog/reduce-cold-start-time-llm-inference">How To Reduce Cold Start Times For LLM Inference (Scale AI)</A>
								<DT><A HREF="https://github.com/facebookresearch/HolisticTraceAnalysis">facebookresearch/HolisticTraceAnalysis</A>
								<DT><A HREF="https://github.com/huggingface/hf_transfer">huggingface/hf_transfer</A>
								<DT><A HREF="https://docs.ffcv.io/bottleneck_doctor.html">The Bottleneck Doctor — FFCV documentation</A>
								<DT><A HREF="https://github.com/coreweave/tensorizer">coreweave/tensorizer: Module, Model, and Tensor Serialization/Deserialization</A>
								<DT><A HREF="https://github.com/allenai/cached_path">allenai/cached_path: A file utility for accessing both local and remote files through a unified interface.</A>
								<DT><A HREF="https://openai.com/index/scaling-kubernetes-to-7500-nodes/">Scaling Kubernetes to 7,500 nodes | OpenAI -&gt; CPU &amp; GPU balloons</A>
								<DT><A HREF="https://www.usenix.org/conference/osdi20/presentation/gujarati">Serving DNNs like Clockwork: Performance Predictability from the Bottom Up | USENIX</A>
								<DT><A HREF="https://www.outerport.com/blog/fast-flux-load">How fast can you load a FLUX (LoRA) model?</A>
							</DL><p>
							<DT><H3 FOLDED>serving-chat</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/chat/">HuggingChat</A>
								<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/1082">Support for HF Chat templates? · Issue #1082 · huggingface/text-generation-inference</A>
								<DT><A HREF="https://huggingface.co/docs/transformers/main/chat_templating">Templates for Chat Models</A>
							</DL><p>
							<DT><H3 FOLDED>serving-miscellaneous</H3>
							<DL><p>
								<DT><A HREF="https://github.com/cloudflare/quiche">cloudflare/quiche: 🥧 Savoury implementation of the QUIC transport protocol and HTTP/3</A>
								<DT><A HREF="https://cloudflare-quic.com/">QUIC | Cloudflare</A>
								<DT><A HREF="https://www.youtube.com/watch?v=FqljO9B5grM">Impeccable API Design: What you MUST CONSIDER before deploying APIs to production</A>
								<DT><A HREF="https://github.com/w3c/webtransport/blob/main/explainer.md">webtransport/explainer.md at main · w3c/webtransport</A>
								<DT><A HREF="https://github.com/Motsepe-Jr/AI-research-papers-pseudo-code/blob/main/Distributed%20Inference%20Papers/Fast_Distributed_Inference_Serving_for_Large_Language_Models.ipynb">AI-research-papers-pseudo-code/Distributed Inference Papers/Fast_Distributed_Inference_Serving_for_Large_Language_Models.ipynb at main · Motsepe-Jr/AI-research-papers-pseudo-code · GitHub</A>
								<DT><A HREF="https://www.flickr.com/photos/neurollero/sets/366106/with/51970885/">neuro | Flickr</A>
								<DT><A HREF="https://jina.ai/news/inference-how-can-jina-ai-offer-the-best-in-class-model-as-a-service-so-affordably/">Jina: The Science of Model Deployment</A>
								<DT><A HREF="https://docs.cerebras.net/en/latest/wsc/getting-started/csctl.html#csctl">csctl: CLI tool for job monitoring — Cerebras Developer Documentation</A>
								<DT><A HREF="https://github.com/alibaba/hiactor">alibaba/hiactor: Hiactor is a distributed C++ actor framework.</A>
								<DT><A HREF="https://github.com/hpcaitech/EnergonAI">hpcaitech/EnergonAI: Large-scale model inference.</A>
								<DT><A HREF="https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/EVAL_GAUNTLET.md">llm-foundry/scripts/eval/local_data/EVAL_GAUNTLET.md at main · mosaicml/llm-foundry</A>
							</DL><p>
							<DT><H3 FOLDED>serving-hosting-container</H3>
							<DL><p>
								<DT><A HREF="https://github.com/awslabs/llm-hosting-container">awslabs/llm-hosting-container: Large Language Model Hosting Container</A>
							</DL><p>
							<DT><H3 FOLDED>serving-diffusion</H3>
							<DL><p>
								<DT><H3 FOLDED>alibaba-diffusers-api</H3>
								<DL><p>
									<DT><A HREF="https://github.com/alibaba/diffusers-api">alibaba/diffusers-api</A>
									<DT><A HREF="https://github.com/alibaba/diffusers-api/blob/main/diffusers/app.py#L28">diffusers-api/diffusers/app.py at main · alibaba/diffusers-api</A>
									<DT><A HREF="https://github.com/alibaba/diffusers-api/blob/main/diffusers/utils/image_process.py#L156">diffusers-api/diffusers/utils/image_process.py: computer vision algorithms</A>
								</DL><p>
								<DT><H3 FOLDED>DeepCache</H3>
								<DL><p>
									<DT><H3 FOLDED>tmp</H3>
									<DL><p>
										<DT><A HREF="https://medium.com/swlh/writing-c-when-youre-a-java-developer-memory-management-7c42e222645e">Writing C++ When You’re a Java Developer: Memory Management | by Alexandre Lombard | The Startup | Medium</A>
										<DT><A HREF="https://mypy.readthedocs.io/en/stable/protocols.html">Protocols and structural subtyping - mypy 1.9.0 documentation</A>
										<DT><A HREF="https://colab.research.google.com/github/google/seqio/blob/main/seqio/notebooks/Basics_Task_and_Mixtures.ipynb#scrollTo=QTyxusscJgwO">[seqio basics] Task and Mixtures.ipynb - Colab</A>
										<DT><A HREF="https://github.com/google-research-datasets/natural-questions">google-research-datasets/natural-questions: Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems.</A>
										<DT><A HREF="https://twitter.com/i/bookmarks?post_id=1778234586599727285">https://twitter.com/i/bookmarks?post_id=1778234586599727285</A>
										<DT><A HREF="https://arxiv.org/abs/2404.07143">[2404.07143] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</A>
										<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8">LLM Inference Series: 4. KV caching, a deeper look | by Pierre Lienhart | Medium</A>
										<DT><A HREF="https://github.com/openai/triton/blob/main/python/triton/testing.py">triton/python/triton/testing.py at main · openai/triton</A>
										<DT><A HREF="https://github.com/horseee/DeepCache/blob/master/stable_diffusion.py#L29">DeepCache/stable_diffusion.py at master · horseee/DeepCache</A>
										<DT><A HREF="https://huggingface.co/docs/diffusers/main/en/optimization/deepcache">DeepCache</A>
										<DT><A HREF="https://arxiv.org/pdf/2312.00858.pdf">https://arxiv.org/pdf/2312.00858.pdf</A>
									</DL><p>
									<DT><A HREF="https://github.com/horseee/DeepCache">horseee/DeepCache: [CVPR 2024] DeepCache: Accelerating Diffusion Models for Free</A>
									<DT><A HREF="https://huggingface.co/docs/diffusers/main/en/optimization/deepcache">DeepCache</A>
									<DT><A HREF="https://github.com/dbolya/tomesd">dbolya/tomesd: Speed up Stable Diffusion with this one simple trick!</A>
									<DT><A HREF="https://github.com/chengzeyi/stable-fast/issues/110">Support for DeepCache · Issue #110 · chengzeyi/stable-fast</A>
									<DT><A HREF="https://developer.nvidia.com/blog/nvidia-h200-tensor-core-gpus-and-nvidia-tensorrt-llm-set-mlperf-llm-inference-records/">NVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM Inference Records | NVIDIA Technical Blog</A>
									<DT><A HREF="https://blogs.nvidia.com/blog/tensorrt-llm-inference-mlperf/">NVIDIA Hopper Leaps Ahead in Generative AI at MLPerf | NVIDIA Blog</A>
									<DT><A HREF="https://arxiv.org/abs/2312.00858">[2312.00858] DeepCache: Accelerating Diffusion Models for Free</A>
									<DT><A HREF="https://developer.nvidia.com/blog/nvidia-h200-tensor-core-gpus-and-nvidia-tensorrt-llm-set-mlperf-llm-inference-records/">NVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM</A>
									<DT><A HREF="https://blogs.nvidia.com/blog/tensorrt-llm-inference-mlperf/">NVIDIA Hopper Leaps Ahead in Generative AI at MLPerf</A>
								</DL><p>
								<DT><H3 FOLDED>diffusers</H3>
								<DL><p>
									<DT><H3 FOLDED>diffusers-imports</H3>
									<DL><p>
										<DT><A HREF="https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py">optuna/optuna/integration/__init__.py at master</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/42cae93b942ec904ead46c26c42be24422adc92c/src/diffusers/utils/import_utils.py#L760">diffusers/src/diffusers/utils/import_utils.py</A>
										<DT><A HREF="https://github.com/huggingface/diffusers/blob/67bef2027cc461af5bbe73b3c0f35bb1350f5aa8/src/diffusers/pipelines/consistency_models/__init__.py">diffusers/src/diffusers/pipelines/consistency_models/__init__.py</A>
									</DL><p>
									<DT><A HREF="https://github.com/huggingface/diffusers/blob/fe5f035f797a5fa663a98030c9d0ec2f982cd09d/docs/source/en/using-diffusers/custom_pipeline_overview.md">diffusers/docs/source/en/using-diffusers/custom_pipeline_overview.md</A>
								</DL><p>
								<DT><H3 FOLDED>image-process</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/controlnet_aux">huggingface/controlnet_aux</A>
								</DL><p>
								<DT><H3 FOLDED>diffusion-generation-server</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/diffusers/tree/main/examples/server">diffusers/examples/server at main · huggingface/diffusers</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=JgP2WgNIq_w">How to Deploy HuggingFace’s Stable Diffusion Pipeline with Triton Inference Server</A>
								<DT><A HREF="https://developer.nvidia.com/blog/nvidia-h200-tensor-core-gpus-and-nvidia-tensorrt-llm-set-mlperf-llm-inference-records/">NVIDIA H200 Tensor Core GPUs and NVIDIA TensorRT-LLM Set MLPerf LLM</A>
								<DT><A HREF="https://mlcommons.org/benchmarks/inference-datacenter/">Benchmark MLPerf Inference: Datacenter | MLCommons V3.1</A>
								<DT><A HREF="https://huggingface.co/ByteDance/SDXL-Lightning">ByteDance/SDXL-Lightning · Hugging Face</A>
								<DT><A HREF="https://github.com/huggingface/diffusers/blob/fe5f035f797a5fa663a98030c9d0ec2f982cd09d/docs/source/en/using-diffusers/custom_pipeline_overview.md">diffusers/docs/source/en/using-diffusers/custom_pipeline_overview.md</A>
								<DT><A HREF="https://www.baseten.co/blog/40-faster-stable-diffusion-xl-inference-with-nvidia-tensorrt/">diffusion-benchmarking</A>
							</DL><p>
							<DT><H3 FOLDED>serving-cache</H3>
							<DL><p>
								<DT><A HREF="https://x.com/i/bookmarks?post_id=1819358570766643223">DeepSeek: context caching on disk</A>
								<DT><A HREF="https://www.eecs.harvard.edu/~michaelm/postscripts/esa2008full.pdf">More Robust Hashing: Cuckoo Hashing with a Stash∗</A>
								<DT><A HREF="https://www.youtube.com/watch?app=desktop&v=VTqVnhL1ClU&list=PL9eL-xg48OM3pnVqFSRyBFleHtBBw-nmZ&index=37&pp=iAQB">Episode 036: A Cache That Always Hits - YouTube</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2404.16283">transmogrifier</A>
							<DT><A HREF="https://www.usenix.org/conference/osdi22/presentation/yu">Orca: A Distributed Serving System for Transformer-Based Generative Models | USENIX</A>
							<DT><A HREF="https://www.run.ai/">Run:ai - AI Optimization and Orchestration</A>
							<DT><A HREF="https://x.com/haoailab/status/1805307696297689119">(1) Hao AI Lab en X: "Multiple LLM serving has emerged as a crucial and costly demand. Want to co-serve multiple LLMs with better utilization? Introducing MuxServe - flexible spatial-temporal multiplexing - up to 1.8x higher throughput Blog: https://t.co/Pep94vUFTw Paper: https://t.co/X1Jhov3QOY https://t.co/mXrHMSLPS1" / X</A>
							<DT><A HREF="https://github.com/Lightning-AI/LitServe">Lightning-AI/LitServe: Deploy AI models at scale. High-throughput serving engine for AI/ML models that uses the latest state-of-the-art model deployment techniques.</A>
							<DT><A HREF="https://github.com/microsoft/ParrotServe">microsoft/ParrotServe: [OSDI'24] Serving LLM-based Applications Efficiently with Semantic Variable</A>
							<DT><A HREF="https://docs.databricks.com/en/machine-learning/model-serving/index.html">Model serving with Databricks | Databricks on AWS</A>
							<DT><A HREF="https://github.com/kvcache-ai/Mooncake">kvcache-ai/Mooncake: Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI.</A>
							<DT><A HREF="https://github.com/replicate/cog">replicate/cog: Containers for machine learning</A>
							<DT><A HREF="https://github.com/AI-Hypercomputer/JetStream">AI-Hypercomputer/JetStream: JetStream is a throughput and memory optimized engine for LLM inference on XLA devices, starting with TPUs (and GPUs in future -- PRs welcome).</A>
							<DT><A HREF="https://github.com/xjdr-alt/entropix/tree/server">xjdr-alt/entropix at server</A>
							<DT><A HREF="https://www.run.ai/blog/serving-large-language-models">What it means to serve an LLM and which serving technology to choose from</A>
							<DT><A HREF="https://github.com/microsoft/sarathi-serve">microsoft/sarathi-serve: A low-latency &amp; high-throughput serving engine for LLMs</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-scheduler</H3>
						<DL><p>
							<DT><H3 FOLDED>TorchX</H3>
							<DL><p>
								<DT><A HREF="https://pytorch.org/torchx/main/basics.html">Basic Concepts — PyTorch/TorchX main documentation</A>
								<DT><A HREF="https://airflow.apache.org/">Apache Airflow</A>
								<DT><A HREF="https://pytorch.org/torchx/main/quickstart.html">Quickstart — PyTorch/TorchX main documentation</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Y7T01L0a4G4">Automating PyTorch Using TorchX to Make Data Centric ML Workflows</A>
							</DL><p>
							<DT><A HREF="https://docs.cerebras.net/en/latest/wsc/getting-started/csctl.html#csctl">csctl: CLI tool for job monitoring — Cerebras Developer Documentation</A>
							<DT><A HREF="https://slurm.schedmd.com/SC22/Slurm-and-or-vs-Kubernetes.pdf">Slurm and kubernetes</A>
							<DT><A HREF="https://slurm.schedmd.com/">slurm.schedmd.com</A>
							<DT><A HREF="https://github.com/GoogleCloudPlatform/hpc-toolkit">GoogleCloudPlatform/hpc-toolkit: Cloud HPC Toolkit is an open-source software offered by Google Cloud which makes it easy for customers to deploy HPC environments on Google Cloud.</A>
							<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-model-parallelism</H3>
						<DL><p>
							<DT><A HREF="https://github.com/bytedance/flux">bytedance/flux: A fast communication-overlapping library for tensor parallelism on GPUs.</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-notebooks</H3>
						<DL><p>
							<DT><A HREF="https://github.com/bigcode-project/Megatron-LM/blob/raymond-notebooks/notebooks/transformer_parameter_count.ipynb">Megatron-LM/transformer_parameter_count.ipynb</A>
							<DT><A HREF="https://www.philschmid.de/gptj-deepspeed-inference#3-optimize-gpt-j-for-gpu-using-deepspeeds-inferenceengine">Accelerate GPT-J inference with DeepSpeed-Inference on GPUs</A>
							<DT><A HREF="https://www.philschmid.de/static-quantization-optimum">Static Quantization with Hugging Face `optimum` for ~3x latency improvements</A>
							<DT><A HREF="https://www.philschmid.de/fine-tune-flan-t5-deepspeed">Fine-tune FLAN-T5 XL/XXL using DeepSpeed &amp; Hugging Face Transformers</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-benchmark</H3>
						<DL><p>
							<DT><A HREF="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</A>
							<DT><A HREF="https://huggingface.co/docs/transformers/benchmarks">https://huggingface.co/docs/transformers/benchmarks</A>
							<DT><A HREF="https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb">notebooks/benchmark.ipynb at main · huggingface/notebooks · GitHub</A>
							<DT><A HREF="https://github.com/huggingface/transformers/blob/main/tests/benchmark/test_benchmark.py">transformers/test_benchmark.py at main · huggingface/transformers · GitHub</A>
							<DT><A HREF="https://github.com/huggingface/transformers/blob/c256bc6d104b1e85a138f3cf0e5f9f85d1197a25/src/transformers/benchmark/benchmark_utils.py#L633">transformers/benchmark_utils.py at c256bc6d104b1e85a138f3cf0e5f9f85d1197a25 · huggingface/transformers</A>
							<DT><A HREF="https://colab.research.google.com/drive/17tla0i10y2fsQF0FNy_yuV6hxqPjqS6h#scrollTo=19ec9a99">transformers.ipynb - Colaboratory</A>
							<DT><A HREF="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py">https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py</A>
							<DT><A HREF="https://pytorch.org/docs/stable/benchmark_utils.html">https://pytorch.org/docs/stable/benchmark_utils.html</A>
							<DT><A HREF="https://github.com/pytorch/pytorch/blob/master/torch/utils/benchmark/examples/compare.py">https://github.com/pytorch/pytorch/blob/master/torch/utils/benchmark/examples/compare.py</A>
							<DT><A HREF="https://github.com/mli/transformers-benchmarks">https://github.com/mli/transformers-benchmarks</A>
							<DT><A HREF="https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling">https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling</A>
							<DT><A HREF="https://github.com/huggingface/transformers-bloom-inference">transformers-bloom-inference</A>
							<DT><A HREF="https://github.com/bigcode-project/Megatron-LM/issues/20">Benchmarking Memory Consumption of Optimizers Adam v.s. Adan</A>
							<DT><A HREF="https://github.com/huggingface/blog/blob/main/bloom-inference-pytorch-scripts.md">Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate</A>
							<DT><A HREF="https://github.com/microsoft/DeepSpeed/tree/master/examples">DeepSpeed/examples</A>
							<DT><A HREF="https://github.com/microsoft/DeepSpeedExamples/blob/master/inference/huggingface/text-generation/inference-test.py">DeepSpeedExamples/inference-test.py</A>
							<DT><A HREF="https://huggingface.co/blog/bloom-inference-pytorch-scripts">Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate</A>
							<DT><A HREF="https://github.com/huggingface/transformers-bloom-inference/blob/main/bloom-inference-scripts/bloom-accelerate-inference.py#L46">transformers-bloom-inference/bloom-accelerate-inference.py</A>
							<DT><A HREF="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference/tree/main/benchmark</A>
						</DL><p>
						<DT><H3 FOLDED>sw-transformer-inference-lectures</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/watch?v=IGu7ivuy1Ag&t=590s">How a Transformer works at inference vs training time - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=7P6wllBoHwU">SparseGPT : Get Rid of 100 Billion Parameters - YouTube</A>
							<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance</A>
						</DL><p>
						<DT><A HREF="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/#distillation">Large Transformer Model Inference Optimization | Lil'Log</A>
						<DT><A HREF="https://www.zhihu.com/people/liang-de-peng">GiantPandaCV (chinese high-proffesional discussions)</A>
						<DT><A HREF="https://websites.umich.edu/~amberljc/file/LLM-Systems-Basics.pdf">LLM-Systems-Basics (Jiachen Liu)</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance</A>
						<DT><A HREF="https://github.com/antferdom/tuning_playbook/blob/main/language_models/Resources.md">tuning_playbook</A>
						<DT><A HREF="https://github.com/NVIDIA/DeepLearningExamples">NVIDIA/DeepLearningExamples</A>
						<DT><A HREF="https://arxiv.org/pdf/2302.14017.pdf">Full Stack Optimization of Transformer Inference: a Survey</A>
						<DT><A HREF="https://huggingface.co/docs/transformers/perf_infer_gpu_one">Efficient Inference on a Single GPU</A>
						<DT><A HREF="https://nat.dev/compare">OpenPlayground</A>
						<DT><A HREF="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/Dev_Guide.md">Transformer Model Optimization Tool Dev Guide</A>
						<DT><A HREF="https://excalidraw.com/">Excalidraw | Hand-drawn look &amp; feel • Collaborative • Secure</A>
						<DT><A HREF="https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices">LLM Inference Performance Engineering: Best Practices | Databricks Blog</A>
						<DT><A HREF="https://rahulschand.github.io/gpu_poor/">Tokens/s simulation</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance (2024 main)</A>
						<DT><A HREF="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/">Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</A>
						<DT><A HREF="https://www.modular.com/blog/how-to-be-confident-in-your-performance-benchmarking">Modular: How to Be Confident in Your Performance Benchmarking</A>
						<DT><A HREF="https://github.com/xjdr-alt/mla_blog_translation">DeepSeek-V2 High-performance Inference Optimization Notes: MLA Optimization</A>
						<DT><A HREF="https://github.com/DefTruth/Awesome-LLM-Inference">DefTruth/Awesome-LLM-Inference: 📖A curated list of Awesome LLM Inference Paper with codes, TensorRT-LLM, vLLM, streaming-llm, AWQ, SmoothQuant, WINT8/4, Continuous Batching, FlashAttention, PagedAttention etc.</A>
						<DT><A HREF="https://github.com/drisspg/transformer_nuggets">drisspg/transformer_nuggets: A place to store reusable transformer components of my own creation or found on the interwebs</A>
						<DT><A HREF="https://github.com/feifeibear/LLMRoofline">feifeibear/LLMRoofline: Compare different hardware platforms via the Roofline Model for LLM inference tasks.</A>
					</DL><p>
					<DT><H3 FOLDED>ML Sys</H3>
					<DL><p>
						<DT><H3 FOLDED>Operating Systems</H3>
						<DL><p>
							<DT><H3 FOLDED>os-configuration</H3>
							<DL><p>
								<DT><H3 FOLDED>bootstrap</H3>
								<DL><p>
									<DT><A HREF="https://www.ventoy.net/en/index.html">Ventoy</A>
								</DL><p>
								<DT><H3 FOLDED>.dotfiles</H3>
								<DL><p>
									<DT><H3 FOLDED>repos</H3>
									<DL><p>
										<DT><A HREF="https://github.com/eieioxyz/Beyond-Dotfiles-in-100-Seconds">eieioxyz/Beyond-Dotfiles-in-100-Seconds</A>
									</DL><p>
									<DT><H3 FOLDED>docs</H3>
									<DL><p>
										<DT><A HREF="https://wiki.archlinux.org/title/Dotfiles">Dotfiles - ArchWiki</A>
										<DT><A HREF="https://dotfiles.github.io/">GitHub does dotfiles - dotfiles.github.io</A>
										<DT><A HREF="https://www.anishathalye.com/2014/08/03/managing-your-dotfiles/">Managing Your Dotfiles</A>
									</DL><p>
									<DT><A HREF="https://superuser.com/questions/886132/where-is-the-zshrc-file-on-mac">Where is the .zshrc file</A>
									<DT><A HREF="https://tldp.org/LDP/abs/html/sample-bashrc.html">Sample .bashrc and .bash_profile Files</A>
									<DT><A HREF="https://www.youtube.com/watch?v=r_MpUP6aKiQ&t=540s">~/.dotfiles in 100 Seconds</A>
									<DT><A HREF="https://www.atlassian.com/git/tutorials/dotfiles">How to store dotfiles</A>
									<DT><A HREF="https://news.ycombinator.com/item?id=11070797">Ask HN: What do you use to manage dotfiles?</A>
									<DT><A HREF="https://superuser.com/questions/39751/add-directory-to-path-if-its-not-already-there/753948#753948">bash - Add directory to $PATH if it's not already there</A>
								</DL><p>
								<DT><A HREF="https://github.com/fastai/fastsetup">fastai/fastsetup: Setup all the things</A>
								<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5d8ca2faf74c494f220c8f71130340b513eea9a9/docker/common/install_pytorch.sh">TensorRT-LLM/docker/common/install_pytorch.sh</A>
								<DT><H3 FOLDED>os-setup</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tinygrad/tinyos">tinygrad/tinyos</A>
								</DL><p>
								<DT><A HREF="https://github.com/tinygrad/tinyos">tinygrad/tinyos</A>
							</DL><p>
							<DT><H3 FOLDED>version control</H3>
							<DL><p>
								<DT><H3 FOLDED>git</H3>
								<DL><p>
									<DT><H3 FOLDED>pre-commit</H3>
									<DL><p>
										<DT><A HREF="https://pre-commit.ci/">pre-commit.ci</A>
										<DT><A HREF="https://pre-commit.com/">pre-commit</A>
										<DT><A HREF="https://github.com/pre-commit/pre-commit">pre-commit/pre-commit: A framework for managing and maintaining multi-language pre-commit hooks.</A>
									</DL><p>
									<DT><H3 FOLDED>monorepo</H3>
									<DL><p>
										<DT><H3 FOLDED>git-subtrees</H3>
										<DL><p>
											<DT><A HREF="https://claude.ai/chat/ccf1d431-fadc-4eae-b3e1-7689cf13dbcc">monorepo modifications methodology</A>
											<DT><A HREF="https://gist.github.com/SKempin/b7857a6ff6bddb05717cc17a44091202">Git Subtree basics</A>
										</DL><p>
										<DT><A HREF="https://github.com/web3infra-foundation/mega?tab=readme-ov-file">mega: unofficial open source implementation of Google Piper</A>
										<DT><A HREF="https://cacm.acm.org/research/why-google-stores-billions-of-lines-of-code-in-a-single-repository/">Why Google Stores Billions of Lines of Code in a Single Repository</A>
									</DL><p>
									<DT><H3 FOLDED>gitignore</H3>
									<DL><p>
										<DT><A HREF="https://stackoverflow.com/questions/2545602/how-can-i-git-ignore-subfolders-subdirectories#:~:text=*%2F*%20ignores%20all%20subdirectories%20but,%2F*%20all%20work%20for%20me.">git rm -r --cached .</A>
									</DL><p>
									<DT><H3 FOLDED>git-submodules</H3>
									<DL><p>
										<DT><A HREF="https://git-scm.com/book/en/v2/Git-Tools-Submodules">Git - Submodules</A>
										<DT><A HREF="https://git-scm.com/book/en/v2/Git-Basics-Working-with-Remotes">Git - Working with Remotes</A>
										<DT><A HREF="https://github.blog/2016-02-01-working-with-submodules/">Working with submodules - The GitHub Blog</A>
										<DT><A HREF="https://claude.ai/chat/d2720f8d-0434-4654-80be-fdbd77a7ff7f">Troubleshooting Git Submodule Setup - Claude</A>
									</DL><p>
									<DT><H3 FOLDED>git-remote</H3>
									<DL><p>
										<DT><A HREF="https://twitter.com/JI/status/1546948817462800384">No more "--set-upstream origin"</A>
										<DT><A HREF="https://claude.ai/chat/538dd4e6-f241-47ce-93f4-7bcbd2095129">Updating Git Remote to Push Local Changes to Fork - Claude</A>
									</DL><p>
									<DT><H3 FOLDED>git-patches</H3>
									<DL><p>
										<DT><A HREF="https://www.specbee.com/blogs/how-create-and-apply-patch-git-diff-and-git-apply-commands-your-drupal-website">git patches</A>
									</DL><p>
									<DT><A HREF="https://github.com/web3infra-foundation/mega">web3infra-foundation/mega: Mega is an unofficial open source implementation of Google Piper.</A>
									<DT><A HREF="https://git-scm.com/docs/git-diff">git-diff</A>
									<DT><A HREF="https://git-scm.com/docs/git-commit">commit</A>
									<DT><A HREF="https://chris.beams.io/posts/git-commit/">How to Write a Git Commit Message</A>
									<DT><A HREF="https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh">Connecting to GitHub with SSH - GitHub Docs</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Basics-Viewing-the-Commit-History">Viewing the Commit History</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Branching-Rebasing">Rebasing</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Tools-Reset-Demystified">Reset Demystified</A>
									<DT><A HREF="https://stackoverflow.com/questions/448919/how-can-i-remove-a-commit-on-github">How can I remove a commit on GitHub?</A>
									<DT><A HREF="https://stackoverflow.com/questions/3701404/how-can-i-list-all-commits-that-changed-a-specific-file">follow file</A>
									<DT><A HREF="https://stackoverflow.com/questions/2505096/cloning-a-private-github-repo">Cloning a private Github repo</A>
									<DT><A HREF="https://stackoverflow.com/questions/68775869/support-for-password-authentication-was-removed-please-use-a-personal-access-to">Support for password authentication was removed</A>
									<DT><A HREF="https://stackoverflow.com/questions/11188801/connect-local-repo-with-remote-repo">connect local repo with remote repo</A>
									<DT><A HREF="https://docs.github.com/en/account-and-profile/setting-up-and-managing-your-github-user-account/managing-email-preferences/setting-your-commit-email-address">Setting your commit email address</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Tools-Submodules">Git - Submodules</A>
									<DT><A HREF="https://git-scm.com/book/en/v2/Git-Basics-Working-with-Remotes">Git - Working with Remotes</A>
									<DT><A HREF="https://github.blog/2016-02-01-working-with-submodules/">Working with submodules - The GitHub Blog</A>
									<DT><A HREF="https://gist.github.com/myusuf3/7f645819ded92bda6677">How effectively delete a git submodule.</A>
									<DT><A HREF="https://www.leshenko.net/p/ugit/#read-tree-to-index">Git Internals - Learn by Building Your Own Git</A>
									<DT><A HREF="https://www.conventionalcommits.org/en/v1.0.0/">Conventional Commits</A>
									<DT><A HREF="https://github.com/triton-lang/triton/issues/4310">install Python module via git commit: example -&gt; pytorch-triton                     3.0.0+a9bc1a3647</A>
								</DL><p>
								<DT><H3 FOLDED>Github</H3>
								<DL><p>
									<DT><A HREF="https://docs.github.com/en/rest/reference/users#get-a-user">Users - GitHub Docs</A>
									<DT><A HREF="https://cli.github.com/">GitHub CLI | Take GitHub to the command line</A>
									<DT><A HREF="https://education.github.com/discount_requests/teacher_application">Request a discount - GitHub Education</A>
								</DL><p>
								<DT><H3 FOLDED>Trunk-based development</H3>
								<DL><p>
									<DT><A HREF="https://www.atlassian.com/continuous-delivery/continuous-integration/trunk-based-development">Trunk-based Development | Atlassian</A>
									<DT><A HREF="https://trunkbaseddevelopment.com/">Introduction</A>
									<DT><A HREF="https://github.com/ezyang/ghstack">ezyang/ghstack: Submit stacked diffs to GitHub on the command line</A>
								</DL><p>
								<DT><H3 FOLDED>Stacking change</H3>
								<DL><p>
									<DT><A HREF="https://graphite.dev/blog/post/DThX8ffP1gmxWJChEv0y">Graphite - Stacking changes</A>
									<DT><A HREF="https://jg.gg/2018/09/29/stacked-diffs-versus-pull-requests/">Stacked Diffs Versus Pull Requests | Jackson Gabbard's Blog</A>
									<DT><A HREF="https://kurtisnusbaum.medium.com/stacked-diffs-keeping-phabricator-diffs-small-d9964f4dcfa6">Stacked Diffs: Keeping Phabricator Diffs Small | by Kurtis Nusbaum | Medium</A>
									<DT><A HREF="https://news.ycombinator.com/item?id=26922633">Stacked Diffs versus Pull Requests (2018) | Hacker News</A>
									<DT><A HREF="https://newsletter.pragmaticengineer.com/p/stacked-diffs">Stacked Diffs (and why you should know about them)</A>
									<DT><A HREF="https://github.com/ezyang/ghstack">ezyang/ghstack: Submit stacked diffs to GitHub on the command line</A>
									<DT><A HREF="https://github.com/modular/stack-pr">modular/stack-pr: A tool for working with stacked PRs on github.</A>
								</DL><p>
								<DT><A HREF="https://pre-commit.ci/">pre-commit.ci</A>
							</DL><p>
							<DT><H3 FOLDED>package manager</H3>
							<DL><p>
								<DT><H3 FOLDED>Brew</H3>
								<DL><p>
									<DT><H3 FOLDED>tap</H3>
									<DL><p>
										<DT><A HREF="https://docs.brew.sh/Taps">Taps (Third-Party Repositories)</A>
									</DL><p>
									<DT><H3 FOLDED>leaves</H3>
									<DL><p>
										<DT><A HREF="https://thoughtbot.com/blog/brew-leaves">brew leaves</A>
									</DL><p>
									<DT><H3 FOLDED>casks</H3>
									<DL><p>
										<DT><A HREF="https://formulae.brew.sh/cask/">homebrew-cask</A>
										<DT><A HREF="https://github.com/Homebrew/homebrew-cask">CLI workflow for the administration of applications as bin</A>
									</DL><p>
									<DT><A HREF="https://brew.sh/">The Missing Package Manager for macOS (or Linux)</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Symbolic_link">Symbolic link</A>
									<DT><A HREF="https://clubmate.fi/make-a-symlink-in-linux-or-mac-os-x/">Make a symlink</A>
									<DT><A HREF="https://formulae.brew.sh/">Formulae</A>
									<DT><A HREF="https://zanshin.net/2014/02/03/how-to-list-brew-dependencies/">List Brew Dependencies</A>
									<DT><A HREF="https://rick.cogley.info/post/use-homebrew-zsh-instead-of-the-osx-default/">Use Homebrew zsh Instead of the OS X Default</A>
									<DT><A HREF="https://stackoverflow.com/questions/55732972/curl-56-libressl-ssl-read-ssl-error-syscall-errno-54/55735219">curl: (56) LibreSSL SSL_read: SSL_ERROR_SYSCALL, errno 54</A>
									<DT><A HREF="https://www.unix.com/os-x-apple-/161123-what-directory-brew-prefix-homebrew.html">directory "$(brew --prefix)"?</A>
									<DT><A HREF="https://docs.brew.sh/Formula-Cookbook">Formula Cookbook — Homebrew Documentation</A>
									<DT><A HREF="https://docs.brew.sh/FAQ">FAQ — Homebrew Documentation</A>
									<DT><A HREF="https://apple.stackexchange.com/questions/373411/how-to-install-a-specific-version-of-ocaml-on-macos">install a specific version</A>
									<DT><A HREF="https://stackoverflow.com/questions/13477363/how-can-i-brew-link-a-specific-version">How can I brew link a specific version?</A>
									<DT><A HREF="https://docs.brew.sh/Tips-N%27-Tricks">Tips and Tricks — Homebrew Documentation</A>
									<DT><A HREF="https://remarkablemark.org/blog/2017/02/03/install-brew-package-version/">How to install an older homebrew package</A>
									<DT><A HREF="https://devhints.io/homebrew">Homebrew cheatsheet</A>
									<DT><A HREF="https://docs.w3cub.com/homebrew/manpage">Brew - Homebrew - W3cubDocs</A>
									<DT><A HREF="https://stackoverflow.com/questions/16246352/how-do-i-specify-ldflags-and-cppflags-for-configure">LDFLAGS and CPPFLAGS</A>
									<DT><A HREF="https://stackoverflow.com/questions/65502748/why-does-brew-cleanup-or-brew-cleanup-n-dont-show-any-output">cleanup</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>windows manager</H3>
							<DL><p>
								<DT><A HREF="https://en.wikipedia.org/wiki/X_Window_System">X Window System</A>
								<DT><A HREF="https://wiki.haskell.org/Xmonad/General_xmonad.hs_config_tips">Xmonad/General xmonad.hs config tips</A>
							</DL><p>
							<DT><H3 FOLDED>Docker</H3>
							<DL><p>
								<DT><H3 FOLDED>docker-examples</H3>
								<DL><p>
									<DT><A HREF="https://github.com/docker/docker-rust-hello">docker/docker-rust-hello: A simple Rust application</A>
									<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo/tree/main">neuralmagic/tensorrt-demo</A>
									<DT><A HREF="https://github.com/alihassanijr/PyTorch-CUDA12">alihassanijr/PyTorch-CUDA12: Nightly PyTorch + CUDA 12 Dockerfile</A>
									<DT><A HREF="https://hub.docker.com/r/xzhao9/gcp-a100-runner-dind">xzhao9/gcp-a100-runner-dind: Tritonbench base enviroment</A>
									<DT><A HREF="https://github.com/xdit-project/HunyuanVideo/blob/main/docker/Dockerfile_xDiT">HunyuanVideo/docker/Dockerfile_xDiT at main: Install Miniconda</A>
								</DL><p>
								<DT><H3 FOLDED>docker-image</H3>
								<DL><p>
									<DT><H3 FOLDED>Dockerfile</H3>
									<DL><p>
										<DT><H3 FOLDED>dockerfile-python</H3>
										<DL><p>
											<DT><A HREF="https://github.com/docker-library/python/blob/bf5951cfa2b2f6c3dabf428549c9dca658ecee81/3.12/bullseye/Dockerfile">python/3.12/bullseye/Dockerfile</A>
											<DT><A HREF="http://dockerfile.github.io/#/python">Python Dockerfile</A>
										</DL><p>
										<DT><H3 FOLDED>dockerfile-ENV</H3>
										<DL><p>
											<DT><A HREF="https://grok.com/chat/e6d885da-d411-4cc2-8911-bc09c38eb46d">Dockerfile Environment Persistence - Grok</A>
										</DL><p>
										<DT><A HREF="http://dockerfile.github.io/">Dockerfile Project - Trusted Automated Docker Builds</A>
										<DT><A HREF="https://docs.docker.com/build/dockerfile/frontend/#stable-channel">Custom Dockerfile syntax | Docker Docs</A>
										<DT><A HREF="https://docs.docker.com/reference/dockerfile/">Dockerfile reference | Docker Docs</A>
										<DT><A HREF="https://github.com/tinygrad/tinygrad/blob/d49d4324a3340609126e64b10f979507636fe5ed/test/Dockerfile#L2">tinygrad/test/Dockerfile (minimal python example)</A>
										<DT><A HREF="https://github.com/docker-library/python/blob/875ce40a1ae98c7c37b1652e65f76376ac93a911/apply-templates.sh#L22">python/apply-templates.sh</A>
									</DL><p>
									<DT><H3 FOLDED>docker-image-optimization</H3>
									<DL><p>
										<DT><A HREF="https://rodneyosodo.medium.com/minimizing-python-docker-images-cf99f4468d39">Minimizing python docker images. During the transition to a micro</A>
									</DL><p>
									<DT><A HREF="https://github.com/docker-library">Docker Official Images</A>
									<DT><A HREF="https://registry.hub.docker.com/">Docker Hub Container Image Library | App Containerization</A>
									<DT><A HREF="https://hub.docker.com/r/chengzeyi/ubuntu-desktop">chengzeyi/ubuntu-desktop - Docker Image | Docker Hub</A>
								</DL><p>
								<DT><H3 FOLDED>docker-container</H3>
								<DL><p>
									<DT><H3 FOLDED>NGC</H3>
									<DL><p>
										<DT><A HREF="https://www.nvidia.com/pt-br/gpu-cloud1/containers/">NGC Containers | NVIDIA</A>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/Dockerfile">pytorch/Dockerfile</A>
										<DT><A HREF="https://catalog.ngc.nvidia.com/containers?filters=platform%7CPyTorch%7Cpltfm_pytorch&orderBy=weightPopularDESC&query=&page=&pageSize=">Pytorch container</A>
										<DT><A HREF="https://github.com/psaboia/devcontainer-nvidia-base">psaboia/devcontainer-nvidia-base: Example of how to setup a NVIDIA DevContainer with GPU Support for Tensorflow/Keras, that follows the page https://alankrantas.medium.com/setup-a-nvidia-devcontainer-with-gpu-support-for-tensorflow-keras-on-windows-d00e6e204630</A>
										<DT><A HREF="https://hub.docker.com/layers/nvidia/cuda/11.8.0-devel-ubuntu22.04/images/sha256-60eda04ab6790aa76d73bf0df245b361eabc6d8f7b6f6cf9846c70f399b9a1eb">Image Layer Details - nvidia/cuda:11.8.0-devel-ubuntu22.04 | Docker Hub</A>
										<DT><A HREF="https://hub.docker.com/r/nvidia/cuda/tags">nvidia/cuda Tags | Docker Hub</A>
										<DT><A HREF="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver">Triton Inference Server | NVIDIA NGC</A>
										<DT><A HREF="https://github.com/sgl-project/tensorrt-demo">sgl-project/tensorrt-demo: prerequisite -&gt; nvidia-container-toolkit</A>
									</DL><p>
									<DT><H3 FOLDED>docker-container-checkpointing</H3>
									<DL><p>
										<DT><A HREF="https://github.com/checkpoint-restore/checkpointctl">checkpoint-restore/checkpointctl: A tool for in-depth analysis of container checkpoints</A>
									</DL><p>
									<DT><A HREF="https://docs.docker.com/reference/cli/docker/container/rm/">docker container rm | Docker Docs</A>
								</DL><p>
								<DT><H3 FOLDED>docker-debug</H3>
								<DL><p>
									<DT><H3 FOLDED>docker-interactive</H3>
									<DL><p>
										<DT><H3 FOLDED>docker-run</H3>
										<DL><p>
											<DT><A HREF="https://github.com/neuralmagic/tensorrt-demo/tree/main">docker run -it -d --net host --shm-size=2g --ulimit memlock=-1 --ulimit stack=67108864 --runtime=nvidia --gpus all -v ${tensorrtllm_backend_dir}:/tensorrtllm_backend  -v $HOME/models:/models -v ${tensorrt_demo_dir}:/root/tensorrt-demo --name triton_server nvcr.io/nvidia/tritonserver:24.04-trtllm-python-py3 bash</A>
											<DT><A HREF="https://claude.ai/chat/cbc3a3dc-bdfa-40ff-aeac-ba722f2b3652">docker run interactive session: decompose full command</A>
											<DT><A HREF="https://datacrunch.io/blog/deepseek-v3-llm-nvidia-h200-gpu-inference-benchmarking">DeepSeek V3 LLM NVIDIA H200 GPU Inference Benchmarking — Blog — DataCrunch</A>
											<DT><A HREF="https://github.com/datacrunch-research/tensorrt-demo">docker exec -it triton_server /bin/bash</A>
											<DT><A HREF="https://docs.docker.com/reference/cli/docker/container/run/">docker run | Docker Docs</A>
											<DT><A HREF="https://stackoverflow.com/questions/47831774/docker-run-with-volume">docker run with --volume</A>
											<DT><A HREF="https://chat.openai.com/c/05357f44-c5ed-4a6d-bce7-5545860507cd">docker run -ti -p 3000:3000 -v $inferno_dir:/source/inferno &lt;image&gt;</A>
										</DL><p>
										<DT><A HREF="https://claude.ai/chat/4d385901-e1fb-45c8-9c7b-d7be6a5cc32c">Executing Commands in a Running Docker Container - Claude</A>
										<DT><A HREF="https://docs.docker.com/storage/volumes/">Volumes | Docker Docs</A>
									</DL><p>
									<DT><H3 FOLDED>dev-container</H3>
									<DL><p>
										<DT><H3 FOLDED>dev-container-examples</H3>
										<DL><p>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/acc466751b2723eb913fd3148b4f054189bbf1ab/.devcontainer/README.md">pytorch/.devcontainer/README.md</A>
											<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/.devcontainer/Dockerfile">pytorch/.devcontainer/Dockerfile</A>
											<DT><A HREF="https://github.com/psaboia/devcontainer-nvidia-base">psaboia/devcontainer-nvidia-base</A>
											<DT><A HREF="https://github.com/devcontainers/images/tree/main/src/base-ubuntu">images/src/base-ubuntu at main · devcontainers/images</A>
										</DL><p>
										<DT><H3 FOLDED>dev-container-template</H3>
										<DL><p>
											<DT><A HREF="https://github.com/devcontainers/template-starter">devcontainers/template-starter: A template explaining how to author custom dev container Templates</A>
											<DT><A HREF="https://containers.dev/templates">Available Dev Container Templates</A>
										</DL><p>
										<DT><H3 FOLDED>dev-container-devcontainer.json</H3>
										<DL><p>
											<DT><A HREF="https://github.com/devcontainers/images">devcontainers/images: Repository for pre-built dev container images published under mcr.microsoft.com/devcontainers</A>
											<DT><A HREF="https://containers.dev/implementors/json_reference/#image-specific">Dev Container metadata reference</A>
										</DL><p>
										<DT><H3 FOLDED>dev-container-nvidia</H3>
										<DL><p>
											<DT><A HREF="https://github.com/psaboia/devcontainer-nvidia-base">psaboia/devcontainer-nvidia-base: Example of how to setup a NVIDIA DevContainer with GPU Support for Tensorflow/Keras, that follows the page https://alankrantas.medium.com/setup-a-nvidia-devcontainer-with-gpu-support-for-tensorflow-keras-on-windows-d00e6e204630</A>
										</DL><p>
										<DT><A HREF="https://containers.dev/implementors/json_reference/">Dev Container metadata reference</A>
										<DT><A HREF="https://github.com/devcontainers">devcontainers</A>
										<DT><A HREF="https://code.visualstudio.com/docs/devcontainers/containers#_create-a-devcontainerjson-file">dev containers &amp; VS code</A>
										<DT><A HREF="https://code.visualstudio.com/docs/devcontainers/containers">Developing inside a Container using Visual Studio Code Remote Development</A>
										<DT><A HREF="https://code.visualstudio.com/docs/devcontainers/create-dev-container">Rebuild: postCreateCommand</A>
										<DT><A HREF="https://code.visualstudio.com/docs/devcontainers/containers#_getting-started">Developing inside a Container using Visual Studio Code Remote Development</A>
										<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#docker">Installing the NVIDIA Container Toolkit</A>
										<DT><A HREF="https://www.youtube.com/watch?v=p9L7YFqHGk4">Customize Dev Containers in VS Code with Dockerfiles and Docker Compose</A>
										<DT><A HREF="https://www.youtube.com/watch?v=BhtxEDwgylU">Use AppMap with VS Code Dev Containers - YouTube</A>
										<DT><A HREF="https://github.com/devcontainers/images/tree/main/src/base-ubuntu">images/src/base-ubuntu at main · devcontainers/images</A>
										<DT><A HREF="https://docs.github.com/en/codespaces/setting-up-your-project-for-codespaces/adding-a-dev-container-configuration/introduction-to-dev-containers">Introduction to dev containers - GitHub Docs</A>
										<DT><A HREF="https://containers.dev/guide/dockerfile">Using Images, Dockerfiles, and Docker Compose</A>
										<DT><A HREF="https://containers.dev/">Development containers</A>
									</DL><p>
									<DT><H3 FOLDED>docker-logs</H3>
									<DL><p>
										<DT><A HREF="https://forums.docker.com/t/capture-ouput-of-docker-build-into-a-log-file/123178">Capture ouput of docker build into a log file?</A>
									</DL><p>
									<DT><A HREF="https://github.com/wagoodman/dive">wagoodman/dive: A tool for exploring each layer in a docker image</A>
								</DL><p>
								<DT><H3 FOLDED>docker-nvidia</H3>
								<DL><p>
									<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide">Installation Guide — NVIDIA drivers</A>
									<DT><A HREF="https://medium.datadriveninvestor.com/setting-up-carla-simulator-for-the-self-driving-cars-specialization-d38d4f6a0486">Setting up CARLA Simulator for the Self-Driving Cars Specialization | by Viridiana Romero Martinez | DataDrivenInvestor</A>
									<DT><A HREF="https://awesomeopensource.com/project/Amin-Tgz/awesome-CARLA">Awesome Carla</A>
									<DT><A HREF="https://hub.docker.com/layers/nvidia/cuda/11.8.0-devel-ubuntu22.04/images/sha256-60eda04ab6790aa76d73bf0df245b361eabc6d8f7b6f6cf9846c70f399b9a1eb">Image Layer Details - nvidia/cuda:11.8.0-devel-ubuntu22.04 | Docker Hub</A>
									<DT><A HREF="https://hub.docker.com/r/nvidia/cuda/tags">nvidia/cuda Tags | Docker Hub</A>
									<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/sample-workload.html">Running a Sample Workload — NVIDIA Container Toolkit 1.16.0 documentation</A>
									<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/docker-specialized.html">Specialized Configurations with Docker — NVIDIA Container Toolkit 1.16.0 documentation</A>
									<DT><A HREF="https://forums.developer.nvidia.com/t/whats-difference-between-gpus-and-runtime-nvidia-for-the-docker-container/283468/3">To put it simply, use --gpus on x86 and --runtime=nvidia on jetson/ARM SBSA system.</A>
								</DL><p>
								<DT><H3 FOLDED>docker-build</H3>
								<DL><p>
									<DT><H3 FOLDED>docker-build-examples</H3>
									<DL><p>
										<DT><A HREF="https://docs.anjuna.io/sgx/latest/getting_started/tutorial_anjuna_in_docker/build_an_image.html">Build a Docker image :: Getting Started</A>
									</DL><p>
									<DT><A HREF="https://github.com/docker/buildx">docker/buildx: Docker CLI plugin for extended build capabilities with BuildKit</A>
									<DT><A HREF="https://docs.docker.com/build/">Overview of Docker Build | Docker Docs</A>
									<DT><A HREF="https://docs.docker.com/build/guide/intro/">Introduction | Docker Docs</A>
									<DT><A HREF="https://docs.docker.com/reference/cli/docker/buildx/build/">docker buildx build | Docker Docs</A>
									<DT><A HREF="https://aistudio.google.com/app/prompts/1KnqPc-9ZNiYGbWD-sO3I0ro5touxoTU0?pli=1">Docker build | Google AI Studio</A>
								</DL><p>
								<DT><H3 FOLDED>linux-namespaces</H3>
								<DL><p>
									<DT><A HREF="https://blog.lizzie.io/linux-containers-in-500-loc.html">Linux containers in 500 lines of code</A>
									<DT><A HREF="https://www.youtube.com/watch?v=-YnMr1lj4Z8&t=174s">How Docker Works - Intro to Namespaces - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=cPGZMt4cJ0I">Introduction to Docker for CTFs - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>docker-footprint</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookincubator/senpai">facebookincubator/senpai: Senpai is an automated memory sizing tool for container applications.</A>
								</DL><p>
								<DT><H3 FOLDED>docker-install</H3>
								<DL><p>
									<DT><A HREF="https://github.com/docker/docker-install">docker/docker-install: Docker installation script</A>
								</DL><p>
								<DT><H3 FOLDED>testcontainers</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=sNg0bnMF_qY">Testcontainers have forever changed the way I write tests - YouTube</A>
									<DT><A HREF="https://testcontainers.com/">Testcontainers</A>
									<DT><A HREF="https://www.youtube.com/watch?v=sNg0bnMF_qY">Testcontainers have forever changed the way I write tests</A>
								</DL><p>
								<DT><H3 FOLDED>docker-registry</H3>
								<DL><p>
									<DT><H3 FOLDED>docker-mirror</H3>
									<DL><p>
										<DT><A HREF="https://github.com/regclient/regclient/blob/main/docs/regsync.md">regclient/docs/regsync.md at main · regclient/regclient</A>
									</DL><p>
									<DT><H3 FOLDED>docker-hub-private</H3>
									<DL><p>
										<DT><A HREF="https://claude.ai/chat/cbc3a3dc-bdfa-40ff-aeac-ba722f2b3652">docker pull: Pulling a Private Docker Image - Claude</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://blog.lizzie.io/linux-containers-in-500-loc.html">Linux containers in 500 lines of code</A>
								<DT><A HREF="https://github.com/wagoodman/dive">wagoodman/dive: A tool for exploring each layer in a docker image</A>
								<DT><A HREF="https://www.youtube.com/watch?v=rIrNIzy6U_g&t=247s">100+ Docker Concepts you Need to Know</A>
								<DT><A HREF="https://gist.github.com/anupambhatnagar/07ebff374bc45e4b63eb42893cca7e87">Commonly used Docker commands</A>
								<DT><A HREF="https://stackoverflow.com/questions/48957195/how-to-fix-docker-got-permission-denied-issue">How to fix docker: Got permission denied issue</A>
								<DT><A HREF="https://stackoverflow.com/questions/51188657/image-is-being-used-by-stopped-container/51189547">image is being used by stopped container</A>
								<DT><A HREF="https://gist.github.com/biera/fa4fcca8a3150dfa2438">remove all docker containers</A>
								<DT><A HREF="https://docs.docker.com/engine/install/linux-postinstall/">Post-installation steps for Linux | NON-ROOT USER</A>
								<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide">Installation Guide — NVIDIA drivers</A>
								<DT><A HREF="https://www.youtube.com/watch?v=J0NuOlA2xDc&t=5s">Never install locally</A>
								<DT><A HREF="https://github.com/docker/buildx">docker/buildx: Docker CLI plugin for extended build capabilities with BuildKit</A>
								<DT><A HREF="https://github.com/facebookincubator/senpai">facebookincubator/senpai: Senpai is an automated memory sizing tool for container applications.</A>
								<DT><A HREF="https://av.tib.eu/media/46137">Senpai - Automatic memory sizing for containers - TIB AV-Portal</A>
								<DT><A HREF="https://gist.github.com/cloneofsimo/e4226f63443bf6386df846b06ff8d420">dockersetup.md</A>
								<DT><A HREF="https://github.com/goldmann/docker-squash">goldmann/docker-squash: Docker image squashing tool</A>
								<DT><A HREF="https://claude.ai/chat/4d385901-e1fb-45c8-9c7b-d7be6a5cc32c">Executing Commands in a Running Docker Container - Claude</A>
							</DL><p>
							<DT><H3 FOLDED>Linux</H3>
							<DL><p>
								<DT><H3 FOLDED>linux-distros</H3>
								<DL><p>
									<DT><H3 FOLDED>ubuntu</H3>
									<DL><p>
										<DT><A HREF="https://askubuntu.com/questions/1162491/how-can-you-tell-the-version-of-ubuntu-on-a-system-in-a-sh-bash-script">print system version</A>
										<DT><A HREF="https://help.ubuntu.com/community/EnvironmentVariables#File-location_related_variables">EnvironmentVariables - Community Help Wiki</A>
										<DT><A HREF="https://x.com/__tinygrad__/status/1869238988877492671/photo/1">Installing Ubuntu, media boost</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>linux-users</H3>
								<DL><p>
									<DT><H3 FOLDED>linux-users-sudo</H3>
									<DL><p>
										<DT><A HREF="https://askubuntu.com/questions/2214/how-do-i-add-a-user-to-the-sudo-group">How do I add a user to the "sudo" group? - Ask Ubuntu</A>
									</DL><p>
									<DT><A HREF="https://manpages.ubuntu.com/manpages/bionic/man8/useradd.8.html">Ubuntu Manpage: useradd - create a new user or update default new user information</A>
									<DT><A HREF="https://learnubuntu.com/list-users/">How to List Users in Ubuntu Command Line</A>
									<DT><A HREF="https://www.cyberciti.biz/faq/create-a-user-account-on-ubuntu-linux/">How to create a user account on Ubuntu Linux - nixCraft</A>
									<DT><A HREF="https://www.geeksforgeeks.org/id-command-in-linux-with-examples/">id command in Linux with examples - GeeksforGeeks</A>
									<DT><A HREF="https://unix.stackexchange.com/questions/3568/how-to-switch-between-users-on-one-terminal">How to switch between users on one terminal?</A>
									<DT><A HREF="https://www.cyberciti.biz/faq/linux-list-users-command/">Linux List All Users In The System Command - nixCraft</A>
									<DT><A HREF="https://chatgpt.com/c/54a41702-74ee-4dc6-a049-4637eb63c617">Create and Switch Users</A>
									<DT><A HREF="https://chatgpt.com/c/67583c20-e1b4-800c-9d82-a189909a793b">Shared Model Weights Setup: Create or Modify users for a shared volume environment using CEPH</A>
								</DL><p>
								<DT><H3 FOLDED>linux-package manager</H3>
								<DL><p>
									<DT><H3 FOLDED>apt-get</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>apt</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>update &amp; upgrade</H3>
									<DL><p>
										<DT><H3 FOLDED>debfoster</H3>
										<DL><p>
											<DT><A HREF="https://manpages.ubuntu.com/manpages/trusty/man8/debfoster.8.html#name">Ubuntu Manpage: debfoster — weed unnecessary Debian packages</A>
											<DT><A HREF="https://ubunlog.com/en/debfoster-clean-maintenance-ubuntu/">Debfoster, clean your system and keep only the important packages | Ubunlog</A>
											<DT><A HREF="https://ubuntuforums.org/showthread.php?t=24403">HOWTO: using debfoster in practice</A>
										</DL><p>
										<DT><A HREF="https://chat.openai.com/c/51c6dd70-1f39-4bf7-843e-837d48b6f0e4">apt-rdepends</A>
										<DT><A HREF="https://askubuntu.com/questions/44122/how-to-upgrade-a-single-package-using-apt-get">apt-get install --only-upgrade &lt;packagename&gt; (unitary upgrade)</A>
									</DL><p>
									<DT><H3 FOLDED>/var/lib/apt/lists</H3>
									<DL><p>
										<DT><A HREF="https://askubuntu.com/questions/179955/var-lib-apt-lists-is-huge">package management - /var/lib/apt/lists is huge - Ask Ubuntu</A>
										<DT><A HREF="https://github.com/devcontainers/images/blob/main/docs/TIPS.md/#why-do-dockerfiles-in-this-repository-use-run-statements-with-commands-separated-by">images/docs/TIPS.md at main · devcontainers/images</A>
									</DL><p>
									<DT><A HREF="https://unix.stackexchange.com/questions/20979/how-do-i-list-all-installed-programs">application - How do I list all installed programs?</A>
									<DT><A HREF="https://unix.stackexchange.com/questions/561263/how-to-get-a-list-of-which-packages-were-installed-with-apt-get-by-a-user-and-no">How to get a list of which packages were installed with apt-get by a user and not by dependencies?</A>
									<DT><A HREF="https://chat.openai.com/c/51c6dd70-1f39-4bf7-843e-837d48b6f0e4">apt &amp; apt-get: /etc/apt/sources.list and other .list files in /etc/apt/sources.list.d/</A>
								</DL><p>
								<DT><H3 FOLDED>linux-configuration</H3>
								<DL><p>
									<DT><A HREF="https://www.omgubuntu.co.uk/2010/05/transfer-your-packages-to-a-clean-install">Transfer your packages to a clean install</A>
									<DT><A HREF="https://launchpad.net/oneconf">OneConf in Launchpad</A>
									<DT><A HREF="https://wiki.ubuntu.com/OneConf">OneConf - Ubuntu Wiki</A>
									<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5d8ca2faf74c494f220c8f71130340b513eea9a9/docker/common/install_pytorch.sh">TensorRT-LLM/docker/common/install_pytorch.sh</A>
								</DL><p>
								<DT><H3 FOLDED>editors</H3>
								<DL><p>
									<DT><H3 FOLDED>VS Code</H3>
									<DL><p>
										<DT><A HREF="https://vscode-docs.readthedocs.io/en/latest/customization/themes/">Themes - vscode-docs</A>
										<DT><A HREF="https://code.visualstudio.com/docs/getstarted/settings">Workspace Settings</A>
									</DL><p>
									<DT><H3 FOLDED>vim</H3>
									<DL><p>
										<DT><H3 FOLDED>configuration</H3>
										<DL><p>
											<DT><A HREF="http://cream.sourceforge.net/home.html">Cream :: a modern configuration</A>
											<DT><A HREF="https://stackoverflow.com/questions/40576522/enable-vi-mouse-wheel-scrolling-using-bash-on-ubuntu-on-windows-10/40715383">Set mouse</A>
										</DL><p>
										<DT><H3 FOLDED>syntax highlighting</H3>
										<DL><p>
											<DT><A HREF="https://vim.fandom.com/wiki/Forcing_Syntax_Coloring_for_files_with_odd_extensions">Forcing Syntax Coloring</A>
											<DT><A HREF="https://www.cyberciti.biz/faq/turn-on-or-off-color-syntax-highlighting-in-vi-or-vim/">Turn On or Off Color Syntax Highlighting</A>
											<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/syntax.html">Huge Vim documentation: syntax</A>
											<DT><A HREF="https://vi.stackexchange.com/questions/5780/list-known-filetypes">List known filetypes</A>
										</DL><p>
										<DT><H3 FOLDED>shortcuts</H3>
										<DL><p>
											<DT><A HREF="http://www.keyxl.com/aaa8263/290/VIM-keyboard-shortcuts.htm">78 Keyboard Shortcuts for VIM</A>
										</DL><p>
										<DT><H3 FOLDED>cheatsheet</H3>
										<DL><p>
											<DT><A HREF="https://www.worldtimzone.com/res/vi.html">Short cheatsheet</A>
											<DT><A HREF="https://vim.rtorr.com/">Medium Vim Cheat Sheet</A>
										</DL><p>
										<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/help.html">Vim documentation: help</A>
										<DT><A HREF="https://unix.stackexchange.com/questions/161821/how-can-i-delete-all-lines-in-a-file-using-vi">Delete all lines frong a given file</A>
										<DT><H3 FOLDED>vim-configuration</H3>
										<DL><p>
											<DT><A HREF="http://cream.sourceforge.net/home.html">Cream :: a modern configuration</A>
											<DT><A HREF="https://stackoverflow.com/questions/40576522/enable-vi-mouse-wheel-scrolling-using-bash-on-ubuntu-on-windows-10/40715383">Set mouse</A>
										</DL><p>
										<DT><H3 FOLDED>vim-syntax-highlighting</H3>
										<DL><p>
											<DT><A HREF="https://vim.fandom.com/wiki/Forcing_Syntax_Coloring_for_files_with_odd_extensions">Forcing Syntax Coloring</A>
											<DT><A HREF="https://www.cyberciti.biz/faq/turn-on-or-off-color-syntax-highlighting-in-vi-or-vim/">Turn On or Off Color Syntax Highlighting</A>
											<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/syntax.html">Huge Vim documentation: syntax</A>
											<DT><A HREF="https://vi.stackexchange.com/questions/5780/list-known-filetypes">List known filetypes</A>
										</DL><p>
										<DT><H3 FOLDED>vim-shortcuts</H3>
										<DL><p>
											<DT><A HREF="http://www.keyxl.com/aaa8263/290/VIM-keyboard-shortcuts.htm">78 Keyboard Shortcuts for VIM</A>
										</DL><p>
										<DT><H3 FOLDED>vim-cheatsheet</H3>
										<DL><p>
											<DT><A HREF="https://www.worldtimzone.com/res/vi.html">Short cheatsheet</A>
											<DT><A HREF="https://vim.rtorr.com/">Medium Vim Cheat Sheet</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>neovim</H3>
									<DL><p>
										<DT><H3 FOLDED>config</H3>
										<DL><p>
											<DT><A HREF="https://medium.com/life-at-moka/step-up-your-game-with-neovim-62ba814166d7">Step Up Your Game with Neovim</A>
											<DT><A HREF="https://github.com/tpope/vim-fugitive">A Git wrapper</A>
											<DT><A HREF="https://github.com/preservim/nerdtree">A tree explorer plugin</A>
											<DT><A HREF="https://vi.stackexchange.com/questions/514/how-do-i-change-the-current-splits-width-and-height">change the current split's width and height</A>
											<DT><A HREF="https://www.youtube.com/watch?v=ZEFXeRIFvN0&t=404s">Command Line: Neovim Installation and Configuration</A>
											<DT><A HREF="https://github.com/junegunn/vim-plug">junegunn/vim-plug: Minimalist Vim Plugin Manager</A>
											<DT><A HREF="https://github.com/morhetz/gruvbox">morhetz/gruvbox: Retro groove color scheme for Vim</A>
											<DT><A HREF="https://www.youtube.com/watch?v=iagjeLuxnMs">My Entire Neovim + Tmux Workflow As A DevOps Engineer On MacOS - YouTube</A>
										</DL><p>
										<DT><H3 FOLDED>installation</H3>
										<DL><p>
											<DT><A HREF="https://github.com/neovim/neovim">neovim/neovim: Vim-fork focused on extensibility and usability</A>
											<DT><A HREF="https://formulae.brew.sh/formula/neovim#default">neovim — Homebrew Formulae</A>
										</DL><p>
										<DT><H3 FOLDED>neovim-config</H3>
										<DL><p>
											<DT><A HREF="https://medium.com/life-at-moka/step-up-your-game-with-neovim-62ba814166d7">Step Up Your Game with Neovim</A>
											<DT><A HREF="https://github.com/tpope/vim-fugitive">A Git wrapper</A>
											<DT><A HREF="https://github.com/preservim/nerdtree">A tree explorer plugin</A>
											<DT><A HREF="https://vi.stackexchange.com/questions/514/how-do-i-change-the-current-splits-width-and-height">change the current split's width and height</A>
											<DT><A HREF="https://www.youtube.com/watch?v=ZEFXeRIFvN0&t=404s">Command Line: Neovim Installation and Configuration</A>
											<DT><A HREF="https://github.com/junegunn/vim-plug">junegunn/vim-plug: Minimalist Vim Plugin Manager</A>
											<DT><A HREF="https://github.com/morhetz/gruvbox">morhetz/gruvbox: Retro groove color scheme for Vim</A>
											<DT><A HREF="https://www.youtube.com/watch?v=iagjeLuxnMs">My Entire Neovim + Tmux Workflow As A DevOps Engineer On MacOS - YouTube</A>
										</DL><p>
										<DT><H3 FOLDED>neovim-installation</H3>
										<DL><p>
											<DT><A HREF="https://github.com/neovim/neovim">neovim/neovim: Vim-fork focused on extensibility and usability</A>
											<DT><A HREF="https://formulae.brew.sh/formula/neovim#default">neovim — Homebrew Formulae</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>sublime text</H3>
									<DL><p>
										<DT><A HREF="https://packagecontrol.io/installation">Installation - Package Control</A>
										<DT><A HREF="https://packagecontrol.io/docs/usage">Usage - Package Control</A>
									</DL><p>
									<DT><A HREF="https://support.typora.io/Spellcheck/">Spellcheck - Typora Support</A>
									<DT><A HREF="https://support.typora.io/Line-Break/">Whitespace and Line Breaks - Typora Support</A>
								</DL><p>
								<DT><H3 FOLDED>linux-disk</H3>
								<DL><p>
									<DT><A HREF="https://man7.org/linux/man-pages/man8/lsblk.8.html">lsblk(8) - Linux manual page</A>
								</DL><p>
								<DT><H3 FOLDED>linux-filesystem</H3>
								<DL><p>
									<DT><H3 FOLDED>distributed-filesystem</H3>
									<DL><p>
										<DT><H3 FOLDED>lustre</H3>
										<DL><p>
											<DT><A HREF="https://www.lustre.org/">Lustre: distributed filesystem</A>
										</DL><p>
										<DT><H3 FOLDED>CEPH</H3>
										<DL><p>
											<DT><A HREF="https://claude.ai/chat/afe41c86-8070-4868-86f3-bef7cecdb346">Optimizing Conda Environments in Multi-Node GPU Clusters - Claude</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>/dev/shm</H3>
									<DL><p>
										<DT><A HREF="https://chatgpt.com/c/67eac713-14fc-800c-bd41-3794b8d7df63">Shm vs Ceph for IO</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>linux-memory</H3>
								<DL><p>
									<DT><H3 FOLDED>LWN.net: What every programmer should know about memory</H3>
									<DL><p>
										<DT><A HREF="https://lwn.net/Articles/250967/">1. Introduction</A>
										<DT><A HREF="https://lwn.net/Articles/252125/">2. CPU caches</A>
										<DT><A HREF="https://lwn.net/Articles/253361/">3. Virtual Memory</A>
										<DT><A HREF="https://lwn.net/Articles/254445/">4. NUMA support</A>
										<DT><A HREF="https://lwn.net/Articles/255364/">5. What programmers can do - cache optimization</A>
										<DT><A HREF="https://lwn.net/Articles/256433/">6. What programmers can do - multi-threaded optimizations</A>
										<DT><A HREF="https://lwn.net/Articles/257209/">7. Memory performance tools</A>
										<DT><A HREF="https://lwn.net/Articles/258154/">8. Future technologies</A>
										<DT><A HREF="https://lwn.net/Articles/258188/">9. Examples and Benchmark Programs: matmul</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=L79vSP8yV2g">Base [4]: Memory Management - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=vHWiDx_l4V0&t=2436s">What's a Memory Allocator Anyway? - Benjamin Feng - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=Sxx0TDPT0t8">🌿 Week 17 Hobby Kernel Dev in C, x86: Physical memory allocator pt2 🌿 - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=OFo5FPt7KYA">ZRAM | A Misunderstood Linux Tool... - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>linux-networking</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=ck4WvYM9V4c">Linux Networking: How The Kernel Handles A TCP Connection - YouTube</A>
									<DT><A HREF="https://chatgpt.com/c/e57a5ce7-00de-486a-a8c2-c95db90cf417">non-blocking flat tree</A>
									<DT><A HREF="https://www.geeksforgeeks.org/iotop-command-in-linux-with-examples/">Comando iotop en Linux con ejemplos - GeeksforGeeks</A>
									<DT><A HREF="https://www.net.in.tum.de/fileadmin/TUM/NET/NET-2024-04-1/NET-2024-04-1_16.pdf">The Path of a Packet Through the Linux Kernel</A>
								</DL><p>
								<DT><H3 FOLDED>dkms</H3>
								<DL><p>
									<DT><A HREF="https://askubuntu.com/questions/408605/what-does-dkms-do-how-do-i-use-it">What does DKMS do? How do I use it? - Ask Ubuntu</A>
									<DT><A HREF="https://github.com/dell/dkms">dell/dkms: Dynamic Kernel Module Support</A>
								</DL><p>
								<DT><H3 FOLDED>shell</H3>
								<DL><p>
									<DT><H3 FOLDED>shell-languages</H3>
									<DL><p>
										<DT><H3 FOLDED>Bash</H3>
										<DL><p>
											<DT><H3 FOLDED>bash-error-handling</H3>
											<DL><p>
												<DT><A HREF="https://chatgpt.com/c/73e6a395-5da9-448f-b956-56c47bc98735">set -Eeuo</A>
											</DL><p>
											<DT><H3 FOLDED>bash-set</H3>
											<DL><p>
												<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html">The Set Builtin (Bash Reference Manual)</A>
											</DL><p>
											<DT><H3 FOLDED>bash-completions</H3>
											<DL><p>
												<DT><A HREF="https://itnext.io/programmable-completion-for-bash-on-macos-f81a0103080b">Programmable Completion for Bash</A>
												<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/Programmable-Completion.html">Programmable Completion (Bash Manual)</A>
											</DL><p>
											<DT><H3 FOLDED>bash-colors</H3>
											<DL><p>
												<DT><A HREF="https://www.cyberciti.biz/faq/how-to-turn-on-or-off-colors-in-bash/">How To Turn On/Off Colors For ls Command</A>
												<DT><A HREF="https://linoxide.com/how-tos/change-linux-shell-prompt-with-different-colors/">How to Change Bash Shell Prompt Colorful</A>
												<DT><A HREF="https://www.cyberciti.biz/faq/apple-mac-osx-terminal-color-ls-output-option/">How to enable colorized output for ls command</A>
											</DL><p>
											<DT><H3 FOLDED>bash-script</H3>
											<DL><p>
												<DT><A HREF="https://linuxize.com/post/bash-comments/">Writing Comments</A>
												<DT><A HREF="https://stackoverflow.com/questions/17066250/create-timestamp-variable-in-bash-script">Create timestamp variable in bash script - Stack Overflow</A>
											</DL><p>
											<DT><A HREF="https://itnext.io/programmable-completion-for-bash-on-macos-f81a0103080b">Programmable Completion for Bash</A>
											<DT><A HREF="https://itnext.io/upgrading-bash-on-macos-7138bd1066ba">Upgrading</A>
											<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/Programmable-Completion.html">Programmable Completion (Bash Manual)</A>
											<DT><A HREF="https://www.cyberciti.biz/faq/how-to-turn-on-or-off-colors-in-bash/">How To Turn On/Off Colors For ls Command</A>
											<DT><A HREF="https://linoxide.com/how-tos/change-linux-shell-prompt-with-different-colors/">How to Change Bash Shell Prompt Colorful</A>
											<DT><A HREF="https://tldp.org/LDP/abs/html/">Advanced Bash-Scripting Guide</A>
											<DT><A HREF="https://stackoverflow.com/questions/589149/bash-script-to-cd-to-directory-with-spaces-in-pathname">cd to directory with WHITESPACES in pathname</A>
											<DT><A HREF="https://stackoverflow.com/questions/5130968/how-can-i-copy-the-output-of-a-command-directly-into-my-clipboard">copy the output of a command into clipboard</A>
											<DT><A HREF="https://www.cyberciti.biz/faq/apple-mac-osx-terminal-color-ls-output-option/">How to enable colorized output for ls command</A>
											<DT><A HREF="https://linuxize.com/post/bash-comments/">Writing Comments</A>
											<DT><A HREF="https://stackoverflow.com/questions/17066250/create-timestamp-variable-in-bash-script">Create timestamp variable in bash script - Stack Overflow</A>
											<DT><A HREF="https://github.com/dylanaraps/pure-bash-bible">dylanaraps/pure-bash-bible: 📖 A collection of pure bash alternatives to external processes.</A>
											<DT><A HREF="https://github.com/dylanaraps/pure-bash-bible">dylanaraps/pure-bash-bible</A>
											<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/">Top (Bash Reference Manual)</A>
											<DT><A HREF="https://stackoverflow.com/questions/44222883/run-a-shell-script-and-immediately-background-it-however-keep-the-ability-to-in">&amp; - Run a shell script and immediately background</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>shell-editors</H3>
									<DL><p>
										<DT><H3 FOLDED>VS Code</H3>
										<DL><p>
											<DT><A HREF="https://vscode-docs.readthedocs.io/en/latest/customization/themes/">Themes - vscode-docs</A>
										</DL><p>
										<DT><H3 FOLDED>vim</H3>
										<DL><p>
											<DT><H3 FOLDED>configuration</H3>
											<DL><p>
												<DT><A HREF="http://cream.sourceforge.net/home.html">Cream :: a modern configuration</A>
												<DT><A HREF="https://stackoverflow.com/questions/40576522/enable-vi-mouse-wheel-scrolling-using-bash-on-ubuntu-on-windows-10/40715383">Set mouse</A>
											</DL><p>
											<DT><H3 FOLDED>syntax highlighting</H3>
											<DL><p>
												<DT><A HREF="https://vim.fandom.com/wiki/Forcing_Syntax_Coloring_for_files_with_odd_extensions">Forcing Syntax Coloring</A>
												<DT><A HREF="https://www.cyberciti.biz/faq/turn-on-or-off-color-syntax-highlighting-in-vi-or-vim/">Turn On or Off Color Syntax Highlighting</A>
												<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/syntax.html">Huge Vim documentation: syntax</A>
												<DT><A HREF="https://vi.stackexchange.com/questions/5780/list-known-filetypes">List known filetypes</A>
											</DL><p>
											<DT><H3 FOLDED>shortcuts</H3>
											<DL><p>
												<DT><A HREF="http://www.keyxl.com/aaa8263/290/VIM-keyboard-shortcuts.htm">78 Keyboard Shortcuts for VIM</A>
											</DL><p>
											<DT><H3 FOLDED>cheatsheet</H3>
											<DL><p>
												<DT><A HREF="https://www.worldtimzone.com/res/vi.html">Short cheatsheet</A>
												<DT><A HREF="https://vim.rtorr.com/">Medium Vim Cheat Sheet</A>
											</DL><p>
											<DT><A HREF="http://vimdoc.sourceforge.net/htmldoc/help.html">Vim documentation: help</A>
											<DT><A HREF="https://unix.stackexchange.com/questions/161821/how-can-i-delete-all-lines-in-a-file-using-vi">Delete all lines frong a given file</A>
										</DL><p>
										<DT><H3 FOLDED>neovim</H3>
										<DL><p>
											<DT><H3 FOLDED>config</H3>
											<DL><p>
												<DT><A HREF="https://medium.com/life-at-moka/step-up-your-game-with-neovim-62ba814166d7">Step Up Your Game with Neovim</A>
												<DT><A HREF="https://github.com/tpope/vim-fugitive">A Git wrapper</A>
												<DT><A HREF="https://github.com/preservim/nerdtree">A tree explorer plugin</A>
												<DT><A HREF="https://vi.stackexchange.com/questions/514/how-do-i-change-the-current-splits-width-and-height">change the current split's width and height</A>
												<DT><A HREF="https://www.youtube.com/watch?v=ZEFXeRIFvN0&t=404s">Command Line: Neovim Installation and Configuration</A>
												<DT><A HREF="https://github.com/junegunn/vim-plug">junegunn/vim-plug: Minimalist Vim Plugin Manager</A>
												<DT><A HREF="https://github.com/morhetz/gruvbox">morhetz/gruvbox: Retro groove color scheme for Vim</A>
												<DT><A HREF="https://www.youtube.com/watch?v=iagjeLuxnMs">My Entire Neovim + Tmux Workflow As A DevOps Engineer On MacOS - YouTube</A>
											</DL><p>
											<DT><H3 FOLDED>installation</H3>
											<DL><p>
												<DT><A HREF="https://github.com/neovim/neovim">neovim/neovim: Vim-fork focused on extensibility and usability</A>
												<DT><A HREF="https://formulae.brew.sh/formula/neovim#default">neovim — Homebrew Formulae</A>
											</DL><p>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>tmux</H3>
									<DL><p>
										<DT><A HREF="https://wiki.archlinux.org/index.php/Tmux#256_colors">tmux - ArchWiki</A>
										<DT><A HREF="https://stackoverflow.com/questions/18760281/how-to-increase-scrollback-buffer-size-in-tmux">scroll - How to increase scrollback buffer size?</A>
										<DT><A HREF="https://mutelight.org/practical-tmux">Practical Tmux</A>
										<DT><A HREF="https://www.runrails.com/tmux/scrolling-in-tmux/#:~:text=2%20%2D%20With%20keyboard%20shortcuts,around%20with%20the%20arrow%20keys.&text=Just%20as%20with%20the%20mouse,to%20add%20them%20to%20your%20.&text=Note%20that%20you%20have%20to,bound%20as%20the%20command%20key.">How to scroll back in Tmux</A>
									</DL><p>
									<DT><H3 FOLDED>fuzzy finder</H3>
									<DL><p>
										<DT><A HREF="https://github.com/junegunn/fzf#using-homebrew">junegunn/fzf: A command-line fuzzy finder</A>
										<DT><A HREF="https://www.youtube.com/watch?v=qgG5Jhi_Els">Vim universe. fzf - command line fuzzy finder - YouTube</A>
										<DT><A HREF="https://www.youtube.com/watch?v=hJzqEAf2U4I">I made the greatest tool ever! | tmux &amp; cht.sh &amp; fzf - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>shell-profiles</H3>
									<DL><p>
										<DT><A HREF="https://unix.stackexchange.com/questions/476593/when-should-i-use-bashrc-and-when-profile">When should I use .bashrc and when .profile?</A>
									</DL><p>
									<DT><H3 FOLDED>grep</H3>
									<DL><p>
										<DT><H3 FOLDED>ripgrep</H3>
										<DL><p>
											<DT><A HREF="https://github.com/BurntSushi/ripgrep">ripgrep: ripgrep recursively searches directories for a regex pattern</A>
											<DT><A HREF="https://github.com/BurntSushi/ripgrep/blob/master/GUIDE.md">ripgrep/GUIDE.md</A>
											<DT><A HREF="https://github.com/BurntSushi/ripgrep/issues/623">hidden files to be searched by default · Issue #623 · BurntSushi/ripgrep</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>man &amp; info &amp; help</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=vnBCnd2L0dY">Linux Experts Read 'info' Pages (NOT 'man' pages) - YouTube</A>
										<DT><A HREF="https://man7.org/linux/man-pages/man1/ldd.1.html">ldd - print shared object dependencies</A>
									</DL><p>
									<DT><H3 FOLDED>file manager</H3>
									<DL><p>
										<DT><H3 FOLDED>yazi</H3>
										<DL><p>
											<DT><A HREF="https://github.com/sxyazi/yazi">sxyazi/yazi: 💥 Blazing fast terminal file manager written in Rust, based on async I/O.</A>
											<DT><A HREF="https://yazi-rs.github.io/docs/configuration/overview">Configuration | Yazi</A>
											<DT><A HREF="https://yazi-rs.github.io/docs/quick-start/">shell wrapper: change dir</A>
											<DT><A HREF="https://github.com/sxyazi/yazi/issues/801">yazi-adaptor not compiling · Issue #801 · sxyazi/yazi</A>
										</DL><p>
									</DL><p>
									<DT><H3 FOLDED>shell-pipes</H3>
									<DL><p>
										<DT><A HREF="https://github.com/akavel/up">akavel/up: Ultimate Plumber is a tool for writing Linux pipes with instant live preview</A>
									</DL><p>
									<DT><H3 FOLDED>shell-trash</H3>
									<DL><p>
										<DT><A HREF="https://github.com/andreafrancia/trash-cli">andreafrancia/trash-cli: Command line interface to the freedesktop.org trashcan.</A>
									</DL><p>
									<DT><H3 FOLDED>std-out-err</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tiangolo/typer/blob/04eba6b70203287176d2823753513226bf778872/docs/tutorial/printing.md#standard-output-and-standard-error">Standard Output and Standard Error</A>
									</DL><p>
									<DT><H3 FOLDED>shell-automation</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=9KAp_zWeI34">Automating Everything in Linux with ENTR! - YouTube</A>
										<DT><A HREF="https://eradman.com/entrproject/">entr(1)</A>
										<DT><A HREF="https://github.com/tinygrad/tinyos">tinygrad/tinyos</A>
									</DL><p>
									<DT><H3 FOLDED>linux-monitoring</H3>
									<DL><p>
										<DT><H3 FOLDED>disk-usage</H3>
										<DL><p>
											<DT><A HREF="https://github.com/Canop/dysk">Canop/dysk: A linux utility to get information on filesystems, like df but better</A>
											<DT><A HREF="https://github.com/Byron/dua-cli">Byron/dua-cli: View disk space usage and delete unwanted data, fast.</A>
											<DT><A HREF="https://github.com/KSXGitHub/parallel-disk-usage">KSXGitHub/parallel-disk-usage: Highly parallelized, blazing fast directory tree analyzer</A>
											<DT><A HREF="https://github.com/bootandy/dust">bootandy/dust: A more intuitive version of du in rust</A>
										</DL><p>
										<DT><A HREF="https://github.com/aristocratos/btop">aristocratos/btop: A monitor of resources</A>
										<DT><A HREF="https://github.com/Syllo/nvtop">Syllo/nvtop: GPU &amp; Accelerator process monitoring for AMD, Apple, Huawei, Intel, NVIDIA and Qualcomm</A>
										<DT><A HREF="https://github.com/XuehaiPan/nvitop">XuehaiPan/nvitop: An interactive NVIDIA-GPU process viewer and beyond, the one-stop solution for GPU process management.</A>
										<DT><A HREF="https://github.com/htop-dev/htop">htop-dev/htop: htop - an interactive process viewer</A>
										<DT><A HREF="https://github.com/ClementTsang/bottom">ClementTsang/bottom: Yet another cross-platform graphical process/system monitor.</A>
										<DT><A HREF="https://netflixtechblog.com/tagged/observability">Observability – Netflix TechBlog</A>
										<DT><A HREF="https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55">Linux Performance Analysis in 60,000 Milliseconds | by Netflix Technology Blog | Netflix TechBlog</A>
									</DL><p>
									<DT><A HREF="https://ss64.com/osx/">An A-Z Index of the Apple macOS command line</A>
									<DT><A HREF="https://jqlang.github.io/jq/">jq</A>
									<DT><A HREF="https://gist.github.com/sts10/daadbc2f403bdffad1b6d33aff016c0a">A curated list of command-line utilities written in Rust</A>
									<DT><A HREF="https://github.com/sharkdp/bat">bat: A cat(1) clone with wings.</A>
									<DT><A HREF="https://github.com/sxyazi/yazi">sxyazi/yazi: 💥 Blazing fast terminal file manager written in Rust, based on async I/O.</A>
									<DT><A HREF="https://github.com/Byron/dua-cli">Byron/dua-cli: View disk space usage and delete unwanted data, fast.</A>
									<DT><A HREF="https://github.com/sharkdp/fd">sharkdp/fd: A simple, fast and user-friendly alternative to 'find'</A>
									<DT><A HREF="https://github.com/Canop/dysk">Canop/dysk: A linux utility to get information on filesystems, like df but better</A>
									<DT><A HREF="https://github.com/Syllo/nvtop">Syllo/nvtop: GPU &amp; Accelerator process monitoring for AMD, Apple, Huawei, Intel, NVIDIA and Qualcomm</A>
									<DT><A HREF="https://github.com/Xfennec/progress">Xfennec/progress: Linux tool to show progress for cp, mv, dd, ... (formerly known as cv)</A>
									<DT><A HREF="https://github.com/atuinsh/atuin">atuinsh/atuin: ✨ Magical shell history</A>
									<DT><A HREF="https://github.com/facebookarchive/pcicrawler">facebookarchive/pcicrawler: pcicrawler is a Python based command line interface tool which can be used to display, filter and export information about PCI (Peripheral Component Interconnect) or PCIe buses and devices, as well as PCI topology.</A>
									<DT><A HREF="http://www.faqs.org/faqs/unix-faq/shell/shell-differences/">UNIX shell differences and how to change your shell</A>
									<DT><A HREF="https://johndjameson.com/blog/updating-your-shell-with-homebrew/">Updating Your Shell with Homebrew</A>
									<DT><A HREF="https://unix.stackexchange.com/questions/167631/finding-the-original-file-of-a-symbolic-link/167632">readlink - Finding the original file of a symbolic link</A>
									<DT><A HREF="https://ss64.com/osx/ln.html">ln Man Page - Symbolic links</A>
									<DT><A HREF="https://www.baeldung.com/linux/head-tail-commands">Head &amp; Tail</A>
									<DT><A HREF="https://explainshell.com/explain?cmd=tr+%22%3A%22+%22%5Cn%22+%3C%3C%3C%22%24PATH%22">explainshell.com - tr ":" "\n" &lt;&lt;&lt;"$PATH"</A>
									<DT><A HREF="https://explainshell.com/">explainshell.com - match command-line arguments to their help text</A>
									<DT><A HREF="https://github.com/BurntSushi/ripgrep">BurntSushi/ripgrep: ripgrep recursively searches directories for a regex pattern while respecting your gitignore</A>
									<DT><A HREF="https://github.com/chubin/cheat.sh">chubin/cheat.sh: the only cheat sheet you need</A>
									<DT><A HREF="https://man7.org/linux/man-pages/man1/dmesg.1.html">dmesg: print or control the kernel ring buffer</A>
									<DT><A HREF="https://explainshell.com/">explainshell.com</A>
									<DT><A HREF="https://github.com/Xfennec/progress">Xfennec/progress: Linux tool to show progress for cp, mv, dd</A>
									<DT><A HREF="https://jqlang.github.io/jq/">jq: JSON processor</A>
									<DT><A HREF="https://man7.org/linux/man-pages/man1/du.1.html">du(1) - Linux manual page (du -sh &lt;dir&gt;)</A>
									<DT><A HREF="https://man7.org/linux/man-pages/man1/ldd.1.html">ldd - print shared object dependencies</A>
									<DT><A HREF="https://github.com/facebookincubator/below">facebookincubator/below: A time traveling resource monitor for modern Linux systems</A>
									<DT><A HREF="https://github.com/aristocratos/btop">aristocratos/btop: A monitor of resources</A>
									<DT><A HREF="https://github.com/robbmcleod/cpufeature">robbmcleod/cpufeature: Python module for detection of CPU features</A>
									<DT><A HREF="https://github.com/ClementTsang/bottom">ClementTsang/bottom: Yet another cross-platform graphical process/system monitor.</A>
									<DT><A HREF="https://github.com/zellij-org/zellij">zellij-org/zellij: A terminal workspace with batteries included</A>
									<DT><A HREF="https://github.com/Orange-OpenSource/hurl">Orange-OpenSource/hurl: Hurl, run and test HTTP requests with plain text.</A>
								</DL><p>
								<DT><H3 FOLDED>std-out-err</H3>
								<DL><p>
									<DT><A HREF="https://github.com/tiangolo/typer/blob/04eba6b70203287176d2823753513226bf778872/docs/tutorial/printing.md#standard-output-and-standard-error">Standard Output and Standard Error</A>
								</DL><p>
								<DT><H3 FOLDED>UNIX</H3>
								<DL><p>
									<DT><H3 FOLDED>learning</H3>
									<DL><p>
										<DT><H3 FOLDED>$PATH</H3>
										<DL><p>
											<DT><A HREF="https://askubuntu.com/questions/600018/how-to-display-path-as-one-directory-per-line">How to display $PATH as one directory per line?</A>
											<DT><A HREF="https://kb.iu.edu/d/acar">Set or modify a path in Unix</A>
										</DL><p>
										<DT><A HREF="https://kb.iu.edu/d/acmq">current values of all environment variables and functions</A>
										<DT><A HREF="https://tldp.org/LDP/Linux-Filesystem-Hierarchy/html/usr.html">/usr</A>
										<DT><A HREF="https://refspecs.linuxfoundation.org/FHS_3.0/fhs/ch04s09.html">4.9. /usr/local : Local hierarchy</A>
										<DT><A HREF="https://blog.balthazar-rouberol.com/text-processing-in-the-shell">Text processing in the shell</A>
									</DL><p>
									<DT><H3 FOLDED>building</H3>
									<DL><p>
										<DT><A HREF="http://www.lemis.com/grog/Documentation/Lions/book.pdf">Book: Commentary on the sixth edition</A>
									</DL><p>
									<DT><A HREF="https://kb.iu.edu/d/affo">What do some common Unix file extensions mean?</A>
								</DL><p>
								<DT><H3 FOLDED>GNU</H3>
								<DL><p>
									<DT><A HREF="https://melpa.org/#/">MELPA</A>
									<DT><A HREF="https://www.gnu.org/software/emacs/download.html#nonfree">Emacs download</A>
									<DT><A HREF="https://www.gnu.org/software/emacs/refcards/pdf/refcard.pdf">GNU Emacs Reference Card</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/GNU_Autotools">GNU Autotools - Wikipedia</A>
									<DT><A HREF="https://www.gnu.org/software/bash/manual/html_node/">Top (Bash Reference Manual)</A>
								</DL><p>
								<DT><H3 FOLDED>ssh</H3>
								<DL><p>
									<DT><H3 FOLDED>clusterssh</H3>
									<DL><p>
										<DT><A HREF="https://github.com/duncs/clusterssh">duncs/clusterssh: Cluster SSH - Cluster Admin Via SSH</A>
										<DT><A HREF="https://www.hackplayers.com/2016/11/clusterssh-o-como-manejar-varios-ssh.html">ClusterSSH o cómo manejar varias sesiones SSH de forma concurrente</A>
									</DL><p>
									<DT><A HREF="https://docs.digitalocean.com/products/droplets/how-to/add-ssh-keys/create-with-openssh/">How to Create SSH Keys with OpenSSH on MacOS or Linux :: DigitalOcean Documentation</A>
									<DT><A HREF="https://docs.digitalocean.com/products/droplets/how-to/connect-with-ssh/">How to Connect to Droplets with SSH :: DigitalOcean Documentation</A>
									<DT><A HREF="https://linuxize.com/post/using-the-ssh-config-file/">Using the SSH Config File | Linuxize</A>
									<DT><A HREF="https://www.openssh.com/">OpenSSH</A>
									<DT><A HREF="https://www.ssh.com/academy/ssh/keygen">What is ssh-keygen &amp; How to Use It to Generate a New SSH Key?</A>
									<DT><A HREF="https://www.ssh.com/academy/ssh/copy-id">What is ssh-copy-id? How ssh-copy-id works?</A>
									<DT><A HREF="https://aistudio.google.com/app/prompts/1v0BYzOpN_R8BUbFEIYQsoqZV1QWHRE1c?pli=1">torch.distributed torchrun | Google AI Studio</A>
								</DL><p>
								<DT><H3 FOLDED>linux-checkpoint</H3>
								<DL><p>
									<DT><A HREF="https://github.com/checkpoint-restore/criu">checkpoint-restore/criu: Checkpoint/Restore tool</A>
								</DL><p>
								<DT><H3 FOLDED>linux-kernel-versions</H3>
								<DL><p>
									<DT><H3 FOLDED>linux-kernel-6.10</H3>
									<DL><p>
										<DT><A HREF="https://www.youtube.com/watch?v=mphUtoCD5SU">Kernel 6.10 | Locked &amp; Optimized - YouTube</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://ss64.com/osx/">An A-Z Index of the Apple macOS command line</A>
								<DT><A HREF="https://learnubuntu.com/list-users/">How to List Users in Ubuntu Command Line</A>
								<DT><A HREF="https://unix.stackexchange.com/questions/20979/how-do-i-list-all-installed-programs">application - How do I list all installed programs?</A>
								<DT><A HREF="https://unix.stackexchange.com/questions/561263/how-to-get-a-list-of-which-packages-were-installed-with-apt-get-by-a-user-and-no">How to get a list of which packages were installed with apt-get by a user and not by dependencies?</A>
								<DT><A HREF="https://www.cyberciti.biz/faq/create-a-user-account-on-ubuntu-linux/">How to create a user account on Ubuntu Linux - nixCraft</A>
								<DT><A HREF="https://www.youtube.com/watch?v=vnBCnd2L0dY">Linux Experts Read 'info' Pages (NOT 'man' pages) - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=d0gS5TXarXc&t=80s">Signals. I spent 2 years to understand this part</A>
								<DT><A HREF="https://github.com/facebookincubator/below?tab=readme-ov-file">facebookincubator/below: A time traveling resource monitor for modern Linux systems</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Ia5jyz8sOCM">No really, how does Linux run executables?</A>
							</DL><p>
							<DT><H3 FOLDED>macOS</H3>
							<DL><p>
								<DT><H3 FOLDED>M1</H3>
								<DL><p>
									<DT><A HREF="https://support.apple.com/en-us/HT211861">Rosetta 2 overview</A>
									<DT><A HREF="https://medium.com/mkdir-awesome/how-to-install-x86-64-homebrew-packages-on-apple-m1-macbook-54ba295230f">How to Install x86_64 Homebrew Packages on Apple M1 MacBook</A>
									<DT><A HREF="https://news.ycombinator.com/item?id=25132217">Run x86 Apps (including homebrew) in the Terminal on Apple Silicon | Hacker News</A>
									<DT><A HREF="https://wickedchicken.github.io/post/macos-nix-setup/">MacOS Nix Setup (an alternative to Homebrew)</A>
									<DT><H3 FOLDED>monitoring</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tlkh/asitop">tlkh/asitop: Perf monitoring CLI tool for Apple Silicon (nvtop)</A>
									</DL><p>
									<DT><H3 FOLDED>m1-monitoring</H3>
									<DL><p>
										<DT><A HREF="https://github.com/tlkh/asitop">tlkh/asitop: Perf monitoring CLI tool for Apple Silicon (nvtop)</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>finder</H3>
								<DL><p>
									<DT><A HREF="https://support.apple.com/guide/mac-help/see-the-devices-connected-to-your-mac-mchlp1039/mac">Finder options: Connected devices</A>
									<DT><A HREF="https://mac4u.tech/how-to-see-hidden-files-and-folders-in-macos-big-sur/">How to see hidden files and folders in Apple macOS Big Sur</A>
								</DL><p>
								<DT><H3 FOLDED>xterm</H3>
								<DL><p>
									<DT><A HREF="https://zsh.sourceforge.io/Doc/Release/Prompt-Expansion.html#Prompt-Expansion">zsh: 13 Prompt Expansion</A>
									<DT><A HREF="https://www.makeuseof.com/customize-zsh-prompt-macos-terminal/">PS1</A>
									<DT><A HREF="https://www.cyberciti.biz/faq/change-default-shell-to-bash-on-macos-catalina/">chsh (Catalina/BigSur)</A>
								</DL><p>
								<DT><A HREF="https://support.apple.com/en-hk/guide/terminal/apdc6c1077b-5d5d-4d35-9c19-60f2397b2369/2.10/mac/10.15">launchd - manage daemons and agents</A>
								<DT><A HREF="https://discussions.apple.com/thread/250756110">show hidden files and folders</A>
								<DT><A HREF="https://www.cyberciti.biz/faq/apple-mac-osx-terminal-color-ls-output-option/">How to enable colorized output for ls command</A>
								<DT><A HREF="https://embeddedartistry.com/blog/2017/02/24/installing-llvm-clang-on-osx/">Installing LLVM/Clang on OS X</A>
								<DT><A HREF="https://ss64.com/osx/">An A-Z Index of the Apple macOS command line</A>
							</DL><p>
							<DT><H3 FOLDED>build from source</H3>
							<DL><p>
								<DT><A HREF="https://serverfault.com/questions/46381/learning-to-compile-things-from-source-on-unix-linux-osx">compile things from source</A>
								<DT><A HREF="https://zig.news/kristoff/compile-a-c-c-project-with-zig-368j">Compile a C/C++ Project with Zig - Zig NEWS</A>
								<DT><A HREF="https://www.youtube.com/watch?v=wFlyUzUVFhw">Zig Build System &amp; How to Build Software From Source • Andrew Kelley • GOTO 2023 - YouTube</A>
								<DT><H3 FOLDED>make</H3>
								<DL><p>
									<DT><H3 FOLDED>debug</H3>
									<DL><p>
										<DT><A HREF="https://chat.openai.com/c/926b55b6-8995-4593-aff8-3150238c1398">.PHONY: debug \n debug: @echo</A>
									</DL><p>
									<DT><H3 FOLDED>makefile</H3>
									<DL><p>
										<DT><A HREF="https://github.com/pytorch/pytorch/blob/main/docker.Makefile">pytorch/docker.Makefile at main · pytorch/pytorch</A>
									</DL><p>
									<DT><A HREF="https://chat.openai.com/c/926b55b6-8995-4593-aff8-3150238c1398">.PHONY: debug \n debug: @echo</A>
									<DT><H3 FOLDED>untitled folder</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>make-debug</H3>
									<DL><p>
										<DT><A HREF="https://chat.openai.com/c/926b55b6-8995-4593-aff8-3150238c1398">.PHONY: debug \n debug: @echo</A>
									</DL><p>
									<DT><H3 FOLDED>cmake</H3>
									<DL><p>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>from-source-linker-(ld)</H3>
								<DL><p>
									<DT><A HREF="https://stackoverflow.com/questions/9688200/difference-between-shared-objects-so-static-libraries-a-and-dlls-so">Difference shared objects (.so), static libraries (.a), and DLL's (.so)</A>
								</DL><p>
								<DT><H3 FOLDED>shared-libraries</H3>
								<DL><p>
									<DT><A HREF="https://www.hpc.dtu.dk/?page_id=1180">LD_LIBRARY_PATH – or: How to get yourself into trouble!</A>
									<DT><A HREF="https://askubuntu.com/questions/1461829/set-ld-library-path-for-with-symlinks">environment variables - Set LD_LIBRARY_PATH for with symlinks - Ask Ubuntu</A>
									<DT><A HREF="https://help.ubuntu.com/community/EnvironmentVariables#File-location_related_variables">EnvironmentVariables - Community Help Wiki</A>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=wFlyUzUVFhw">Zig Build System &amp; How to Build Software From Source • Andrew Kelley</A>
							</DL><p>
							<DT><H3 FOLDED>build-system</H3>
							<DL><p>
								<DT><H3 FOLDED>bazel</H3>
								<DL><p>
									<DT><H3 FOLDED>bazel-installation</H3>
									<DL><p>
										<DT><A HREF="https://blog.bazel.build/2018/08/22/bazel-homebrew.html">Bazel in Homebrew - Bazel</A>
										<DT><A HREF="https://docs.bazel.build/versions/main/install-os-x.html#install-with-installer-mac-os-x">Installing Bazel on macOS - Bazel main</A>
										<DT><A HREF="https://github.com/bazelbuild/bazelisk">bazelbuild/bazelisk: A user-friendly launcher for Bazel.</A>
										<DT><A HREF="https://formulae.brew.sh/formula/bazelisk">bazelisk — Homebrew Formulae</A>
									</DL><p>
									<DT><H3 FOLDED>bazel-examples</H3>
									<DL><p>
										<DT><A HREF="https://github.com/bazelbuild/examples/tree/main/bzlmod/01-depend_on_bazel_module">examples/bzlmod/01-depend_on_bazel_module</A>
										<DT><A HREF="https://github.com/grpc/grpc/blob/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/protos/BUILD">grpc/examples/protos/BUILD "helloworld"</A>
									</DL><p>
									<DT><H3 FOLDED>bazel-v7</H3>
									<DL><p>
										<DT><A HREF="https://github.com/bazelbuild/bazel/issues/18958">enable_bzlmod: Flip the default value of `--enable_bzlmod` to true · Issue #18958 · bazelbuild/bazel</A>
									</DL><p>
									<DT><H3 FOLDED>bazel-modules</H3>
									<DL><p>
										<DT><H3 FOLDED>bazel-dependency-graph</H3>
										<DL><p>
											<DT><A HREF="https://bazel.build/tutorials/cpp-dependency">Review the dependency graph  |  Bazel</A>
										</DL><p>
										<DT><A HREF="https://bazel.build/external/module">Bazel modules</A>
										<DT><A HREF="https://bazel.build/rules/lib/globals/module">MODULE.bazel files  |  Bazel</A>
										<DT><A HREF="https://docs.google.com/document/d/1moQfNcEIttsk6vYanNKIy3ZuK53hQUFq1b1r0rmsYVg/edit">Bazel External Dependencies Overhaul</A>
										<DT><A HREF="https://github.com/bazelbuild/bazel/issues/18958">enable_bzlmod: Flip the default value of `--enable_bzlmod` to true · Issue #18958 · bazelbuild/bazel</A>
									</DL><p>
									<DT><H3 FOLDED>bazel-starlark</H3>
									<DL><p>
										<DT><A HREF="https://github.com/google/starlark-go">google/starlark-go: Starlark in Go: the Starlark configuration language, implemented in Go</A>
										<DT><A HREF="https://github.com/search?q=repo%3Azml%2Fzml++language%3AStarlark&type=code">zml building examples</A>
									</DL><p>
									<DT><H3 FOLDED>bazel-rules</H3>
									<DL><p>
										<DT><H3 FOLDED>cpp</H3>
										<DL><p>
											<DT><A HREF="https://github.com/bazelbuild/examples/blob/main/cpp-tutorial/stage1/main/hello-world.cc">examples/cpp-tutorial/stage1/main/hello-world.cc at main · bazelbuild/examples</A>
											<DT><A HREF="https://docs.bazel.build/versions/1.0.0/tutorial/cpp.html#understand-the-build-file">Build Tutorial - C++ - Bazel 1.0.0</A>
											<DT><A HREF="https://bazel.build/tutorials/cpp-dependency">Review the dependency graph  |  Bazel</A>
										</DL><p>
										<DT><H3 FOLDED>rust</H3>
										<DL><p>
											<DT><A HREF="https://news.ycombinator.com/item?id=24847702">Not exactly true. Google uses Bazel for Rust code</A>
											<DT><A HREF="https://github.com/bazelbuild/rules_rust/tree/main/examples/ffi">rules_rust</A>
										</DL><p>
									</DL><p>
									<DT><A HREF="https://github.com/OasisDigital/bazelcon-2019">OasisDigital/bazelcon-2019: Bazel examples for Bazel Boot Camp</A>
									<DT><A HREF="https://www.google.com/search?q=papers+on+Googles+build+infrastructure+bazel&client=safari&rls=en&sxsrf=AOaemvLW8YwdatuWueomwqIEZZgYu-7-Cw%3A1642121078072&ei=dsfgYYvjA9DSa727vMgG&ved=0ahUKEwiL9qTcgbD1AhVQ6RoKHb0dD2kQ4dUDCA0&uact=5&oq=papers+on+Googles+build+infrastructure+bazel&gs_lcp=Cgdnd3Mtd2l6EAMyBwghEAoQoAE6BwgAEEcQsAM6BQghEKsCSgQIQRgASgQIRhgAUJsDWOEJYLELaAFwAHgAgAGVAYgB5gWSAQMwLjaYAQCgAQHIAQjAAQE&sclient=gws-wiz">papers on Googles build infrastructure bazel - Google Search</A>
									<DT><A HREF="https://oasisdigital.com/class/bazel">Building with Bazel | Oasis Digital</A>
									<DT><A HREF="https://www.buildbuddy.io/">BuildBuddy</A>
									<DT><A HREF="https://bazel.build/reference/glossary#dependency">Bazel Glossary</A>
									<DT><A HREF="https://docs.google.com/document/d/1moQfNcEIttsk6vYanNKIy3ZuK53hQUFq1b1r0rmsYVg/edit">Bazel External Dependencies Overhaul - Documentos de Google</A>
									<DT><A HREF="https://www.visualcapitalist.com/wp-content/uploads/2017/02/1276_lines_of_code_sep2015_fb.png">1276_lines_of_code_sep2015_fb.png 1.276×4.670 pixels</A>
									<DT><A HREF="https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext#:~:text=Google%2DScale&text=The%20Google%20codebase%20includes%20approximately,Google's%20entire%2018%2Dyear%20existence">Why Google Stores Billions of Lines of Code in a Single Repository (monorepo)</A>
									<DT><A HREF="https://github.com/jetstack/cert-manager/pull/4184/files/611bac67cf6dc8b58130b9cb43486d4ddda1b387#diff-23618edb36c476683f5a0c453d4d8fdb296b4c9fc3b21163dbd05b0c7d038708">code example in Go</A>
									<DT><A HREF="https://docs.bazel.build/versions/1.0.0/tutorial/cpp.html#understand-the-build-file">Build Tutorial - C++ - Bazel 1.0.0</A>
									<DT><A HREF="https://www.reddit.com/r/bazel/">bazel subreddit</A>
									<DT><A HREF="https://blog.bazel.build/2024/12/09/bazel-8-release.html">Bazel 8.0 LTS - Bazel</A>
								</DL><p>
								<DT><H3 FOLDED>facebook-getdeps</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebook/proxygen/blob/8e338869dc41dee6d197d5cf627482f0a9159bb8/build/fbcode_builder/README.md">proxygen/build/fbcode_builder/README.md at 8e338869dc41dee6d197d5cf627482f0a9159bb8 · facebook/proxygen</A>
								</DL><p>
								<DT><H3 FOLDED>language-interoperability</H3>
								<DL><p>
									<DT><A HREF="https://github.com/kriasoft/relay-starter-kit/blob/c985206/src/utils/password_hash.cc">NAPI C++ example</A>
									<DT><A HREF="https://koistya.medium.com/how-to-call-c-c-code-from-node-js-86a773033892">How to call C/C++ code from Node.js</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/Interface_description_language">Interface description language (IDL) - Wikipedia</A>
									<DT><A HREF="https://cxx.rs/">Rust ❤️ C++</A>
								</DL><p>
								<DT><H3 FOLDED>build-system-papers</H3>
								<DL><p>
									<DT><A HREF="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45880.pdf">google CI</A>
									<DT><A HREF="https://www.microsoft.com/en-us/research/uploads/prod/2018/03/build-systems-final.pdf">Build Systems à la Carte</A>
								</DL><p>
								<DT><A HREF="https://scons.org/">SCons: A software construction tool - SCons</A>
								<DT><A HREF="https://docs.engflow.com/docs/re.html">Remote Execution Service | Documentation</A>
								<DT><A HREF="https://github.com/web3infra-foundation/mega">web3infra-foundation/mega: Mega is an unofficial open source implementation of Google Piper.</A>
								<DT><A HREF="https://buck2.build/">Buck2 build system website | Buck2</A>
								<DT><A HREF="https://github.com/facebook/buck2/tree/main/examples/hello_world">buck2/examples/hello_world at main · facebook/buck2</A>
								<DT><A HREF="https://github.com/facebook/buck2/blob/main/examples/hello_world/BUCK">buck2/examples/hello_world/BUCK at main · facebook/buck2</A>
								<DT><A HREF="https://github.com/facebook/buck2">facebook/buck2: Build system, successor to Buck</A>
							</DL><p>
							<DT><H3 FOLDED>os-file-formats</H3>
							<DL><p>
								<DT><H3 FOLDED>parquet</H3>
								<DL><p>
								</DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=bISBNVtXZ6M">Nimble, A New Columnar File Format - Yoav Helfman, Meta - YouTube</A>
								<DT><A HREF="https://github.com/facebookexternal/nimble">facebookexternal/nimble: New file format for storage of large columnar datasets.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-ZtyRgD1c40">Velox and Composable Data Management - Pedro Pedreira, Meta - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=bISBNVtXZ6M&t=1169s">Nimble, A New Columnar File Format - Yoav Helfman, Meta - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>os-emulation</H3>
							<DL><p>
								<DT><A HREF="https://fms.komkon.org/EMUL8/HOWTO.html">HOWTO: Writing a Computer Emulator</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Kq849CpGd88">QEMU Performance</A>
							</DL><p>
							<DT><H3 FOLDED>security</H3>
							<DL><p>
								<DT><A HREF="https://arstechnica.com/security/2024/07/secure-boot-is-completely-compromised-on-200-models-from-5-big-device-makers/">Secure Boot is completely broken on 200+ models from 5 big device makers | Ars Technica</A>
								<DT><A HREF="https://www.youtube.com/watch?v=eKpv5xjSqs0">this is getting ridiculous... - YouTube</A>
							</DL><p>
							<DT><A HREF="https://en.wikipedia.org/wiki/POSIX">POSIX</A>
							<DT><A HREF="https://www.youtube.com/watch?v=d0gS5TXarXc&t=80s">Signals. I spent 2 years to understand this part</A>
							<DT><A HREF="https://github.com/sdmg15/Best-websites-a-programmer-should-visit?tab=readme-ov-file">sdmg15/Best-websites-a-programmer-should-visit: :link: Some useful websites for programmers.</A>
						</DL><p>
						<DT><H3 FOLDED>ml-sys-networking</H3>
						<DL><p>
							<DT><H3 FOLDED>IP</H3>
							<DL><p>
								<DT><A HREF="https://lwn.net/Articles/960913/">So you think you understand IP fragmentation? [LWN.net]</A>
							</DL><p>
							<DT><H3 FOLDED>RDMA</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=kDJHA7TNtDk">NSDI '23 - Empowering Azure Storage with RDMA - YouTube</A>
								<DT><A HREF="https://le.qun.ch/en/blog/2024/12/25/libfabric-efa-0-intro/">Harnessing 3200 Gbps Network: A Journey with RDMA, EFA, and libfabric</A>
							</DL><p>
							<DT><A HREF="https://www.linkedin.com/pulse/network-acceleration-genai-workloads-sharada-yeluri-g8xbc/?trackingId=4J97SZQpTwenJBkn4Hr6nA%3D%3D">(1) In Network Acceleration for AI/ML Workloads | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/pulse/gpu-fabrics-genai-workloads-sharada-yeluri-j8ghc/?trackingId=5mCxOsDBSoG07wlKbx6o2g%3D%3D">(1) GPU Fabrics for GenAI Workloads | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/pulse/network-acceleration-genai-workloads-sharada-yeluri-g8xbc/?trackingId=JYRFyD33QkmHY5eoE24rCg%3D%3D">(1) In Network Acceleration for AI/ML Workloads | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/pulse/llm-inference-hwsw-optimizations-sharada-yeluri-wfdyc/?trackingId=IAo88qPrQLeVto9JLwix6w%3D%3D">(1) LLM Inference - HW/SW Optimizations | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/pulse/tearing-down-memory-wall-sharada-yeluri/?trackingId=dn9NrSPARD2R%2BUQ9KN6CXQ%3D%3D">(1) Tearing Down the Memory Wall | LinkedIn</A>
							<DT><A HREF="https://www.youtube.com/watch?v=ck4WvYM9V4c">Linux Networking: How The Kernel Handles A TCP Connection - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Systems Performance: Enterprise and the Cloud</H3>
						<DL><p>
							<DT><H3 FOLDED>bpf</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/bpftoolkit">Netflix-Skunkworks/bpftoolkit</A>
								<DT><A HREF="https://www.youtube.com/watch?v=7pmXdG8-7WU&list=PLZT-fvrVTVfPVLkSYPlGo6Vwp9uze9H2q&index=5">Netflix talks about Extended BPF - A new software type</A>
								<DT><A HREF="https://manpages.ubuntu.com/manpages/focal/en/man8/biosnoop.bt.8.html">biosnoop.bt - Block I/O tracing tool, showing per I/O latency</A>
								<DT><A HREF="https://github.com/aquasecurity/tracee">aquasecurity/tracee: Linux Runtime Security and Forensics using eBPF</A>
								<DT><A HREF="https://github.com/Netflix/bpftop">Netflix/bpftop: bpftop provides a dynamic real-time view of running eBPF programs. It displays the average runtime, events per second, and estimated total CPU % for each program.</A>
							</DL><p>
							<DT><H3 FOLDED>WSPerfLab</H3>
							<DL><p>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/WSPerfLab">Netflix-Skunkworks/WSPerfLab: Project for testing web-service implementations.</A>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/WSPerfLab/blob/master/test-results/RxNetty_vs_Tomcat_April2015.pdf">RxNetty vs Tomcat Performance Results</A>
							</DL><p>
							<DT><H3 FOLDED>Service Capacity Modeling</H3>
							<DL><p>
								<DT><H3 FOLDED>AWS</H3>
								<DL><p>
									<DT><A HREF="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html">Amazon EBS volume types - Amazon EBS</A>
								</DL><p>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/service-capacity-modeling">Netflix-Skunkworks/service-capacity-modeling</A>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/service-capacity-modeling/blob/main/notebooks/demo.ipynb">service-capacity-modeling/notebooks/demo.ipynb at main · Netflix-Skunkworks/service-capacity-modeling</A>
							</DL><p>
							<DT><H3 FOLDED>sys-perftools</H3>
							<DL><p>
								<DT><H3 FOLDED>Performance Monitoring Counters (PMC)</H3>
								<DL><p>
									<DT><A HREF="https://github.com/brendangregg/pmc-cloud-tools">brendangregg/pmc-cloud-tools: PMC (Performance Monitoring Counter) tools for the cloud</A>
									<DT><A HREF="https://www.brendangregg.com/blog/2017-05-04/the-pmcs-of-ec2.html">The PMCs of EC2: Measuring IPC</A>
									<DT><A HREF="https://twitter.com/__tinygrad__/status/1768408123826721045">tinybox shows:</A>
								</DL><p>
								<DT><A HREF="https://manpages.ubuntu.com/manpages/focal/en/man8/biosnoop.bt.8.html">biosnoop.bt - Block I/O tracing tool, showing per I/O latency</A>
								<DT><A HREF="https://github.com/hoytech/vmtouch">hoytech/vmtouch: Portable file system cache diagnostics and control</A>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/corepipe">Netflix-Skunkworks/corepipe: perform a coredump of a running process</A>
								<DT><A HREF="https://github.com/Netflix-Skunkworks/diffy">netflix/diffy: digital forensics and incident response (DFIR)</A>
								<DT><A HREF="https://www.redhat.com/sysadmin/stop-using-telnet-test-port">Stop using Telnet to test ports | Red Hat</A>
								<DT><A HREF="https://manpages.ubuntu.com/manpages/focal/en/man8/iotop.8.html">Ubuntu Manpage: iotop - simple top-like I/O monitor</A>
								<DT><A HREF="https://github.com/wagoodman/dive">wagoodman/dive: A tool for exploring each layer in a docker image</A>
								<DT><A HREF="https://justine.lol/rusage/">Portable rusage command: Report resource usage statistics when launching cmd programms</A>
								<DT><A HREF="https://chat.openai.com/c/6f34e9bd-60ae-4c54-848e-479b93d3e219">df -h: list disk usage of volumes or mount points</A>
								<DT><A HREF="https://mariusschulz.com/blog/fast-searching-with-ripgrep">Fast Searching with ripgrep</A>
								<DT><A HREF="https://github.com/facebookincubator/below">facebookincubator/below: A time traveling resource monitor for modern Linux systems</A>
							</DL><p>
							<DT><H3 FOLDED>observability</H3>
							<DL><p>
								<DT><A HREF="https://netflixtechblog.com/tagged/observability">Observability – Netflix TechBlog</A>
								<DT><A HREF="https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55">Linux Performance Analysis in 60,000 Milliseconds | by Netflix Technology Blog | Netflix TechBlog</A>
							</DL><p>
							<DT><H3 FOLDED>linux-perf</H3>
							<DL><p>
								<DT><A HREF="https://github.com/NAThompson/performance_tuning_tutorial">NAThompson/performance_tuning_tutorial: Performance Tuning Tutorial given at Oak Ridge National Laboratory</A>
							</DL><p>
							<DT><H3 FOLDED>cluster-health</H3>
							<DL><p>
								<DT><A HREF="https://github.com/imbue-ai/cluster-health">imbue-ai/cluster-health</A>
								<DT><A HREF="https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55">Linux Performance Analysis in 60,000 Milliseconds | by Netflix Technology Blog | Netflix TechBlog</A>
								<DT><A HREF="https://imbue.com/research/70b-infrastructure/">From bare metal to a 70B model: infrastructure set-up and scripts - imbue</A>
								<DT><A HREF="https://github.com/HanGuo97/cluster-utils/blob/master/server.py">cluster-utils/server.py at master · HanGuo97/cluster-utils</A>
							</DL><p>
							<DT><A HREF="https://www.brendangregg.com/">Performance Monitoring Counters (PMC)</A>
							<DT><A HREF="https://www.brendangregg.com/linuxperf.html">Linux Performance</A>
							<DT><A HREF="https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55">Linux Performance Analysis in 60,000 Milliseconds | by Netflix Technology Blog | Netflix TechBlog</A>
							<DT><A HREF="https://github.com/quickwit-oss/quickwit">quickwit-oss/quickwit: Cloud-native search engine for observability. An open-source alternative to Datadog, Elasticsearch, Loki, and Tempo.</A>
							<DT><A HREF="https://www.brendangregg.com/Slides/SBSRE_perf_meetup_aug2017.pdf">Netflix Performance Meetup (2017)</A>
							<DT><A HREF="https://github.com/facebook/watchman">facebook/watchman: Watches files and records, or triggers actions, when they change.</A>
							<DT><A HREF="https://www.linkedin.com/feed/update/urn:li:activity:7189636365620240384/">Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services</A>
						</DL><p>
						<DT><H3 FOLDED>Distributed Systems</H3>
						<DL><p>
							<DT><H3 FOLDED>Large-Scale Information Retrieval Systems</H3>
							<DL><p>
								<DT><A HREF="https://videolectures.net/wsdm09_dean_cblirs/">Videolectures</A>
								<DT><A HREF="https://x.com/arvidkahl/status/1803441726767366420">(1) Arvid Kahl en X: "Full-text search on 500GB+ of data is keeping me awake at night. MySQL's full-text index just can't handle this. Often takes minutes. And Meilisearch, as fast as it is, is hard to wrangle to get it to get only precise results. Anyone here experienced with search at this size?" / X</A>
								<DT><A HREF="https://www.postgresql.org/docs/current/textsearch.html">PostgreSQL: Documentation: 16: Chapter 12. Full Text Search</A>
								<DT><A HREF="https://hive.apache.org/">Apache Hive</A>
								<DT><A HREF="https://prestodb.io/">Presto: Free, Open-Source SQL Query Engine for any Data</A>
								<DT><A HREF="https://github.com/quickwit-oss/tantivy">quickwit-oss/tantivy: Tantivy is a full-text search engine library inspired by Apache Lucene and written in Rust</A>
							</DL><p>
							<DT><H3 FOLDED>distributed-sys-lectures</H3>
							<DL><p>
								<DT><H3 FOLDED>6.824</H3>
								<DL><p>
									<DT><A HREF="https://pdos.csail.mit.edu/6.824/index.html">6.824 Home Page: Spring 2021</A>
									<DT><A HREF="https://mit-6824-notes.book.triplez.cn/lectures/1-introduction/">Lecture 1: Introduction | MIT 6.824 Notebook</A>
									<DT><A HREF="https://wizardforcel.gitbooks.io/distributed-systems-engineering-lecture-notes/content/l01-intro.html">Introduction · Distributed Systems Engineering Lecture notes (MIT 6.824)</A>
									<DT><A HREF="https://www.youtube.com/watch?v=3HunZHHrk1Q">Remote Procedure Call (RPC)</A>
									<DT><A HREF="https://learncs.me/mit/6.824">NOTES</A>
									<DT><H3 FOLDED>source code</H3>
									<DL><p>
										<DT><A HREF="https://github.com/glodknife/MIT-6.824-2016/blob/46100bcf7b276b8824a8aac979e7c4e12b217038/src/kvpaxos/server.go#L58">MIT-6.824-2016/server.go</A>
										<DT><A HREF="https://github.com/glodknife/MIT-6.824-2016/tree/master/src">MIT-6.824-2016/src at master · glodknife/MIT-6.824-2016</A>
									</DL><p>
									<DT><H3 FOLDED>labs</H3>
									<DL><p>
									</DL><p>
									<DT><H3 FOLDED>questions</H3>
									<DL><p>
									</DL><p>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>distributed-sys-key-value store</H3>
							<DL><p>
								<DT><H3 FOLDED>REST</H3>
								<DL><p>
									<DT><A HREF="https://medium.com/extend/what-is-rest-a-simple-explanation-for-beginners-part-1-introduction-b4a072f8740f">What is REST?</A>
									<DT><A HREF="https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm">Representational State Transfer (REST)</A>
									<DT><A HREF="https://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm">Architectural Styles and the Design of Network-based Software Architectures</A>
								</DL><p>
								<DT><H3 FOLDED>Seaweedfs</H3>
								<DL><p>
									<DT><A HREF="https://github.com/chrislusf/seaweedfs">chrislusf/seaweedfs</A>
								</DL><p>
								<DT><H3 FOLDED>Bigtable</H3>
								<DL><p>
									<DT><A HREF="https://cloud.google.com/bigtable">Bigtable |  Google Cloud</A>
								</DL><p>
								<DT><H3 FOLDED>kv-python</H3>
								<DL><p>
									<DT><H3 FOLDED>uWSGI</H3>
									<DL><p>
										<DT><A HREF="https://flask.palletsprojects.com/en/1.1.x/deploying/uwsgi/">uWSGI — Flask Documentation (1.1.x)</A>
										<DT><A HREF="https://uwsgi-docs.readthedocs.io/en/latest/">uWSGI — uWSGI 2.0 documentation</A>
										<DT><A HREF="https://uwsgi-docs.readthedocs.io/en/latest/Download.html">Getting uWSGI — uWSGI 2.0 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>Flask</H3>
									<DL><p>
										<DT><A HREF="https://flask.palletsprojects.com/en/1.1.x/">Welcome to Flask — Flask Documentation (1.1.x)</A>
									</DL><p>
									<DT><H3 FOLDED>Plyvel</H3>
									<DL><p>
										<DT><A HREF="https://plyvel.readthedocs.io/en/latest/">Plyvel — Plyvel 1.3.0 documentation</A>
										<DT><A HREF="https://readthedocs.org/projects/plyvel/downloads/pdf/latest/">plyvel docs</A>
										<DT><A HREF="https://githubmemory.com/repo/wbolster/plyvel/issues/114?page=2">installation failed on mac osx 10.15 - plyvel</A>
										<DT><A HREF="https://github.com/wbolster/plyvel/issues/13">IOError: IO error: lock · Issue #13 · wbolster/plyvel</A>
									</DL><p>
									<DT><H3 FOLDED>Gunicorn</H3>
									<DL><p>
										<DT><A HREF="https://gunicorn.org/">Gunicorn - Python WSGI HTTP Server for UNIX</A>
									</DL><p>
									<DT><A HREF="https://docs.python.org/3/library/json.html">json — JSON encoder and decoder</A>
									<DT><A HREF="https://stackoverflow.com/questions/6269765/what-does-the-b-character-do-in-front-of-a-string-literal">'b' character in front of a string literal? -- Python</A>
									<DT><A HREF="https://ipython.org/install.html">Installing IPython — IPython</A>
									<DT><A HREF="https://stackoverflow.com/questions/56658553/module-not-found-error-in-vs-code-despite-the-fact-that-i-installed-it">Module not found error in VS code despite the fact that I installed it - Stack Overflow</A>
									<DT><H3 FOLDED>tempfile</H3>
									<DL><p>
										<DT><A HREF="https://docs.python.org/3/library/tempfile.html">tempfile — Generate temporary files and directories</A>
										<DT><A HREF="http://www.chiark.greenend.org.uk/doc/python-pyxattr/html/module.html">Interface to extended filesystem attributes</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>leveldb</H3>
								<DL><p>
									<DT><A HREF="https://github.com/bagonyi/homebrew-formulae">bagonyi/homebrew-formulae -- 1.22 Stable</A>
									<DT><A HREF="https://github.com/google/leveldb/blob/master/doc/index.md#concurrency">google/leveldb</A>
									<DT><A HREF="https://github.com/google/leveldb/blob/master/doc/index.md#concurrency">leveldb/index.md at master · google/leveldb</A>
									<DT><A HREF="https://en.wikipedia.org/wiki/LevelDB">LevelDB - Wikipedia</A>
								</DL><p>
								<DT><A HREF="http://localhost:3000/">‎localhost:3000</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cAFjZ1gXBxc&t=6222s">Geohot : A distributed key value store in under 1000 lines</A>
								<DT><A HREF="https://httpstatuses.com/">HTTP Status Codes — httpstatuses.com</A>
								<DT><A HREF="https://stackoverflow.com/questions/11583562/how-to-kill-a-process-running-on-particular-port-in-linux">lsof - How to kill a process running on particular port in Linux?</A>
								<DT><A HREF="https://www.youtube.com/watch?v=v44bAtgEEUw">Building Redis From Scratch In Golang</A>
								<DT><A HREF="https://github.com/danielealbano/cachegrand">danielealbano/cachegrand: cachegrand - a modern data ingestion, processing and serving platform built for today's hardware</A>
							</DL><p>
							<DT><H3 FOLDED>distributed-sys-queue-systems</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=yKPVZgA6Oe0">What makes Kafka special? | System Design - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>distributed-sys-actor-model</H3>
							<DL><p>
								<DT><A HREF="http://dist-prog-book.com/chapter/3/message-passing.html#akka">Message Passing and the Actor Model</A>
							</DL><p>
							<DT><H3 FOLDED>blockchain</H3>
							<DL><p>
								<DT><A HREF="https://bitcoin.org/bitcoin.pdf">Bitcoin whitepaper</A>
							</DL><p>
							<DT><H3 FOLDED>databases</H3>
							<DL><p>
								<DT><H3 FOLDED>db-relational</H3>
								<DL><p>
									<DT><H3 FOLDED>postgreSQL</H3>
									<DL><p>
										<DT><A HREF="https://stackoverflow.com/questions/17633422/psql-fatal-database-user-does-not-exist">psql: FATAL: database "&lt;user&gt;" does not exist</A>
										<DT><A HREF="https://www.postgresql.org/docs/9.2/app-psql.html">PostgreSQL: Documentation: 9.2: psql</A>
										<DT><A HREF="https://alvinalexander.com/blog/post/postgresql/log-in-postgresql-database/">How to log into a Postgresql database</A>
										<DT><A HREF="https://stackoverflow.com/questions/26040493/how-to-show-data-in-a-table-by-using-psql-command-line-interface">How to show data in a table by using psql</A>
										<DT><A HREF="https://stackoverflow.com/questions/48180282/how-to-populate-a-heroku-postgresql-database-with-a-sql-file">How to populate a heroku postgresql database with a sql file</A>
										<DT><A HREF="https://stackoverflow.com/questions/21307786/load-sql-file-data-in-to-single-table-in-postgres">Load sql file data in to single table in postgres</A>
										<DT><A HREF="https://dba.stackexchange.com/questions/46125/why-does-postgres-generate-an-already-used-pk-value">already used PK value?</A>
										<DT><A HREF="https://stackoverflow.com/questions/64354458/named-parameter-not-bound-date-format-native-query-in-spring-boot">Named parameter not bound :</A>
										<DT><A HREF="https://devcenter.heroku.com/articles/heroku-cli-commands">Heroku CLI Commands | Heroku Dev Center</A>
										<DT><A HREF="https://www.postgresql.org/docs/9.5/sql-insert.html">PostgreSQL: Documentation: 9.5: INSERT</A>
										<DT><A HREF="https://ketansingh.me/posts/how-postgres-stores-rows/">How Postgres Stores Rows</A>
									</DL><p>
									<DT><A HREF="https://www.w3schools.com/sql/sql_groupby.asp">w3schools sql clauses</A>
									<DT><A HREF="https://wiki.eclipse.org/EclipseLink/UserGuide/JPA/Basic_JPA_Development/Querying/JPQL">JPA</A>
									<DT><A HREF="https://en.wikibooks.org/wiki/Structured_Query_Language/Create_Table">Structured Query Language/Create Table</A>
									<DT><A HREF="https://www.w3schools.com/sql/">SQL Tutorial</A>
									<DT><A HREF="https://www.pearson.com/us/higher-education/program/Garcia-Molina-Database-Systems-The-Complete-Book-2nd-Edition/PGM2429.html">Database Systems: The Complete Book, 2nd Edition | Pearson</A>
								</DL><p>
								<DT><A HREF="https://dbdb.io/db/leveldb">Database of Databases - LevelDB</A>
								<DT><A HREF="https://web.stanford.edu/class/cs245/">CS 245: Principles of Data-Intensive Systems (Winter 2021)</A>
								<DT><A HREF="https://towardsdatascience.com/10-quick-sql-tips-after-writing-daily-in-sql-for-3-years-37bdba0637d0">10 Quick SQL Tips After Writing Daily in SQL for 3 Years | Mar, 2022</A>
								<DT><A HREF="https://engineering.fb.com/2022/04/26/developer-tools/sql-notebooks/">SQL Notebooks: Combining the power of Jupyter and SQL editors for data analytics - Engineering at Meta</A>
								<DT><A HREF="http://airbnb.io/airpal/">A web-based query execution tool built on top of Facebook's PrestoDB.</A>
								<DT><A HREF="https://www.youtube.com/watch?v=-1VGwmFKKf8">The Billion Rows Challenge in Rust - an intro to Rust for data engineering - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=zSn8il5Mo5s">The Rise of Oracle, SQL and the Relational Database - YouTube</A>
							</DL><p>
							<DT><H3 FOLDED>canary vs shadow traffic</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/shorts/ckYBLT1VG-M">Shadow Trafficking Explained - YouTube</A>
								<DT><A HREF="https://x.com/basetenco/status/1852025274734719061">Baseten en X: "We’re excited to launch canary deployments on Baseten! 🐦 🎉 Canary deployments let you gradually shift traffic to new model deployments, with seamless rollback if needed. Learn more in our launch blog 👇 https://t.co/brn8pDqlai" / X</A>
							</DL><p>
							<DT><A HREF="https://jsonformatter.curiousconcept.com/#">JSON Formatter &amp; Validator</A>
							<DT><A HREF="https://www.youtube.com/watch?v=VWrpnT8rwVY">"Functional distributed systems beyond request/response" by Melinda Lu - YouTube</A>
							<DT><A HREF="https://canvas.mit.edu/courses/11164">6.829 Computer Networks</A>
							<DT><A HREF="https://raft.github.io/">Raft Consensus Algorithm</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/%CE%A0-calculus">π-calculus - Wikipedia</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/Petri_net">Petri net - Wikipedia</A>
							<DT><A HREF="http://dist-prog-book.com/chapter/3/message-passing.html#akka">Message Passing and the Actor Model</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Vch4BWUVzMM">SimTigerBeetle (Director's Cut!): Distributed systems failure simulator</A>
							<DT><A HREF="https://www.youtube.com/watch?v=DfLKd3WlTuw">Programming Distributed Systems - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=m3HwXlQPCEU">Testing a Single-Node, Single Threaded, Distributed System Written in 1985 By Will Wilson - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>Web</H3>
						<DL><p>
							<DT><H3 FOLDED>web scrapping</H3>
							<DL><p>
								<DT><A HREF="https://www.selenium.dev/documentation/webdriver/locating_elements/">Locating elements | Selenium</A>
								<DT><A HREF="https://stackoverflow.com/questions/61308799/unable-to-locate-elements-in-selenium-python">Unable to locate elements in Selenium (Python)</A>
								<DT><A HREF="https://www.guru99.com/accessing-forms-in-webdriver.html#2">Selenium Form WebElement: TextBox, Button, sendkeys(), click()</A>
								<DT><A HREF="https://www.selenium.dev/documentation/webdriver/keyboard/">Keyboard | Selenium</A>
								<DT><A HREF="https://www.w3.org/TR/webdriver/#keyboard-actions">WebDriver</A>
								<DT><A HREF="https://stackoverflow.com/questions/2632677/python-integer-incrementing-with">i++</A>
								<DT><A HREF="https://towardsdatascience.com/web-automation-nightmares-6-tricks-to-overcome-them-4241089953e3">Web Automation Nightmares: 6 Tricks to Overcome Them | by Aw Khai Sheng | Towards Data Science</A>
								<DT><A HREF="https://stackoverflow.com/questions/33225947/can-a-website-detect-when-you-are-using-selenium-with-chromedriver/33403473#33403473">selenium fingerprint</A>
							</DL><p>
							<DT><H3 FOLDED>architecture</H3>
							<DL><p>
								<DT><A HREF="https://orkhanscience.medium.com/software-architecture-patterns-5-mins-read-e9e3c8eb47d2">Software Architecture Patterns: 4 minute read</A>
								<DT><A HREF="https://www.infoq.com/">InfoQ: Software Development News, Trends &amp; Best Practices</A>
								<DT><A HREF="https://en.wikipedia.org/wiki/Pattern-Oriented_Software_Architecture">Pattern-Oriented Software Architecture - Wikipedia</A>
								<DT><A HREF="https://www.amazon.com/Pattern-Oriented-Software-Architecture-System-Patterns/dp/0471958697">Pattern-Oriented Software Architecture Volume 1: A System of Patterns: Buschmann, Frank, Meunier, Regine, Rohnert, Hans, Sommerlad, Peter, Stal, Michael, Michael Stal</A>
								<DT><A HREF="https://www.amazon.com/Principles-Computer-System-Design-Introduction/dp/0123749573">Principles of Computer System Design: An Introduction: Saltzer, Jerome, Kaashoek, M. Frans: 9780123749574: Amazon.com: Books</A>
								<DT><A HREF="https://medium.com/@olgamitroshyna/software-architecture-i-wish-i-had-known-about-this-earlier-4df43eae57db">Software Architecture: I wish I had known about this earlier...</A>
							</DL><p>
							<DT><H3 FOLDED>dynamic-web-page</H3>
							<DL><p>
								<DT><H3 FOLDED>JavaScript</H3>
								<DL><p>
									<DT><A HREF="https://www.awwwards.com/websites/">Website Design Inspiration</A>
									<DT><A HREF="https://github.com/nhn/tui.image-editor#powerful-filter-function">Full-featured photo image editor using canvas</A>
									<DT><A HREF="http://vanilla-js.com/">Vanilla JS</A>
									<DT><A HREF="https://create.editorx.com/html/editor/web/renderer/new?metaSiteId=0c39fce3-65b4-4514-a1f0-81618914e347&siteId=2dd00095-350d-41e3-8997-5af977b6a6b2">Editor X</A>
									<DT><A HREF="https://getbootstrap.com/">Bootstrap</A>
									<DT><A HREF="https://css-tricks.com/seamless-responsive-photo-grid/">https://css-tricks.com/seamless-responsive-photo-grid/</A>
									<DT><A HREF="https://codepen.io/ibrahima92/full/zYYqqbZ">Image gallery</A>
									<DT><A HREF="https://codepen.io/marcobiedermann/full/vYYyVzK">CSS Grid Gallery</A>
									<DT><A HREF="https://www.freecodecamp.org/news/how-to-deploy-a-static-website-for-free-in-only-3-minutes-with-google-drive/">How to deploy a static website</A>
									<DT><A HREF="https://codepen.io/topics/gsap">CodePen Topics</A>
								</DL><p>
								<DT><H3 FOLDED>Maven</H3>
								<DL><p>
									<DT><A HREF="https://maven.apache.org/run.html">Maven – Running Apache Maven</A>
									<DT><A HREF="https://maven.apache.org/pom.html#Dependency_Management">POM Reference</A>
									<DT><A HREF="http://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html">Guide to installing 3rd party JARs</A>
									<DT><A HREF="https://www.pegaxchange.com/2017/10/19/setup-maven-java-project-macos/">Setting up Maven on Mac OS and Creating Java Project</A>
								</DL><p>
								<DT><H3 FOLDED>Spring</H3>
								<DL><p>
									<DT><H3 FOLDED>jsp</H3>
									<DL><p>
										<DT><A HREF="https://stackoverflow.com/questions/2148658/iterate-over-elements-of-list-and-map-using-jstl-cforeach-tag">Iterate over elements of List and Map</A>
										<DT><A HREF="https://docs.oracle.com/cd/E17802_01/products/products/jsp/jstl/1.1/docs/tlddocs/index.html">All Tags / Functions</A>
										<DT><A HREF="http://www.w3big.com/jsp/jstl-core-foreach-tag.html">C: forEach</A>
									</DL><p>
									<DT><A HREF="https://start.spring.io/">Spring Initializr</A>
									<DT><A HREF="https://docs.spring.io/spring-boot/docs/current/maven-plugin/reference/html/#repackage">Spring Boot Maven Plugin Documentation</A>
									<DT><A HREF="https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#boot-features-spring-mvc-template-engines">Spring Boot Reference Documentation</A>
									<DT><A HREF="https://spring.io/guides/gs/spring-boot/">Building an Application with Spring Boot</A>
									<DT><A HREF="https://spring.io/guides/gs/serving-web-content/">Serving Web Content with Spring MVC</A>
									<DT><A HREF="https://stackoverflow.com/questions/36697663/circular-view-path-error-spring-boot/41918545">Circular View path error Spring boot</A>
									<DT><A HREF="https://spring.io/guides/tutorials/rest/">Building REST services with Spring</A>
									<DT><A HREF="https://docs.spring.io/spring-data/jpa/docs/current/reference/html/#repositories">Spring Data JPA - Reference Documentation</A>
									<DT><A HREF="https://spring.io/projects/spring-hateoas">Spring HATEOAS</A>
									<DT><A HREF="http://keenformatics.blogspot.com/2013/08/how-to-solve-json-infinite-recursion.html">How To Solve JSON infinite recursion Stackoverflow</A>
									<DT><A HREF="https://docs.spring.io/spring-boot/docs/2.1.5.RELEASE/reference/html/boot-features-testing.html">46. Testing</A>
									<DT><A HREF="https://www.javadoc.io/doc/org.mockito/mockito-core/2.23.4/org/mockito/Mockito.html">Mockito (Mockito 2.23.4 API)</A>
									<DT><A HREF="https://www.callicoder.com/hibernate-spring-boot-jpa-one-to-many-mapping-example/">JPA / Hibernate One to Many Mapping Example</A>
									<DT><A HREF="https://openclassrooms.com/en/courses/5684146-create-web-applications-efficiently-with-the-spring-boot-mvc-framework/6157116-make-services-unit-testable-using-dependency-injection">services unit testable using dependency injection</A>
									<DT><A HREF="https://www.baeldung.com/get-user-in-spring-security">Retrieve User Information in Spring Security</A>
									<DT><A HREF="https://petstore.swagger.io/#/">Swagger UI</A>
									<DT><A HREF="https://octoperf.com/blog/2018/03/08/securing-rest-api-spring-security/#testing-the-application-1">Securing a Rest API with Spring Security</A>
									<DT><A HREF="https://docs.spring.io/spring-hateoas/docs/current/api/org/springframework/hateoas/EntityModel.html">EntityModel (Spring HATEOAS 1.3.0 API)</A>
								</DL><p>
								<DT><A HREF="https://tools.ietf.org/html/rfc8288#section-1">RFC 8288 - Web Linking</A>
								<DT><A HREF="https://tools.ietf.org/html/rfc3986">RFC 3986 - Uniform Resource Identifier (URI)</A>
								<DT><A HREF="https://tools.ietf.org/html/rfc7807">RFC 7807 - Problem Details for HTTP APIs</A>
								<DT><A HREF="http://stateless.co/hal_specification.html">The Hypertext Application Language</A>
								<DT><A HREF="https://fetch.spec.whatwg.org/#origin-header">Fetch Standard</A>
								<DT><A HREF="https://www.objectdb.com/java/jpa/query/jpql/expression">JPA Query Language (JPQL / Criteria) Expression Syntax</A>
								<DT><A HREF="https://stackoverflow.com/questions/6842289/jpa-multiple-select-query">JPA multiple select query</A>
								<DT><A HREF="https://en.wikibooks.org/wiki/Java_Persistence/Querying#Query_Results">Java Persistence/Querying</A>
							</DL><p>
							<DT><H3 FOLDED>static-web-page</H3>
							<DL><p>
								<DT><A HREF="https://pages.github.com/">GitHub Pages | Websites for you and your projects, hosted directly from your GitHub repository. Just edit, push, and your changes are live.</A>
							</DL><p>
							<DT><A HREF="https://cloud.google.com/blog/products/application-development/rest-vs-rpc-what-problems-are-you-trying-to-solve-with-your-apis">REST vs. RPC: what problems are you trying to solve with your APIs? | Google Cloud Blog</A>
							<DT><A HREF="https://twitter.com/bibryam/status/1531316906581495811">2021 State of the API design - a visual report</A>
						</DL><p>
						<DT><H3 FOLDED>HPC</H3>
						<DL><p>
							<DT><H3 FOLDED>hpc-simulation</H3>
							<DL><p>
								<DT><A HREF="https://www.gameenginebook.com/">Game Engine Architecture</A>
							</DL><p>
							<DT><H3 FOLDED>hpc-observability</H3>
							<DL><p>
								<DT><A HREF="https://www.smartmontools.org/">smartmontools</A>
								<DT><A HREF="https://twitter.com/marcan42/status/1361151198921826308?lang=en">M1 SSD degradation</A>
								<DT><A HREF="https://serverfault.com/questions/480726/how-to-measure-total-writes-performed-to-ssd-in-linux">ubuntu - How to measure total writes performed to SSD in Linux? - Server Fault</A>
							</DL><p>
							<DT><A HREF="https://web.stanford.edu/class/cs245/">CS 245: Principles of Data-Intensive Systems (Winter 2021)</A>
							<DT><A HREF="https://biojulia.net/post/hardware/">What scientists must know about hardware to write fast code</A>
							<DT><A HREF="https://unias.github.io/docklet/book/en/notebook/gallery.html#linguistics-and-text-mining">Scientific Computation · GitBook</A>
							<DT><A HREF="https://willcrichton.net/notes/k-corrset/">Analyzing Data 180,000x Faster with Rust</A>
						</DL><p>
						<DT><H3 FOLDED>ml-sys-storage</H3>
						<DL><p>
							<DT><A HREF="https://github.com/louwrentius/fio-plot">louwrentius/fio-plot: Create charts from FIO storage benchmark tool output</A>
							<DT><A HREF="https://github.com/datenlord/datenlord?tab=readme-ov-file">datenlord/datenlord: DatenLord, Computing Defined Storage, an application-orientated, cloud-native distributed storage system</A>
							<DT><A HREF="https://github.com/axboe/fio">axboe/fio: Flexible I/O Tester</A>
							<DT><A HREF="https://github.com/louwrentius/showtools">louwrentius/showtools: Shows detailed disk or network device information</A>
							<DT><A HREF="https://github.com/datenlord/s3-server">datenlord/s3-server: Generic S3 server implementation</A>
						</DL><p>
						<DT><H3 FOLDED>Cloud</H3>
						<DL><p>
							<DT><H3 FOLDED>GenAI Infrastructure</H3>
							<DL><p>
								<DT><H3 FOLDED>Open Compute Project (OCP)</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=2l6gI-ksdKs">What's inside a Facebook Datacenter Open Compute Rack? - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=PrOEznqzq40">Intro to the Open Compute Project OCP - YouTube</A>
									<DT><A HREF="https://www.cio.com/article/250874/how-and-why-facebook-excels-at-data-center-efficiency.html#:~:text=Facebook's%20data%20center%20design%2C%20which,in%20hours%20rather%20than%20months.">How (and Why) Facebook Excels at Data Center Efficiency</A>
									<DT><A HREF="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/">OCP Summit 2022: Open hardware for AI infrastructure</A>
									<DT><A HREF="https://www.opencompute.org/projects/cooling-environments">Cooling Environments » Open Compute Project</A>
									<DT><A HREF="https://www.youtube.com/watch?v=2udD56VHEK0">Facebook's Open Networking Hardware - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>GPU Platform</H3>
								<DL><p>
									<DT><A HREF="https://cos.googlesource.com/cos/tools">cos/tools - Git at Google</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/cos-gpu-installer">GoogleCloudPlatform/cos-gpu-installer: Scripts to build and use a container to install GPU drivers on Container-Optimized OS images</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/hpc-toolkit">GoogleCloudPlatform/hpc-toolkit: Cloud HPC Toolkit is an open-source software offered by Google Cloud which makes it easy for customers to deploy HPC environments on Google Cloud.</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/slurm-gcp#hybrid">GoogleCloudPlatform/slurm-gcp</A>
									<DT><A HREF="https://docs.coreweave.com/coreweave-machine-learning-and-ai/get-started-with-ml-and-ai#sunk-slurm-on-kubernetes">Sunk: SLURM on Kubernetes (CoreWeave)</A>
									<DT><A HREF="https://github.com/coreweave/kubernetes-cloud">coreweave/kubernetes-cloud: Getting Started with the CoreWeave Kubernetes GPU Cloud</A>
									<DT><A HREF="https://googlecloudplatform.github.io/magic-modules/">Overview | Magic Modules</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/ml-testing-accelerators/tree/52b290a149b760b270085d4f8191188f986047ed">GoogleCloudPlatform/ml-testing-accelerators</A>
								</DL><p>
								<DT><A HREF="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">Building Meta’s GenAI Infrastructure - Engineering at Meta</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ptGDaGUXInw">Mark Russinovich | Generative AI in the Cloud: Inside Microsoft AI Innovation - YouTube</A>
								<DT><A HREF="https://arxiv.org/pdf/2311.18677.pdf">Splitwise: Efficient Generative LLM Inference Using Phase Splitting (MS)</A>
								<DT><A HREF="https://github.com/S-LoRA/S-LoRA">S-LoRA/S-LoRA: S-LoRA: Serving Thousands of Concurrent LoRA Adapters</A>
								<DT><A HREF="https://github.com/predibase/lorax">predibase/lorax: Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs</A>
								<DT><A HREF="https://github.com/sabetAI/BLoRA">sabetAI/BLoRA: batched loras</A>
								<DT><A HREF="https://arxiv.org/abs/2305.07759">[2305.07759] TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</A>
								<DT><A HREF="https://arxiv.org/abs/2306.11644">[2306.11644] Textbooks Are All You Need</A>
								<DT><A HREF="https://arxiv.org/pdf/2310.02238.pdf">Who’s Harry Potter? Approximate Unlearning in LLMs</A>
								<DT><A HREF="https://news.microsoft.com/source/features/innovation/azure-quantum-majorana-topological-qubit/">In a historic milestone, Azure Quantum demonstrates formerly elusive physics needed to build scalable topological qubits - Source</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Rk3nTUfRZmo&t=330s">What runs ChatGPT? Inside Microsoft's AI supercomputer</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ptGDaGUXInw">Mark Russinovich | Generative AI in the Cloud: Inside Microsoft AI Innovation</A>
								<DT><A HREF="https://news.microsoft.com/source/features/innovation/azure-quantum-majorana-topological-qubit/">elusive physics needed to build scalable topological qubits</A>
							</DL><p>
							<DT><H3 FOLDED>cloud-system</H3>
							<DL><p>
								<DT><A HREF="https://github.com/google/paxml">google/paxml: Pax is a Jax-based machine learning framework for training large scale models. Pax allows for advanced and fully configurable experimentation and parallelization, and has demonstrated industry leading model flop utilization rates.</A>
								<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/pax">JAX-Toolbox/rosetta/rosetta/projects/pax at main · NVIDIA/JAX-Toolbox</A>
								<DT><A HREF="https://nats.io/">NATS.io – Cloud Native, Open Source, High-performance Messaging</A>
								<DT><A HREF="https://websites.umich.edu/~amberljc/file/LLM-Systems-Basics.pdf">LLM-Systems-Basics (Jiachen Liu)</A>
							</DL><p>
							<DT><H3 FOLDED>Kubernetes</H3>
							<DL><p>
								<DT><H3 FOLDED>GKE</H3>
								<DL><p>
									<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/whats-new-with-gke-at-google-cloud-next">What’s new with GKE at Google Cloud Next</A>
								</DL><p>
								<DT><H3 FOLDED>Dynamic Resource Allocation</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna">Dynamic Resource Allocation for GPUs in Kubernetes - Google Docs</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-local</H3>
								<DL><p>
									<DT><A HREF="https://kind.sigs.k8s.io/">kind: local run</A>
								</DL><p>
								<DT><H3 FOLDED>kserve</H3>
								<DL><p>
									<DT><A HREF="https://github.com/kserve/modelmesh-serving">kserve/modelmesh-serving: Controller for ModelMesh</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-basics</H3>
								<DL><p>
									<DT><A HREF="https://hamel.dev/notes/k8s/02-Basics.html">k8s</A>
									<DT><A HREF="https://www.youtube.com/watch?v=90kZRyPcRZw">Kubernetes Deconstructed: Understanding Kubernetes by Breaking It</A>
								</DL><p>
								<DT><H3 FOLDED>Device sharig strategies</H3>
								<DL><p>
									<DT><H3 FOLDED>Time-Slicing</H3>
									<DL><p>
										<DT><A HREF="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html">Time-Slicing GPUs in Kubernetes — NVIDIA GPU Operator 23.9.2 documentation</A>
									</DL><p>
									<DT><H3 FOLDED>NVIDIA Multi-Instance GPU</H3>
									<DL><p>
										<DT><A HREF="https://www.nvidia.com/en-us/technologies/multi-instance-gpu/">Multi-Instance GPU (MIG) | NVIDIA</A>
									</DL><p>
									<DT><H3 FOLDED>CUDA Multi-Process Service (MPS)</H3>
									<DL><p>
										<DT><A HREF="https://docs.nvidia.com/deploy/mps/index.html">Multi-Process Service :: GPU Deployment and Management Documentation</A>
										<DT><A HREF="https://www.olcf.ornl.gov/wp-content/uploads/2021/06/MPS_ORNL_20210817.pdf">Introduction (slides)</A>
									</DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=Q2GuTUO170w">Sharing Is Caring: GPU Sharing and CDI in Device Plugins - Evan Lezar, NVIDIA &amp; David Porter, Google - YouTube</A>
									<DT><A HREF="https://www.youtube.com/watch?v=jbpIFCkEEng">Mastering GPU Management in Kubernetes Using the Operator Pattern (2024)</A>
								</DL><p>
								<DT><A HREF="https://cloud.google.com/anthos-config-management/docs/concepts/kustomize">Configure Kubernetes with Kustomize  |  Anthos Config Management  |  Google Cloud</A>
								<DT><A HREF="https://www.youtube.com/watch?v=06bKlSmVwIg">CNCF Live Webinar: Overcoming the GPU shortage with virtual Kubelets &amp; distributed cloud - YouTube</A>
								<DT><A HREF="https://virtual-kubelet.io/">Virtual Kubelet | Home</A>
								<DT><A HREF="https://github.com/virtual-kubelet/virtual-kubelet">virtual-kubelet/virtual-kubelet: Virtual Kubelet is an open source Kubernetes kubelet implementation.</A>
								<DT><A HREF="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet | Kubernetes</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cwiAW5TZsfo">On-Demand Systems and Scaled Training Using the JobSet API - Abdullah Gharaibeh &amp; Vanessa Sochat - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=Q2GuTUO170w">Sharing Is Caring: GPU Sharing and CDI in Device Plugins - Evan Lezar, NVIDIA &amp; David Porter, Google - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=jbpIFCkEEng">Mastering GPU Management in Kubernetes Using the Operator Pattern (2024)</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cwiAW5TZsfo">On-Demand Systems and Scaled Training Using the JobSet API</A>
							</DL><p>
							<DT><H3 FOLDED>job scheduler</H3>
							<DL><p>
								<DT><H3 FOLDED>SLURM</H3>
								<DL><p>
									<DT><H3 FOLDED>SLURM-containers</H3>
									<DL><p>
										<DT><H3 FOLDED>pyxis</H3>
										<DL><p>
											<DT><A HREF="https://github.com/NVIDIA/pyxis">NVIDIA/pyxis: Container plugin for Slurm Workload Manager</A>
										</DL><p>
										<DT><H3 FOLDED>enroot</H3>
										<DL><p>
											<DT><A HREF="https://github.com/NVIDIA/enroot">NVIDIA/enroot: A simple yet powerful tool to turn traditional container/OS images into unprivileged sandboxes.</A>
										</DL><p>
										<DT><A HREF="https://github.com/NVIDIA/pyxis">NVIDIA/pyxis: Container plugin for Slurm Workload Manager</A>
										<DT><A HREF="https://github.com/NVIDIA/enroot">NVIDIA/enroot: A simple yet powerful tool to turn traditional container/OS images into unprivileged sandboxes.</A>
										<DT><A HREF="https://chatgpt.com/c/67f94568-2c2c-800c-b9cf-a97b0777ad7c">SLURM Pyxis Enroot AI</A>
									</DL><p>
									<DT><A HREF="https://slurm.schedmd.com/">slurm.schedmd.com</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/slurm-gcp">GoogleCloudPlatform/slurm-gcp</A>
									<DT><A HREF="https://github.com/GoogleCloudPlatform/hpc-toolkit">GoogleCloudPlatform/hpc-toolkit: Cloud HPC Toolkit is an open-source software offered by Google Cloud which makes it easy for customers to deploy HPC environments on Google Cloud.</A>
									<DT><A HREF="https://github.com/facebookincubator/submitit">facebookincubator/submitit: Python 3.8+ toolbox for submitting jobs to Slurm</A>
									<DT><A HREF="https://github.com/NVIDIA/pyxis">NVIDIA/pyxis: Container plugin for Slurm Workload Manager</A>
									<DT><A HREF="https://github.com/datacrunch-research/micro/commit/d28f998269e735a540797589e7916be59c478dec">slurm guide · datacrunch-research/micro@d28f998</A>
									<DT><A HREF="https://github.com/datacrunch-research/micro/blob/main/slurm/multinode-slurm.md">micro/slurm/multinode-slurm.md at main · datacrunch-research/micro</A>
								</DL><p>
								<DT><H3 FOLDED>job-manager-kubernetes</H3>
								<DL><p>
									<DT><H3 FOLDED>GKE</H3>
									<DL><p>
										<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/whats-new-with-gke-at-google-cloud-next">What’s new with GKE at Google Cloud Next</A>
									</DL><p>
									<DT><H3 FOLDED>Dynamic Resource Allocation</H3>
									<DL><p>
										<DT><A HREF="https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna">Dynamic Resource Allocation for GPUs in Kubernetes - Google Docs</A>
										<DT><A HREF="https://www.youtube.com/watch?v=1QfShSQLsbs">Unlocking the Full Potential of GPUs for AI Workloads on Kubernetes - Kevin Klues, NVIDIA - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>local</H3>
									<DL><p>
										<DT><A HREF="https://kind.sigs.k8s.io/">kind: local run</A>
									</DL><p>
									<DT><H3 FOLDED>kserve</H3>
									<DL><p>
										<DT><A HREF="https://github.com/kserve/modelmesh-serving">kserve/modelmesh-serving: Controller for ModelMesh</A>
									</DL><p>
									<DT><H3 FOLDED>basics</H3>
									<DL><p>
										<DT><A HREF="https://hamel.dev/notes/k8s/02-Basics.html">k8s</A>
										<DT><A HREF="https://www.youtube.com/watch?v=90kZRyPcRZw">Kubernetes Deconstructed: Understanding Kubernetes by Breaking It Down - Carson Anderson, DOMO - YouTube</A>
									</DL><p>
									<DT><H3 FOLDED>utils</H3>
									<DL><p>
										<DT><A HREF="https://github.com/alibaba/kt-connect">alibaba/kt-connect: A toolkit for Integrating with your kubernetes dev environment more efficiently</A>
										<DT><A HREF="https://github.com/alibaba/kubeskoop">alibaba/kubeskoop</A>
									</DL><p>
									<DT><A HREF="https://cloud.google.com/anthos-config-management/docs/concepts/kustomize">Configure Kubernetes with Kustomize  |  Anthos Config Management  |  Google Cloud</A>
									<DT><A HREF="https://www.youtube.com/watch?v=06bKlSmVwIg">CNCF Live Webinar: Overcoming the GPU shortage with virtual Kubelets &amp; distributed cloud - YouTube</A>
									<DT><A HREF="https://virtual-kubelet.io/">Virtual Kubelet | Home</A>
									<DT><A HREF="https://github.com/virtual-kubelet/virtual-kubelet">virtual-kubelet/virtual-kubelet: Virtual Kubelet is an open source Kubernetes kubelet implementation.</A>
									<DT><A HREF="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet | Kubernetes</A>
									<DT><A HREF="https://www.youtube.com/watch?v=cwiAW5TZsfo">On-Demand Systems and Scaled Training Using the JobSet API - Abdullah Gharaibeh &amp; Vanessa Sochat - YouTube</A>
									<DT><A HREF="https://github.com/kubernetes/kubernetes/issues/95492">Kubernetes won't run 50,000 Jobs · Issue #95492 · kubernetes/kubernetes</A>
									<DT><A HREF="https://www.cncf.io/blog/2020/08/10/why-the-kubernetes-scheduler-is-not-enough-for-your-ai-workloads/">Scheduler (Level 2)</A>
									<DT><A HREF="https://www.run.ai/">Run:ai - AI Optimization and Orchestration</A>
									<DT><A HREF="https://www.cncf.io/wp-content/uploads/2020/10/Kube-two-level-RM.pdf">Kubernetes native two-level resource managment for AI workloads</A>
									<DT><A HREF="https://github.com/project-codeflare/multi-cluster-app-dispatcher">project-codeflare/multi-cluster-app-dispatcher: Holistic job manager on Kubernetes</A>
									<DT><A HREF="https://github.com/bentoml/Yatai">bentoml/Yatai: Model Deployment at Scale on Kubernetes 🦄️</A>
									<DT><A HREF="https://docs.aws.amazon.com/parallelcluster/latest/ug/build-image.html">buildImage - AWS ParallelCluster</A>
									<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/high-performance-aiml-storage-through-local-ssd-support-on-gke">High performance AI/ML storage through Local SSD support on GKE | Google Cloud Blog</A>
								</DL><p>
								<DT><A HREF="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/core/exp_manager.html">Experiment Manager — NVIDIA NeMo</A>
								<DT><A HREF="https://github.com/google-deepmind/xmanager">google-deepmind/xmanager: A platform for managing machine learning experiments</A>
								<DT><A HREF="https://storage.googleapis.com/gresearch/xmanager/deepmind_xmanager_slides.pdf">XManager (Slides)</A>
								<DT><A HREF="https://github.com/google/xpk">google/xpk: xpk (Accelerated Processing Kit) is a software tool to help Cloud developers to orchestrate training jobs on accelerators such as TPUs and GPUs on GKE.</A>
								<DT><A HREF="https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/gke-a100-jax/train.py">ai-on-gke/gke-a100-jax/train.py at main · GoogleCloudPlatform/ai-on-gke</A>
								<DT><A HREF="https://github.com/NVIDIA/pyxis">NVIDIA/pyxis: Container plugin for Slurm Workload Manager</A>
							</DL><p>
							<DT><H3 FOLDED>Infrasctructure as Code</H3>
							<DL><p>
								<DT><H3 FOLDED>Ansible</H3>
								<DL><p>
									<DT><A HREF="https://www.youtube.com/watch?v=2c5b8Olk6Fk">Ansible and Automation before Mexico Vacation - Skills to get engineering roles. - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>Terraform</H3>
								<DL><p>
									<DT><A HREF="https://googlecloudplatform.github.io/magic-modules/">Magic Modules: Extending Terraform</A>
								</DL><p>
								<DT><H3 FOLDED>Pulumi</H3>
								<DL><p>
									<DT><H3 FOLDED>xAI</H3>
									<DL><p>
										<DT><A HREF="https://x.ai/career/">Join xAI</A>
										<DT><A HREF="https://boards.greenhouse.io/xai/jobs/4099452007">Job Application for Infrastructure Engineer at xAI</A>
										<DT><A HREF="https://boards.greenhouse.io/xai/jobs/4099456007">Job Application for AI Engineer &amp; Researcher at xAI</A>
										<DT><A HREF="https://www.pulumi.com/solutions/ai/">AI Infrastructure at Any Scale | Pulumi</A>
										<DT><A HREF="https://github.com/pulumi/pulumi">pulumi/pulumi: Pulumi - Infrastructure as Code in any programming language. Build infrastructure intuitively on any cloud using familiar languages 🚀</A>
									</DL><p>
									<DT><A HREF="https://www.pulumi.com/solutions/ai/">AI Infrastructure at Any Scale | Pulumi</A>
								</DL><p>
								<DT><A HREF="https://github.com/kykosic/remote">kykosic/remote: Simple CLI tool for managing remote development instances</A>
								<DT><A HREF="https://github.com/pulumi/pulumi">pulumi/pulumi: Pulumi - Infrastructure as Code in any programming language. Build infrastructure intuitively on any cloud using familiar languages 🚀</A>
								<DT><A HREF="https://github.com/matklad/xshell">matklad/xshell: Making Rust a Better Bash</A>
							</DL><p>
							<DT><H3 FOLDED>ml-sys-cloud-benchmarking</H3>
							<DL><p>
								<DT><H3 FOLDED>Artificial Analysis</H3>
								<DL><p>
									<DT><A HREF="https://artificialanalysis.ai/">Model &amp; API Providers Analysis | Artificial Analysis</A>
								</DL><p>
								<DT><A HREF="https://artificialanalysis.ai/">Model &amp; API Providers Analysis | Artificial Analysis</A>
								<DT><A HREF="https://artificialanalysis.ai/models/gpt-4o-mini">GPT-4o mini - Quality, Performance &amp; Price Analysis | Artificial Analysis</A>
								<DT><A HREF="https://artificialanalysis.ai/models/deepseek-v2">DeepSeek-V2 - Quality, Performance &amp; Price Analysis | Artificial Analysis</A>
								<DT><A HREF="https://x.com/ArtificialAnlys/status/1813975855468560621">(1) Artificial Analysis en X: "GPT-4o Mini, announced today, is very impressive for how cheap it is being offered 👀 With a MMLU score of 82% (reported by TechCrunch), it surpasses the quality of other smaller models including Gemini 1.5 Flash (79%) and Claude 3 Haiku (75%). What is particularly exciting is https://t.co/5x7dTvpEQL" / X</A>
								<DT><A HREF="https://twitter.com/ArtificialAnlys">(1) Artificial Analysis (@ArtificialAnlys) / X</A>
								<DT><A HREF="https://twitter.com/lqiao/status/1767406833692737971">Fireworks beats Groq (Lin Qiao)</A>
								<DT><A HREF="https://www.linkedin.com/feed/update/urn:li:activity:7189636365620240384/">Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services</A>
								<DT><A HREF="https://llm-qoe.github.io/">Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services · Andes Blog</A>
							</DL><p>
							<DT><H3 FOLDED>ml-sys-cloud-providers</H3>
							<DL><p>
								<DT><H3 FOLDED>Google Cloud Platform</H3>
								<DL><p>
									<DT><H3 FOLDED>Google Collab</H3>
									<DL><p>
										<DT><H3 FOLDED>third-party apps integration</H3>
										<DL><p>
											<DT><H3 FOLDED>Github</H3>
											<DL><p>
												<DT><A HREF="https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d">How to use Google Colab with GitHub via Google Drive - medium</A>
												<DT><A HREF="https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228">Google Drive + Google Colab + GitHub; Don’t Just Read, Do It!</A>
												<DT><A HREF="https://towardsdatascience.com/colaboratory-drive-github-the-workflow-made-simpler-bde89fba8a39">Colaboratory + Drive + Github</A>
												<DT><A HREF="https://medium.com/@ashwindesilva/how-to-use-google-colaboratory-to-clone-a-github-repository-e07cf8d3d22b">git clone</A>
											</DL><p>
											<DT><H3 FOLDED>Google Drive</H3>
											<DL><p>
												<DT><A HREF="https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d">How to use Google Colab with GitHub via Google Drive - medium</A>
												<DT><A HREF="https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228">Google Drive + Google Colab + GitHub; Don’t Just Read, Do It!</A>
												<DT><A HREF="https://towardsdatascience.com/colaboratory-drive-github-the-workflow-made-simpler-bde89fba8a39">Colaboratory + Drive + Github</A>
											</DL><p>
											<DT><A HREF="https://medium.com/@ashwindesilva/how-to-use-google-colaboratory-to-clone-a-github-repository-e07cf8d3d22b">Basic interoperability usage: Google Drive-Github</A>
										</DL><p>
										<DT><H3 FOLDED>remote rendering</H3>
										<DL><p>
											<DT><A HREF="https://davidrpugh.github.io/stochastic-expatriate-descent/openai/binder/google-colab/2020/04/16/remote-rendering-gym-envs.html">Rendering OpenAI Gym Envs on Binder and Google Colab</A>
											<DT><A HREF="https://stackoverflow.com/questions/53472940/nameerror-name-base-is-not-defined-openai-gym">Cannot render display - NameError: name 'base' is not defined OpenAI Gym - Stack Overflow</A>
											<DT><A HREF="https://colab.research.google.com/github/davidrpugh/stochastic-expatriate-descent/blob/2020-04-16-remote-rendering-gym-envs/_notebooks/2020-04-16-remote-rendering-gym-envs.ipynb#scrollTo=iS3_S__W01cs">remote-rendering-gym-envs.ipynb - Colaboratory</A>
										</DL><p>
										<DT><A HREF="https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403">memory mapped file formats like HDF5 (aka H5) or LMDB</A>
									</DL><p>
									<DT><A HREF="https://cloud.google.com/products/calculator/#id=4b1ece6b-db16-4e2b-bdf2-0d7370d75fb7">Google Cloud Pricing Calculator</A>
									<DT><A HREF="https://cloud.google.com/compute/gpus-pricing#tpus">GPU pricing  |  Compute Engine: Virtual Machines (VMs)  |  Google Cloud</A>
									<DT><A HREF="https://cloud.google.com/compute/docs/gpus">GPU platforms  |  Compute Engine Documentation  |  Google Cloud</A>
								</DL><p>
								<DT><H3 FOLDED>CoreWeave</H3>
								<DL><p>
									<DT><A HREF="https://docs.coreweave.com/">CoreWeave Cloud - CoreWeave</A>
									<DT><A HREF="https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer">https://docs.coreweave.com/compass/examples/triton-inference-server-fastertransformer</A>
								</DL><p>
								<DT><H3 FOLDED>AWS DJL</H3>
								<DL><p>
									<DT><A HREF="https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-fastertransformer.html">Large model inference with FasterTransformer and DJL Serving - Amazon SageMaker</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>CI/CD</H3>
							<DL><p>
								<DT><H3 FOLDED>github-actions</H3>
								<DL><p>
									<DT><A HREF="https://github.com/nektos/act">nektos/act: Run your GitHub Actions locally 🚀</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://twitter.com/TigerBeetleDB/status/1729054660223553933">(1) TigerBeetle en X: "How do you catch up (and overtake) 30 years of test time? - We've ramped to 100 CPU cores. - 100 simultaneous TigerBeetle simulations, 24x7. - Each simulation (in 3.3 seconds) tests 39 minutes on avg. of real world runtime. 10 cores = 20 years 100 cores = 200 years (every day) https://t.co/mDDCBE74nY" / X</A>
							<DT><A HREF="https://www.tritondatacenter.com/">Take Control of Your Cloud Data | Triton DataCenter</A>
							<DT><A HREF="https://github.com/GoogleCloudPlatform/slurm-gcp">GoogleCloudPlatform/slurm-gcp</A>
							<DT><A HREF="https://github.com/quickwit-oss/quickwit">quickwit-oss/quickwit: Cloud-native search engine for observability. An open-source alternative to Datadog, Elasticsearch, Loki, and Tempo.</A>
							<DT><A HREF="https://gpulist.ai/">gpulist</A>
							<DT><A HREF="https://twitter.com/TigerBeetleDB/status/1729054660223553933">(1) TigerBeetle en X: "How do you catch up (and overtake) 30 years of test time?</A>
							<DT><A HREF="https://github.com/quickwit-oss/quickwit">quickwit-oss/quickwit: Cloud-native search engine for observability. An open-source alternative to Datadog, Elasticsearch</A>
						</DL><p>
						<DT><H3 FOLDED>sw-testing</H3>
						<DL><p>
							<DT><H3 FOLDED>sw-testing-errors</H3>
							<DL><p>
								<DT><A HREF="https://www.inngest.com/blog/python-errors-as-values">Python errors as values: Comparing useful patterns from Go and Rust - Inngest Blog</A>
								<DT><A HREF="https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/convert.py#L4">errors as values: safetensors/bindings/python/convert.py</A>
							</DL><p>
							<DT><H3 FOLDED>sw-testing-exception-handling</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>sw-testing-bugs</H3>
							<DL><p>
								<DT><A HREF="https://github.com/ProjectPhysX/FluidX3D/releases/tag/v2.16">Release FluidX3D v2.16 (bug fixes) · ProjectPhysX/FluidX3D</A>
								<DT><A HREF="https://private-user-images.githubusercontent.com/90851087/327656146-d013fc95-d977-460f-9de8-2a6fc7ddaec5.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTQ3MzM2NDUsIm5iZiI6MTcxNDczMzM0NSwicGF0aCI6Ii85MDg1MTA4Ny8zMjc2NTYxNDYtZDAxM2ZjOTUtZDk3Ny00NjBmLTlkZTgtMmE2ZmM3ZGRhZWM1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA1MDMlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNTAzVDEwNDkwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQ4ZTJiY2I4NTU2MzBmZmFkNWJiMzI1MzdkYjc4NTg1N2Q5ZjU0ZmZlYmQ3MTNlODFkNDE1NjYyMjlkNmZmY2EmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.QXKhYOoKDu0s5FNsEZsht_Psxxfb1exaRzg8-OrI45Q">327656146-d013fc95-d977-460f-9de8-2a6fc7ddaec5.png 1.886×960 pixels</A>
								<DT><A HREF="https://chat.openai.com/">github-bug-issues</A>
							</DL><p>
							<DT><H3 FOLDED>Deterministic Simulation Testing</H3>
							<DL><p>
								<DT><A HREF="https://www.polarsignals.com/blog/posts/2024/05/28/mostly-dst-in-go">(Mostly) Deterministic Simulation Testing in Go</A>
							</DL><p>
							<DT><A HREF="https://twitter.com/karpathy/status/1786085254006202541">(1) Andrej Karpathy en X: "Clearly LLMs must one day run in Space Step 1 we harden llm.c to pass the NASA code standards and style guides, certifying that the code is super safe, safe enough to run in Space. https://t.co/tYGrfdka4X (see the linked PDF) LLM training/inference in principle should be super..." / X</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/The_Power_of_10:_Rules_for_Developing_Safety-Critical_Code">The Power of 10: Rules for Developing Safety-Critical Code</A>
							<DT><A HREF="https://x.com/realGeorgeHotz/status/1874228009550700809">(2) George Hotz 🌑 en X: "@ns123abc @AMD We still have a very long journey ahead. For sure our driver has bugs to find and performance optimizations to make. The key thing is that it's 696 lines, written in Python, and easy to understand and debug. Testing and simplicity are our strengths." / X</A>
						</DL><p>
						<DT><H3 FOLDED>programming-behaviour-invariance</H3>
						<DL><p>
							<DT><A HREF="https://www.youtube.com/shorts/M-VU0fLjIUU">This Is A Game Changer - YouTube</A>
							<DT><A HREF="https://softwareengineering.stackexchange.com/questions/32727/what-are-invariants-how-can-they-be-used-and-have-you-ever-used-it-in-your-pro">What are invariants, how can they be used, and have you ever used it in your program? - Software Engineering Stack Exchange</A>
							<DT><A HREF="https://wiki.python.org/moin/UsingAssertionsEffectively">UsingAssertionsEffectively - Python Wiki</A>
						</DL><p>
						<DT><H3 FOLDED>mlsys-research-groups</H3>
						<DL><p>
							<DT><H3 FOLDED>MadSys Research Group</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://github.com/madsys-dev">MadSys Research Group</A>
						</DL><p>
						<DT><H3 FOLDED>mlsys-inference</H3>
						<DL><p>
							<DT><H3 FOLDED>inference.net</H3>
							<DL><p>
								<DT><A HREF="https://www.inference.net/">custom scheduling and orchestration software that aggregates these small chunks across data centers to run AI models on compute that would otherwise go unused.</A>
							</DL><p>
						</DL><p>
						<DT><A HREF="https://notes.ekzhang.com/events/nysrg">NYSRG (k8s, git, S3)</A>
						<DT><A HREF="https://imbue.com/research/70b-infrastructure/">From bare metal to a 70B model: infrastructure set-up and scripts - imbue</A>
						<DT><A HREF="https://websites.umich.edu/~amberljc/file/LLM-Systems-Basics.pdf">LLM-Systems-Basics (Jiachen Liu)</A>
						<DT><A HREF="https://web.stanford.edu/class/cs245/">CS 245: Principles of Data-Intensive Systems (Winter 2021)</A>
						<DT><A HREF="https://www.youtube.com/watch?v=TOyD-5QgpuE">Design a Code Execution System | System Design - YouTube</A>
						<DT><A HREF="https://www.youtube.com/watch?v=jFrGhodqC08">The cloud is over-engineered and overpriced (no music) - YouTube</A>
						<DT><A HREF="https://github.com/cuda-mode/awesomeMLSys">cuda-mode/awesomeMLSys: An ML Systems Onboarding list</A>
						<DT><A HREF="https://www.youtube.com/watch?v=GA4ONupSl8Y">Two Decades of Hardware Optimizations Down The Drain - YouTube</A>
						<DT><A HREF="https://12factor.net/">The Twelve-Factor App</A>
						<DT><A HREF="https://discord.com/blog/how-discord-supercharges-network-disks-for-extreme-low-latency">How Discord Supercharges Network Disks for Extreme Low Latency</A>
						<DT><A HREF="https://x.com/mvpatel2000/status/1803941734629388590">(1) Mihir Patel en X: "@StasBekman See blog https://t.co/A0JWbNWHVl in particular the HSDP section. When you shard across many GPUs with HSDP, you have multiple replicas. When a node goes down (take top left red square), we can bring up a new node and restore it's copy by pulling from top left green square (1/n) https://t.co/5jsGSr3ca6" / X</A>
						<DT><A HREF="https://github.com/hao-ai-lab">Hao AI Lab</A>
						<DT><A HREF="https://github.com/imbue-ai/cluster-health">imbue-ai/cluster-health</A>
						<DT><A HREF="https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55">Linux Performance Analysis in 60,000 Milliseconds | by Netflix Technology Blog | Netflix TechBlog</A>
						<DT><A HREF="https://github.com/facebookresearch/DCPerf/tree/main/packages/feedsim/third_party/src/scripts">DCPerf/packages/feedsim/third_party/src/scripts at main · facebookresearch/DCPerf</A>
						<DT><A HREF="https://github.com/linkedin/school-of-sre?tab=readme-ov-file">linkedin/school-of-sre: At LinkedIn, we are using this curriculum for onboarding our entry-level talents into the SRE role.</A>
						<DT><A HREF="http://mlforsystems.org/">Announcement | ML For Systems</A>
						<DT><A HREF="https://github.com/stas00/ml-engineering/tree/master/compute/accelerator#power-consumption">ml-engineering/compute/accelerator at master · stas00/ml-engineering</A>
						<DT><A HREF="https://github.com/MLSys-Learner-Resources/Awesome-MLSys-Blogger">MLSys-Learner-Resources/Awesome-MLSys-Blogger: The repository has collected a batch of noteworthy MLSys bloggers (Algorithms/Systems)</A>
					</DL><p>
					<DT><H3 FOLDED>sw-training-infrastructure</H3>
					<DL><p>
						<DT><H3 FOLDED>multi-threading</H3>
						<DL><p>
							<DT><H3 FOLDED>OpenMP</H3>
							<DL><p>
								<DT><A HREF="https://github.com/jakaspeh/concurrency/blob/master/ompForSchedule.cpp">ompForSchedule.cpp examples</A>
								<DT><A HREF="http://jakascorner.com/blog/2016/06/omp-for-scheduling.html">OpenMP: For &amp; Scheduling</A>
								<DT><A HREF="https://smileipic.github.io/tutorials/perfs_parallel_computing.html">Parallel computing — Smilei tutorials X.Y documentation</A>
								<DT><A HREF="https://github.com/facebookincubator/dispenso">facebookincubator/dispenso: The project provides high-performance concurrency, enabling highly parallel computation.</A>
							</DL><p>
							<DT><H3 FOLDED>MPI</H3>
							<DL><p>
								<DT><A HREF="https://mpi4py.readthedocs.io/en/stable/">MPI for Python — MPI for Python 3.1.6 documentation</A>
								<DT><A HREF="https://mpitutorial.com/tutorials/">Tutorials · MPI Tutorial</A>
							</DL><p>
							<DT><A HREF="https://www.pythonpool.com/cant-pickle-local-object/">Pickling local variables context</A>
							<DT><A HREF="http://jakascorner.com/blog/2016/06/omp-for-scheduling.html">OpenMP: For &amp; Scheduling</A>
							<DT><A HREF="https://www.youtube.com/watch?v=Iqrd9vsLrak">Coroutine Patterns: Problems and Solutions Using Coroutines in a Modern Codebase</A>
							<DT><A HREF="https://x.com/7etsuo/status/1823584755398664370">Tetsuo en X: "Mastering Multithreading in C: A Comprehensive Guide to Concurrency Primitives" / X</A>
						</DL><p>
						<DT><H3 FOLDED>model-parallelism-mpi</H3>
						<DL><p>
							<DT><A HREF="https://www.open-mpi.org/">Open MPI: Open Source High Performance Computing</A>
						</DL><p>
						<DT><H3 FOLDED>llm.c</H3>
						<DL><p>
							<DT><H3 FOLDED>llm.c-videos</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=l8pRSuU81PU">Let's reproduce GPT-2 (124M) - YouTube</A>
								<DT><A HREF="https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4577s">Let's build GPT: from scratch, in code, spelled out. - YouTube</A>
							</DL><p>
							<DT><A HREF="https://github.com/karpathy/llm.c">karpathy/llm.c: LLM training in simple, raw C/CUDA</A>
							<DT><A HREF="https://github.com/Chillee/llm.c/commit/c39de5916835b5ade292bc96a8b81de4a517972e">attach a simple tutorial (layernorm)</A>
							<DT><A HREF="https://gist.github.com/geohot/7c9f10f5770f058a1de6ef0598e4c9d8">Outputted llm.c from tinygrad</A>
							<DT><A HREF="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/core/providers/cuda/math/softmax_warpwise_impl.cuh">onnxruntime/onnxruntime/core/providers/cuda/math/softmax_warpwise_impl.cuh</A>
							<DT><A HREF="https://x.com/karpathy/status/1799949853289804266">(1) Andrej Karpathy en X: "📽️ New 4 hour (lol) video lecture on YouTube: "Let’s reproduce GPT-2 (124M)" https://t.co/NMIVD1V6zr The video ended up so long because it is... comprehensive: we start with empty file and end up with a GPT-2 (124M) model: - first we build the GPT-2 network - then we optimize https://t.co/NDqTrLbrO4" / X</A>
							<DT><A HREF="https://github.com/karpathy/llm.c/discussions/481">Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 · karpathy/llm.c · Discussion #481</A>
							<DT><A HREF="https://arxiv.org/abs/2212.14034">[2212.14034] Cramming: Training a Language Model on a Single GPU in One Day</A>
							<DT><A HREF="https://x.com/karpathy/status/1841536804073439268">Andrej Karpathy en X: "I gave a talk at GPU MODE workshop last week on llm.c - the origin story of llm.c - being naked in the world without PyTorch and having to re-invent Array, Autograd, Device, Dtype, Compile, Distributed - how to port a PyTorch layer to 1) explicit PyTorch - and then to 2) write https://t.co/u8JXyy90VE" / X</A>
						</DL><p>
						<DT><H3 FOLDED>torchtitan</H3>
						<DL><p>
							<DT><H3 FOLDED>torchtitan-train-configs</H3>
							<DL><p>
								<DT><A HREF="https://github.com/pytorch/torchtitan/blob/7281e0be8feeb607f3c3f12cc3ceaafed87912c9/train_configs/llama3_405b.toml">torchtitan/train_configs/llama3_405b.toml at 7281e0be8feeb607f3c3f12cc3ceaafed87912c9 · pytorch/torchtitan</A>
								<DT><A HREF="https://github.com/pytorch/torchtitan/blob/main/train_configs/debug_model.toml">torchtitan/train_configs/debug_model.toml at main · pytorch/torchtitan</A>
							</DL><p>
							<DT><H3 FOLDED>lingua</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/lingua">facebookresearch/lingua: Meta Lingua: a lean, efficient, and easy-to-hack codebase to research LLMs.</A>
							</DL><p>
							<DT><A HREF="https://github.com/pytorch/torchtitan">pytorch/torchtitan: A native PyTorch Library for large model training</A>
							<DT><A HREF="https://github.com/pytorch/torchtune">pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning</A>
							<DT><A HREF="https://github.com/huggingface/nanotron">huggingface/nanotron: Minimalistic large language model 3D-parallelism training</A>
							<DT><A HREF="https://github.com/pytorch-labs/gpt-fast">pytorch-labs/gpt-fast: Simple and efficient pytorch-native transformer text generation in &lt;1000 LOC of python.</A>
							<DT><A HREF="https://github.com/pytorch-labs/gpt-fast/blob/main/tp.py#L124">gpt-fast/tp.py at main · pytorch-labs/gpt-fast</A>
							<DT><A HREF="https://volcengine.github.io/veScaleWeb/">veScale</A>
							<DT><A HREF="https://github.com/volcengine/veScale">volcengine/veScale: A PyTorch Native LLM Training Framework</A>
							<DT><A HREF="https://arxiv.org/abs/2402.15627">[2402.15627] MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs</A>
							<DT><A HREF="https://arxiv.org/abs/2408.03505">[2408.03505] Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation</A>
							<DT><A HREF="https://www.youtube.com/watch?v=VYWRjcUqW6w">Lecture 39: Torchtitan - YouTube</A>
						</DL><p>
						<DT><H3 FOLDED>nanotron</H3>
						<DL><p>
							<DT><H3 FOLDED>nanotron-dataloader</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://github.com/huggingface/nanotron/tree/main">huggingface/nanotron: Minimalistic large language model 3D-parallelism training</A>
							<DT><A HREF="https://jeremybernste.in/modula/history/">The science of scale - Modula documentation</A>
						</DL><p>
						<DT><H3 FOLDED>sw-Megatron-LM</H3>
						<DL><p>
							<DT><H3 FOLDED>NeMo</H3>
							<DL><p>
							</DL><p>
							<DT><H3 FOLDED>examples</H3>
							<DL><p>
								<DT><A HREF="https://github.com/bigcode-project/Megatron-LM/blob/c4468364e4225b32adb648d7918a3e645ff9e409/examples/pretrain_gpt_1B_santacoder.sh">BigCode: pretrain_gpt_1_santacoder.sh</A>
								<DT><A HREF="https://github.com/THUDM/CodeGeeX/blob/f1afae7b26ceaa040adcd4b4812b64f259b05647/scripts/pretrain_codegeex.sh#L47">CodeGeeX/pretrain_codegeex.sh</A>
							</DL><p>
							<DT><A HREF="https://huggingface.co/blog/bloom-megatron-deepspeed">The Technology Behind BLOOM Training</A>
							<DT><A HREF="https://github.com/microsoft/Megatron-DeepSpeed/blob/main/megatron/training.py#L137">training.py</A>
							<DT><A HREF="https://nvidia.github.io/apex/">Apex (A PyTorch Extension) — Apex 0.1.0 documentation</A>
							<DT><A HREF="https://huggingface.co/blog/megatron-training">How to train a Language Model with Megatron-LM</A>
							<DT><A HREF="https://github.com/microsoft/Megatron-DeepSpeed/blob/7bbd7f0e390b120898235c446f076d8de219d474/megatron/arguments.py#L24">Megatron-DeepSpeed/arguments.py</A>
							<DT><A HREF="https://pytorch.org/docs/stable/distributed.html">Distributed communication package - torch.distributed — PyTorch 2.0 documentation</A>
							<DT><A HREF="https://github.com/mli/transformers-benchmarks/blob/main/transformers.ipynb">transformers-benchmarks/transformers.ipynb at main · mli/transformers-benchmarks</A>
						</DL><p>
						<DT><H3 FOLDED>sw-FSDP</H3>
						<DL><p>
							<DT><A HREF="https://pytorch.org/docs/stable/distributed.html#profiling-collective-communication">Distributed communication package - torch.distributed — PyTorch 2.0</A>
							<DT><A HREF="https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md">torchtitan/docs/fsdp.md at main · pytorch/torchtitan</A>
						</DL><p>
						<DT><H3 FOLDED>open-lm</H3>
						<DL><p>
							<DT><A HREF="https://github.com/mlfoundations/open_lm">mlfoundations/open_lm: A repository for research on medium sized language models.</A>
						</DL><p>
						<DT><H3 FOLDED>jax-training</H3>
						<DL><p>
							<DT><H3 FOLDED>FLAX</H3>
							<DL><p>
								<DT><A HREF="https://www.youtube.com/watch?v=__eG63ZP_5g">Day 2 Talks: JAX, Flax &amp; Transformers 🤗 - YouTube</A>
								<DT><A HREF="https://github.com/google/maxtext">google/maxtext: A simple, performant and scalable Jax LLM!</A>
								<DT><H3 FOLDED>flaxformer</H3>
								<DL><p>
									<DT><A HREF="https://github.com/google/flaxformer">google/flaxformer</A>
								</DL><p>
							</DL><p>
							<DT><H3 FOLDED>Haliax</H3>
							<DL><p>
								<DT><A HREF="https://github.com/stanford-crfm/haliax">stanford-crfm/haliax: Named Tensors for Legible Deep Learning in JAX</A>
								<DT><A HREF="https://levanter.readthedocs.io/en/latest/Fine-Tuning/">Custom Fine-Tuning: Alpaca Tutorial - Levanter</A>
								<DT><A HREF="https://twitter.com/dlwh/status/1716901560532521100">(David Hall): Haliax release</A>
								<DT><A HREF="https://twitter.com/dlwh/status/1716900734120464834/photo/1">Named tensor library</A>
							</DL><p>
							<DT><H3 FOLDED>t5x + seqio</H3>
							<DL><p>
								<DT><A HREF="https://github.com/google-research/text-to-text-transfer-transformer">google-research/text-to-text-transfer-transformer: Code for the paper "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"</A>
								<DT><A HREF="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md">Checkpoints</A>
								<DT><A HREF="https://github.com/google/flaxformer/blob/main/flaxformer/architectures/t5/t5_1_1.py#L70">flaxformer/flaxformer/architectures/t5/t5_1_1.py at main · google/flaxformer</A>
								<DT><A HREF="https://github.com/google/maxtext">google/maxtext: A simple, performant and scalable Jax LLM!</A>
								<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x</A>
							</DL><p>
							<DT><H3 FOLDED>Paxml</H3>
							<DL><p>
								<DT><A HREF="https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/pax">JAX-Toolbox/rosetta/rosetta/projects/pax at main · NVIDIA/JAX-Toolbox</A>
								<DT><A HREF="https://github.com/google/paxml">google/paxml: Pax is a Jax-based machine learning framework for training large scale models. Pax allows for advanced and fully configurable experimentation and parallelization, and has demonstrated industry leading model flop utilization rates.</A>
								<DT><A HREF="https://github.com/google/praxis/tree/main">google/praxis</A>
							</DL><p>
							<DT><H3 FOLDED>Maxtext</H3>
							<DL><p>
								<DT><A HREF="https://github.com/google/maxtext">google/maxtext: A simple, performant and scalable Jax LLM!</A>
								<DT><A HREF="https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e">the world’s largest distributed LLM training job on TPU v5e | Google Cloud Blog</A>
							</DL><p>
							<DT><H3 FOLDED>Levanter</H3>
							<DL><p>
								<DT><A HREF="https://github.com/stanford-crfm/levanter">stanford-crfm/levanter: Legibile, Scalable, Reproducible Foundation Models with Named Tensors and Jax</A>
								<DT><A HREF="https://crfm.stanford.edu/2023/06/16/levanter-1_0-release.html">Levanter — Legible, Scalable, Reproducible Foundation Models with JAX</A>
								<DT><H3 FOLDED>configs</H3>
								<DL><p>
									<DT><A HREF="https://github.com/NbAiLab/nb-levanter/blob/main/configs/mimir-mistral-7b-extended.yaml">nb-levanter/configs/mimir-mistral-7b-extended.yaml</A>
								</DL><p>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2311.08105">[2311.08105] DiLoCo: Distributed Low-Communication Training of Language Models</A>
						</DL><p>
						<DT><H3 FOLDED>hf-accelerate</H3>
						<DL><p>
							<DT><H3 FOLDED>hf-accelerate-inference</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/docs/transformers/perf_infer_gpu_one">Efficient Inference on a Single GPU</A>
							</DL><p>
							<DT><A HREF="https://github.com/huggingface/accelerate">huggingface/accelerate: Train and use PyTorch models with multi-GPU, TPU, mixed-precision</A>
							<DT><A HREF="https://huggingface.co/blog/bloom-inference-pytorch-scripts">Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate</A>
							<DT><A HREF="https://huggingface.co/docs/accelerate/usage_guides/big_modeling">Handling big models for inference (Sharding &amp; device map)</A>
							<DT><A HREF="https://huggingface.co/docs/transformers/perf_train_gpu_many">Efficient Training on Multiple GPUs</A>
							<DT><A HREF="https://github.com/huggingface/accelerate/blob/d4f5fd694e3acf9178b8075ea2379fd1839954f2/docs/source/usage_guides/big_modeling.mdx">Handling big models for inference</A>
							<DT><A HREF="https://github.com/huggingface/transformers/issues/16884">Transformers model shards</A>
							<DT><A HREF="https://towardsdatascience.com/sharded-a-new-technique-to-double-the-size-of-pytorch-models-3af057466dba">Sharded</A>
							<DT><A HREF="https://github.com/kingoflolz/mesh-transformer-jax">Mesh-Transformer-jax: Model parallel transformers in JAX and Haiku</A>
						</DL><p>
						<DT><H3 FOLDED>sw-mistral</H3>
						<DL><p>
							<DT><A HREF="https://github.com/stanford-crfm/mistral">Mistral: Framework for transparent and accessible LLMs training, built with Hugging Face 🤗 Transformers</A>
						</DL><p>
						<DT><H3 FOLDED>zero-deepspeed</H3>
						<DL><p>
							<DT><A HREF="https://www.deepspeed.ai/tutorials/inference-tutorial/">Getting Started with DeepSpeed for Inferencing Transformer based Models - DeepSpeed</A>
							<DT><A HREF="https://www.deepspeed.ai/tutorials/inference-tutorial/#end-to-end-gpt-neo-27b-inference">GPT-NEOX-20B</A>
							<DT><A HREF="https://www.philschmid.de/bert-deepspeed-inference">Accelerate BERT inference with DeepSpeed-Inference on GPUs</A>
							<DT><A HREF="https://www.deepspeed.ai/tutorials/megatron/">Megatron-LM GPT2 - DeepSpeed</A>
							<DT><A HREF="https://www.deepspeed.ai/tutorials/zero/#zero-overview">Zero Redundancy Optimizer</A>
							<DT><A HREF="https://github.com/huggingface/transformers/blob/main/tests/deepspeed/ds_config_zero2.json">transformers/ds_config_zero2.json</A>
							<DT><A HREF="https://github.com/microsoft/Megatron-DeepSpeed">microsoft/Megatron-DeepSpeed: Ongoing research training transformer language models at scale, including: BERT &amp; GPT-2</A>
							<DT><A HREF="https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed">Huggingface Transformers DeepSpeed Integration</A>
						</DL><p>
						<DT><H3 FOLDED>mosaicml-training</H3>
						<DL><p>
							<DT><H3 FOLDED>composer</H3>
							<DL><p>
								<DT><A HREF="https://github.com/mosaicml/composer">mosaicml/composer: Supercharge Your Model Training</A>
							</DL><p>
							<DT><A HREF="https://www.mosaicml.com/">MosaicML | Home</A>
							<DT><A HREF="https://www.mosaicml.com/blog/coreweave-nvidia-h100-part-1?utm_source=twitter&utm_medium=social&utm_campaign=h100-blog-post">Benchmarking Large Language Models on NVIDIA H100 GPUs with CoreWeave (Part 1)</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-eth-zurich</H3>
						<DL><p>
							<DT><A HREF="https://github.com/eth-easl/fmengine">eth-easl/fmengine: Utilities for Training Very Large Models</A>
						</DL><p>
						<DT><H3 FOLDED>tensorflow-training</H3>
						<DL><p>
							<DT><H3 FOLDED>tensorflow-installation</H3>
							<DL><p>
								<DT><A HREF="https://velog.io/@qone/Apple-silicon-m1-%EB%A7%A5%EC%97%90-tensorflow%EC%84%A4%EC%B9%98%ED%95%98%EA%B3%A0-GPU%EA%B0%80%EC%86%8D-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0">Enable tensorflow metal (ARM GPU)</A>
								<DT><A HREF="https://gist.github.com/juliasilge/035d54c59436604d6142ebebf29aa224?permalink_comment_id=3981784">Installing R + Tensorflow on M1</A>
								<DT><A HREF="https://developer.apple.com/metal/tensorflow-plugin/">Tensorflow Plugin - Metal - Apple Developer</A>
								<DT><A HREF="https://github.com/robotology/robotology-superbuild/blob/master/doc/install-mambaforge.md#macos">Install mambaforge</A>
							</DL><p>
							<DT><H3 FOLDED>tensorflow-errors</H3>
							<DL><p>
								<DT><A HREF="https://stackoverflow.com/questions/58012741/error-importing-tensorflow-alreadyexistserror-another-metric-with-the-same-nam">two Keras packages installed</A>
								<DT><A HREF="https://stackoverflow.com/questions/57082918/tensorflow-attributeerror-module-tensorflow-python-ops-nn-has-no-attribute">calling v1 API when using v2</A>
							</DL><p>
							<DT><H3 FOLDED>tensorflow-data</H3>
							<DL><p>
								<DT><A HREF="https://github.com/google-research/google-research/blob/master/protein_lm/data.py">TFRecords $ data.py</A>
								<DT><A HREF="https://github.com/google/gin-config">Gin provides a lightweight configuration framework for Python</A>
							</DL><p>
							<DT><H3 FOLDED>tensorflow-docs</H3>
							<DL><p>
								<DT><A HREF="https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.10/tensorflow/g3doc/tutorials/mnist/tf/index.md">TensorFlow - TensorFlow Mechanics 101</A>
								<DT><A HREF="https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.10/tensorflow/g3doc/how_tos/documentation/index.md">TensorFlow - Writing TensorFlow Documentation</A>
							</DL><p>
							<DT><H3 FOLDED>tensorflow-visualization</H3>
							<DL><p>
								<DT><A HREF="https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin">Visualizing Data using the Embedding Projector in TensorBoard  |  TensorFlow</A>
								<DT><A HREF="http://projector.tensorflow.org/">Embedding projector - visualization of high-dimensional data</A>
							</DL><p>
							<DT><H3 FOLDED>tensorflow-configuration</H3>
							<DL><p>
								<DT><A HREF="https://github.com/google/gin-config">Gin provides a lightweight configuration framework for Python</A>
							</DL><p>
							<DT><A HREF="https://www.tensorflow.org/api_docs/python/tf/nn">Module: tf.nn  |  TensorFlow Core v2.8.0 (API)</A>
						</DL><p>
						<DT><A HREF="https://github.com/kingoflolz/mesh-transformer-jax">Mesh-Transformer-jax: Model parallel transformers in JAX and Haiku</A>
						<DT><H3 FOLDED>training-infra-configuration</H3>
						<DL><p>
							<DT><H3 FOLDED>Hydra</H3>
							<DL><p>
								<DT><A HREF="https://github.com/facebookresearch/hydra/blob/main/examples/experimental/rerun/my_app.py">hydra/examples/experimental/rerun/my_app.py at main · facebookresearch/hydra</A>
								<DT><A HREF="https://github.com/facebookresearch/hydra">facebookresearch/hydra: Hydra is a framework for elegantly configuring complex applications</A>
								<DT><A HREF="https://blog.helsing.ai/strongly-typed-structured-configuration-in-hydra-8fb43522d224">Strongly-typed structured configuration in Hydra | by Robert Fink | Helsing Blog</A>
							</DL><p>
							<DT><H3 FOLDED>Gin</H3>
							<DL><p>
							</DL><p>
							<DT><A HREF="https://twitter.com/karpathy/status/1529159374672830464">how to flexibly configure and instantiate neural net architectures and trainers</A>
							<DT><A HREF="https://twitter.com/karpathy/status/1528808361558306817">How they store, catalogue, override, manage and plumb hyperparamaters configs</A>
							<DT><A HREF="https://github.com/google/gin-config">Gin provides a lightweight configuration framework for Python</A>
						</DL><p>
						<DT><H3 FOLDED>full-model-gradient-updates</H3>
						<DL><p>
							<DT><H3 FOLDED>full-model-gradient-updates-notebooks</H3>
							<DL><p>
								<DT><A HREF="https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es">finetune-gpt-j-6B-8bit.ipynb</A>
							</DL><p>
							<DT><A HREF="https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/deepseed-flan-t5-summarization.ipynb">Fine-tune FLAN-T5 XL/XXL using DeepSpeed &amp; Hugging Face Transformers</A>
							<DT><A HREF="https://lightning.ai/pages/blog/how-to-finetune-gpt-like-large-language-models-on-a-custom-dataset/">How To Finetune GPT Like Large Language Models on a Custom Dataset - Lightning AI</A>
							<DT><A HREF="https://arxiv.org/abs/2405.05904#:~:text=We%20demonstrate%20that%20large%20language,consistent%20with%20the%20model's%20knowledge.">[2405.05904] Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-peft</H3>
						<DL><p>
							<DT><H3 FOLDED>training-infra-LoRA</H3>
							<DL><p>
								<DT><A HREF="https://github.com/tloen/alpaca-lora">Code for reproducing the Stanford Alpaca InstructLLaMA</A>
								<DT><A HREF="https://www.youtube.com/watch?v=4LiKEghyoBo">LoRA - Low Rank Adaptation of Large Language Model: Source Code - YouTube</A>
								<DT><A HREF="https://github.com/punica-ai/punica">punica-ai/punica: Serving multiple LoRA finetuned LLM as one</A>
								<DT><A HREF="https://github.com/predibase/lorax">predibase/lorax: Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs</A>
								<DT><A HREF="https://github.com/S-LoRA/S-LoRA">S-LoRA/S-LoRA: S-LoRA: Serving Thousands of Concurrent LoRA Adapters</A>
							</DL><p>
							<DT><H3 FOLDED>training-infra-prompt-tuning</H3>
							<DL><p>
								<DT><A HREF="https://colab.research.google.com/github/corolla-johnson/mkultra/blob/master/tuning_finetune.ipynb">tuning_finetune_alice.ipynb - Colaboratory</A>
								<DT><A HREF="https://colab.research.google.com/github/corolla-johnson/mkultra/blob/master/tuning_world_info.ipynb#scrollTo=qyfpqA5po5FX">tuning_finetune_lite_3.ipynb - Colaboratory</A>
							</DL><p>
							<DT><A HREF="https://arxiv.org/abs/2403.14608">[2403.14608] Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey</A>
							<DT><A HREF="https://github.com/huggingface/peft">huggingface/peft: 🤗 PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.</A>
							<DT><A HREF="https://www.youtube.com/watch?v=KoOlcX3XLd4">EMNLP 2022 Tutorial - "Modular and Parameter-Efficient Fine-Tuning for NLP Models" - YouTube</A>
							<DT><A HREF="https://www.youtube.com/watch?v=gCcgM7sl4Q4">LaMA-Adapter Finetuning: Source Code: Paper: PseudoCode</A>
							<DT><A HREF="https://twitter.com/giffmana/status/1776156462252761222">(1) Lucas Beyer (bl16) en X: "A bit late, but I just read ReFT, here's a quick thread. - A PEFT method - acts on activs `h` -&amp;gt; small inference overhead 1/5 https://t.co/iN1emxyXQ6" / X</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-huggingface</H3>
						<DL><p>
							<DT><H3 FOLDED>hf-datasets</H3>
							<DL><p>
								<DT><H3 FOLDED>hf-datasets-preprocessing</H3>
								<DL><p>
									<DT><A HREF="https://github.com/leogao2/lm_dataformat">LM_Dataformat: Utilities for storing data for LM training</A>
									<DT><A HREF="https://github.com/GEM-benchmark/NL-Augmenter">GEM-benchmark/NL-Augmenter: NL-Augmenter 🦎 → 🐍 A Collaborative Repository of Natural Language Transformations</A>
								</DL><p>
								<DT><A HREF="https://huggingface.co/datasets/the_pile">the_pile · Datasets at Hugging Face</A>
								<DT><A HREF="https://huggingface.co/docs/datasets/v1.15.1/installation.html">Installation — datasets 1.15.1 documentation</A>
								<DT><A HREF="https://huggingface.co/course/chapter5/5">Creating your own dataset - Hugging Face Course</A>
								<DT><A HREF="https://huggingface.co/docs/datasets/loading">In-Memory data</A>
								<DT><A HREF="https://huggingface.co/docs/datasets/process">Datasets: Process</A>
							</DL><p>
							<DT><H3 FOLDED>hf-hub</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/docs/huggingface_hub/v0.10.1/en/how-to-upstream">Hub Client Library</A>
							</DL><p>
							<DT><H3 FOLDED>hf-eval</H3>
							<DL><p>
								<DT><A HREF="https://huggingface.co/spaces/evaluate-metric/code_eval">Code Eval - a Hugging Face Space by evaluate-metric</A>
							</DL><p>
							<DT><H3 FOLDED>hf-transformers</H3>
							<DL><p>
								<DT><H3 FOLDED>hf-transformers-releases</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/transformers/releases/tag/v4.36.0">Release v4.36: Mixtral, Llava/BakLlava, SeamlessM4T v2, AMD ROCm, F.sdpa wide-spread support · huggingface/transformers</A>
								</DL><p>
								<DT><H3 FOLDED>hf-transformers-t5</H3>
								<DL><p>
									<DT><A HREF="https://twitter.com/LiamFedus/status/1536791574612303872">Switch Transformer models in T5X/JAX (1.6T param)</A>
									<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x: Text-To-Text Transfer Transformer</A>
									<DT><A HREF="https://github.com/google-research/t5x">google-research/t5x</A>
									<DT><A HREF="https://colab.research.google.com/drive/1xx7SgdLaAu23YFBirXmaQViDr8caowX_">T0 by mishin_learning.ipynb - Colaboratory</A>
									<DT><A HREF="https://github.com/bigscience-workshop/t-zero/blob/master/inference/README.md">Running inference with T0</A>
								</DL><p>
								<DT><H3 FOLDED>hf-transformers-generation</H3>
								<DL><p>
									<DT><A HREF="https://github.com/huggingface/transformers/releases/tag/v4.26.0">Release v4.26.0: Generation configs, image processors, backbones and plenty of new models! · huggingface/transformers</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model">Generation Config: Text generation strategies</A>
									<DT><A HREF="https://huggingface.co/gpt2/blob/main/generation_config.json">generation_config.json · gpt2 at main</A>
									<DT><A HREF="https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationConfig">Generation</A>
									<DT><A HREF="https://github.com/huggingface/transformers/blob/e3d832ff87c6ec997125deaa4f1b239db8f9e613/src/transformers/generation/configuration_utils.py#L663">transformers/configuration_utils.py at e3d832ff87c6ec997125deaa4f1b239db8f9e613 · huggingface/transformers · GitHub</A>
									<DT><H3 FOLDED>Pipelines</H3>
									<DL><p>
										<DT><A HREF="https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/pipelines#transformers.TextGenerationPipeline">Pipelines</A>
									</DL><p>
								</DL><p>
								<DT><H3 FOLDED>hf-transformers-models</H3>
								<DL><p>
									<DT><A HREF="https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel">OpenAI GPT2</A>
								</DL><p>
								<DT><H3 FOLDED>hf-transformers-training</H3>
								<DL><p>
								</DL><p>
								<DT><A HREF="https://twitter.com/yacineMTB/status/1691208981698498560">HF Transformers Software Complexity</A>
								<DT><A HREF="https://github.com/THUDM/SwissArmyTransformer">Flexible and powerful library to develop your own Transformer variants</A>
								<DT><A HREF="https://huggingface.co/spaces/nielsr/LayoutLMv2-FUNSD">LayoutLMv 2 FUNSD - a Hugging Face Space by nielsr</A>
								<DT><A HREF="https://huggingface.co/transformers/v4.8.0/glossary.html">Glossary — transformers 4.7.0 documentation</A>
								<DT><A HREF="https://pypi.org/project/cascades/">cascades · PyPI</A>
								<DT><A HREF="https://colab.research.google.com/drive/14wnxMvD9zsiBQo2FtTpxn6w2cpXCcb-7">OPT (30B) - Colaboratory</A>
								<DT><A HREF="https://huggingface.co/docs/transformers/model_doc/auto">Auto Classes</A>
								<DT><A HREF="https://twitter.com/NeelNanda5/status/1582876613397467137">Interpretability</A>
								<DT><A HREF="https://github.com/NielsRogge/Transformers-Tutorials?search=1">Transformers-Tutorials/ at master: NielsRogge</A>
								<DT><A HREF="https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L223">transformers/trainer.py at main · huggingface/transformers · GitHub</A>
							</DL><p>
							<DT><A HREF="https://huggingface.co/">The AI community building the future.</A>
							<DT><A HREF="https://www.philschmid.de/bert-deepspeed-inference">Accelerate BERT inference with DeepSpeed-Inference on GPUs</A>
							<DT><A HREF="https://huggingface.co/search/full-text">Full Text Search - Hugging Face</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-job-scheduler</H3>
						<DL><p>
							<DT><H3 FOLDED>SLURM</H3>
							<DL><p>
								<DT><A HREF="https://oatml.cs.ox.ac.uk/code.html#JvASlurm">Reproducibility and Code - OATML</A>
								<DT><A HREF="https://github.com/y0ast/slurm-for-ml">y0ast/slurm-for-ml: A Machine Learning workflow for Slurm.</A>
							</DL><p>
							<DT><A HREF="https://github.com/it4innovations/hyperqueue">It4innovations/hyperqueue: Scheduler for sub-node tasks for HPC systems with batch scheduling</A>
							<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
						</DL><p>
						<DT><H3 FOLDED>training-infra-model-registry</H3>
						<DL><p>
							<DT><A HREF="https://twitter.com/_ScottCondron/status/1529420711034556416/photo/1">Centralized place for ML teams</A>
							<DT><A HREF="https://wandb.ai/home">Home – Weights &amp; Biases</A>
						</DL><p>
						<DT><H3 FOLDED>training-job-scheduler</H3>
						<DL><p>
							<DT><H3 FOLDED>SLURM</H3>
							<DL><p>
								<DT><H3 FOLDED>slurm-submitit</H3>
								<DL><p>
									<DT><A HREF="https://github.com/facebookincubator/submitit">facebookincubator/submitit: Python 3.8+ toolbox for submitting jobs to Slurm</A>
								</DL><p>
								<DT><A HREF="https://slurm.schedmd.com/">slurm.schedmd.com</A>
								<DT><A HREF="https://github.com/GoogleCloudPlatform/slurm-gcp">GoogleCloudPlatform/slurm-gcp</A>
								<DT><A HREF="https://github.com/GoogleCloudPlatform/hpc-toolkit">GoogleCloudPlatform/hpc-toolkit: Cloud HPC Toolkit is an open-source software offered by Google Cloud which makes it easy for customers to deploy HPC environments on Google Cloud.</A>
								<DT><A HREF="https://oatml.cs.ox.ac.uk/code.html#JvASlurm">Reproducibility and Code - OATML</A>
								<DT><A HREF="https://github.com/y0ast/slurm-for-ml">y0ast/slurm-for-ml: A Machine Learning workflow for Slurm.</A>
							</DL><p>
							<DT><H3 FOLDED>job-manager-kubernetes</H3>
							<DL><p>
								<DT><H3 FOLDED>GKE</H3>
								<DL><p>
									<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/whats-new-with-gke-at-google-cloud-next">What’s new with GKE at Google Cloud Next</A>
								</DL><p>
								<DT><H3 FOLDED>Dynamic Resource Allocation</H3>
								<DL><p>
									<DT><A HREF="https://docs.google.com/document/d/1BNWqgx_SmZDi-va_V31v3DnuVwYnF2EmN7D-O_fB6Oo/edit#heading=h.bxuci8gx6hna">Dynamic Resource Allocation for GPUs in Kubernetes - Google Docs</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-local</H3>
								<DL><p>
									<DT><A HREF="https://kind.sigs.k8s.io/">kind: local run</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-kserve</H3>
								<DL><p>
									<DT><A HREF="https://github.com/kserve/modelmesh-serving">kserve/modelmesh-serving: Controller for ModelMesh</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-basics</H3>
								<DL><p>
									<DT><A HREF="https://hamel.dev/notes/k8s/02-Basics.html">k8s</A>
									<DT><A HREF="https://www.youtube.com/watch?v=90kZRyPcRZw">Kubernetes Deconstructed: Understanding Kubernetes by Breaking It Down - Carson Anderson, DOMO - YouTube</A>
								</DL><p>
								<DT><H3 FOLDED>kubernetes-utils</H3>
								<DL><p>
									<DT><A HREF="https://github.com/alibaba/kt-connect">alibaba/kt-connect: A toolkit for Integrating with your kubernetes dev environment more efficiently</A>
									<DT><A HREF="https://github.com/alibaba/kubeskoop">alibaba/kubeskoop</A>
								</DL><p>
								<DT><A HREF="https://cloud.google.com/anthos-config-management/docs/concepts/kustomize">Configure Kubernetes with Kustomize  |  Anthos Config Management  |  Google Cloud</A>
								<DT><A HREF="https://www.youtube.com/watch?v=06bKlSmVwIg">CNCF Live Webinar: Overcoming the GPU shortage with virtual Kubelets &amp; distributed cloud - YouTube</A>
								<DT><A HREF="https://virtual-kubelet.io/">Virtual Kubelet | Home</A>
								<DT><A HREF="https://github.com/virtual-kubelet/virtual-kubelet">virtual-kubelet/virtual-kubelet: Virtual Kubelet is an open source Kubernetes kubelet implementation.</A>
								<DT><A HREF="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet | Kubernetes</A>
								<DT><A HREF="https://www.youtube.com/watch?v=cwiAW5TZsfo">On-Demand Systems and Scaled Training Using the JobSet API - Abdullah Gharaibeh &amp; Vanessa Sochat - YouTube</A>
								<DT><A HREF="https://github.com/kubernetes/kubernetes/issues/95492">Kubernetes won't run 50,000 Jobs · Issue #95492 · kubernetes/kubernetes</A>
								<DT><A HREF="https://www.cncf.io/blog/2020/08/10/why-the-kubernetes-scheduler-is-not-enough-for-your-ai-workloads/">Scheduler (Level 2)</A>
								<DT><A HREF="https://www.run.ai/">Run:ai - AI Optimization and Orchestration</A>
								<DT><A HREF="https://www.cncf.io/wp-content/uploads/2020/10/Kube-two-level-RM.pdf">Kubernetes native two-level resource managment for AI workloads</A>
								<DT><A HREF="https://github.com/project-codeflare/multi-cluster-app-dispatcher">project-codeflare/multi-cluster-app-dispatcher: Holistic job manager on Kubernetes</A>
								<DT><A HREF="https://github.com/bentoml/Yatai">bentoml/Yatai: Model Deployment at Scale on Kubernetes 🦄️</A>
								<DT><A HREF="https://docs.aws.amazon.com/parallelcluster/latest/ug/build-image.html">buildImage - AWS ParallelCluster</A>
								<DT><A HREF="https://cloud.google.com/blog/products/containers-kubernetes/high-performance-aiml-storage-through-local-ssd-support-on-gke">High performance AI/ML storage through Local SSD support on GKE | Google Cloud Blog</A>
							</DL><p>
							<DT><A HREF="https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/core/exp_manager.html">Experiment Manager — NVIDIA NeMo</A>
							<DT><A HREF="https://github.com/google-deepmind/xmanager">google-deepmind/xmanager: A platform for managing machine learning experiments</A>
							<DT><A HREF="https://storage.googleapis.com/gresearch/xmanager/deepmind_xmanager_slides.pdf">XManager (Slides)</A>
							<DT><A HREF="https://github.com/google/xpk">google/xpk: xpk (Accelerated Processing Kit) is a software tool to help Cloud developers to orchestrate training jobs on accelerators such as TPUs and GPUs on GKE.</A>
							<DT><A HREF="https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/gke-a100-jax/train.py">ai-on-gke/gke-a100-jax/train.py at main · GoogleCloudPlatform/ai-on-gke</A>
							<DT><A HREF="https://github.com/it4innovations/hyperqueue">It4innovations/hyperqueue: Scheduler for sub-node tasks for HPC systems with batch scheduling</A>
							<DT><A HREF="https://github.com/Google/saxml">google/saxml</A>
						</DL><p>
						<DT><H3 FOLDED>training-logs</H3>
						<DL><p>
							<DT><H3 FOLDED>wandb</H3>
							<DL><p>
								<DT><A HREF="https://github.com/huggingface/nanotron/blob/03d67f2103d5be0dc15ea6022a6cf16d6a633064/scripts/log_lighteval_to_wandb.py">nanotron/scripts/log_lighteval_to_wandb.py</A>
							</DL><p>
						</DL><p>
						<DT><A HREF="https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness">Training great LLMs entirely from ground zero in the wilderness as a startup — Yi Tay</A>
						<DT><A HREF="https://x.com/leilavclark/status/1805700631199642094">From bare metal to a 70B model: infrastructure set-up and scripts</A>
						<DT><A HREF="https://github.com/imbue-ai/cluster-health">imbue-ai/cluster-health</A>
						<DT><A HREF="https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py#L148">transformers/training_args.py</A>
						<DT><A HREF="https://github.com/apple/corenet">apple/corenet: CoreNet: A library for training deep neural networks</A>
						<DT><A HREF="https://github.com/pytorch/torchtitan">pytorch/torchtitan: A native PyTorch Library for large model training</A>
						<DT><A HREF="https://developer.nvidia.com/blog/tips-on-scaling-storage-for-ai-training-and-inferencing/">Tips on Scaling Storage for AI Training and Inferencing | NVIDIA Technical Blog</A>
						<DT><A HREF="https://developer.nvidia.com/blog/storage-performance-basics-for-deep-learning/">Storage Performance Basics for Deep Learning | NVIDIA Technical Blog</A>
						<DT><A HREF="https://github.com/alibaba/Megatron-LLaMA">alibaba/Megatron-LLaMA: Best practice for training LLaMA models in Megatron-LM</A>
						<DT><A HREF="https://www.youtube.com/watch?v=HYvhj9W2qHQ">Cloud Native Data Loaders for Machine Learning Using Zarr and Xarray - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>sw-physics-simulation</H3>
					<DL><p>
						<DT><H3 FOLDED>MuJoCo</H3>
						<DL><p>
							<DT><H3 FOLDED>mujoco-installation</H3>
							<DL><p>
								<DT><A HREF="https://formulae.brew.sh/formula/glfw#default">glfw — Homebrew Formulae</A>
								<DT><A HREF="https://blog.birost.com/a?ID=b71caef0-c790-4bde-933c-f3b3a0b3312b">Some tiny steps</A>
								<DT><A HREF="https://www.fullstaq.com/knowledge-hub/blogs/an-alternative-to-macos-dyld-library-path">DYLD_LIBRARY_PATH</A>
								<DT><A HREF="https://medium.com/macos-is-not-linux-and-other-nix-reflections/d-o-y-ou-ld-library-path-you-6ab0a6135a33">D(o) Y(ou) LD_LIBRARY_PATH</A>
							</DL><p>
							<DT><A HREF="https://mujoco.org/">MuJoCo — Advanced Physics Simulation</A>
							<DT><A HREF="https://mujoco.readthedocs.io/en/latest/programming.html#initialization">Programming — MuJoCo documentation</A>
							<DT><A HREF="https://twitter.com/GoogleDeepMind/status/1714627619742683245">GPU &amp; TPU acceleration through JAX</A>
						</DL><p>
					</DL><p>
					<DT><H3 FOLDED>sw-graphs</H3>
					<DL><p>
						<DT><H3 FOLDED>graphs-jax</H3>
						<DL><p>
							<DT><A HREF="https://github.com/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb">educational/intro_to_graph_nets_tutorial_with_jraph.ipynb at master · deepmind/educational</A>
							<DT><A HREF="https://github.com/deepmind/jraph">deepmind/jraph: A Graph Neural Network Library in Jax</A>
						</DL><p>
						<DT><A HREF="https://www.dgl.ai/">Deep Graph Library</A>
					</DL><p>
					<DT><H3 FOLDED>sw-graphs-neural-networks</H3>
					<DL><p>
						<DT><A HREF="https://github.com/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb">educational/intro_to_graph_nets_tutorial_with_jraph.ipynb at master · deepmind/educational</A>
						<DT><A HREF="https://github.com/deepmind/jraph">deepmind/jraph: A Graph Neural Network Library in Jax</A>
						<DT><A HREF="https://github.com/a-r-j/graphtype">a-r-j/graphtype: Type hinting for networkx Graphs</A>
					</DL><p>
					<DT><H3 FOLDED>sw-reinforcement-learning</H3>
					<DL><p>
						<DT><H3 FOLDED>rl-openai</H3>
						<DL><p>
							<DT><H3 FOLDED>gym</H3>
							<DL><p>
								<DT><A HREF="https://stackoverflow.com/questions/44150310/openai-gym-nameerror">python - openAi-gym NameError</A>
								<DT><A HREF="https://openai.github.io/mujoco-py/build/html/reference.html#mjviewer-3d-rendering">API reference — mujoco-py 1.50.1.0 documentation</A>
							</DL><p>
							<DT><A HREF="https://beta.openai.com/docs/guides/embeddings/what-are-embeddings">Embeddings - OpenAI API</A>
							<DT><A HREF="http://gptprompts.wikidot.com/intro:logprobs">Logprobs</A>
						</DL><p>
						<DT><A HREF="https://github.com/kaesve/muzero">A clean implementation of MuZero and AlphaZero following the AlphaZero General framework.</A>
						<DT><A HREF="https://github.com/chiamp/muzero-cartpole">chiamp/muzero-cartpole: Applying DeepMind's MuZero algorithm to the cart pole environment in gym</A>
						<DT><A HREF="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">Introduction to Reinforcement Learning with David Silver (Deepmind)</A>
						<DT><A HREF="https://github.com/openai/baselines">openai/baselines: OpenAI Baselines: high-quality implementations of reinforcement learning algorithms</A>
					</DL><p>
					<DT><H3 FOLDED>sw-notebooks</H3>
					<DL><p>
						<DT><H3 FOLDED>jupyter</H3>
						<DL><p>
							<DT><A HREF="https://jupyter.org/">Project Jupyter | Home</A>
							<DT><A HREF="https://stackoverflow.com/questions/50982686/what-is-the-difference-between-jupyter-notebook-and-jupyterlab">python - What is the difference between Jupyter Notebook and JupyterLab? - Stack Overflow</A>
							<DT><A HREF="https://towardsdatascience.com/jypyter-notebook-shortcuts-bf0101a98330">Jupyter Notebook Shortcuts</A>
						</DL><p>
						<DT><A HREF="https://nn.labml.ai/normalization/deep_norm/index.html">Labml.ai</A>
						<DT><A HREF="https://notebooks.quantumstat.com/">NLP</A>
						<DT><A HREF="https://github.com/amrzv/awesome-colab-notebooks">awesome-colab-notebooks: Collection of google colaboratory notebooks for fast and easy experiments</A>
					</DL><p>
					<DT><H3 FOLDED>sw-interoperability</H3>
					<DL><p>
						<DT><H3 FOLDED>webassembly</H3>
						<DL><p>
							<DT><H3 FOLDED>wasm-internals</H3>
							<DL><p>
								<DT><A HREF="https://developer.mozilla.org/en-US/docs/WebAssembly/Understanding_the_text_format">Understanding WebAssembly text format - WebAssembly | MDN</A>
								<DT><A HREF="https://www.youtube.com/watch?v=ojYEfRye6aE&t=13s">HELLO WEBASSEMBLY - wat</A>
								<DT><A HREF="https://developer.mozilla.org/en-US/docs/WebAssembly/Text_format_to_wasm">Converting WebAssembly text format to wasm - WebAssembly | MDN</A>
								<DT><A HREF="https://rustwasm.github.io/docs.html">Rust and WebAssembly Documentation | Rust and WebAssembly</A>
							</DL><p>
							<DT><H3 FOLDED>wasm-cpp</H3>
							<DL><p>
								<DT><H3 FOLDED>Emscripten</H3>
								<DL><p>
									<DT><A HREF="https://emscripten.org/">Main — Emscripten 3.0.1-git (dev) documentation</A>
									<DT><H3 FOLDED>installation</H3>
									<DL><p>
										<DT><A HREF="https://emscripten.org/docs/building_from_source/toolchain_what_is_needed.html#toolchain-what-you-need">Emscripten Toolchain Requirements — Emscripten 3.1.9</A>
										<DT><A HREF="https://formulae.brew.sh/formula/emscripten">emscripten — Homebrew Formulae</A>
										<DT><A HREF="https://formulae.brew.sh/formula/llvm#default">llvm — Homebrew Formulae</A>
									</DL><p>
								</DL><p>
								<DT><A HREF="https://medium.com/@tdeniffel/pragmatic-compiling-from-c-to-webassembly-a-guide-a496cc5954b8">Pragmatic compiling of C++ to WebAssembly. A Guide. | by Thomas Deniffel | Medium</A>
								<DT><A HREF="https://web.dev/loading-wasm/">Loading WebAssembly modules efficiently</A>
								<DT><A HREF="https://nodejs.dev/learn/nodejs-with-webassembly">Node.js with WebAssembly</A>
								<DT><A HREF="https://emscripten.org/docs/porting/connecting_cpp_and_javascript/Interacting-with-code.html">Interacting with code — Emscripten 3.1.9-git (dev) documentation</A>
								<DT><A HREF="https://web.dev/emscripten-npm/">Emscripten and npm</A>
								<DT><A HREF="https://log2base2.com/c-with-dsa?utm_src=youtube&utm_target=ycwdeug1&gclid=CjwKCAjwt52mBhB5EiwA05YKo4S_xtW1JucfNad8OMbuQj0b_tg5eej5VfMOx5M8PdeFGvIrlEiHeRoCxHkQAvD_BwE">Learn C Programming | Pointers Visualization | Log2Base2</A>
							</DL><p>
							<DT><A HREF="https://wasmcloud.dev/">wasmCloud Documentation</A>
							<DT><A HREF="https://pragprog.com/titles/khrust/programming-webassembly-with-rust/">Programming WebAssembly with Rust: Unified Development for Web, Mobile, and Embedded Applications by Kevin Hoffman</A>
							<DT><A HREF="https://pspdfkit.com/blog/2017/webassembly-a-new-hope/">WebAssembly: A New Hope | PSPDFKit</A>
							<DT><A HREF="https://github.com/WebAssembly/wabt">WebAssembly/wabt: The WebAssembly Binary Toolkit</A>
						</DL><p>
						<DT><H3 FOLDED>Rust bindings for Python</H3>
						<DL><p>
							<DT><A HREF="https://github.com/prql/prql/tree/main/prql-python">prql/prql-python at main · prql/prql</A>
							<DT><A HREF="https://github.com/PyO3/pyo3">PyO3/pyo3: Rust bindings for the Python interpreter</A>
							<DT><A HREF="https://pyo3.rs/v0.14.5/index.html">Introduction - PyO3 user guide</A>
							<DT><A HREF="https://medium.com/@MatthieuL49/a-mixed-rust-python-project-24491e2af424">How to Mix Rust and Python in Your Project | by Matt | Medium</A>
							<DT><A HREF="https://github.com/PyO3/pyo3/issues/941">Running rust unit tests · Issue #941 · PyO3/pyo3</A>
							<DT><A HREF="http://saidvandeklundert.net/learn/2021-11-18-calling-rust-from-python-using-pyo3/">Calling Rust from Python using PyO3</A>
						</DL><p>
					</DL><p>
					<DT><H3 FOLDED>sw-profiling</H3>
					<DL><p>
						<DT><H3 FOLDED>sw-profiling-micro-benchmarking</H3>
						<DL><p>
							<DT><H3 FOLDED>MUST TRACK AND DO: cupy (RDMA), NCCL perf, fast memcpy using GPUs</H3>
							<DL><p>
								<DT><A HREF="https://github.com/alpa-projects/alpa/blob/main/playground/other/test_cupy_partial_transfer.py">alpa/playground/other/test_cupy_partial_transfer.py at main · alpa-projects/alpa</A>
								<DT><A HREF="https://github.com/cupy/cupy/blob/cf9b4a517725b803fba4828ed0b1c93a231cc5da/cupyx/distributed/_nccl_comm.py#L56">cupy/cupyx/distributed/_nccl_comm.py at cf9b4a517725b803fba4828ed0b1c93a231cc5da · cupy/cupy</A>
								<DT><A HREF="https://github.com/microsoft/debugpy/issues/783">use vscode to remote debug python program with tmux session · Issue #783 · microsoft/debugpy</A>
								<DT><A HREF="https://gist.github.com/geohot/a7b10a1fb8da8621c65412c028c73512">wtf_cuda.py</A>
								<DT><A HREF="https://gist.github.com/geohot/e11dc1b1058ed9e0bc6610249263b024">Test Bandwidth of all reduce</A>
								<DT><A HREF="https://gist.github.com/geohot/3b23bb6be846d3097834d07806ca6563">Fast memcpy using GPUs</A>
								<DT><A HREF="https://colab.research.google.com/drive/1Zk_jYW-Ptf-5svBDFSeBp8HSTnlZnNzY#scrollTo=jICeB9AhtHLz">wtf_cuda.ipynb - Colaboratory</A>
								<DT><A HREF="https://github.com/datacrunch-research/micro">datacrunch-research/micro: Miscellaneous microbenchmarking playground</A>
							</DL><p>
						</DL><p>
						<DT><A HREF="https://www.techrepublic.com/article/how-to-check-drive-space-on-linux-from-the-command-line/">How to check drive space on Linux from the command line</A>
						<DT><A HREF="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-profiler-to-analyze-long-running-jobs%23using-profiler-to-analyze-long-running-jobs">PyTorch Profiler — PyTorch Tutorials 1.12.1+cu102 documentation</A>
						<DT><A HREF="https://odsc.medium.com/optimizing-pytorch-performance-batch-size-with-pytorch-profiler-80e0bf39e80e">Optimizing PyTorch Performance: Batch Size with PyTorch Profiler | by ODSC - Open Data Science | Medium</A>
						<DT><A HREF="https://github.com/Syllo/nvtop">Syllo/nvtop: GPUs process monitoring for AMD, Intel and NVIDIA</A>
						<DT><A HREF="https://docs.contrastsecurity.com/en/python-middleware.html">Configure middleware</A>
						<DT><A HREF="https://www.cyberciti.biz/open-source/install-ncdu-on-linux-unix-ncurses-disk-usage/">ncdu</A>
						<DT><A HREF="https://unix.stackexchange.com/questions/125429/tracking-down-where-disk-space-has-gone-on-linux">du</A>
						<DT><A HREF="https://serverfault.com/questions/517483/how-to-read-memory-usage-in-htop">How to read memory usage in htop?</A>
					</DL><p>
					<DT><H3 FOLDED>concurrency</H3>
					<DL><p>
						<DT><A HREF="https://go.dev/blog/codelab-share">Share Memory By Communicating - The Go Programming Language</A>
						<DT><A HREF="https://go.dev/ref/mem">The Go Memory Model - The Go Programming Language</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Concurrency_(computer_science)">Concurrency (computer science) - Wikipedia</A>
						<DT><A HREF="https://stackoverflow.com/questions/36391421/explain-dont-communicate-by-sharing-memory-share-memory-by-communicating">go - Explain: Don't communicate by sharing memory; share memory by communicating - Stack Overflow</A>
						<DT><A HREF="https://man7.org/linux/man-pages/man7/shm_overview.7.html">shm_overview(7) - Linux manual page</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Communicating_sequential_processes">Communicating sequential processes - Wikipedia</A>
						<DT><A HREF="https://github.com/LaurentMazare/hojo">LaurentMazare/hojo: A small python library to run iterators in a separate process</A>
						<DT><A HREF="https://www.youtube.com/watch?v=Iqrd9vsLrak">Coroutine Patterns: Problems and Solutions Using Coroutines in a Modern Codebase</A>
					</DL><p>
					<DT><H3 FOLDED>sw-diffusion-optimization</H3>
					<DL><p>
						<DT><H3 FOLDED>Pseudo-Linear Multistep Sampling (PLMS)</H3>
						<DL><p>
							<DT><A HREF="https://arxiv.org/abs/2312.00858">[2312.00858] DeepCache: Accelerating Diffusion Models for Free</A>
							<DT><A HREF="https://github.com/horseee/DeepCache">horseee/DeepCache: [CVPR 2024] DeepCache: Accelerating Diffusion Models for Free</A>
						</DL><p>
						<DT><H3 FOLDED>minSDXL</H3>
						<DL><p>
							<DT><A HREF="https://github.com/cloneofsimo/minSDXL">cloneofsimo/minSDXL: Huggingface-compatible SDXL Unet implementation that is readily hackable</A>
						</DL><p>
						<DT><H3 FOLDED>stable-diffusion.cpp</H3>
						<DL><p>
							<DT><A HREF="https://github.com/leejet/stable-diffusion.cpp">leejet/stable-diffusion.cpp: Stable Diffusion in pure C/C++</A>
						</DL><p>
						<DT><A HREF="https://www.vrushankdes.ai/diffusion-inference-optimization">Diffusion Inference Optimization</A>
						<DT><A HREF="https://huggingface.co/blog/annotated-diffusion">The Annotated Diffusion Model</A>
						<DT><A HREF="https://huggingface.co/blog/deploy-deepfloydif-using-bentoml#building-and-serving-a-bento">Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action</A>
						<DT><A HREF="https://huggingface.co/docs/diffusers/main/en/stable_diffusion#memory">Effective and efficient diffusion</A>
						<DT><A HREF="https://huggingface.co/docs/diffusers/main/en/optimization/fp16">Memory and speed</A>
						<DT><A HREF="https://github.com/tmabraham/diffusion_reading_group">Diffusion Reading Group at EleutherAI</A>
						<DT><A HREF="https://huggingface.co/blog/sdxl_ort_inference">Accelerating SD Turbo and SDXL Turbo Inference with ONNX Runtime and Olive</A>
						<DT><A HREF="https://github.com/siliconflow/onediff/tree/main">siliconflow/onediff: OneDiff: An out-of-the-box acceleration library for diffusion models.</A>
					</DL><p>
					<DT><H3 FOLDED>sw-embeddings</H3>
					<DL><p>
						<DT><H3 FOLDED>embedding-database</H3>
						<DL><p>
							<DT><A HREF="https://github.com/chroma-core/chroma">chroma-core/chroma: the open source embedding database</A>
							<DT><A HREF="https://www.trychroma.com/">Chroma: Open-source embedding database</A>
							<DT><A HREF="https://www.pinecone.io/">Vector Database for Vector Search | Pinecone</A>
							<DT><A HREF="https://github.com/milvus-io/milvus">milvus-io/milvus: A cloud-native vector database, storage for next generation AI applications</A>
							<DT><A HREF="https://milvus.io/">Vector database - Milvus</A>
						</DL><p>
						<DT><A HREF="https://carbon.now.sh/?bg=rgba%28171%2C+184%2C+195%2C+1%29&t=seti&wt=none&l=auto&width=680&ds=true&dsyoff=20px&dsblur=68px&wc=true&wa=true&pv=56px&ph=56px&ln=false&fl=1&fm=Hack&fs=14px&lh=133%25&si=false&es=2x&wm=false">Carbon | Create and share beautiful images of your source code</A>
						<DT><A HREF="https://twitter.com/hxiao/status/1683369244937838597">JinaAI 35 M to 6 B</A>
						<DT><A HREF="https://platform.openai.com/docs/api-reference/embeddings">API Reference - OpenAI API</A>
						<DT><A HREF="https://github.com/Gage-Technologies/embedding-server/commit/d77b9c0dd0070a3ce643f0c7fe5579524951e2ba#diff-1164ff7fc1a41ffa864162f3fca2f0407c544ece208ef32c7853b6b34c6eb445">TGI</A>
						<DT><A HREF="https://github.com/huggingface/text-generation-inference/issues/199">[Feature] Return embeddings · Issue #199 · huggingface/text-generation-inference</A>
						<DT><A HREF="https://www.youtube.com/watch?v=93yueQQnqpM">RetrievalQA with LLaMA 2 70b &amp; Chroma DB - YouTube</A>
						<DT><A HREF="https://github.com/HKUNLP/instructor-embedding">HKUNLP/instructor-embedding: [ACL 2023] One Embedder, Any Task: Instruction-Finetuned Text Embeddings</A>
					</DL><p>
					<DT><H3 FOLDED>sw-audio</H3>
					<DL><p>
						<DT><A HREF="https://github.com/samim23/polymath">samim23/polymath: Convert any music library into a music production sample-library with ML</A>
						<DT><A HREF="https://github.com/spotify/basic-pitch">spotify/basic-pitch: A lightweight yet powerful audio-to-MIDI converter with pitch bend detection</A>
					</DL><p>
					<DT><H3 FOLDED>sw-docs</H3>
					<DL><p>
						<DT><H3 FOLDED>markup languages</H3>
						<DL><p>
							<DT><H3 FOLDED>Markdown</H3>
							<DL><p>
								<DT><A HREF="https://support.typora.io/Markdown-Reference/">Reference</A>
								<DT><A HREF="http://theme.typora.io/">Themes Gallery — Typora</A>
								<DT><A HREF="https://pandoc.org/#">Pandoc - About</A>
								<DT><A HREF="https://sindresorhus.com/github-markdown-css/">Manual</A>
								<DT><A HREF="https://github.com/microsoft/markitdown">microsoft/markitdown: Python tool for converting files and office documents to Markdown.</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>LaTeX</H3>
						<DL><p>
							<DT><A HREF="https://github.com/Armael/coq-procrastination/blob/master/manual/manual.tex">{coql}</A>
							<DT><A HREF="https://www.overleaf.com/learn/latex/Page_size_and_margins">Page size and margins</A>
							<DT><A HREF="https://www.overleaf.com/learn/latex/Paragraph_formatting">Paragraph formatting</A>
							<DT><A HREF="https://www.overleaf.com/learn/latex/Sections_and_chapters">Sections and chapters</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/36880/insert-a-blank-page-after-current-page">Insert a blank page after current page</A>
							<DT><A HREF="https://www.overleaf.com/learn/latex/Bold,_italics_and_underlining">Bold, italics and underlining - Overleaf, Online LaTeX Editor</A>
							<DT><A HREF="https://dle.rae.es/eximir?m=form">eximir | Definición | Diccionario de la lengua española | RAE - ASALE</A>
							<DT><A HREF="http://www.personal.ceu.hu/tex/breaking.htm">Line and Page Breaking</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/325297/how-to-scale-a-tikzcd-diagram">How to scale a tikzcd diagram-adjust size</A>
							<DT><A HREF="https://tools.ietf.org/doc/texlive-doc/latex/tikz-cd/tikz-cd-doc.pdf">diagrams</A>
							<DT><A HREF="http://www.ccs.neu.edu/home/wand/csg264/latex/mathpartir/mathpartir.pdf">inference rules</A>
							<DT><A HREF="https://www.overleaf.com/project/60c202634ef716ece06a7e7b">Inference Rules ith the semantic Package</A>
							<DT><A HREF="https://www.geeksforgeeks.org/inequalities-in-latex/">Inequalities in LaTeX</A>
							<DT><A HREF="https://almdemo.polarion.com/polarion/help/index.jsp?topic=%2Fcom.polarion.xray.doc.user%2Fguide%2Fxid1661363.html">Help - Polarion ALM Platform</A>
							<DT><A HREF="https://truben.no/latex/table/">Untitled - Table Editor</A>
							<DT><A HREF="https://nasa.github.io/nasa-latex-docs/html/">NASA-LaTeX-Docs — NASA-LaTeX-Docs documentation</A>
							<DT><A HREF="https://nasa.github.io/nasa-latex-docs/html/examples/table.html">LaTeX Tables — NASA-LaTeX-Docs documentation</A>
							<DT><A HREF="https://people.engr.tamu.edu/hlee42/csce222/truth-table.pdf">disjunction truth table</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/61033/setting-toc-depth-not-working">table of contents - Setting TOC depth</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/422197/latex-environment-to-write-in-plain-text-mode">write in plain text mode</A>
							<DT><A HREF="https://liliaquituisacasamaniego.com/2013/03/25/latex-file-ended-while-scanning-use-of-writefile/">LaTeX – file ended while scanning use of @writefile</A>
							<DT><A HREF="https://llevatilde.es/palabra/extr%C3%A1e">¿Lleva tilde extrae? | LlevaTilde.es</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/30757/change-the-word-chapter-to-something-else">naming - Change the word "Chapter" to something else - TeX - LaTeX Stack Exchange</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/12597/renaming-the-bibliography-page-using-bibtex">Renaming the bibliography page using BibTeX</A>
							<DT><A HREF="http://metodos.fam.cie.uva.es/~latex/apuntes/apuntes19.pdf">Bibtex</A>
							<DT><A HREF="https://tex.stackexchange.com/questions/329707/how-to-increase-textheight-without-changing-the-topmargin">How to increase \textheight, without changing the topmargin - TeX - LaTeX Stack Exchange</A>
							<DT><A HREF="https://en.wikipedia.org/wiki/List_of_mathematical_symbols_by_subject#Group_theory">List of mathematical symbols by subject - Wikipedia</A>
							<DT><A HREF="https://twitter.com/BorisAKnyazev/status/1531275326482956291">"Dear paper writers, consider using \𝘶𝘴𝘦𝘱𝘢𝘤𝘬𝘢𝘨𝘦[𝗯𝗮𝗰𝗸𝗿𝗲𝗳=𝗽𝗮𝗴𝗲]{𝘩𝘺𝘱𝘦𝘳𝘳𝘦𝘧} instead of simply \𝘶𝘴𝘦𝘱𝘢𝘤𝘬𝘢𝘨𝘦{𝘩𝘺𝘱𝘦𝘳𝘳𝘦𝘧}</A>
						</DL><p>
						<DT><H3 FOLDED>diverso</H3>
						<DL><p>
							<DT><A HREF="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html">Directives — Sphinx documentation</A>
							<DT><A HREF="https://mkdocs.readthedocs.io/en/0.9/user-guide/writing-your-docs/">MkDocs</A>
							<DT><A HREF="https://docs.readthedocs.io/en/stable/tutorial/index.html#getting-started">Read the Docs tutorial — Tutorial</A>
							<DT><A HREF="https://carla.readthedocs.io/en/latest/">CARLA Simulator (Example)</A>
							<DT><H3 FOLDED>Read the docs template</H3>
							<DL><p>
								<DT><A HREF="https://github.com/readthedocs/tutorial-template/">readthedocs/tutorial-template: Template for the Read the Docs tutorial</A>
								<DT><A HREF="https://docs.readthedocs.io/en/stable/tutorial/index.html">Read the Docs tutorial — Read the Docs user documentation 7.6.0 documentation</A>
								<DT><A HREF="https://docs.readthedocs.io/en/stable/tutorial/index.html#getting-started">Read the Docs tutorial — Read the Docs user documentation 7.6.0 documentation</A>
								<DT><A HREF="https://www.ericholscher.com/blog/2016/jul/1/sphinx-and-rtd-for-writers/">An introduction to Sphinx and Read the Docs for Technical Writers — Eric Holscher</A>
								<DT><A HREF="https://github.com/diverso-lab/core/wiki/1.-Home">1. Home · diverso-lab/core Wiki</A>
								<DT><A HREF="https://www.wordreference.com/es/translation.asp?tranword=languagje">languagje - English-Spanish Dictionary - WordReference.com</A>
							</DL><p>
							<DT><A HREF="https://www.ericholscher.com/blog/2016/jul/1/sphinx-and-rtd-for-writers/">An introduction to Sphinx and Read the Docs for Technical Writers</A>
						</DL><p>
						<DT><H3 FOLDED>tools</H3>
						<DL><p>
							<DT><A HREF="https://readthedocs.org/">Home | Read the Docs</A>
							<DT><H3 FOLDED>Github pages</H3>
							<DL><p>
								<DT><A HREF="https://jekyllrb.com/">Jekyll • Simple, blog-aware, static sites | Transform your plain text into static websites and blogs</A>
								<DT><A HREF="https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/about-github-pages-and-jekyll">Set up site with Jekyll</A>
								<DT><A HREF="https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/creating-a-github-pages-site-with-jekyll">Creating a GitHub Pages site with Jekyll - GitHub Docs</A>
							</DL><p>
							<DT><H3 FOLDED>gitbooks</H3>
							<DL><p>
								<DT><A HREF="https://www.gitbook.com/">GitBook - Where software teams break knowledge silos.</A>
								<DT><A HREF="https://docs.gitbook.com/editing-content/editing-pages/change-requests">Change requests - GitBook Documentation</A>
								<DT><A HREF="https://docs.gitbook.com/integrations/git-sync/enabling-github-sync">Enabling GitHub Sync - GitBook Documentation</A>
								<DT><A HREF="https://www.garyng.xyz/gtil-gitbook/GitBook/relative-internal-links-in-gitbook.html">Relative Internal Links in GitBook · Today I Learned...</A>
								<DT><A HREF="https://seadude.gitbooks.io/learn-gitbook/content/chapter1/internal.html">Internal Links · Learn Gitbook</A>
								<DT><A HREF="https://docs.gitbook.com/editing-content/rich-text">Rich text - GitBook Documentation</A>
								<DT><A HREF="https://stackoverflow.com/questions/2822089/how-to-link-to-part-of-the-same-document-in-markdown">multimarkdown - How to link to part of the same document in Markdown? - Stack Overflow</A>
							</DL><p>
							<DT><A HREF="https://www.mkdocs.org/">MkDocs</A>
							<DT><H3 FOLDED>Jekyll</H3>
							<DL><p>
								<DT><A HREF="https://jekyllrb.com/">Home</A>
								<DT><A HREF="https://jekyllrb.com/docs/installation/macos/">Jekyll installation on macOS</A>
								<DT><A HREF="https://github.com/sighingnow/jekyll-gitbook">jekyll-gitbook</A>
								<DT><A HREF="https://www.youtube.com/watch?v=HVLl8GaduPQ">Video overview</A>
								<DT><A HREF="https://jekyllrb.com/docs/front-matter/">Front Matter</A>
								<DT><A HREF="https://mademistakes.com/mastering-jekyll/how-to-link/#how-to-link">How to use URLs and links in Jekyll - Made Mistakes</A>
								<DT><A HREF="https://github.com/benbalter/jekyll-relative-links">benbalter/jekyll-relative-links: A Jekyll plugin to convert relative links to markdown files to their rendered equivalents</A>
								<DT><A HREF="https://unruffled-ardinghelli-55d901.netlify.app/">Latex Jekyll</A>
								<DT><A HREF="http://jekyllthemes.org/page2/">Jekyll Themes</A>
							</DL><p>
						</DL><p>
						<DT><H3 FOLDED>grammarly</H3>
						<DL><p>
							<DT><A HREF="https://support.grammarly.com/hc/en-us/articles/4412828867085-My-cursor-disappeared-in-the-Grammarly-Editor-for-Windows-and-Mac">My cursor disappeared in the Grammarly Editor</A>
						</DL><p>
						<DT><A HREF="https://towardsdatascience.com/how-to-easily-draw-neural-network-architecture-diagrams-a6b6138ed875">Diagrams: Neural Network</A>
						<DT><A HREF="https://github.com/kennethleungty/Neural-Network-Architecture-Diagrams">GitHub - kennethleungty/Neural-Network-Architecture-Diagrams: Diagrams for visualizing neural network architecture (Created with diagrams.net)</A>
						<DT><A HREF="https://x.com/_xjdr/status/1827737014315483483">(1) xjdr en X: "This is great The number 1 question I get asked by traditional enterprise execs is "what can I be doing right now to get ready for AI at our company" and this is my number 1 response (organize for LLM search, rewrite for LLM consumption)." / X</A>
					</DL><p>
				</DL><p>
				<DT><H3 FOLDED>Mathematics</H3>
				<DL><p>
					<DT><H3 FOLDED>institutions</H3>
					<DL><p>
						<DT><A HREF="https://www.ias.edu/video">Institute for Advanced Study</A>
						<DT><A HREF="https://simons.berkeley.edu/">Simons Institute for the Theory of Computing</A>
						<DT><A HREF="https://plato.stanford.edu/entries/hilbert-program/">Hilbert’s Program (Stanford Encyclopedia of Philosophy)</A>
					</DL><p>
					<DT><H3 FOLDED>computing</H3>
					<DL><p>
						<DT><A HREF="https://www.abelard.org/turpap2/tp2-ie.asp">On computable numbers, with an application to the Entscheidungsproblem - A. M. Turing, 1936</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/certified+programming">certified programming</A>
					</DL><p>
					<DT><H3 FOLDED>lambda calculus</H3>
					<DL><p>
					</DL><p>
					<DT><H3 FOLDED>calculus of constructions</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/calculus+of+constructions">calculus of constructions</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/pure+type+system">pure type system</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/lambda-calculus">lambda-calculus</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/computational+trinitarianism">computational trinitarianism</A>
					</DL><p>
					<DT><H3 FOLDED>algebra</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/algebraically+injective+object">algebraically injective object</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/injection">injection</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/homomorphism">homomorphism</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/disjoint+subsets">disjoint subsets</A>
						<DT><A HREF="https://linear.axler.net/">Linear Algebra Done Right (Sheldon Axler)</A>
					</DL><p>
					<DT><H3 FOLDED>logic</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/predicate+logic">predicate logic</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/ex+falso+quodlibet">ex falso quodlibet</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/inductive+reasoning">inductive reasoning</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/proposition">proposition</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/decidable+proposition">decidable proposition</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/modus+ponens">modus ponens</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/natural+deduction">natural deduction</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/relation">relation</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/proof">proof</A>
					</DL><p>
					<DT><H3 FOLDED>calculus on manifolds</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set in nLab</A>
					</DL><p>
					<DT><H3 FOLDED>type theory</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/preimage">preimage</A>
						<DT><A HREF="https://pjreddie.com/coq-tactics/">Coq Tactic Index</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Martin-L%C3%B6f+dependent+type+theory">Martin-Löf dependent type theory in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Phenomenology+of+Spirit">Phenomenology of Spirit in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/semantics">semantics</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/function+extensionality">function extensionality</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/identity+type">identity type</A>
						<DT><A HREF="https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F2699407&file=p75-wadler-supp.pdf">Propositions as Types-Philip Wadler</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/dependent+type+theory">dependent type theory</A>
						<DT><A HREF="http://www-sop.inria.fr/members/Yves.Bertot/tsinghua/tsinghua-1.pdf">Introduction to dependent types in Coq</A>
						<DT><A HREF="https://medium.com/background-thread/the-future-of-programming-is-dependent-types-programming-word-of-the-day-fcd5f2634878">The Future of Programming is Dependent Types</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/type">type</A>
					</DL><p>
					<DT><H3 FOLDED>proof assistant</H3>
					<DL><p>
						<DT><A HREF="https://coq.inria.fr/distrib/current/refman/">Introduction and Contents — Coq 8.13.0 documentation</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/ML_(programming_language)">ML (programming language) - Wikipedia</A>
						<DT><A HREF="https://ocaml.org/">OCaml – OCaml</A>
						<DT><A HREF="https://gmplib.org/list-archives/gmp-announce/2020-November/000049.html">GMP 6.2.1 released</A>
						<DT><A HREF="https://formulae.brew.sh/formula/coq#default">coq — Homebrew Formulae</A>
						<DT><A HREF="https://formulae.brew.sh/cask/coqide#default">coqide — Homebrew Formulae</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/separation+algebra">separation algebra</A>
						<DT><H3 FOLDED>Coq</H3>
						<DL><p>
							<DT><A HREF="https://ncatlab.org/nlab/show/definition">definition in nLab</A>
							<DT><A HREF="https://wiki.portal.chalmers.se/cse/pmwiki.php/ForMath/ForMath">ForMath: Formalisation of Mathematics</A>
							<DT><A HREF="https://drops.dagstuhl.de/opus/volltexte/2006/432/">DROPS - Introduction to the Flyspeck Project</A>
							<DT><A HREF="https://ncatlab.org/nlab/show/Archive+of+Formal+Proofs">Archive of Formal Proofs</A>
							<DT><A HREF="https://ncatlab.org/nlab/show/Coq">Coq</A>
							<DT><A HREF="https://coq.inria.fr/distrib/current/refman/language/core/basic.html">Coq 8.13.1 documentation</A>
							<DT><A HREF="ftp://mizar.uwb.edu.pl/pub/qed/manifesto">QED manifesto</A>
						</DL><p>
					</DL><p>
					<DT><H3 FOLDED>set theory</H3>
					<DL><p>
						<DT><A HREF="https://en.wikipedia.org/wiki/Groupoid">Groupoid - Wikipedia</A>
						<DT><A HREF="https://www.pinterest.es/pin/156077943317011320/">Number theory sets</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/function">function in nLab</A>
					</DL><p>
					<DT><H3 FOLDED>category theory</H3>
					<DL><p>
						<DT><A HREF="https://arxiv.org/abs/1809.05923">What is Applied Category Theory?</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Timeline+of+category+theory+and+related+mathematics">Timeline of category theory and related mathematics</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/generalized+element">generalized element</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/morphism">morphism</A>
					</DL><p>
					<DT><H3 FOLDED>proof theory</H3>
					<DL><p>
						<DT><A HREF="https://www.google.com/search?client=safari&rls=en&sxsrf=ALeKk02CUmoD_hscdezGf0jizv1UxWm0Zw%3A1595607125474&ei=VQgbX-C-HIGOlwSBh5r4Dg&q=David+Hilbert+proof+theory&oq=David+Hilbert+proof+theory&gs_lcp=CgZwc3ktYWIQAzoHCC4QJxCTAjoHCAAQFBCHAjoCCAA6AgguOgcIIxDqAhAnOgcILhDqAhAnOgQIIxAnOgQILhBDOggILhDHARCjAjoFCC4QkQI6BwguEBQQhwI6CAguEJECEJMCOgUILhDLAToFCAAQywE6CAguEMcBEK8BOgYIABAWEB46BwghEAoQoAE6CAghEBYQHRAeOgUIIRCgAVCo4kJYjq1DYIiwQ2gIcAB4AIABtQGIAc8YkgEENS4yM5gBAKABAaoBB2d3cy13aXqwAQrAAQE&sclient=psy-ab&ved=0ahUKEwigvajfo-bqAhUBx4UKHYGDBu8Q4dUDCAs&uact=5">David Hilbert proof theory - Google Search</A>
						<DT><A HREF="https://www.marxists.org/reference/subject/philosophy/works/ge/hilbert.htm">Foundations of Mathematics By David Hilbert (1927)</A>
					</DL><p>
					<DT><H3 FOLDED>univalence foundations</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/univalence+axiom#InSimplicialSets">univalence axiom in nLab</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence">Curry–Howard correspondence - Wikipedia</A>
						<DT><A HREF="https://www.idris-lang.org/">Idris: A Language for Type-Driven Development</A>
						<DT><A HREF="https://www.fstar-lang.org/tutorial/">F* Tutorial</A>
						<DT><A HREF="http://goto.ucsd.edu/~ravi/research/oopsla12-djs.pdf">Dependent Types for JavaScript</A>
						<DT><A HREF="http://smallcultfollowing.com/babysteps/blog/2016/11/02/associated-type-constructors-part-1-basic-concepts-and-introduction/">Associated type constructors</A>
						<DT><A HREF="https://crates.io/crates/type_level_values">type_level_values - crates.io: Rust </A>
						<DT><A HREF="https://nalgebra.org/">nalgebra linear-algebra library | nalgebra</A>
						<DT><A HREF="https://webusers.imj-prg.fr/~leila.schneps/grothendieckcircle/Letters/GS.pdf">GROTHENDIECK-SERRE CORRESPONDENCE</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Groupoid">Groupoid - Wikipedia</A>
						<DT><A HREF="https://www.probabilitycourse.com/chapter1/1_2_3_cardinality.php">Cardinality | Finite Sets | Infinite Sets </A>
						<DT><A HREF="https://www.math.ias.edu/~vladimir/Site3/Univalent_Foundations.html">Univalent Foundations of Mathematics</A>
						<DT><A HREF="https://www.math.ias.edu/~vladimir/Foundations_library/toc.html">Table of contents</A>
						<DT><A HREF="https://medium.com/background-thread/the-future-of-programming-is-dependent-types-programming-word-of-the-day-fcd5f2634878">The Future of Programming is Dependent Types</A>
						<DT><A HREF="https://flint.cs.yale.edu/flint/software.html">Yale FLINT Group: Software</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Axiomatic_system">Axiomatic system - Wikipedia</A>
						<DT><A HREF="https://www.google.com/search?client=safari&rls=en&q=Martin+lof+type+theory&ie=UTF-8&oe=UTF-8">Martin lof type theory - Google Search</A>
						<DT><A HREF="https://www.google.com/search?client=safari&rls=en&q=formal+language&ie=UTF-8&oe=UTF-8">formal language - Google Search</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/HomePage">nLab</A>
						<DT><A HREF="https://github.com/UniMath/Foundations">UniMath/Foundations: Voevodsky's original development of the univalent foundations of mathematics in Coq</A>
						<DT><A HREF="https://github.com/UniMath/UniMath">UniMath/UniMath: This coq library aims to formalize a substantial body of mathematics using the univalent point of view.</A>
						<DT><A HREF="https://github.com/EgbertRijke/HoTT-Intro">EgbertRijke/HoTT-Intro: An introductory course to Homotopy Type Theory</A>
						<DT><A HREF="https://arxiv.org/abs/1401.0053">Experimental library of univalent formalization of mathematics</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set in nLab</A>
					</DL><p>
					<DT><H3 FOLDED>homotopy type theory</H3>
					<DL><p>
						<DT><A HREF="https://en.wikipedia.org/wiki/Homotopy">Homotopy - Wikipedia</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/simplicial+complex#RemarkOnTerminology">simplicial complex in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/simplicial+complex">simplicial complex in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/simplicial+set">simplicial set</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/semi-simplicial+set">semi-simplicial set</A>
						<DT><A HREF="https://en.wikipedia.org/wiki/Modus_ponens">Modus ponens - Wikipedia</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/family+of+sets">family of sets in nLab</A>
						<DT><A HREF="https://golem.ph.utexas.edu/category/2013/06/the_hott_book.html#more">The HoTT Book | The n-Category Café</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/homotopy.io">homotopy.io in nLab</A>
						<DT><A HREF="http://www.andrew.cmu.edu/user/erijke/hott/">Introduction to Homotopy Type Theory</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/homotopy">homotopy in nLab</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/function+extensionality">function extensionality</A>
					</DL><p>
					<DT><H3 FOLDED>Statistics</H3>
					<DL><p>
						<DT><A HREF="http://www.stat.cmu.edu/~larry/=stat705/">10-705 Intermediate Statistics, Fall 2020</A>
						<DT><A HREF="http://pub.math.leidenuniv.nl/~szabobt/STAN.html">Statistiek AN, 2018-2019</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/permutation">permutation</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/probability+distribution">probability distribution</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/supervised+learning">supervised learning</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/probability+density">probability density</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Banach+space">Banach space</A>
						<DT><A HREF="https://ncatlab.org/nlab/show/Wasserstein+metric">Wasserstein metric</A>
						<DT><A HREF="https://www.youtube.com/watch?v=R9aMV8QIEB0">The Best Book Ever Written on Mathematical Statistics - YouTube</A>
					</DL><p>
					<DT><H3 FOLDED>TeX</H3>
					<DL><p>
						<DT><A HREF="https://sourabhbajaj.com/mac-setup/LaTeX/">LaTeX · macOS Setup Guide</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/195291/inserting-graph-from-a-software-in-latex">Inserting graph from a software in Latex - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="https://en.wikibooks.org/wiki/LaTeX/Mathematics">LaTeX/Mathematics - Wikibooks, open books for an open world</A>
						<DT><A HREF="http://tex.loria.fr/general/mil.pdf">‎tex.loria.fr/general/mil.pdf</A>
						<DT><A HREF="https://en.wikibooks.org/wiki/LaTeX/Basics">LaTeX/Basics - Wikibooks, open books for an open world</A>
						<DT><A HREF="https://support.typora.io/Math/#limitations">Math and Academic Functions</A>
						<DT><A HREF="http://nickgeorge.net/programming/latex_setup/">Setting up LaTeX on a Mac</A>
						<DT><A HREF="https://www.overleaf.com/project/5fb2ebb49f6f3f20dfb6e766">Example - Online LaTeX Editor Overleaf</A>
						<DT><A HREF="https://i0.wp.com/texblog.org/Wordpress/wp-content/uploads/2008/02/font-modifications-latex.png">font-modifications</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/115432/best-practice-for-typesetting-quantifiers">spacing - Best practice for typesetting quantifiers? - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols">List of LaTeX mathematical symbols - OeisWiki</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/351660/double-superscript">Double Superscript - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="http://www.personal.ceu.hu/tex/breaking.htm">LaTeX Line and Page Breaking</A>
						<DT><A HREF="https://www.overleaf.com/learn/latex/sections_and_chapters">Sections and chapters - Overleaf, Online LaTeX Editor</A>
						<DT><A HREF="https://www.overleaf.com/learn/latex/Creating_a_document_in_LaTeX">Creating a document in LaTeX - Overleaf, Online LaTeX Editor</A>
						<DT><A HREF="https://www.lipsum.com/">Lorem Ipsum - All the facts - Lipsum generator</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/539796/subsubsubsection-paragraph-not-showing-in-toc">table of contents - Subsubsubsection (paragraph) not showing in TOC - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="https://www.overleaf.com/learn/latex/Bold,_italics_and_underlining">Bold, italics and underlining - Overleaf, Online LaTeX Editor</A>
						<DT><A HREF="https://www.overleaf.com/learn/latex/Inserting_Images">Inserting Images - Overleaf, Online LaTeX Editor</A>
						<DT><A HREF="https://stackoverflow.com/questions/3175105/inserting-code-in-this-latex-document-with-indentation">Inserting code in this LaTeX document with indentation - Stack Overflow</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/85904/showcase-of-beautiful-title-page-done-in-tex">typography - Showcase of beautiful title page done in TeX - TeX - LaTeX Stack Exchange</A>
						<DT><A HREF="https://github.com/ajvondrak/coq/blob/master/coq.tex">coq/coq.tex at master · ajvondrak/coq</A>
						<DT><A HREF="https://tex.stackexchange.com/questions/8357/how-to-have-local-package-override-default-package/8359#8359">Internal directories structure</A>
						<DT><A HREF="https://github.com/amunn/make-local-texmf">texmf hierarchy</A>
						<DT><A HREF="https://coq.inria.fr/refman/using/tools/coqdoc.html">coqdoc</A>
					</DL><p>
					<DT><H3 FOLDED>basic algebra</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/injection">injection in nLab</A>
					</DL><p>
					<DT><H3 FOLDED>foundations</H3>
					<DL><p>
						<DT><A HREF="https://ncatlab.org/nlab/show/foundation+of+mathematics#EilenbergSteenrod">foundation of mathematics</A>
					</DL><p>
					<DT><A HREF="https://terrytao.wordpress.com/2009/01/01/245b-notes-0-a-quick-review-of-measure-and-integration-theory/">245B, notes 0: A quick review of measure and integration theory | What's new</A>
					<DT><A HREF="https://papers.labml.ai/paper/16a1cbf8f68f11ecb9b9d35608ee6155">Pen and Paper Exercises in Machine Learning</A>
					<DT><H3 FOLDED>Sheaf Theory</H3>
					<DL><p>
					</DL><p>
					<DT><H3 FOLDED>Differential Geometry</H3>
					<DL><p>
					</DL><p>
					<DT><H3 FOLDED>Algebraic Topology</H3>
					<DL><p>
					</DL><p>
					<DT><H3 FOLDED>Chaos: Non-Linear Dynamical Systems</H3>
					<DL><p>
						<DT><A HREF="https://www.wolframalpha.com/widgets/view.jsp?id=c731077c04035ac9e92a3706288db18f">Wolfram|Alpha Widgets: "Logistic Map" - Free Mathematics Widget</A>
						<DT><A HREF="https://geoffboeing.com/2015/03/chaos-theory-logistic-map/">Chaos Theory and the Logistic Map – Geoff Boeing</A>
						<DT><A HREF="https://github.com/brorson/FeigenbaumConstants/blob/master/CAJUNTalk.pdf">FeigenbaumConstants/CAJUNTalk.pdf at master · brorson/FeigenbaumConstants</A>
						<DT><A HREF="https://github.com/gboeing/pynamical/blob/39478f13c98d86f5fafc23875e68c99dd15fc879/pynamical/pynamical.py#L186">pynamical/pynamical.py at 39478f13c98d86f5fafc23875e68c99dd15fc879 · gboeing/pynamical</A>
						<DT><A HREF="https://github.com/gboeing/pynamical">gboeing/pynamical: Pynamical is a Python package for modeling and visualizing discrete nonlinear dynamical systems, chaos, and fractals.</A>
					</DL><p>
				</DL><p>
				<DT><H3 FOLDED>ai-jobs</H3>
				<DL><p>
					<DT><H3 FOLDED>ai-jobs-hiring</H3>
					<DL><p>
						<DT><H3 FOLDED>hiring-popular</H3>
						<DL><p>
							<DT><A HREF="https://www.indeed.com/q-Math-Software-Engineer-jobs.html">Math Software Engineer Jobs, Employment | Indeed.com</A>
							<DT><A HREF="https://www.simplyhired.com/search?q=mathematical+software+developer&job=RsU31mOh80Jmmo7WTgmur_p1hgEjCAW3Ni0kvM1M5qdfpIHKCj7tvw">SimplyHired</A>
							<DT><A HREF="https://www.glassdoor.com/Job/remote-mathematics-jobs-SRCH_IL.0,6_IS11047_KO7,18.htm">Glassdoor</A>
							<DT><A HREF="https://engineering.atspotify.com/">Spotify Engineering</A>
							<DT><A HREF="https://www.workatastartup.com/application/preview">Y Combinator's Work at a Startup</A>
							<DT><A HREF="https://jobs.lever.co/cohere/fab6d469-b35c-4c72-be88-30fe3effe570/apply">Cohere - Machine Learning Intern (Fall 2022)</A>
						</DL><p>
						<DT><H3 FOLDED>how we hire</H3>
						<DL><p>
							<DT><A HREF="https://careers.google.com/how-we-hire/#step-your-resume">How we hire - Google Careers</A>
							<DT><A HREF="https://www.google.com/about/careers/applications/jobs/results/">Search Jobs — Google Careers</A>
							<DT><A HREF="https://scholar.google.com/citations?hl=en&user=aON3WZYAAAAJ&view_op=list_works&gmla=AOV7GLM2TckIKC7Mc7ZA3FSqKOLUKYf7X8mE915uFnfQ_2qb17xgxLafbFNftpTSWd_anXcnGVCsYkOw1KOcHcUL">‪Antonio J. Dominguez‬ - ‪Google Scholar‬</A>
							<DT><A HREF="https://wellfound.com/jobs/2708656-principal-software-engineer-c-cuda-raptor">Principal Software Engineer C++ / CUDA (Raptor) at SpaceX • Hawthorne | Wellfound (formerly AngelList Talent)</A>
						</DL><p>
						<DT><A HREF="https://boards.greenhouse.io/spacex/jobs/4867945002?gh_jid=4867945002">Job Application for Summer 2021 Associate Engineer - SpaceX</A>
						<DT><A HREF="https://jobs.raytheonmissilesanddefense.com/search-jobs?orgIds=30457&ac=22805&ascf=%5B%7B%22key%22:%22job_type%22,%22value%22:%22College%20Jobs%22%7D%5D">Search our Job Opportunities at Raytheon Technologies</A>
						<DT><A HREF="https://www.works-hub.com/login">WorksHub</A>
						<DT><A HREF="https://www.isomorphiclabs.com/blog">Isomorphic Labs | Blog</A>
						<DT><A HREF="https://www.talentbyblind.com/profile">Talent By Blind | Get More Offers</A>
						<DT><A HREF="https://www.lifeatspotify.com/students">Students | Life at Spotify</A>
						<DT><A HREF="https://adobe.wd5.myworkdayjobs.com/en-US/external_university/userHome">Adobe careers</A>
						<DT><A HREF="https://hnhiring.com/">All Jobs From Hacker News 'Who is Hiring?' Posts | HNHIRING</A>
						<DT><A HREF="https://www.spinics.net/lists/pgsql-jobs/index.html#01055">Postgresql Jobs</A>
						<DT><A HREF="https://www.wecanbeheroes.io/startup/iomed-0">IOMED jobs: Your career at one of Europe's coolest startups</A>
						<DT><A HREF="https://wasmer.io/">Wasmer - The Universal WebAssembly Runtime</A>
						<DT><A HREF="https://angel.co/jobs">Startup Jobs | AngelList Talent</A>
						<DT><A HREF="https://ai-jobs.net/">Jobs in AI/ML and Big Data | ai-jobs.net</A>
						<DT><A HREF="https://x.company/">X, the moonshot factory</A>
						<DT><A HREF="https://read.cv/open-roles">Open Roles</A>
						<DT><A HREF="https://himalayas.app/onboarding/talent">Job Seeker Sign Up | Himalayas</A>
						<DT><A HREF="https://apply.workable.com/huggingface/?lng=en">Hugging Face - Current Openings</A>
						<DT><A HREF="https://wellfound.com/jobs">Find Startup Jobs Near You and Remote Jobs | Wellfound (formerly AngelList Talent)</A>
						<DT><A HREF="https://startup.jobs/?q=ai">Startup Jobs – Developer, designer, marketing, sales jobs, and more</A>
					</DL><p>
					<DT><H3 FOLDED>ai-jobs-research</H3>
					<DL><p>
						<DT><H3 FOLDED>jobs-efficient-ai</H3>
						<DL><p>
							<DT><A HREF="https://openai.com/careers/gpu-kernels-engineer/">GPU Kernels Engineer | OpenAI | OpenAI</A>
							<DT><A HREF="https://jobs.lever.co/cohere/9ce64f5b-73f7-4a66-900b-c9dee2c5fba6">Cohere - Member of Technical Staff, Model Efficiency</A>
							<DT><A HREF="https://openai.com/careers/research-engineer-post-training-model-optimization/">Research Engineer, Post-training Model Optimization | OpenAI | OpenAI</A>
							<DT><A HREF="https://openai.com/careers/software-engineer-triton-compiler/">Software Engineer, Triton Compiler | OpenAI | OpenAI</A>
							<DT><A HREF="https://x.com/nadavrot/status/1796312670779650460">(1) Nadav Rotem en X: "We are looking for GPU compiler engineers, CUDA experts, PTX aficionados, MLIR experts, Triton lovers, IEEE 754 ninjas, and people that don't need to heat their house in winter because their GPU is hot enough. We require a U.S. work permit. Please send me resumes in PDF format." / X</A>
							<DT><A HREF="https://featuresandlabels.notion.site/We-are-hiring-fal-ai-37eece7cf700403fbb63b61b757684c4">🚀 We are hiring @ fal.ai!</A>
						</DL><p>
						<DT><H3 FOLDED>jobs-pre-training</H3>
						<DL><p>
							<DT><A HREF="https://boards.greenhouse.io/anthropic/jobs/4020189008">Job Application for Engineering Manager, Pretraining Data Platform at Anthropic</A>
						</DL><p>
						<DT><H3 FOLDED>jobs-research-interships</H3>
						<DL><p>
							<DT><A HREF="https://jobs.lever.co/cohere/0bb9479c-36a5-4a2f-ad47-6e5ca2fb37ec">Cohere - Research Internship</A>
						</DL><p>
						<DT><H3 FOLDED>jobs-inference</H3>
						<DL><p>
							<DT><H3 FOLDED>jobs-edge-inference</H3>
							<DL><p>
								<DT><A HREF="https://www.linkedin.com/jobs/view/3982459914/?refId=282fb2a3-462d-40f0-8ecc-df6f91e43130&trackingId=JZ69HVkmSEKA9xvzDIgh%2Bw%3D%3D&trk=flagship3_job_home_savedjobs">(1) ML Engineer L4, Consumer Inference | Netflix | LinkedIn</A>
							</DL><p>
							<DT><A HREF="https://openai.com/careers/distributed-systemsml-engineer/">Distributed Systems/ML Engineer | OpenAI | OpenAI</A>
							<DT><A HREF="https://openai.com/careers/platform-ml-engineering-manager-inference/">Platform ML Engineering Manager, Inference | OpenAI | OpenAI</A>
							<DT><A HREF="https://openai.com/careers/research-engineer-post-training-model-optimization/">Research Engineer, Post-training Model Optimization | OpenAI | OpenAI</A>
							<DT><A HREF="https://openai.com/careers/engineering-manager-ai-inference-systems/">Engineering Manager, AI Inference Systems | OpenAI | OpenAI</A>
							<DT><A HREF="https://openai.com/careers/software-engineer-model-inference-2/">Software Engineer, Model Inference | OpenAI | OpenAI</A>
							<DT><A HREF="https://www.linkedin.com/jobs/view/3982463651/?refId=282fb2a3-462d-40f0-8ecc-df6f91e43130&trackingId=h9VprUs6Qq25r308hAChSg%3D%3D&trk=flagship3_job_home_savedjobs">(1) Research Scientist (L6) - Machine Learning and Inference Research | Netflix | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/jobs/view/3970207835/?refId=282fb2a3-462d-40f0-8ecc-df6f91e43130&trackingId=Hv%2BsaKKaTfWiNWRe9EjApQ%3D%3D&trk=flagship3_job_home_savedjobs">(1) Machine Learning Engineer | Replicate | LinkedIn</A>
							<DT><A HREF="https://www.linkedin.com/jobs/view/3959597204/?refId=282fb2a3-462d-40f0-8ecc-df6f91e43130&trackingId=5TkyftfTTNuKwtlBc3v5HQ%3D%3D&trk=flagship3_job_home_savedjobs">(1) Machine Learning Engineer - Inference | Together AI | LinkedIn</A>
							<DT><A HREF="https://boards.greenhouse.io/anthropic/jobs/4020356008">Job Application for Software Engineer, Inference at Anthropic</A>
						</DL><p>
						<DT><H3 FOLDED>jobs-gpu-platform</H3>
						<DL><p>
							<DT><A HREF="https://openai.com/careers/engineering-manager-gpu-platform/">Engineering Manager, GPU Platform | OpenAI | OpenAI</A>
						</DL><p>
						<DT><A HREF="https://www.anthropic.com/jobs">Jobs \ Anthropic</A>
						<DT><A HREF="https://openai.com/careers/search/?c=ai-scientist%2Calignment%2Cfrontiers%2Chuman-data%2Cplatform%2Cpost-training%2Cpre-training%2Cpreparedness%2Csafety-systems%2Csora%2Cresearch">OpenAI</A>
					</DL><p>
					<DT><A HREF="https://engineering.atspotify.com/">Spotify Engineering</A>
					<DT><A HREF="https://www.workatastartup.com/application/preview">Y Combinator's Work at a Startup</A>
					<DT><A HREF="https://jobs.lever.co/cohere/fab6d469-b35c-4c72-be88-30fe3effe570/apply">Cohere - Machine Learning Intern (Fall 2022)</A>
					<DT><A HREF="https://www.levels.fyi/internships/Cohere-ai/Software-Engineer-Intern/">Cohere.ai Software Engineer Intern Salaries | $64.00 / hr | Levels.fyi</A>
					<DT><A HREF="https://www.metacareers.com/v2/jobs/2587807648062439/">Software Engineer, Systems ML - Frameworks / Compilers / Kernels | Meta Careers</A>
					<DT><A HREF="https://www.youtube.com/watch?v=3xD_aUGKk_4">May 2024: The tech layoffs have oversaturated the USA tech job market. - YouTube</A>
					<DT><A HREF="https://www.youtube.com/watch?v=blp7dRsSgBM">Open Data Engineering Q&amp;A - YouTube</A>
					<DT><A HREF="https://featuresandlabels.notion.site/We-are-hiring-fal-ai-37eece7cf700403fbb63b61b757684c4">🚀 We are hiring @ fal.ai!</A>
					<DT><A HREF="https://www.janestreet.com/join-jane-street/position/7449077002/">Machine Learning Performance Engineer :: Jane Street</A>
					<DT><A HREF="https://landing.club/stamplist">StampList: Visa Sponsors | Landing Club</A>
				</DL><p>
				<DT><H3 FOLDED>to-do</H3>
				<DL><p>
					<DT><H3 FOLDED>gRPC / Protos / TGI / Bazel</H3>
					<DL><p>
						<DT><A HREF="https://www.youtube.com/watch?v=kerKXChDmsE&t=961s">Tonic makes gRPC in Rust stupidly simple - YouTube</A>
						<DT><A HREF="https://github.com/fullstorydev/grpcurl">fullstorydev/grpcurl: Like cURL, but for gRPC: Command-line tool for interacting with gRPC servers</A>
						<DT><A HREF="https://github.com/hyperium/tonic/blob/master/examples/helloworld-tutorial.md">tonic/examples/helloworld-tutorial.md at master · hyperium/tonic</A>
						<DT><A HREF="https://github.com/search?q=repo%3Ahuggingface%2Ftext-generation-inference%20tonic&type=code">Code search results</A>
						<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/8a5bcba227fcddff40385e8707ef94c7d6b3b561/server/text_generation_server/server.py">text-generation-inference/server/text_generation_server/server.py at 8a5bcba227fcddff40385e8707ef94c7d6b3b561 · huggingface/text-generation-inference</A>
						<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/Makefile">text-generation-inference/server/Makefile at main · huggingface/text-generation-inference</A>
						<DT><A HREF="https://typer.tiangolo.com/">Typer</A>
						<DT><A HREF="https://protobuf.dev/overview/">Overview | Protocol Buffers Documentation</A>
						<DT><A HREF="https://github.com/grpc/grpc/tree/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/python/uds">grpc/examples/python/uds at d68161a64f191b8d8d5afe0507e7a2291f91ff1a · grpc/grpc</A>
						<DT><A HREF="https://grpc.io/docs/languages/python/quickstart/">Quick start | Python | gRPC</A>
						<DT><A HREF="https://github.com/grpc/grpc/blob/d68161a64f191b8d8d5afe0507e7a2291f91ff1a/examples/protos/BUILD">grpc/examples/protos/BUILD at d68161a64f191b8d8d5afe0507e7a2291f91ff1a · grpc/grpc</A>
						<DT><A HREF="https://chat.openai.com/c/f2b6f035-383c-4639-a4ee-0963305dee4f">ChatGPT</A>
						<DT><A HREF="https://bazel.build/run/bazelrc">Write bazelrc configuration files  |  Bazel</A>
					</DL><p>
					<DT><H3 FOLDED>cutlass &amp; kernel opts / Bazel &amp; Starlark</H3>
					<DL><p>
						<DT><A HREF="https://docs.google.com/document/d/1EhHx1-vVOb8aTcreHIgmrU4JNNmqCsAmniA7j8Lj8Pc/edit">https://docs.google.com/document/d/1EhHx1-vVOb8aTcreHIgmrU4JNNmqCsAmniA7j8Lj8Pc/edit</A>
						<DT><A HREF="https://drive.google.com/drive/u/5/folders/1cWttjKQTTX1GsDEDy9KgG-yHNERuE0VH">Programming Model - Efficient AI - Google Drive</A>
						<DT><A HREF="https://research.colfax-intl.com/">Colfax Research – Contributing to Innovations in Computing</A>
						<DT><A HREF="https://research.colfax-intl.com/wp-content/uploads/2024/01/layout_algebra.pdf">https://research.colfax-intl.com/wp-content/uploads/2024/01/layout_algebra.pdf</A>
						<DT><A HREF="https://research.colfax-intl.com/wp-content/uploads/2023/12/colfax-flashattention.pdf">https://research.colfax-intl.com/wp-content/uploads/2023/12/colfax-flashattention.pdf</A>
						<DT><A HREF="https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/">https://research.colfax-intl.com/nvidia-hopper-gemm-cutlass/</A>
						<DT><A HREF="https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f">LLM Inference Series: 5. Dissecting model performance | by Pierre Lienhart | Feb, 2024 | Medium</A>
						<DT><A HREF="https://bazel.build/start/cpp">Bazel Tutorial: Build a C++ Project</A>
						<DT><A HREF="https://github.com/bazelbuild/rules_cc/blob/bbb0615a8728fe2ee790296f12b325b6d42583a7/examples/my_c_compile/my_c_compile.bzl">rules_cc/examples/my_c_compile/my_c_compile.bzl at bbb0615a8728fe2ee790296f12b325b6d42583a7 · bazelbuild/rules_cc</A>
						<DT><A HREF="https://github.com/bazelbuild/starlark/blob/master/users.md">starlark/users.md at master · bazelbuild/starlark</A>
						<DT><A HREF="https://github.com/bazelbuild/examples/blob/main/cpp-tutorial/stage1/main/hello-world.cc">examples/cpp-tutorial/stage1/main/hello-world.cc at main · bazelbuild/examples</A>
						<DT><A HREF="https://bazel.build/tutorials/cpp-dependency">Review the dependency graph  |  Bazel</A>
						<DT><A HREF="https://mail.google.com/mail/u/1/#inbox?compose=DmwnWrRlQhjWPQkzQHTzgSzxrcjhHfrsZMPWMKfGtPPHGFRMJNVZnsWwgVRcdQBvcdXSZdZHnkjQ">Inbox - antonio.jfdominguez1@gmail.com - Gmail</A>
					</DL><p>
					<DT><H3 FOLDED>GH200</H3>
					<DL><p>
						<DT><A HREF="https://docs.google.com/document/d/1P8MyAWt9AnS9c8yhlodT7RuI8RlFZoMfsgKvGOBNSJM/edit">supermicro_research_gh200 - Google Docs</A>
						<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements">CUDA C++ Programming Guide</A>
						<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-pageable-systems">CUDA C++ Programming Guide</A>
						<DT><A HREF="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-memory-programming">CUDA C++ Programming Guide</A>
						<DT><A HREF="https://docs.nvidia.com/gh200-superchip-benchmark-guide.pdf">https://docs.nvidia.com/gh200-superchip-benchmark-guide.pdf</A>
						<DT><A HREF="https://github.com/pytorch/pytorch/blob/9df4bc6a0dc72caccee142555d1668fad1621206/benchmarks/transformer/better_transformer_vs_mha_functional.py#L114">pytorch/benchmarks/transformer/better_transformer_vs_mha_functional.py at 9df4bc6a0dc72caccee142555d1668fad1621206 · pytorch/pytorch</A>
						<DT><A HREF="https://github.com/NVIDIA/GPUStressTest/tree/main">NVIDIA/GPUStressTest: GPU Stress Test is a tool to stress the compute engine of NVIDIA Tesla GPU’s by running a BLAS matrix multiply using different data types. It can be compiled and run on both Linux and Windows.</A>
						<DT><A HREF="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#removing-cuda-toolkit-and-driver">CUDA Installation Guide for Linux</A>
						<DT><A HREF="https://github.com/google/jax/blob/main/jax/experimental/mosaic/gpu/examples/flash_attention.py#L146-L354">jax/jax/experimental/mosaic/gpu/examples/flash_attention.py at main · google/jax</A>
						<DT><A HREF="https://arxiv.org/pdf/2407.07850">https://arxiv.org/pdf/2407.07850</A>
						<DT><A HREF="https://www.youtube.com/playlist?list=LL">Liked videos - YouTube</A>
						<DT><A HREF="https://translate.google.com/?sl=ko&tl=en&text=%EB%AF%B8%ED%8C%85%20%EB%82%A0%EC%A7%9C%20%ED%88%AC%ED%91%9C%20https%3A%2F%2Fcalendly.com%2Fd%2Fcp2p-jnv-2tz%2Fdiffusion-kernel-starter-meeting%0A%EB%AF%B8%ED%8C%85%20%EC%9E%A5%EC%86%8C%20https%3A%2F%2Fmeet.google.com%2Fwiw-ahbw-mrd%0A%EA%B8%88%EC%9A%94%EC%9D%BC%20%EC%A0%90%EC%8B%AC%EC%A0%95%EB%8F%84%EA%B9%8C%EC%A7%80%20%ED%88%AC%ED%91%9C%20%EB%B6%80%ED%83%81%EB%93%9C%EB%A6%BD%EB%8B%88%EB%8B%A4!%0A%ED%98%B9%EC%8B%9C%EB%9D%BC%EB%8F%84%20%EB%A7%81%ED%81%AC%EC%97%90%20%EB%AC%B8%EC%A0%9C%20%EC%9E%88%EC%9C%BC%EB%A9%B4%20%EB%A7%90%EC%94%80%ED%95%B4%EC%A3%BC%EC%84%B8%EC%9A%94.%20(edited)%20&op=translate">Google Translate</A>
						<DT><A HREF="https://www.youtube.com/watch?v=RciT5fcuN1E">High Performance LLMs in Jax 2024 -- Session 2 - YouTube</A>
						<DT><A HREF="https://x.com/RajaXg/status/1812721241985610147">(1) Raja Koduri en X: "GPU Architecture Impact" / X</A>
						<DT><A HREF="https://mail.google.com/mail/u/0/#inbox?compose=lqrsljbQBGtpQrNrqLstRTMZsFhZtlDgJWQKxPQtKgBjChrtvCSmwNpfjNNbjggGBxqXGcPJRwBjGCJxK">Inbox - antonio.jfdominguez@gmail.com - Gmail</A>
						<DT><A HREF="https://developer.nvidia.com/blog/nvidia-transitions-fully-towards-open-source-gpu-kernel-modules/">NVIDIA Transitions Fully Towards Open-Source GPU Kernel Modules | NVIDIA Technical Blog</A>
						<DT><A HREF="https://outlook.office.com/mail/">Email - Antonio Dominguez - Outlook</A>
					</DL><p>
					<DT><A HREF="https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/cli.py">text-generation-inference/server/text_generation_server/cli.py at main · huggingface/text-generation-inference</A>
					<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5ddb6bf218ed16a2dcf0058f20c59a247e180fd2/tensorrt_llm/runtime/model_runner.py#L582">TensorRT-LLM/tensorrt_llm/runtime/model_runner.py at 5ddb6bf218ed16a2dcf0058f20c59a247e180fd2 · NVIDIA/TensorRT-LLM</A>
					<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/blob/5ddb6bf218ed16a2dcf0058f20c59a247e180fd2/examples/multimodal/run.py#L920">TensorRT-LLM/examples/multimodal/run.py at 5ddb6bf218ed16a2dcf0058f20c59a247e180fd2 · NVIDIA/TensorRT-LLM</A>
					<DT><A HREF="https://nvidia.github.io/TensorRT-LLM/installation/build-from-source-linux.html#building-the-python-bindings-for-the-c-runtime">Building from Source Code on Linux — tensorrt_llm documentation</A>
					<DT><A HREF="https://github.com/huggingface/optimum-nvidia">huggingface/optimum-nvidia</A>
					<DT><A HREF="https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit">torch.compile, the missing manual - Google Docs</A>
					<DT><A HREF="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/openai_triton/manual_plugin">TensorRT-LLM/examples/openai_triton/manual_plugin at main · NVIDIA/TensorRT-LLM</A>
					<DT><A HREF="https://jax.readthedocs.io/en/latest/pallas/tpu/matmul.html">Matrix Multiplication — JAX documentation</A>
					<DT><A HREF="https://github.com/orgs/triton-inference-server/repositories">triton-inference-server repositories</A>
					<DT><A HREF="https://github.com/InternLM/lmdeploy">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.</A>
				</DL><p>
			</DL><p>
			<DT><H3 FOLDED>software foundations</H3>
			<DL><p>
				<DT><H3 FOLDED>software-foundations-exercises</H3>
				<DL><p>
					<DT><A HREF="https://github.com/frankYaohua/software-foundations/blob/master/Basics.v">software-foundations/Basics.v at master · frankYaohua/software-foundations</A>
					<DT><A HREF="https://www.seas.upenn.edu/~cis500/current/exams/index.html">Directory Index</A>
					<DT><A HREF="https://github.com/coq-community/coq-art/blob/v8.13.0/ch5_everydays_logic/SRC/class.v">coq-art/class.v at v8.13.0 · coq-community/coq-art</A>
					<DT><A HREF="https://github.com/SatyendraBanjare/software-foundations">Repository 2</A>
					<DT><A HREF="https://github.com/paulkoerbitz/software-foundations">Repository 3</A>
					<DT><A HREF="https://github.com/StevenWongChess/software-foundations/blob/master/IndProp.v">Repository 4</A>
					<DT><A HREF="https://www.cs.uic.edu/~mansky/teaching/cs494sf/sp19/exams.html">CS 494 SF: Software Foundations · Exams</A>
					<DT><A HREF="https://github.com/steven7woo/Coq-CIS500/blob/master/For_wenrui/Since_HW6/Imp.v">Coq-CIS500/Imp.v at master · steven7woo/Coq-CIS500</A>
					<DT><A HREF="https://github.com/coq-community/coq-art/blob/v8.13.0/ch2_types_expressions/SRC/Zbtree.v">Binary tree</A>
					<DT><A HREF="https://github.com/bishboria/software-foundations/blob/master/Imp.v">Imp poset</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-environment</H3>
				<DL><p>
					<DT><A HREF="https://proofgeneral.github.io/">Proof General</A>
					<DT><A HREF="https://scholar.princeton.edu/scuellar/blog/how-i-set-my-mac-brew-opam-emacs-and-coq">How I set up my mac (brew, opam, emacs and coq)</A>
					<DT><A HREF="https://www.dc.fi.udc.es/staff/freire/coqdoc/pauillac.inria.fr/coq/doc/no15.htm">5.4 Compiled files</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-lectures</H3>
				<DL><p>
					<DT><A HREF="https://deepspec.org/event/dsss18/">DeepSpec: The Science of Deep Specification</A>
					<DT><A HREF="https://www.labri.fr/perso/casteran/CoqArt/">Coq'Art Home page</A>
					<DT><A HREF="https://github.com/plclub/cis670-16fa/blob/master/notes/IntuitionisticPropositionalLogicLecture.pdf">Introduction to Intuitionistic Propositional Logic</A>
					<DT><A HREF="https://www.nii.ac.jp/event/upload/talk-ShonanMeetings100th.pdf">Principles, examples and main applications</A>
					<DT><A HREF="https://www.ercim.eu/publication/Ercim_News/enw36/filiatre.html">Certification of Imperative Programs in Coq</A>
					<DT><A HREF="https://www.brics.dk/RS/97/18/BRICS-RS-97-18.pdf">How to Believe a Machine-Checked Proof</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-theory</H3>
				<DL><p>
					<DT><A HREF="https://ncatlab.org/nlab/show/certified+programming">certified programming</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-self-modified-code</H3>
				<DL><p>
					<DT><A HREF="https://authors.library.caltech.edu/">Welcome to CaltechAUTHORS - CaltechAUTHORS</A>
				</DL><p>
				<DT><H3 FOLDED>software-foundations-resources</H3>
				<DL><p>
					<DT><A HREF="https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/fisher">Using Formal Methods to Eliminate Exploitable Bugs | USENIX</A>
					<DT><A HREF="https://madiot.fr/files/reportru.pdf">Specification of imperative languages using operational semantics in Coq</A>
					<DT><A HREF="http://why3.lri.fr/">Why3</A>
					<DT><A HREF="https://hal.inria.fr/hal-01094195/document">Intro to the Calculus of Inductive Constructions</A>
					<DT><A HREF="http://www.cse.chalmers.se/research/group/logic/TypesSS05/Extra/filliatre.pdf">HL</A>
					<DT><A HREF="http://nickbenton.name/coqasm.pdf">x86</A>
					<DT><A HREF="https://www.lri.fr/~paulin/LASER/course-notes.pdf">Coq overview</A>
					<DT><A HREF="https://www.dragonwasrobot.com/mathematics/2015/09/26/an-interpreter-a-compiler-and-a-virtual-machine.html">An interpreter, a compiler, and a virtual machine</A>
					<DT><A HREF="https://link.springer.com/chapter/10.1007/978-3-540-70594-9_3">Next generation programming languages</A>
				</DL><p>
				<DT><A HREF="https://softwarefoundations.cis.upenn.edu/">Software Foundations</A>
				<DT><A HREF="http://www.cs.cornell.edu/courses/cs4160/2020sp/sf/lf/terse/toc.html">Alternative version</A>
				<DT><A HREF="https://xavierleroy.org/courses/Eugene-2011/">Proving a Compiler</A>
				<DT><A HREF="https://nickdrozd.github.io/">Something Something Programming | Mostly thoughts about programming. Maybe other stuff too.</A>
			</DL><p>
			<DT><A HREF="https://www.youtube.com/watch?v=7A0YjbLGLXo">Bitcoin Mining in One Minute - YouTube</A>
		</DL><p>
	</DL><p>
</HTML>
